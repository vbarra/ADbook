{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5dc15f",
   "metadata": {},
   "source": [
    "# Statistique descriptive\n",
    "## Définitions\n",
    "Dans la suite, nous nous intéressons à des unités statistiques ou individus statistiques ou unités d'observation (individus,  entreprises,  ménages, données abstraites...). Bien que le cas infini soit envisageable, nous nous restreignons ici à l'étude d'un nombre fini de ces unités. Un ou plusieurs caractères (ou variables) est mesuré sur chaque unité. Les variables sont désignées par simplicité par une lettre. Leurs valeurs possibles sont appelées modalités et l'ensemble des valeurs possibles ou des modalités est appelé le domaine. L'ensemble des individus statistiques forme la population.\n",
    "### Typologie des variables\n",
    "La typologie des variables définit le type de problème statistique que l'on doit aborder :\n",
    "\n",
    "```{index} Variable ; qualitative\n",
    "```\n",
    "````{prf:definition} Variable qualitative\n",
    "La variable est dite qualitative lorsque les modalités sont des catégories. Suivant qu'il existe une relation d'ordre sur les catégories, on distingue :\n",
    "- la variable qualitative nominale, si les modalités  ne peuvent pas être ordonnées\n",
    "- la variable qualitative ordinale, si les modalités peuvent être ordonnées\n",
    "````\n",
    "\n",
    "```{index} Variable ; quantitative\n",
    "```\n",
    "````{prf:definition} Variable quantitative\n",
    "La variable est dite quantitative lorsque les modalités sont des valeurs numériques (scalaires ou vectorielles) :\n",
    "- la variable est quantitative discrète si les modalités forment un ensemble dénombrable\n",
    "- la variable quantitative est continue si les modalités vivent dans un espace continu.\n",
    "````\n",
    "\n",
    "Dans certains cas (l'âge par exemple), une variable d'un type (quantitative continue ici) peut être exprimée d'une autre manière pour des raisons pratiques de collecte ou de mesure. De même, les variables qualitatives ordinales peuvent être codées, par exemple selon une échelle de satisfaction.\n",
    "\n",
    "```{index} Statistique ; série\n",
    "```\n",
    "````{prf:definition} Série statistique\n",
    "On appelle série statistique une suite de $n$ valeurs prises par une variable $X$ sur les unités d'observation, notées $x_1\\cdots x_n$.\n",
    "````\n",
    "\n",
    "\n",
    "### Variable qualitative nominale\n",
    "```{index} Variable ; qualitative ; nominale\n",
    "```\n",
    "Une variable qualitative nominale a des valeurs distinctes qui ne peuvent pas être ordonnées. On note $J$ le nombre de valeurs distinctes ou de modalités, notées $x_1\\cdots x_J$. On appelle effectif d'une modalité ou d'une valeur distincte le nombre de fois que cette modalité (ou valeur distincte) apparaît dans la série statistique. On note $n_j$ l'effectif de la modalité $x_j$. La fréquence d'une modalité $j$ est  alors égale à $f_j=\\frac{n_j}{n}$.\n",
    "\n",
    "Le tableau statistique d'une variable qualitative nominale peut être représenté par deux types de graphiques. Les effectifs sont représentés par un diagramme en tuyau d'orgue et les fréquences par un diagramme en secteurs. Pour ce dernier, si le nombre de modalités devient trop important, la représentation perd de son intérêt.\n",
    "\n",
    "![](./images/baton.png)\n",
    "\n",
    "\n",
    "### Variable qualitative ordinale\n",
    "```{index} Variable ; qualitative ; ordinale\n",
    "```\n",
    "Le domaine peut être muni d'une relation d'ordre.  Les valeurs distinctes d'une variable ordinale peuvent donc être ordonnées $x_1\\leq x_2\\cdots\\leq  x_J$, à permutation près dans l'ordre croissant des indices. L'effectif cumulé $N_j$ et la fréquence cumulée $F_j$ des variables sont alors définis par \n",
    "$(\\forall j\\in[\\![1,J]\\!])\\quad N_j=\\displaystyle\\sum_{i=1}^j n_i\\quad \\textrm {et}\\quad F_j=\\displaystyle\\sum_{i=1}^j f_i$\n",
    "\n",
    "Les fréquences et les effectifs (cumulés ou non) peuvent être représentés sous la forme d'un diagramme en tuyaux d'orgue.\n",
    "\n",
    "### Variable quantitative discrète\n",
    "```{index} Variable ; quantitative ; discrète\n",
    "```\n",
    "Le domaine d'une telle variable est dénombrable. Comme pour les variables qualitatives ordinales, on peut calculer les effectifs (cumulés ou non) et les fréquences (cumulées ou non). \n",
    "\n",
    "La répartition des valeurs de la variable peut être représentée par un diagramme en bâtonnets. Les fréquences cumulées  sont visualisées par la fonction de répartition de la variable , définie par \n",
    "\n",
    "$F(x) = \\left \\{\n",
    "\\begin{eqnarray}\n",
    "0&\\textrm{ si} &x<x_1\\\\\n",
    "F_j &\\textrm{ si}&  x\\in[x_j,x_{j+1}[\\\\\n",
    "1& \\textrm{ si}&  x_J\\leq x\n",
    "\\end{eqnarray}\\right .$\n",
    "\n",
    "![](./images/baton2.png)\n",
    "\n",
    "### Variable quantitative continue\n",
    "```{index} Variable ; quantitative ; continue\n",
    "```\n",
    "Le domaine d'une  variable quantitative continue est infini et est assimilé à $\\mathbb{R}$ ou à un intervalle de $\\mathbb{R}$. Cependant, la mesure étant limitée en précision, on peut traiter ces variables comme des variables discrètes.\n",
    "\n",
    "La représentation graphique de ces variables (et la construction du tableau statistique) passe par le regroupement des modalités ou valeurs en classes. Le tableau ainsi construit est souvent appelé distribution groupée. La classe $j$ est l'ensemble des valeurs incluses dans $[c^-_j,c^+_j[$, où $c^-_j$ et $c^+_j$ sont les bornes inférieure et supérieure de la classe. Sur cet intervalle, on peut calculer la fréquence $f_j$ de la classe, la fréquence cumulée, l'effectif $n_j$... La répartition en classes nécessite de définir a priori le nombre de classes $J$ et l'amplitude $a_j$ des intervalles. Si elles peuvent être définies de manière empirique, quelques règles permettent d'établir $J$ et l'amplitude pour une série statistique de $n$ observations. Par exemple :\n",
    "- $J=1+3.3log_{10}(n)$ (règle de Sturge)\n",
    "- $J=2.5\\sqrt[4\\,]{n}$ (règle de Yule)\n",
    "\n",
    "La représentation graphique se fait par exemple par histogramme. \n",
    "Les histogrammes sont des représentations de la distribution des données, agrégées par intervalles. A partir de l'étendue des données, on subdivise l'intervalle en $k$ bins, de tailles $t_k$ non nécessairement identiques, et on compte le nombre d'individus $n_k$ rentrant dans chaque bin. L'histogramme peut alors être :\n",
    "- non normalisé : $h_k = n_k$\n",
    "- normalisé: $h_k = n_k/t_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebebc14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/vibarra/Documents/Cours/FISA/envbook/ADbook/_build/jupyter_execute/statsdescriptives_1_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.loadtxt(\"./data/data.csv\", delimiter=\",\")[:,1]\n",
    "\n",
    "# Comptage des individus\n",
    "def count(X, bins):\n",
    "    def findBin(x, bins):\n",
    "        for i, bin in enumerate(bins):\n",
    "            left, right = bin\n",
    "            if left <= x and x < right:\n",
    "                return i\n",
    "        return None\n",
    "    \n",
    "    count = [0] * len(bins)\n",
    "    for x in X:\n",
    "        i = findBin(x, bins)\n",
    "        if i != None:\n",
    "            count[i] += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "        \n",
    "# Affichage de l'histogramme\n",
    "def plot_hist(X,  bin_min, bin_max, bin_width,normed=True):\n",
    "    bins =[ [i, i+bin_width] for i in np.arange(bin_min, bin_max, bin_width) ]\n",
    "    bin_left = [ l for l, r in bins ]\n",
    "    bin_widths = [ r-l  for l,r in bins ]\n",
    "    bin_height = [ \n",
    "        float(c) / w if normed else c \n",
    "        for c,w in zip(count(X, bins), bin_widths)\n",
    "    ]\n",
    "    plt.bar(bin_left,width=bin_width,height=bin_height)\n",
    "    plt.tight_layout()\n",
    "\n",
    "bin_min = min(X)\n",
    "bin_max = max(X)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "for subplot, binsize in ((141, 5),(142, 20), (143, 80), (144, 1000)):\n",
    "    title = 'Taille des bins : ', binsize\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plot_hist(X, bin_min, bin_max, binsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125daaf",
   "metadata": {},
   "source": [
    "Le choix de la largeur $t$ des bins dépend des données, et par exemple on a : \n",
    "- Loi de Scott : $t = \\frac{3.5 \\sigma}{Card(X)^{1/3}}$, où $\\sigma$ est l'écart type des données.\n",
    "- Loi de Freedman–Diaconis : $ t = \\frac{2 IQR}{Card(X)^{1/3}}$, où $IQR$ est la distance interquartile.\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "```{prf:remark}\n",
    ":class: dropdown\n",
    "Toutes les classes n'ont pas nécessairement la même amplitude\n",
    "```\n",
    "\n",
    "Les effectifs (ou les fréquences) sont représenté(e)s par un histogramme. Si l'on s'intéresse à la représentation des effectifs (resp. des fréquences), la densité d'effectif $h_j$ (resp. de fréquence $d_j$),  définie par $h_j=\\frac{n_j}{a_j}$ (resp. $d_j=\\frac{f_j}{a_j}$), détermine la hauteur du rectangle représentant la classe $j$. L'aire de l'histogramme est égale à l'effectif total $n$ pour l'histogramme des effectifs, et à 1 pour l'histogramme des fréquences.\n",
    "\n",
    "Comme dans le cas discret, la fonction de répartition peut être calculée de la manière suivante :\n",
    "\n",
    "$F(x) = \\left \\{\n",
    "\\begin{eqnarray}\n",
    "0&\\textrm{ si}& x<c^-_1\\\\\n",
    "F_{j-1}+\\frac{f_j}{c^+_j-c^-_j}(x-c^-_j) &\\textrm{ si}& x\\in[c^-_j,c^+_j[\\\\\n",
    "1& \\textrm{ si}&c^+_J\\leq x\n",
    "\\end{eqnarray}\\right .$\n",
    "\n",
    "\n",
    "## Pré-traitement des données\n",
    "\n",
    "Faire une analyse de données, c'est traiter un tableau de taille $n\\times d$ où $n$ est le nombre d'individus et $d$ le nombre de variables (caractères) mesurées sur ces individus. En raison de la colecte des données, des erreurs de mesure ou d'autres facteurs, ce tableau est parfois incomplet et il convient de le prétraiter pour pouvoir effectuer l'analyse.\n",
    "\n",
    "### Points aberrants\n",
    "Une anomalie (ou point aberrant, ou outlier) est une observation (ou un sous-ensemble d'observations) qui semble incompatible avec le reste de l'ensemble de données.\n",
    "\n",
    "S'il est parfois possible d'identifier graphiquement ces points aberrants à l'aide de boîtes à moustaches (voir {ref}`boxplot`), il existe une vaste littérature sur la détection d'anomalies qu'il n'est pas possible d'aborder ici. De plus, suivant le type de données manipulées (données séquentielles ou non), le type de méthode peut être différent. On mentionne donc ici quelques techniques simples :\n",
    "\n",
    "- le détecteur de Hampel : on considère que $x_i$ est un point aberrant si \n",
    "\n",
    "$$|x_i-x_{\\frac12}|>3.MADM$$ \n",
    "\n",
    "où $MADM = 1.4826.|x_i-x_{\\frac12}|_\\frac12$, et où $y_{\\frac12}$ est la médiane des données $y$ \n",
    "- la règle empirique de l'écart-type : on considère que $x_i$ est un point aberrant si \n",
    "\n",
    "$$|x_i-\\bar x|>3.\\sigma$$\n",
    "\n",
    "où  $\\bar x$ (respectivement $\\sigma$) est la moyenne (resp. l'écart-type ) des données.\n",
    "- la méthode LOF (Local Outlier Factor) qui repose sur le concept de densité locale, où la localité est donnée par les $k$ voisins les plus proches, dont la distance est utilisée pour estimer la densité. En comparant la densité locale d'un objet aux densités locales de ses voisins, il est possible d'identifier des régions de densité similaire et des points dont la densité est nettement inférieure à celle de leurs voisins. Ces derniers sont considérés comme des valeurs aberrantes. La densité locale est estimée par la distance typique à laquelle un point peut être atteint à partir de ses voisins. \n",
    "- la méthode COF (Connectivity based Outlier Factor) basée sur le même principe que LOF, à ceci près que l'estimation de densité est effectuée en utilisant le minimum de la somme des distances reliant tous les voisins d'un point donné.\n",
    "\n",
    "### Données manquantes\n",
    "Lors de la collecte des données, il arrive que certaines d'entre elles ne soient pas disponibles ou enregistrées. On distingue trois types de données manquantes :\n",
    "\n",
    "1. les données manquant de manière complètement aléatoire :  la probabilité qu'une donnée soit manquante ne dépend pas des valeurs connues ni de la valeur manquante elle-même.\n",
    "2. les données manquant de manière aléatoire :  la probabilité qu'une donnée soit manquante peut dépendre de valeurs connues (d'autres variables parmi les $d$), mais pas de la variable dont les valeurs sont manquantes.\n",
    "3. les données manquant de manière non aléatoire : la probabilité qu'une donnée soit manquante dépend d'autres variables qui ont également des valeurs manquantes, ou elle dépend de la variable elle-même.\n",
    "\n",
    "\n",
    "Pour résoudre ce problème de données manquantes, dans la mesure où ces dernières ne sont pas trop nombreuses, on a recours à des techniques d'**imputation**.\n",
    "\n",
    "Dans le cas d'une imputation simple (une seule donnée manquante), on peut par exemple remplacer la valeur manquante dans une colonne $j\\in[\\![1,p]\\!]$ par :\n",
    "\n",
    "-  une valeur fixe\n",
    "-  une statistique sur la colonne $j$ (la plus petite ou la plus grande valeur, la moyenne de la colonne, la valeur la plus fréquente...)\n",
    "-  une valeur issue des $k$ plus proches voisins de la ligne du tableau où la valeur en position $j$ est manquante\n",
    "-  une valeur calculée par régression (voir chapitre correspondant) sur l'ensemble du tableau\n",
    "-  la valeur précédente (ou suivante) dans le cas où la colonne est une série ordonnée ou temporelle.\n",
    "\n",
    "\n",
    "Le code suivant remplace les valeurs manquantes (\\texttt{np.nan}) par la moyenne de la colonne qui contient ces valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e492b26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 789.73950195  778.48028564  820.66223145  951.75146484 1142.41357422\n 1340.40576172 1485.84899902 1686.71533203 1672.89733887 1683.21887207\n 1733.16369629 1708.79638672 1717.03918457 1686.7277832  1614.70727539\n 1662.88574219 1819.65490723 2039.40917969 2054.3815918  1864.98071289\n 1558.92687988 1121.76098633  901.66052246  840.00140381  807.53955078\n  787.1607666   798.59619141  907.33856201 1122.89770508 1301.90332031\n 1447.79833984 1504.14526367 1264.86633301 1517.98571777 1497.65588379\n 1535.8704834  1470.72045898 1519.08056641 1354.71630859 1490.4708252\n 1644.65637207 1699.89355469 1937.18054199 1814.11157227 1398.07824707\n 1113.33337402  930.78491211  872.48132324  778.56170654  724.97277832\n  729.55133057  839.13671875 1005.11901855 1165.40319824 1342.07983398\n 1395.83935547 1445.84289551 1410.65881348 1397.22436523 1401.24816895\n 1466.2557373  1570.61340332 1513.4041748  1542.7833252  1638.52062988\n 1692.25976562 1747.94372559 1622.26184082 1328.52502441 1036.2199707\n  878.88452148  806.00726318  769.5848999   746.6829834   777.1796875\n  851.1953125 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m imp \u001b[38;5;241m=\u001b[39m SimpleImputer(missing_values\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m imp\u001b[38;5;241m.\u001b[39mfit([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/impute/_base.py:550\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 550\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/impute/_base.py:343\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/impute/_base.py:326\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    323\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:900\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 900\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    901\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 789.73950195  778.48028564  820.66223145  951.75146484 1142.41357422\n 1340.40576172 1485.84899902 1686.71533203 1672.89733887 1683.21887207\n 1733.16369629 1708.79638672 1717.03918457 1686.7277832  1614.70727539\n 1662.88574219 1819.65490723 2039.40917969 2054.3815918  1864.98071289\n 1558.92687988 1121.76098633  901.66052246  840.00140381  807.53955078\n  787.1607666   798.59619141  907.33856201 1122.89770508 1301.90332031\n 1447.79833984 1504.14526367 1264.86633301 1517.98571777 1497.65588379\n 1535.8704834  1470.72045898 1519.08056641 1354.71630859 1490.4708252\n 1644.65637207 1699.89355469 1937.18054199 1814.11157227 1398.07824707\n 1113.33337402  930.78491211  872.48132324  778.56170654  724.97277832\n  729.55133057  839.13671875 1005.11901855 1165.40319824 1342.07983398\n 1395.83935547 1445.84289551 1410.65881348 1397.22436523 1401.24816895\n 1466.2557373  1570.61340332 1513.4041748  1542.7833252  1638.52062988\n 1692.25976562 1747.94372559 1622.26184082 1328.52502441 1036.2199707\n  878.88452148  806.00726318  769.5848999   746.6829834   777.1796875\n  851.1953125 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [4, 5]])\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b768a0",
   "metadata": {},
   "source": [
    "Dans le cas d'une imputation multiple, où un sous-ensemble de valeurs doit être comblé, on peut adopter la stratégie suivante : \n",
    "\n",
    "1. Effectuer une imputation simple pour toutes les valeurs manquantes de l'ensemble de données.\n",
    "2. Remettre les valeurs manquantes d'une variable $j\\in[\\![1,p]\\!]$ à \"manquante\".\n",
    "3. Former un modèle pour prédire les valeurs manquantes de $j$ en utilisant les valeurs disponibles de la variable $j$ en tant que variable dépendante et les autres variables de l'ensemble de données comme indépendantes.\n",
    "4. Prédire les valeurs manquantes dans la colonne $j$ en utilisant le modèle entraîné à l'étape 3.\n",
    "5. Répéter les étapes 2 à 4 pour toutes les autres colonnes présentant des valeurs manquantes.\n",
    "6. Répéter l'étape 2-5 jusqu'à convergence (ou un nombre maximal d'itérations)\n",
    "7. Répéter les étapes 1-6 plusieurs fois avec différentes initialisations de nombres aléatoires pour créer différentes versions de l'ensemble de données complet/imputé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "imp = IterativeImputer(max_iter=5, random_state=0)\n",
    "X = [[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]]\n",
    "imp.fit(X)\n",
    "print((imp.transform(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504658a",
   "metadata": {},
   "source": [
    "### Transformation des données qualitatives\n",
    "Pour pouvoir être traitées numériquement, les données qualitatives doivent être transformées. Plusieurs techniques existent parmi lesquelles :\n",
    "\n",
    "- pour le cas des variables ordinales : on utilise le rang pour encoder les modalités de la variable. Par exemple, pour un niveau de diplomation Brevet$<$Bac$<$Licence$<$Master$<$Doctorat, on codera Licence par 3 et Doctorat par 5.\n",
    "-  le one-hot encoding : pour une variable qualitative présentant $J$ modalités, on construit un vecteur de taille $J$ dont les composantes sont toutes nulles sauf la $J$-ème qui vaut 1. Par exemple, si $J$=3, on construit 1 vecteur de taille 3, et pour un individu ayant la modalité 2, on le code en (0 1 0). Lorsque $J$ est élevé, on se retrouve avec un jeu de données volumineux.\n",
    "-  les méthodes de plongement (embedding) : utilisées principalement en apprentissage profond (Deep learning) pour le traitement du langage naturel, ces classes de méthodes construisent une représentation de chaque modalité d'une variable qualitative en un vecteur numérique de taille fixe et choisie. Pour le mot \"rouge\" de la variable \"couleur\", par exemple, l'encodage peut par exemple être représenté par le vecteur (0.31 0.57 0.12). En pratique, le calcul de ces représentations s'effectue classiquement par l'entraînement d'un réseau de neurones ayant pour entrée uniquement les variables qualitatives. Tout d'abord, un encodage one-hot est appliqué à la variable afin d'être mise en entrée du réseau, qui n'accepte que les entrées numériques. La sortie d'une des couches cachées du réseau constitue alors le vecteur recherché. On concatène ensuite ce vecteur aux données initiales, utilisées dans l'ajustement du modèle final. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Normalisation\n",
    "Il arrive que les données collectées ne soient pas du même ordre de grandeur, notamment en raison des unités de mesure (un individu mesuré par sa taille en millimètres et son poids en tonnes par exemple). Cette différence de valeur absolue introduit un biais dans l'analyse des données ({ref}`figure 1<biais>`) qu'il convient de corriger. C'est le processus de normalisation des données.\n",
    "\n",
    "Pour une colonne $j\\in[\\![1,p]\\!]$, on dispose de $n$ valeurs $x_{ij},i\\in[\\![1,n]\\!]$. On note : $x_{min} = \\displaystyle\\min_{i\\in[\\![1,n]\\!]}x_{ij}$, $x_{max} = \\displaystyle\\max_{i\\in[\\![1,n]\\!]}x_{ij}$,   $\\bar x_j$ la moyenne des $x_{ij}$, $\\sigma_j$ leur écart-type, $x_\\frac14, x_\\frac12$ et $x_\\frac34$ les premier, deuxième et troisième quartiles. On distingue alors classiquement trois types de normalisation : \n",
    "\n",
    "1. la normalisation min-max : $x_{ij} = \\frac{x_{ij}-x_{min}}{x_{max}-x_{min}}$\n",
    "2. la normalisation standard : $x_{ij}=\\frac{x_{ij}-\\bar x_j}{\\sigma_j}$\n",
    "3. la normalisation robuste : $x_{ij}=\\frac{x_{ij}-x_\\frac12}{x_\\frac34-x_\\frac14}$\n",
    "\n",
    "\n",
    "```{figure} ./images/normdonnees.png\n",
    ":name: normalisation\n",
    "\n",
    "Effet des différents types de normalisation.\n",
    "``` \n",
    "\n",
    "\n",
    "\n",
    "La normalisation standard dépend de la présence de points aberrants (qui affectent la moyenne).\n",
    "\n",
    "\n",
    "```{figure} ./images/normK.png\n",
    ":name: biais\n",
    "\n",
    "Effet de la normalisation sur un algorithme de classification (voir chapitre correspondant). En haut un jeu de données avec deux nuages de points allongés selon l'axe des $x$, certainement en raison d'une différence d'échelle entre les unités de mesure de $x$ et $y$. Au milieu une classification par $k$-moyennes, $k$=2 sans normalisation, en utilisant la distance euclidienne. Les deux classes sont séparées suivant l'axe des $x$, ne reflétant pas la répartition naturelle des points. En bas, après normalisation, les deux nuages de points sont correctement séparés\n",
    "``` \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Statistique descriptive univariée\n",
    "```{index} Statistique ; univariée\n",
    "```\n",
    "La statistique descriptive univariée consiste à étudier un ensemble d'unités d'observations, lorsque celles-ci sont décrites par une seule variable. \n",
    "\n",
    "Soit donc $X$ une variable et $x_j,j\\in [\\![1,n]\\!]$ l'ensemble des valeurs prises par cette variable, $n_i$ étant le nombre de fois où la valeur $x_i$ est prise. $X$ peut être qualitative ou quantitative, les paramètres de description décrits dans la suite s'appliqueront à l'une de ces natures ou au deux.\n",
    "\n",
    "\n",
    "### Paramètres de position\n",
    "Plusieurs paramètres permettent de décrire la position \"la plus représentative\" d'une variable :\n",
    "````{prf:definition} Mode\n",
    "Le mode est la valeur distincte correspondant à l'effectif le plus élevé. Il est noté $x_M$.\n",
    "````\n",
    "Le mode peut être calculé pour tout type de variable, n'est pas nécessairement unique. Lorsqu'une variable continue est découpée en classes, il est possible de définir une classe modale (classe correspondant à l'effectif le plus élevé)\n",
    "````{prf:definition} Moyennes\n",
    "Les moyennes ne peuvent être définies que sur des variables quantitatives. Plusieurs moyennes peuvent être calculées, parmi lesquelles :\n",
    "- la moyenne **arithmétique**  $\\bar{x} = \\frac{1}{n}{\\displaystyle\\sum_{i=1}^nx_i}=  \\frac{1}{n}{\\displaystyle\\sum_{i=1}^J n_ix_i}$. C'est le moment à l'origine d'ordre 1.\n",
    "- la moyenne **géométrique** : si les $x_i$ sont positifs, la moyenne géométrique est la quantité $G=\\left (\\displaystyle\\prod_{i=1}^n x_i\\right )^\\frac{1}{n}$. C'est donc l'exponentielle de la moyenne arithmétique des logarithmes des valeurs observées. \n",
    "- la moyenne **harmonique** : si les $x_i$ sont positifs, la moyenne harmonique est définie par $H=\\frac{n}{\\displaystyle\\sum_{i=1}^J 1/x_i}$\n",
    "- la moyenne **pondérée** : dans certains cas, on n'accorde pas la même importance à toutes les observations (fiabilité, confiance...). La moyenne pondérée est alors définie par \n",
    "$\\bar{x}_w= \\frac{\\displaystyle\\sum_{i=1}^n w_ix_i}{\\displaystyle\\sum_{i=1}^n w_i}$\n",
    "````\n",
    "\n",
    "Dans le cas où $\\forall i,w_i=1/n$, la moyenne pondérée est la moyenne arithmétique. De plus, dans tous les cas, on peut montrer que $H\\leq G\\leq \\bar{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.loadtxt(\"./data/data.csv\", delimiter=\",\")[:,1]\n",
    "\n",
    "\n",
    "def ArithmeticMean(X):\n",
    "    # calculable directement avec np.mean(X)\n",
    "    return float(sum(X)) / len(X)\n",
    "\n",
    "def GeometricMean(X):\n",
    "    n=len(X)\n",
    "    p=1 \n",
    "    for i in range(n):\n",
    "        p*=X[i] \n",
    "    return float(p**(1/n))\n",
    "\n",
    "def HarmonicMean(X):\n",
    "    n=len(X)\n",
    "    s=0\n",
    "    for i in range(n):\n",
    "        s += 1/X[i] \n",
    "    return len(X) / s\n",
    "\n",
    "def WeightedMean(X):\n",
    "    # Exemples de poids\n",
    "    w = np.random.rand(len(X))\n",
    "    return np.average(X,weights=w)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "plt.plot(X, [0.01]*len(X), '|', color='k',label='Points')\n",
    "for method, style, title in ((ArithmeticMean,'r','Arithmétique'),(GeometricMean,'b','Géométrique'),\n",
    "                             (HarmonicMean,'g','Harmonique'),(WeightedMean,'y', 'Pondérée')):\n",
    "    m=method(X)\n",
    "    print (method.__name__, \" : \",m)\n",
    "    plt.plot([m,m],[0,0.2],style,label=title)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4f1cf",
   "metadata": {},
   "source": [
    "````{prf:definition} Médiane\n",
    "La médiane, notée $x_\\frac{1}{2}$ est la valeur centrale de la série statistique triée par ordre croissant. \n",
    "````\n",
    "\n",
    "En d'autres termes, c'est la valeur de la série triée telle qu'au moins 50\\% des effectifs soient inférieurs à $x_\\frac{1}{2}$. Elle peut être calculée sur des variables quantitatives ou qualitatives ordinales (dans le cas où des échelles de valeur ont été définies).\n",
    "````{prf:definition} Quantiles\n",
    "Le quantile d'ordre $p$ est défini par $x_p=F^{-1}(p)$, où $F$ est la fonction de répartition. \n",
    "````\n",
    "\n",
    "La notion de quantile généralise la notion de médiane. Si la fonction de répartition était continue et strictement croissante, la définition de $x_p$ serait unique. Or $F$ est discontinue et définie par paliers et les valeurs de quantiles varient suivant par exemple l'utilisation ou non d'une méthode d'interpolation de $F$. Pour calculer $x_p$, on peut par exemple considérer que si $np$ est pair, \n",
    "$x_p=\\frac{x_{np}+x_{np+1}}{2}$\n",
    "on remarque alors que la médiane est le quantile d'ordre $\\frac{1}{2}$\n",
    "et sinon\n",
    "$x_p=x_{\\lceil{np}\\rceil}$\n",
    "En particulier, un quartile est chacune des 3 valeurs qui divisent les données triées en 4 parts égales, de sorte que chaque partie représente 1/4 de l'échantillon de population. On note $Q_i$ le $i^e$ quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.loadtxt(\"./data/data.csv\", delimiter=\",\")[:,1]\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "plt.plot(X, [0.01]*len(X), '|', color='k',label='Points')\n",
    "    \n",
    "for q, style  in ((25,'r'),(50,'b'),(75,'g')):\n",
    "    m=np.percentile(X,q)\n",
    "    print (\"quartile \", q, \" : \",m)\n",
    "    plt.plot([m,m],[0,0.2],style,label=q)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81e465",
   "metadata": {},
   "source": [
    "### Paramètres de dispersion\n",
    "Il est très souvent utile d'apprécier la dispersion des mesures autour du paramètre de position. Pour cela, sur des variables quantitatives uniquement, plusieurs outils sont disponibles :\n",
    "````{prf:definition} Etendue\n",
    "L'étendue est la simple différence entre la plus grande et la plus petite valeur observée.\n",
    "````\n",
    "\n",
    "\n",
    "````{prf:definition}  Déviation maximale\n",
    "La déviation maximale est définie par \n",
    "   $ maxdev(X) = max \\{ |x_i - \\bar{x}| \\,|\\, i\\in[\\![1,n]\\!]\\}$\n",
    "````\n",
    "\n",
    "\n",
    "````{prf:definition}  Déviation moyenne absolue\n",
    "La déviation moyenne absolue est définie par \n",
    "   $ mad(X) = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n |x_i - \\bar{x}|$\n",
    "````\n",
    "\n",
    "\n",
    "\n",
    "````{prf:definition} Distance interquartile\n",
    "La distance interquartile $Q_3-Q_1$ est la différence entre le troisième et le premier quartile. C'est une statistique robuste aux points aberrants.\n",
    "````\n",
    "\n",
    "````{prf:definition} Variance\n",
    "La variance est la somme des carrés des écarts à la moyenne, normalisée par le nombre d'observations\n",
    "$\\sigma^2 = \\frac{1}{n}\\displaystyle\\sum_{i=1}^n\\left (x_i-\\bar{x}\\right )^2$\n",
    "````\n",
    "Cette variance est dite biaisée. La variance non biaisée est obtenue en divisant non pas par $n$, mais par $n-1$.\n",
    "\n",
    "````{prf:definition} Ecart type\n",
    "L'écart type est la racine carrée de la variance. \n",
    "````\n",
    "\n",
    "````{prf:definition} Ecart moyen absolu\n",
    "L'écart moyen absolu est la somme des valeurs absolues des écarts à la moyenne divisée par le nombre d'observations. \n",
    "````\n",
    "Notons qu'il s'agit de la distance $L_1$ du vecteur des observations au vecteur composé de la valeur moyenne, divisé par le nombre d'observations. La variance est la distance $L_2$ entre ces deux vecteurs. Lorsque la distance est calculée par rapport au vecteur composé de la valeur médiane, on parle d'écart médian absolu.\n",
    "\n",
    "\n",
    "![](./images/dispersion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.loadtxt(\"./data/data.csv\", delimiter=\",\")[:,1]\n",
    "\n",
    "def max_dev(X):\n",
    "    m = np.mean(X)\n",
    "    return max(abs(x - m) for x in X)\n",
    "\n",
    "def mad(X):\n",
    "    m = np.mean(X)\n",
    "    return sum(abs(x - m) for x in X) / float(len(X))\n",
    "\n",
    "def sigma(X):\n",
    "    m = np.mean(X)\n",
    "    return math.pow(sum((x - m)**2 for x in X) / len(X), 0.5)\n",
    "\n",
    "def IQR(X): return np.percentile(X,75) - np.percentile(X,25)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.rcParams['font.size'] = '16'\n",
    "plt.plot(X, [0.01]*len(X), '|', color='k',label='Points')\n",
    "m = np.mean(X)\n",
    "for method, pos,style,  in ((max_dev,0.5,'r'),(mad,0.6,'b'),(sigma,0.7,'g'),(IQR,0.8,'y')):\n",
    "    s=method(X)\n",
    "    print (method.__name__, \" : \",m, \"+/-\",s)\n",
    "    plt.plot([m,m],[0,1],'black' )\n",
    "    plt.plot([m-s,m-s],[0,1],style,label=method.__name__)\n",
    "    plt.plot([m+s,m+s],[0,1],style)\n",
    "    plt.plot([m-s,m+s],[pos,pos],style)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef6892",
   "metadata": {},
   "source": [
    "### Paramètres de forme\n",
    "Les paramètres de forme sont souvent calculés en référence à la forme de la loi normale, pour évaluer la symétrie, l'aplatissement ou la dérive par rapport à cette loi.\n",
    "````{prf:definition} Skewness\n",
    "$$g_1 = \\frac{m_3}{\\sigma^3}$$\n",
    "````\n",
    "Le skewness est également appelé coefficient d'asymétrie de Fisher.\n",
    "````{prf:definition} Kurtosis\n",
    "$$K=\\frac{m_4}{m_2^2}$$\n",
    "````\n",
    "$K$ permet de mesurer l'aplatissement.\n",
    "````{prf:definition} Coefficient d'asymétrie de Yule\n",
    "$$A_Y = \\frac{x_{3/4}+x_{1/4}-2x_{1/2}}{x_{3/4}-x_{1/4}}$$\n",
    "````\n",
    "Ce coefficient est fondé sur les positions de trois quartiles (le premier, la médiane et le troisième) et est normalisé par la distance interquartile.\n",
    "````{prf:definition} Coefficient d'asymétrie de Pearson\n",
    "$$A_P = \\frac{\\bar{x}-x_M}{\\sigma}$$\n",
    "````\n",
    "Ce coefficient est fondé sur la comparaison de la moyenne et du mode, et est normalisé par l'écart type.\n",
    "\n",
    "\n",
    "Tous les coefficients d'asymétrie ont des propriétés similaires : ils sont nuls si la distribution est symétrique, négatifs si la distribution est allongée à gauche (left asymmetry), et positifs si la distribution est allongée à droite (right asymmetry).\n",
    "\n",
    "\n",
    "On peut aussi chercher à mesurer l'aplatissement (ou kurtosis) d'une distribution de mesure. Dans ce cas, on utilise le coefficient d'aplatissement de Pearson ou de Fisher, respectivement donnés par \n",
    "$\\beta_2=\\frac{m_4}{\\sigma^4}\\quad\\textrm{et}\\quad g_2=\\beta_2-3$\n",
    "\n",
    "\n",
    "Une distribution est alors dite :\n",
    "- mésokurtique si $g_2$ est proche de 0\n",
    "- leptokurtique si $g_2>0$ (queues plus longues et distribution plus pointue)\n",
    "- platykyrtique si $g_2<0$ (queues plus courtes et distribution arrondie).\n",
    "\n",
    "\n",
    "\n",
    "(boxplot)=\n",
    "#### Pour résumer\n",
    "Les principales statistiques d'une série statistique peuvent être résumées dans des **boîtes à moustache**, qui permettent de voir sur un même graphique :\n",
    "\n",
    "- la médiane\n",
    "- une boîte entre les premier et le troisième quartile\n",
    "- l'étendue \n",
    "- les points aberrants.\n",
    "\n",
    "\n",
    "Ce mode de représentation consiste à dessiner une boîte dont les extrémités dépendent du premier et du troisième quartiles $Q_1$ et $Q_3$ , en ajoutant une barre à l’intérieur\n",
    "matérialisant le second quartile  $Q_2$ (la valeur médiane de l’échantillon). A cette boîte, on ajoute des “moustaches” dont les extrémités dépendent :\n",
    "- soit des valeurs extrémales prises par l’échantillon (minimum et maximum);\n",
    "- soit de la plus petite et de la plus grande valeur de l’échantillon appartenant à l’intervalle $[Q_1 -\\delta, Q_3+\\delta ]$. La grandeur $\\delta$ est une mesure de la dispersion des données. Généralement, on utilise $\\delta = 1.5(Q_3-Q_1)$. \n",
    "\n",
    "Les valeurs de l’ échantillon en dehors des moustaches sont parfois matérialisées par des points et sont alors considérées comme les points aberrants de l'échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53676a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def annotate_boxplot(bpdict, annotate_params=None,\n",
    "                     x_offset=0.05, x_loc=0,\n",
    "                     text_offset_x=35,\n",
    "                     text_offset_y=20):\n",
    "\n",
    "    if annotate_params is None:\n",
    "        annotate_params = dict(xytext=(text_offset_x, text_offset_y), textcoords='offset points', arrowprops={'arrowstyle':'->'})\n",
    "\n",
    "    plt.annotate('Médiane', (x_loc + 1 + x_offset, bpdict['medians'][x_loc].get_ydata()[0]), **annotate_params)\n",
    "    plt.annotate('$Q_1$', (x_loc + 1 + x_offset, bpdict['boxes'][x_loc].get_ydata()[0]), **annotate_params)\n",
    "    plt.annotate('$Q_3$', (x_loc + 1 + x_offset, bpdict['boxes'][x_loc].get_ydata()[2]), **annotate_params)\n",
    "    plt.annotate('$Q_1-1.5(Q_3-Q_1)$', (x_loc + 1 + x_offset, bpdict['caps'][x_loc*2].get_ydata()[0]), **annotate_params)\n",
    "    plt.annotate('$Q_3+1.5(Q_3-Q_1)$', (x_loc + 1 + x_offset, bpdict['caps'][(x_loc*2)+1].get_ydata()[0]), **annotate_params)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Données': np.random.normal(scale=4, size=150)})\n",
    "\n",
    "bpdict = df.boxplot(grid=False,whis=1.5, return_type='dict')\n",
    "annotate_boxplot(bpdict, x_loc=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667514a",
   "metadata": {},
   "source": [
    "### La description ne fait pas tout...\n",
    "La description d'un ensemble de valeurx $x_j$ par la moyenne, la variance, voire le comportement linéaire (coefficient de corrélation, voir plus loin) peut ne pas suffire à comprendre la distribution des données. Un exemple classique (analyse bivariée, section suivante) est le quartet d'Anscombe (figure ci-dessous), constitué de quatre ensembles de points  $(x,y)\\in\\mathbb{R}^2$ de même propriétés statistiques (moyenne, variance, coefficient de régression linéaire) mais qui sont distribués de manière totalement différente dans le plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "\n",
    "datasets = {\n",
    "    '1.': (x, y1),\n",
    "    '2.': (x, y2),\n",
    "    '3.': (x, y3),\n",
    "    '4.': (x4, y4)\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(6, 6),\n",
    "                        gridspec_kw={'wspace': 0.08, 'hspace': 0.18})\n",
    "axs[0, 0].set(xlim=(2, 15), ylim=(2, 14))\n",
    "\n",
    "for ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.plot(x, y, '.',c='r')\n",
    "\n",
    "    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n",
    "    ax.axline(xy1=(0, p0), slope=p1, color='b', lw=2)\n",
    "\n",
    "    stats = (f'$\\\\bar x$ = {np.mean(y):.3f}\\n'\n",
    "             f'$\\\\sigma$ = {np.std(y):.3f}\\n'\n",
    "             f'$r$ = {np.corrcoef(x, y)[0][1]:.3f}')\n",
    "    ax.text(0.95, 0.07, stats, fontsize=9, \n",
    "            transform=ax.transAxes, horizontalalignment='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbcfc8",
   "metadata": {},
   "source": [
    "## Statistique descriptive bivariée\n",
    "On s'intéresse à deux variables $x$ et $y$, mesurées sur les $n$ unités d'observation. La série statistique est alors une suite de $n$ couples $(x_i,y_i)$ des valeurs prises par les deux variables sur chaque individu.\n",
    "\n",
    "### Cas de deux variables quantitatives\n",
    "Le couple est un couple de valeurs numériques. C'est donc un point dans le plan $\\mathbb{R}^2$. Les variables $x$ et $y$ peuvent être analysées séparément, en opérant une statistique univariée sur chacune de ces variables. Les paramètres calculés (de position, de dispersion...) sont dits marginaux. Cependant, il est intéressant d'étudier le lien entre ces deux variables, par l'intermédiaire des valeurs des couples. On définit pour cela un certain nombre d'outils :\n",
    "````{prf:definition} Covariance\n",
    "La covariance de $x$ et $y$ est définie par :\n",
    "$\\sigma_{xy}=\\frac{1}{n}\\displaystyle\\sum_{i=1}^n\\left (x_i-\\bar{x}\\right )\\left (y_i-\\bar{y}\\right )$\n",
    "````\n",
    "\n",
    "```{index} Corrélation ; coefficient\n",
    "```\n",
    "\n",
    "```{index} Détermination ; coefficient\n",
    "```\n",
    "\n",
    "````{prf:definition} Coefficient de corrélation\n",
    "Le coefficient de corrélation  de deux variables $x$ et $y$ est défini par \n",
    "$r_{xy}=\\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}$.\n",
    "Le coefficient de détermination est le carré du coefficient de corrélation.\n",
    "```` \n",
    "Le coefficient de corrélation est donc la covariance normalisée par les écarts types marginaux des variables. Il mesure la dépendance linéaire entre $x$ et $y$. Il est compris dans l'intervalle [-1,1] est est positif (resp. négatif) si les points sont alignés le long d'une droite croissante (resp. décroissante), d'autant plus grand en valeur absolue que la dépendance linéaire est vérifiée. Dans le cas où le coefficient est nul, il n'existe pas de dépendance linéaire.\n",
    "\n",
    "Pour connaître plus précisément la relation linéaire qui lie $x$ et $y$, on effectue une régression linéaire en calculant par exemple la droite de régression : si $y=a+bx$, il est facile de montrer que \n",
    "$b=\\frac{\\sigma_{xy}}{\\sigma_x^2}\\quad\\textrm{et}\\quad a=\\bar{y}-b\\bar{x}$\n",
    "\n",
    "et la droite de régression s'écrit $y-\\bar{y}=\\frac{\\sigma_{xy}}{\\sigma_x^2}\\left ( x-\\bar{x}\\right )$.\n",
    "\n",
    "A partir de cette droite, on peut calculer les valeurs ajustées, obtenues à partir de la droite de régression : $y^*_i=a+bx_i$. Ce sont les valeurs théoriques des $y_i$ et les résidus $e_i=y_i-y_i^*$ représentent la partie inexpliquée des $y_i$ par la droite de régression (ceux là même que l'on essaye de minimiser par la méthode des moindres carrés). Nous reviendrons dans le chapitre sur la régression sur l'analyse de ces résidus.\n",
    "\n",
    "### Cas de deux variables qualitatives\n",
    "\n",
    "Le couple est un couple de valeurs $(x_i,y_i)$ où $x_i$ et $y_i$ prennent comme valeurs des modalités qualitatives. Notons $x_1\\cdots x_J$ et $y_1\\cdots y_K$ les valeurs distinctes prises. \n",
    "\n",
    "Les données peuvent être regroupées sous la forme d'un **tableau de contingence** prenant la forme suivante :\n",
    "```{index} Tableau ; contingence\n",
    "```\n",
    "```{index} Contingence ; tableau\n",
    "```\n",
    "\n",
    "$\\begin{array}{c|ccccc|c}\n",
    "&y_1&\\cdots&y_k&\\cdots&y_K&total\\\\\n",
    "\\hline\n",
    "x_1&n_{11}&\\cdots&n_{1k}&\\cdots&n_{1K}&n_{1.}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "x_j&n_{j1}&\\cdots&n_{jk}&\\cdots&n_{jK}&n_{j.}\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "x_J&n_{J1}&\\cdots&n_{Jk}&\\cdots&n_{JK}&n_{J.}\\\\\n",
    "\\hline\n",
    "total&n_{.1}&\\cdots&n_{.k}&\\cdots&n_{.K}&n\\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "où $n_{j.}$ (resp $n_{.k}$ )sont les effectifs marginaux représentant le nombre de fois où $x_j$ (resp. $y_k$) apparaît, et $n_{jk}$ le nombre d'apparition du couple $(x_j,y_k)$.\n",
    "\n",
    "Le tableau des fréquences $f_{jk}$ s'obtient en divisant tous les effectifs par la taille $n$ dans ce tableau.\n",
    "\n",
    "Un tel tableau s'interprète toujours en comparant les fréquences en lignes ou les fréquences en colonnes (profils lignes ou colonnes), définies  respectivement par \n",
    "$f_k^{(j)}= \\frac{n_{jk}}{n_{j.}}=\\frac{f_{jk}}{f_{j.}}\\quad\\textrm{ et }\\quad f_j^{(k)}= \\frac{n_{jk}}{n_{.k}}=\\frac{f_{jk}}{f_{.k}}$\n",
    "\n",
    "Si l'on cherche un lien entre les variables, on construit un tableau d'effectifs théoriques qui représente la situation où les variables ne sont pas liées (indépendance). Ce tableau est constitué des effectifs \n",
    "$n_{jk}^*=\\frac{n_{j.}n_{.k}}{n}$\n",
    "Les effectifs observés $n_{jk}$ ont les mêmes marges que les $n_{jk}^*$, et les écarts à l'indépendance sont calculés par la différence $e_{jk}=n_{jk}-n_{jk}^*$\n",
    "\n",
    "```{index} Khi-deux\n",
    "```\n",
    "\n",
    "La dépendance du tableau se mesure au moyen du khi-deux défini par \n",
    "$\\chi^2_{obs}= \\displaystyle\\sum_{k=1}^K\\displaystyle\\sum_{j=1}^J\\frac{e_{jk}^2}{n_{jk}^*}$\n",
    "qui peut être normalisé pour ne plus dépendre du nombre d'observations :\n",
    "$\\phi^2=\\frac{\\chi^2_{obs}}{n}$\n",
    "\n",
    "La construction du tableau des effectifs théoriques et sa comparaison au tableau des observations permet dans un premier temps de mettre en évidence les associations significatives entre modalités des deux variables. Pour cela, on calcule la contribution au $\\chi^2$ des modalités $j$ et $k$ :\n",
    "\n",
    "$$\\frac{1}{\\chi^2_{obs}}\\frac{\\left (n_{jk}-n_{j.}n_{.k}\\right )^2}{n_{jk}^*}$$\n",
    "\n",
    "Le signe de la différence $n_{jk}-n_{jk}^*$ indique alors s'il y a une association positive ou négative entre les modalités $j$ et $k$.\n",
    "\n",
    "Plus généralement, le $\\chi^2_{obs}$ est un indicateur de liaison entre les variables.  Dans le cas où $\\chi^2_{obs}=0$, il y a indépendance. Pour rechercher la borne supérieure du khi-deux et voir dans quel cas elle est atteinte, on développe le carré et on obtient \n",
    "\n",
    "$$\\chi^2_{obs} = n\\left [\\displaystyle\\sum_{k=1}^K\\displaystyle\\sum_{j=1}^J \\frac{n_{jk}^2}{n_{j.}n_{.k}} -1\\right ]$$\n",
    "\n",
    "Comme $\\frac{n_{jk}}{n_{.k}}\\leq 1$ on a $ \\frac{n_{jk}^2}{n_{j.}n_{.k}} \\leq \\frac{n_{jk}}{n_{.k}}$ d'où\n",
    "\n",
    "$$\\displaystyle\\sum_{k=1}^K\\displaystyle\\sum_{j=1}^J\\frac{n_{jk}^2}{n_{j.}n_{.k}}\\leq \\displaystyle\\sum_{k=1}^K\\displaystyle\\sum_{j=1}^J \\frac{n_{jk}}{n_{.k}} = \\displaystyle\\sum_{k=1}^K \\frac{\\displaystyle\\sum_{j=1}^J n_{jk}}{n_{.k}}=\\displaystyle\\sum_{k=1}^K \\frac{n_{.k}}{n_{.k}}=1$$\n",
    "\n",
    "d'où $\\chi^2_{obs}\\leq n(K-1)$. On pourrait de même montrer que $\\chi^2_{obs}\\leq n(J-1)$ et donc $\\phi^2\\leq min(J-1,K-1)$.\n",
    "\n",
    "La borne est atteinte dans le cas de la dépendance fonctionnelle (si $\\forall j \\frac{n_{jk}}{n_{j.}}=1$, i.e. il n'existe qu'une case non nulle dans chaque ligne.)\n",
    "\n",
    "A partir de ce khi-deux normalisé, on calcule finalement plusieurs coefficients permettant de mesurer l'indépendance, et parmi ceux-ci citons :\n",
    "\n",
    "- le coefficient de Cramer:\n",
    "$V=\\sqrt{\\frac{\\phi^2}{min(J-1,K-1)}}$\n",
    "- le coefficient de contingence de Pearson :\n",
    "$C = \\sqrt{\\frac{\\phi^2}{\\phi^2 + 1}}$\n",
    "- le coefficient de Tschuprow :\n",
    "$T = \\sqrt{\\frac{\\phi^2}{\\sqrt{(K-1)(J-1)}}}$\n",
    "\n",
    "\n",
    "Ces coefficients sont tous compris entre 0 (indépendance) et 1 (dépendance fonctionnelle). Pour estimer à partir de quelle valeur la dépendance fonctionnelle est significative, on procède de la manière suivante : si les $n$ observations étaient prélevées dans une population où les variables sont indépendantes, on recherche les valeurs probables de $\\chi^2_{obs}$.\n",
    "\n",
    "En s'appuyant sur la loi multinomiale et le test du $\\chi^2$, on montre que $\\chi^2_{obs}$ est une réalisation d'une variable aléatoire $Z$ suivant approximativement une loi $\\chi^2_{(K-1)(J-1)}$. \n",
    "\n",
    "\n",
    "```{prf:remark}\n",
    ":class: dropdown\n",
    "Soient $U_1\\ldots U_p$ $p$ variables i.i.d de loi normale centrée réduite. On appelle loi du $\\chi^2$ à $p$ degrés de liberté la loi de la variable $\\displaystyle\\sum_{i=1}^pU_i^2$.\n",
    "```\n",
    "\n",
    "\n",
    "En effet, les $e_{jk}$ sont liées par $(K-1)(J-1)$ relations linéaires puisqu'on estime les probabilités de réalisation de $x_j$ et $y_k$ respectivement par $n_{j,.}/n$ et $n_{.k}/n$. Il suffit alors de fixer un risque d'erreur $\\alpha$ (une valeur qui, s'il y avait indépendance, n'aurait qu'une probabilité faible d'être dépassée), et on rejette l'hypothèse d'indépendance si $\\chi^2_{obs}$  est supérieur à la valeur critique qu'une variable $\\chi^2_{(K-1)(J-1)}$ a une probabilité $\\alpha$ de dépasser.\n",
    "L'espérance d'un $\\chi^2_{(K-1)(J-1)}$ étant égale à son degré de liberté, $\\chi^2_{obs}$ est d'autant plus grand que le nombre de modalités $J$ et/ou $K$ est grand. \n",
    "\n",
    "\n",
    "D'autres indices existent, qui ne dépendent pas de $\\chi^2_{obs}$, comme par exemple\n",
    "\n",
    "$\\begin{equation} G^2 = 2\\displaystyle\\sum_{k=1}^K\\displaystyle\\sum_{j=1}^J n_{jk} ln \\left (\\frac{ n_{jk}}{ n^*_{jk}} \\right )\\end{equation}$\n",
    "\n",
    "qui sous l'hypothèse d'indépendance suit une loi $\\chi^2_{(K-1)(J-1)}$.\n",
    "\n",
    "### Cas d'une variable quantitative et d'une variable qualitative\n",
    "On s'intéresse ici au cas où les modalités $x_i$ sont qualitatives, et où $y$ est une variable quantitative, dont les modalités sont des réalisations d'une variable aléatoire $Y$.\n",
    "Le rapport de corrélation théorique entre $x$ et $Y$ est défini par \n",
    "\n",
    "$$\\eta^2_{Y\\mid x} = \\frac{\\sigma^2_{\\mathbb{E}_{Y\\mid x}}}{\\sigma^2_Y}$$\n",
    "\n",
    "Si $n_j$ est le nombre d'observations de la modalité $x_j,j\\in[\\![1\\,J]\\!]$, $y_{ij}$ la valeur de $Y$ du $i^e$ individu de la modalité $j$, $\\bar{y}_1\\ldots \\bar{y}_J$ sont les moyennes de $Y$ pour ces modalités et $\\bar{y}$ la moyenne totale sur les $n$ individus, le rapport de corrélation empirique est défini par \n",
    "\n",
    "\n",
    "$$e^2 = \\frac{\\frac{1}{n}\\displaystyle\\sum_{j=1}^J n_j\\left (\\bar{y}_j-\\bar{y}\\right )^2}{\\sigma^2_y}$$\n",
    "\n",
    "\n",
    "La quantité \n",
    "\n",
    "$\\sigma^2_\\cap = \\frac{1}{n}\\displaystyle\\sum_{j=1}^J n_j\\sigma_j^2$\n",
    "\n",
    " avec $\\sigma_j^2 =  \\frac{1}{n_j}\\displaystyle\\sum_{i=1}^{n_j}\\left (y_{ij}-\\bar{y}_j \\right )^2$,  est appelée variance intra groupe (ou intra classe), et donne une idée de la variabilité à l'intérieur de chaque modalité. \n",
    " La quantité \n",
    " $\\sigma_\\cup = \\frac{1}{n}\\displaystyle\\sum_{j=1}^J n_j\\left (\\bar{y}_j-\\bar{y}\\right )^2$\n",
    " est la variance inter groupes (ou inter classes), et mesure la variabilité entre les différentes modalités.\n",
    "\n",
    " Le théorème de décomposition de la variance (ou théorème de Huygens) affirme que la variance totale $\\sigma^2_y$, calculée sans distinction de modalité s'écrit :\n",
    " $\\sigma^2_y = \\sigma^2_\\cap + \\sigma^2_\\cup$\n",
    " \n",
    " De ces définitions, on a alors :\n",
    "\n",
    "- $e^2=0$ si toutes les moyennes de $Y$ sont égales, d'où l'absence de dépendance en moyenne\n",
    "- $e^2=1$ si tous les individus d'une modalité de $x$ ont même valeur de $Y$ et ceci pour chaque modalité \n",
    "- $e^2$ permet de comprendre, via le théorème de Huygens,  quelle variation est prédominante dans la variance totale. Ainsi par exemple, si la variable quantitative est la note d'un élève à un examen, et la variable qualitative son assiduité au cours correspondant, la variabilité entre les notes obtenues dans toute la promotion dépend de deux\n",
    "facteurs : le fait que les étudiants assistent ou pas aux cours, et le fait qu'à assiduité\n",
    "égale (i.e. à l'intérieur d'une même modalité d'assiduité) les étudiants n'ont pas le même niveau. $e^2$  permet alors de savoir lequel de ces deux facteurs est prédominant\n",
    "pour expliquer la variabilité des notes dans toute la promotion.\n",
    "\n",
    "\n",
    "Pour déterminer à partir de quelle valeur $e^2$ est significatif, on compare donc $\\sigma^2_\\cap$ à $\\sigma^2_\\cup$. On peut montrer que si le rapport de corrélation théorique est nul, alors la variable $\\frac{\\left (\\frac{e^2}{J-1}\\right )}{\\left (\\frac{1-e^2}{n-J}\\right )}$ suit une loi de Fisher Snedecor, en supposant que les distributions conditionnelles de $Y$ pour chaque modalité de $X$ sont gaussiennes, de même espérance et de même variance. \n",
    "\n",
    "```{prf:remark}\n",
    ":class: dropdown\n",
    "Soient $U$ et $V$ deux variables aléatoires indépendantes suivant respectivement des lois $\\chi^2_n$ et $\\chi^2_p$. On définit la loi de Fisher Snedecor par $F(n,p)=\\frac{U/n}{V/P}$) $F(J-1,n-J)$\n",
    "```\n",
    "\n",
    "## Vers une analyse multivariée\n",
    "Bien évidemment, dans la majorité des cas, un individu sera décrit par $p\\geq 2$ variables. Si certains algorithmes de statistique descriptive multidimensionnelle sont abordés dans ce cours, il est néanmoins possible d'avoir une première approche exploratoire de ce cas.\n",
    "\n",
    "### Matrices de covariance et de corrélation\n",
    "La première idée, lorsque l'on a observé $d$ variables sur $n$ individus, est de calculer les $d$ variances de ces variables, et les $\\frac{p(p-1)}{2}$ covariances. Ces mesures sont regroupées dans une matrice $p\\times p$, symétrique, semi définie positive, appelée matrice de variance-covariance (ou matrice des covariances), et classiquement notée $\\boldsymbol\\Sigma$.\n",
    "\n",
    "De même, on peut former la matrice des corrélations entre les variables, à diagonale unité et symétrique. La matrice résultante, notée $\\mathbf R$, est également semi définie positive et sa représentation graphique en fausses couleurs permet d'apprécier les dépendances linéaires entre variables.\n",
    "\n",
    "\n",
    "![](./images/batiments.png)\n",
    "\n",
    "\n",
    "Dans le cas de variables qualitatives, les coefficients de corrélation peuvent être remplacés par les coefficients de Cramer, de Tschuprow...\n",
    "\n",
    "### Tableaux de nuages\n",
    "On peut proposer à partir de là des représentations entre sous-ensembles de variables. La figure suivante propose un exemple de tels tableaux, parfois appelés splom (Scatter PLOt Matrix) :\n",
    "\n",
    "- la partie triangulaire supérieure représente les nuages de points de couples de variables\n",
    "- la diagonale représente les histogrammes des variables\n",
    "- la partie trianglaire inférieure donne le coefficient de corrélation entre les deux variables, et une estimation de la densité de la distribution 2D des données\n",
    "\n",
    "\n",
    "![](./images/batiments2.png)\n",
    "\n",
    "\n",
    "\n",
    "### Tableaux de Burt\n",
    "Le tableau de Burt est une généralisation particulière de la table de contingence dans le cas où l'on étudie simultanément $p$ variables qualitatives $X_1\\ldots X_p$. Notons $c_j$ le nombre de modalités de $X_j$ et posons $c=\\displaystyle\\sum_{j=1}^p c_j$. \n",
    "\n",
    "```{index} Tableau ; Burt\n",
    "```\n",
    "```{index} Burt ; tableau\n",
    "```\n",
    "\n",
    "Le tableau de Burt est une matrice carrée symétrique de taille $c$, constituée de $p^2$ sous-matrices. Chacune des $p$ sous-matrices diagonales est relative à l'une des $p$ variables, la $j^e$ étant carrée de taille $c_j$, diagonale, et de coefficients diagonaux les effectifs marginaux de $X_j$. La sous-matrice dans le bloc $(k,l)$ du tableau, $k\\neq l$, est la table de contingence des variables $X_k$ et $X_l$."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "source_map": [
   11,
   93,
   138,
   206,
   212,
   229,
   237,
   309,
   350,
   371,
   386,
   435,
   469,
   524,
   550,
   556,
   593
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}