
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sélection de variables &#8212; Analyse de données</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Analyse en composantes principales" href="acp.html" />
    <link rel="prev" title="Statistique descriptive" href="statsdescriptives.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="intro.html">
  <img src="_static/isimainp.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Rappels.html">
  Rappels de probabilité
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="elemstats.html">
  Elements de statistiques
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="statsdescriptives.html">
  Statistique descriptive
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Sélection de variables
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="acp.html">
  Analyse en composantes principales
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="regression.html">
  Régression
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="clustering.html">
  Quelques méthodes de classification
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="afc.html">
  Analyse Factorielle des correspondances
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="acm.html">
  Analyse des correspondances multiples
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="genindex.html">
  Index
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage mt-5 pt-1 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Définitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#caracteristiques-des-methodes-de-selection">
   Caractéristiques des méthodes de sélection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initialisation">
     Initialisation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exploration-des-sous-ensembles">
     Exploration des sous-ensembles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluation-des-sous-ensembles">
     Evaluation des sous-ensembles
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#filtres">
       Filtres
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#methodes-enveloppantes">
       Méthodes enveloppantes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#methodes-integrees">
       Méthodes intégrées
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quelques-methodes-de-selection">
   Quelques méthodes de sélection
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#suppression-des-descripteurs-a-variance-faible">
     Suppression des descripteurs à variance faible
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithmes-de-selection-sequentielle">
     Algorithmes de sélection séquentielle
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithme-focus">
     Algorithme Focus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithme-relief">
     Algorithme relief
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#methode-sac">
     Méthode SAC
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithme-rfe">
     Algorithme RFE
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="selection-de-variables">
<h1>Sélection de variables<a class="headerlink" href="#selection-de-variables" title="Permalink to this headline">#</a></h1>
<p>On s’intéresse ici à <span class="math notranslate nohighlight">\(n\)</span> individus  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> décrits par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives ou caractéristiques (features), <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^d\)</span>. Avec l’avènement des Big Data, et la généralisation des capteurs, <span class="math notranslate nohighlight">\(d\)</span> peut être très grand (plusieurs milliers), et analyser telles quelles les données brutes devient difficile d’un point de vue calculatoire et interprétation. De plus, il est rare que les caractéristiques soient totalement utiles et indépendantes.</p>
<p>Une étape souvent utilisée en analyse de données consiste donc à prétraiter cet espace, par exemple pour :</p>
<ul class="simple">
<li><p>le transformer en un format compatible avec des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité temporelle des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité spatiale du problème traité</p></li>
<li><p>découpler des variables et chercher les dépendances</p></li>
<li><p>introduire des a priori, ou des propriétés importantes pour les algorithmes (données centrées normées, descripteurs épars…)</p></li>
<li><p>permettre une interprétation plus intuitive et/ou graphique (<a class="reference internal" href="#tsne"><span class="std std-ref">figure 2</span></a>)</p></li>
</ul>
<div class="figure align-default" id="tsne">
<img alt="_images/tsne.png" src="_images/tsne.png" />
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Exemple de réduction de dimension (source: Maaten &amp; Hinton, 2008). Des images 28<span class="math notranslate nohighlight">\(\times\)</span> 28 de chiffres manuscrits sont représentées par un vecteur de 784 valeurs, puis transformés en vecteurs de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> pour les projeter dans le plan. La méthode utilisée permet d’optimiser la transformation de sorte à ce que les images représentant le même chiffre soient regroupées dans des nuages compacts.</span><a class="headerlink" href="#tsne" title="Permalink to this image">#</a></p>
</div>
<p>Deux stratégies peuvent alors être utilisées :</p>
<ol class="simple">
<li><p>sélectionner un sous-ensemble des variables initiales comme descripteurs des individus</p></li>
<li><p>calculer de nouveaux descripteurs à partir des variables initiales.</p></li>
</ol>
<p>Nous nous intéressons ici à la première approche, la seconde (extraction de caractéristiques) étant abordée pour une approche linéaire dans le chapitre sur l’analyse en composantes principales.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 12 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les méthodes d’extraction de caractéristiques peuvent être soit linéaires (on recherche des combinaisons linéaires des variables initiales  permettant d’optimiser un cerrtain critère), ou non linéaires (on parle également de manifold learning)</p>
</div>
</div><div class="section" id="definitions">
<h2>Définitions<a class="headerlink" href="#definitions" title="Permalink to this headline">#</a></h2>
<p>La sélection de caractéristiques consiste à choisir parmi les <span class="math notranslate nohighlight">\(d\)</span> descripteurs d’un ensemble d’individus <span class="math notranslate nohighlight">\(\mathbf x_i,i\in[\![1,n]\!]\)</span>, un sous-ensemble de  <span class="math notranslate nohighlight">\(t&lt;d\)</span>  caractéristiques jugées “les plus pertinentes”, les <span class="math notranslate nohighlight">\(d-t\)</span> restantes étant ignorées.</p>
<p>On note <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span> les <span class="math notranslate nohighlight">\(d\)</span> caractéristiques.  On note <span class="math notranslate nohighlight">\(Perf\)</span> une fonction qui permet d’évaluer un sous-ensemble de caractéristiques, et on suppose que <span class="math notranslate nohighlight">\(Perf\)</span> atteint son maximum pour le meilleur sous-ensemble de caractéristiques (“le plus pertinent”). Le problème de sélection se formule donc comme un problème d’optimisation</p>
<div class="math notranslate nohighlight">
\[\hat{F} = Arg\displaystyle\max_{U\subset F} Perf(U)\]</div>
<p>le cardinal <span class="math notranslate nohighlight">\(|\hat{F|}\)</span> de <span class="math notranslate nohighlight">\(\hat{F}\)</span> étant soit contrôlé par l’utilisateur, soit défini par l’algorithme de sélection.</p>
<p>On distingue alors trois stratégies :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(|\hat{F|}\)</span> est défini par l’utilisateur et l’optimisation s’effectue sur tous les sous-ensembles ayant ce cardinal</p></li>
<li><p>On connaît une mesure minimale de performance <span class="math notranslate nohighlight">\(\gamma\)</span>  et la sélection recherche le plus petit sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> dont la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> est supérieure ou égale à <span class="math notranslate nohighlight">\(\gamma\)</span></p></li>
<li><p>On cherche un compromis entre l’amélioration de la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> et la réduction de la taille du sous ensemble.</p></li>
</ul>
<p>La mesure de pertinence d’une caractéristique est donc au centre des algorithmes de sélection. Plusieurs définitions sont possibles, et nous dirons ici  qu’une caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> est :</p>
<ul class="simple">
<li><p>pertinente si son absence entraîne une détérioration significative de la performance de l’algorithme utilisé en aval (classification ou régression)</p></li>
<li><p>peu pertinente si elle n’est pas pertinente et s’il existe un sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> tel que la performance de <span class="math notranslate nohighlight">\(U\cup\{f_i\}\)</span> est significativement meilleure que la peformance de <span class="math notranslate nohighlight">\(U\)</span></p></li>
<li><p>non pertinente, si elle ne rentre pas dans les deux premières définitions. En général, ces caractéristiques sont supprimées.</p></li>
</ul>
</div>
<div class="section" id="caracteristiques-des-methodes-de-selection">
<h2>Caractéristiques des méthodes de sélection<a class="headerlink" href="#caracteristiques-des-methodes-de-selection" title="Permalink to this headline">#</a></h2>
<p>Une méthode de sélection basée sur l’optimisation de <span class="math notranslate nohighlight">\(Perf\)</span> utilise généralement trois étapes. Les  deux dernières sont itérées jusqu’à un test d’arrêt.</p>
<div class="section" id="initialisation">
<h3>Initialisation<a class="headerlink" href="#initialisation" title="Permalink to this headline">#</a></h3>
<p>L’initialisation consiste à choisir l’ensemble de départ des caractéristiques. Il peut s’agir de l’ensemble vide, de <span class="math notranslate nohighlight">\(F\)</span> tout entier, ou un sous-ensemble quelconque <span class="math notranslate nohighlight">\(U\subset F\)</span>.</p>
</div>
<div class="section" id="exploration-des-sous-ensembles">
<h3>Exploration des sous-ensembles<a class="headerlink" href="#exploration-des-sous-ensembles" title="Permalink to this headline">#</a></h3>
<p>A partir de cette initialisation, les stratégies d’exploration des sous-ensembles de caractéristiques se déclinent en trois catégories :</p>
<ol class="simple">
<li><p>génération exhaustive : tous les sous-ensembles de caractéristiques sont évalués. Si elle garantit de trouver la valeur optimale, cette méthode n’est que peu applicable dès que <span class="math notranslate nohighlight">\(|F|\)</span> devient important (<span class="math notranslate nohighlight">\(2^{|F|}\)</span> sous-ensembles possibles)</p></li>
<li><p>génération heuristique : une génération itérative est effectuée, chaque itération permettant de sélectionner ou de rejeter une ou plusieurs caractéristiques. La génération peut être ascendante (ajout de caractéristiques à partir de l’ensemble vide), descendante (suppression de caractéristiques à partir de <span class="math notranslate nohighlight">\(F\)</span>), ou mixte.</p></li>
<li><p>génération stochastique : pour un ensemble de données et une initialisation définie, une stratégie de recherche heuristique retourne toujours le même sous-ensemble, ce qui la rend très sensible au changement
de l’ensemble de données. La génération stochastique génère aléatoirement un nombre fini de sous-ensembles de caractéristiques afin de sélectionner le meilleur. La convergence est sous-optimale mais peut s’avérer préférable dans des algorithmes d’apprentissage, par exemple pour éviter le phénomène de surapprentissage.</p></li>
</ol>
</div>
<div class="section" id="evaluation-des-sous-ensembles">
<h3>Evaluation des sous-ensembles<a class="headerlink" href="#evaluation-des-sous-ensembles" title="Permalink to this headline">#</a></h3>
<div class="section" id="filtres">
<h4>Filtres<a class="headerlink" href="#filtres" title="Permalink to this headline">#</a></h4>
<p>Le critère d’évaluation utilisé évalue la pertinence d’une caractéristique selon des mesures
qui reposent sur les propriétés de données d’apprentissage.</p>
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> exemples  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> , on note <span class="math notranslate nohighlight">\(\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d\)</span>  une donnée d’apprentissage (la <span class="math notranslate nohighlight">\(j^e\)</span> caractéristique <span class="math notranslate nohighlight">\(f_j\)</span> ayant donc pour valeur <span class="math notranslate nohighlight">\(x_{ij}\)</span>) , d’étiquette <span class="math notranslate nohighlight">\(y_i\)</span> (en classification ou régression). Les méthodes de type filtres calculent un score pour évaluer le degré de pertinence de chacune des caractéristiques <span class="math notranslate nohighlight">\(f_i\)</span> , parmi lesquelles on peut citer</p>
<ul class="simple">
<li><p>Le critère de corrélation, utilisé en classification binaire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_i =\frac{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )\left (y_{k} -\mu_k\right )}{\sqrt{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )^2\displaystyle\sum_{k=1}^n\left (y_{k} -\mu_k\right )^2}}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mu_i\)</span> (resp. <span class="math notranslate nohighlight">\(\mu_k\)</span>) est la moyenne de la caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> observée sur <span class="math notranslate nohighlight">\(\mathbf x_1\cdots \mathbf x_n\)</span> (resp. moyenne des étiquettes)</p>
<ul class="simple">
<li><p>Le critère de Fisher,  qui permet de mesurer dans un problème de classification à <span class="math notranslate nohighlight">\(C\)</span> classes le degré de séparabilité des classes à l’aide
d’une caractéristique donnée</p></li>
</ul>
<div class="math notranslate nohighlight">
\[F_i = \frac{\displaystyle\sum_{c=1}^C n_c\left (\mu_c^i-\mu_i \right )^2}{\displaystyle\sum_{c=1}^C n_c(\Sigma_c^i)^2}\]</div>
<p>où <span class="math notranslate nohighlight">\(n_c, \mu_c^i\)</span> et <span class="math notranslate nohighlight">\(\Sigma_c^i\)</span> sont l’effectif, la moyenne et l’écart-type de la caractéristique  <span class="math notranslate nohighlight">\(f_i\)</span> dans la classe <span class="math notranslate nohighlight">\(c\)</span></p>
<ul class="simple">
<li><p>l’information mutuelle</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(i) = \displaystyle\sum_{\mathbf x_i} \displaystyle\sum_{y}P(X=\mathbf x_i,Y=y)log\left ( \frac{P(X=\mathbf x_i,Y=y)}{P(X=\mathbf x_i)P(Y=y)}\right )\]</div>
<p>qui mesure la dépendance entre les distributions de deux populations. Ici <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont deux variables aléatoires dont les réalisations sont les valeurs de <span class="math notranslate nohighlight">\(f_i\)</span> et des étiquettes de classes. Les probabilités sont estimées de manière fréquentiste.</p>
<p>Dans l’exemple suivant, on choisit de garder <span class="math notranslate nohighlight">\(|\hat{F|}=2\)</span> descripteurs, en contrôlant la pertinence par l’information mutuelle en classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant : &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">mutual_info_classif</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données après : &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant :  (150, 4)
Taille des données après :  (150, 2)
Variables sélectionnées :  [False False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="methodes-enveloppantes">
<h4>Méthodes enveloppantes<a class="headerlink" href="#methodes-enveloppantes" title="Permalink to this headline">#</a></h4>
<p>Le principal inconvénient des approches précédentes est le fait qu’elles ignorent l’influence des caractéristiques sélectionnées sur la performance de l’algorithme à utiliser par la suite. Les méthodes de type enveloppantes (wrappers)  évaluent un sous-ensemble de caractéristiques par sa performance
de classification en utilisant un algorithme d’apprentissage.  Les sous-ensembles de caractéristiques sélectionnés par cette méthode sont bien adaptés à l’algorithme de classification utilisé, mais ils ne sont pas nécessairement pour un autre. De plus, la complexité de l’algorithme d’apprentissage rend ces méthodes coûteuses.</p>
<p>Les principales différences entre les filtres et les méthodes enveloppantes pour la sélection des caractéristiques sont les suivantes :</p>
<ul class="simple">
<li><p>Les filtres mesurent la pertinence des caractéristiques par leur corrélation avec la variable dépendante, tandis que les méthodes enveloppantes mesurent l’utilité d’un sous-ensemble de caractéristiques en entraînant un modèle sur celles-ci.</p></li>
<li><p>Les filtres sont beaucoup plus rapides que les méthodes enveloppantes car elles n’impliquent pas l’apprentissage des modèles. D’un autre côté, les méthodes enveloppantes sont également très coûteuses en termes de calcul.</p></li>
<li><p>Les filtres utilisent des méthodes statistiques pour l’évaluation d’un sous-ensemble de caractéristiques, tandis que les méthodes enveloppantes utilisent la validation croisée.</p></li>
<li><p>Les filtres peuvent échouer à trouver le meilleur sous-ensemble de caractéristiques dans de nombreuses occasions, mais les méthodes enveloppantes peuvent toujours fournir le meilleur sous-ensemble de caractéristiques.</p></li>
<li><p>L’utilisation d’un sous-ensemble de caractéristiques à partir des méthodes enveloppantes amène plus facilement au phénomène de surapprentissage</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les wrappers sélectionnent les caractéristiques en se fondant sur une estimation du risque réel.</p>
</div>
</div></div>
<div class="section" id="methodes-integrees">
<h4>Méthodes intégrées<a class="headerlink" href="#methodes-integrees" title="Permalink to this headline">#</a></h4>
<p>Les méthodes intégrées incluent la sélection de variables lors du processus d’apprentissage. Un tel mécanisme intégré pour la sélection des caractéristiques peut être trouvé, par
exemple, dans les algorithmes de type SVM,  AdaBoost  ou dans les
arbres de décision.</p>
</div>
</div>
</div>
<div class="section" id="quelques-methodes-de-selection">
<h2>Quelques méthodes de sélection<a class="headerlink" href="#quelques-methodes-de-selection" title="Permalink to this headline">#</a></h2>
<div class="section" id="suppression-des-descripteurs-a-variance-faible">
<h3>Suppression des descripteurs à variance faible<a class="headerlink" href="#suppression-des-descripteurs-a-variance-faible" title="Permalink to this headline">#</a></h3>
<p>Une première idée simple consiste à supprimer les descripteurs ayant une faible variance, ces derniers n’étant pas discriminants dans la définition des individus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Avant sélection, &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Après sélection, &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avant sélection,  (150, 4)
Après sélection,  (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithmes-de-selection-sequentielle">
<h3>Algorithmes de sélection séquentielle<a class="headerlink" href="#algorithmes-de-selection-sequentielle" title="Permalink to this headline">#</a></h3>
<p>Les algorithmes SFS (Sequential Forward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>) et SBS (Sequential Backward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>-rouge) ont été les premiers à être proposés. Ils utilisent des approches heuristiques de recherche en partant, pour la première, d’un ensemble de caractéristiques vide et pour la seconde de  <span class="math notranslate nohighlight">\(F\)</span> tout entier.</p>
<div class="proof algorithm admonition" id="SFS">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algorithmes SFS et SBS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span>, taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\leftarrow F\)</span></span>)</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(d-T\)</span></span>)</p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\( |{F}|\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(|\hat{F}|\)</span></span>)</p>
<ol class="simple">
<li><p>Evaluer <span class="math notranslate nohighlight">\(\{f_j\}\cup \hat{F}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus \{f_j\}\)</span></span>)</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{max}\)</span> = meilleure caractéristique <span class="math notranslate nohighlight">\(\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(f_{min}\)</span>=moins bonne caractéristique</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow\hat{F}\cup\{f_{max}\}, F=F\setminus f_{max}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus\hat{F}f_{min}\)</span></span>)</p></li>
</ol>
</li>
</ol>
</div>
</div><p>L’étape d’évaluation utilise des données d’apprentissage : une heuristique évalue, sur un critère de performance, l’intérêt d’ajouter (ou de supprimer) le descripteur <span class="math notranslate nohighlight">\(f_i\)</span>.</p>
<p>Des variantes autour de ces algorithmes simples ont été proposées depuis et par exemple :</p>
<ul class="simple">
<li><p>il est possible à chaque itération d’inclure (ou d’exclure) un sous-ensemble de caractéristiques, plutôt qu’une seule (méthodes GSFS et GSBS)</p></li>
<li><p>on peut appliquer <span class="math notranslate nohighlight">\(p\)</span> fois SFS puis <span class="math notranslate nohighlight">\(q\)</span> fois SBS, de manière itérative, avec <span class="math notranslate nohighlight">\(p,q\)</span> des paramètres qui peuvent évoluer au cours des itérations (algorithme SFFS et SFBS)</p></li>
</ul>
<p>Dans l’exemple suivant, l’heuristique choisie est l’algorithme des 3 plus proches voisins et la mesure de performance sous-jacente est la mesure de validation croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant sélection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données après sélection&quot;</span><span class="p">,</span><span class="n">sfs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Taille des données après sélection (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithme-focus">
<h3>Algorithme Focus<a class="headerlink" href="#algorithme-focus" title="Permalink to this headline">#</a></h3>
<p>L’algorithme de filtrage Focus (<a class="reference internal" href="#FOCUS">Algorithm 2</a>}) repose sur une recherche exhaustive sur <span class="math notranslate nohighlight">\(F\)</span> pour trouver le sous-ensemble le plus performant de taille optimale.</p>
<div class="proof algorithm admonition" id="FOCUS">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algorithme FOCUS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span>, seuil <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>chaque sous-ensemble <span class="math notranslate nohighlight">\(S_i\)</span> de taille <span class="math notranslate nohighlight">\(i\)</span></p>
<ol class="simple">
<li><p>Si Inconsistance(A,<span class="math notranslate nohighlight">\(S_i\)</span>)&lt;<span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow S_i\)</span></p></li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\hat{F}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="algorithme-relief">
<h3>Algorithme relief<a class="headerlink" href="#algorithme-relief" title="Permalink to this headline">#</a></h3>
<p>La méthode relief en classification binaire (<a class="reference internal" href="#relief">Algorithm 3</a>), propose de calculer une mesure globale de la pertinence des caractéristiques en accumulant la différence des distances entre des exemples d’apprentissage choisis aléatoirement et leurs plus proches voisins de la même classe et de l’autre classe.</p>
<div class="proof algorithm admonition" id="relief">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Algorithme Relief)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , nombre d’itérations <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> un vecteur de poids des caractéristiques, <span class="math notranslate nohighlight">\(w_i\in[-1,1],i\in[\![1,d]\!]\)</span></p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_i\leftarrow 0\)</span></p></li>
</ol>
</li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>Choisir aléatoirement un exemple <span class="math notranslate nohighlight">\(\mathbf x_k\)</span></p></li>
<li><p>Chercher deux plus proches voisins de <span class="math notranslate nohighlight">\(\mathbf x_k\)</span>, l’un (<span class="math notranslate nohighlight">\(\mathbf x_p\)</span>) dans sa  classe, l’autre (<span class="math notranslate nohighlight">\(\mathbf x_q\)</span>) dans l’autre classe</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_j\leftarrow w_j+\frac{1}{nT}\left (|x_{kj} -x_{qj}|-|x_{kj} -x_{pj}| \right )\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="methode-sac">
<h3>Méthode SAC<a class="headerlink" href="#methode-sac" title="Permalink to this headline">#</a></h3>
<p>L’algorithme SAC (Selection Adaptative de Caractéristiques)  construit un ensemble de classifieurs (ou de régresseurs) <span class="math notranslate nohighlight">\((M_1\cdots M_d)\)</span> appris sur chacun des descripteurs et sélectionne les meilleurs par discrimination linéaire de Fisher. Pour ce faire, l’algorithme construit un vecteur dont les éléments sont les performances <span class="math notranslate nohighlight">\(Perf(M_i)\)</span> des modèles <span class="math notranslate nohighlight">\(M_i\)</span>, triés par ordre décroissant. Deux moyennes <span class="math notranslate nohighlight">\(m_1(i)\)</span> et <span class="math notranslate nohighlight">\(m_2(i)\)</span> sont calculées, qui représentent les deux moyennes de performance d’apprentissage qui ont une valeur respectivement plus grande (plus petite) que la performance du modèle <span class="math notranslate nohighlight">\(M_i\)</span> :</p>
<div class="math notranslate nohighlight">
\[m_1(i) = \frac{1}{i}\displaystyle\sum_{j=1}^i Perf (M_j)\textrm{ et } m_2(i) = \frac{1}{d-i}\displaystyle\sum_{j=i+1}^d Perf (M_j)\]</div>
<p>Deux variances des performances  <span class="math notranslate nohighlight">\(v_1^2(i)\)</span> et <span class="math notranslate nohighlight">\( v_2^2(i)\)</span> sont alors calculées à partir de ces moyennes, et le sous-ensemble de caractéristiques sélectionné est celui qui maximise le discriminant de Fisher</p>
<div class="math notranslate nohighlight">
\[\frac{|m_1(i)-m_2(i)|}{v_1^2(i)+v_2^2(i)}\]</div>
</div>
<div class="section" id="algorithme-rfe">
<h3>Algorithme RFE<a class="headerlink" href="#algorithme-rfe" title="Permalink to this headline">#</a></h3>
<p>L’algorithme RLE (Recusrive Feature Elimination) trie les descripteurs en analysant, localement, la sensibilité de la performance.
Étant donné un prédicteur <span class="math notranslate nohighlight">\(f\)</span> qui attribue des poids aux caractéristiques (par exemple, les coefficients d’un modèle linéaire), l’objectif de l’algorithme est de sélectionner les caractéristiques en considérant de manière récursive des ensembles de caractéristiques de plus en plus petits. Tout d’abord, le prédicteur <span class="math notranslate nohighlight">\(f\)</span> est entraîné sur l’ensemble initial de caractéristiques et l’importance de chaque caractéristique est calculée par un algorithme dédié (critère de Gini, entropie…). Les caractéristiques les moins importantes sont éliminées de l’ensemble actuel de caractéristiques. Cette procédure est répétée de manière récursive sur l’ensemble élagué jusqu’à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span> 
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant sélection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classement des variables : &quot;</span><span class="p">,</span><span class="n">s</span><span class="o">.</span><span class="n">ranking_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Variables sélectionnées :  [False False  True  True]
Classement des variables :  [2 3 1 1]
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="statsdescriptives.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Statistique descriptive</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="acp.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Analyse en composantes principales</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.5.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>