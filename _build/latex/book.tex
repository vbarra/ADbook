%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Cours}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Analyse de données}
\date{Oct 20, 2023}
\release{}
\author{Vincent BARRA}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Eléments de vocabulaire}
\end{DUlineblock}

\sphinxAtStartPar
On définit ici de manière informelle les termes utilisés dans la suite :

\index{Population@\spxentry{Population}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Population} : ensemble de cardinalité finie, notée \(N\), ou infinie ;

\end{itemize}

\index{Echantillon@\spxentry{Echantillon}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Echantillon} : sous\sphinxhyphen{}ensemble de la population, de cardinalité \(n\) ;

\end{itemize}

\index{Individu@\spxentry{Individu}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Individu} : sous\sphinxhyphen{}ensemble de la population ou de l’échantillon, de cardinalité 1 ;

\end{itemize}

\index{Caractère@\spxentry{Caractère}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Caractère} : nature de la caractéristique à laquelle on s’intéresse statistiquement. Il peut être \sphinxstylestrong{qualitatif} (nominal ou ordinal) ou \sphinxstylestrong{quantitatif} (discret ou continu).

\end{itemize}

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Probabilités et statistiques}
\end{DUlineblock}

\sphinxAtStartPar
La question qui se pose est la suivante : comment définir ou estimer la valeur de probabilité associée à un  évènement ?
Plusieurs points de vue ont été proposés et adoptés que nous synthétisons très brièvement.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Approche fréquentiste}
\end{DUlineblock}

\sphinxAtStartPar
Ce point de vue historique, souvent présenté comme le plus “naturel” ou “objectif”, consiste à définir une probabilité comme la limite de la fréquence d’observation de la caractéristique lorsque la taille de l’echantillon devient infinie. On suppose ici que les probabilités sont une loi de la nature qu’il faut mesurer par l’expérience. En pratique, la probabilité d’un  évènement est donc estimée/approximée en répétant un très grand nombre de fois l’expérience dans les mêmes conditions. C’est de ces expériences répétées que sont nés les outils de la statistique tels que la régression linéaire ou le test du \(\chi^2\).
On rencontre néanmoins très rapidement des limitations avec ce type d’approche. D’une part, il est impossible d’un point de vue fréquentiste de traiter de petits échantillons de données. De plus, certains types de données ne sont
pas exploitables en raison de leur caractère non expérimental (par exemple, quelle probabilité associer à un évènement du type “nombre de votants aux prochaines élections” qui n’est pas répétable). Enfin, il est parfois difficile de définir un modèle  permettant de modéliser une erreur de mesure ou la variation observée d’un caractère dans une population.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Approche bayésienne}
\end{DUlineblock}

\begin{sphinxShadowBox}
\sphinxstylesidebartitle{T. Bayes}

\sphinxAtStartPar
\sphinxincludegraphics{{bayes}.jpeg}
\end{sphinxShadowBox}

\sphinxAtStartPar
Un point de vue bien différent consiste à définir les probabilités comme une mesure subjective de l’incertitude. Dans ce cadre, tout événement peut être probabilisé à partir d’un a priori de l’observateur. Ce point de vue est appelé Bayésien (fait appel à la règle de Bayes) pour calculer la loi de probabilité des évènements à partir des échantillons de données a posteriori. L’intérêt majeur de ce type de démarche est que tout est probabilisable (jusqu’aux paramètres du modèle utilisé) et qu’il s’appuie sur des résultats de la théorie des probabilités, comme le théorème central limite.
Ce point de vue “subjectif” a longtemps été dénoncé par les adeptes de l’approche fréquentiste qui rejettent l’idée que l’on puisse définir un tel a priori sur les évènements. En effet, l’objection majeure que l’on oppose souvent à la méthodologie bayésienne est que deux observateurs différents, ayant des a priori différents, donneront des résultats ou des interprétations différentes.

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\large Démarche générale}
\end{DUlineblock}

\sphinxAtStartPar
De manière assez générale, une étude statistique consiste à obtenir des informations sur un caractère concernant une population de grande taille en s’appuyant sur celles d’un sous\sphinxhyphen{}ensemble de taille réduite (l’échantillon), afin le plus souvent d’orienter une décision. Le choix de l’échantillon se fera par tirage avec ou sans remise, par tirage uniforme (“au hasard”) ou non (tirage stratifié dans le cas d’un sondage par exemple, ou selon une loi de probabilité précise si une information a priori est disponible).
On estime alors des propriétés d’un caractère de l’échantillon. A partir de ces estimations, on cherche à donner “au mieux” des valeurs aux paramètres correspondant de la population (la moyenne, la variance,…). L’estimation pourra être ponctuelle ou par intervalle de confiance. On pourra également s’intéresser à des tests d’ajustement (ou d’adéquation) à une loi de type donné. La décision, étape finale de l’analyse statistique, se fera par des tests statistiques.
\label{intro:example-0}
\begin{sphinxadmonition}{note}{Example 1}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\index{Statistique@\spxentry{Statistique}!descriptive@\spxentry{descriptive}}\ignorespaces 
\index{Statistique@\spxentry{Statistique}!inférentielle@\spxentry{inférentielle}}\ignorespaces 
\sphinxAtStartPar
La science des statistiques se décompose donc en :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la \sphinxstylestrong{statistique descriptive}, dont l’objectif est de décrire le caractère d’un échantillon en résumant l’information qu’il contient ;

\item {} 
\sphinxAtStartPar
la \sphinxstylestrong{statistique inférentielle}, dont l’objectif est d’inférer, à partir de l’information recueillie sur l’échantillon, des propriétés valables sur la population, de manière la plus fiable possible.

\end{itemize}
\end{sphinxadmonition}

\index{Statistique@\spxentry{Statistique}!exploratoire@\spxentry{exploratoire}}\ignorespaces 
\sphinxAtStartPar
A cette dichotomie s’ajoute la \sphinxstylestrong{statistique exploratoire}, branche de l’analyse de données, qui cherche à comprendre l’organisation des individus de l’échantillon (existe\sphinxhyphen{}t\sphinxhyphen{}il des groupes d’individus semblables ? les caractères mesurés sont\sphinxhyphen{}ils les plus pertinents ?…)

\sphinxAtStartPar
Nous n’abordons pas dans ce cours la statistique inférentielle.

\sphinxAtStartPar
Un cuisinier cherche à savoir si sa sauce est suffisamment salée. Après avoir remué sa casserole, il prélève une cuillérée de sauce (l’échantillon). Il la goûte (il estime le caractère salé de l’échantillon). En fonction du résultat, il décide que la casserole de sauce (la population) est suffisamment salée ou pas.

\sphinxstepscope


\part{Cours}

\sphinxstepscope


\chapter{Rappels de probabilité}
\label{\detokenize{Rappels:rappels-de-probabilite}}\label{\detokenize{Rappels::doc}}

\section{Expérience aléatoire}
\label{\detokenize{Rappels:experience-aleatoire}}

\subsection{Définitions}
\label{\detokenize{Rappels:definitions}}\label{Rappels:expalea}
\begin{sphinxadmonition}{note}{Definition 1 (Expérience aléatoire)}



\sphinxAtStartPar
Une \sphinxstylestrong{expérience aléatoire} est une expérience dont on ne peut prévoir le résultat a priori. Répétée dans des conditions identiques, elle peut donner lieu à des résultats différents.
\end{sphinxadmonition}
\label{Rappels:example-1}
\begin{sphinxadmonition}{note}{Example 2}


\begin{itemize}
\item {} 
\sphinxAtStartPar
Le lancé de dé

\item {} 
\sphinxAtStartPar
Les côtes exactes d’une pièce fabriquée dans un atelier

\end{itemize}
\end{sphinxadmonition}
\label{Rappels:issue}
\begin{sphinxadmonition}{note}{Definition 2 (Issue)}



\sphinxAtStartPar
On appelle \sphinxstylestrong{issue} d’une expérience aléatoire l’un des résultats possibles de cette expérience
\end{sphinxadmonition}
\label{Rappels:univers}
\begin{sphinxadmonition}{note}{Definition 3 (Univers des possibles)}



\sphinxAtStartPar
On appelle \sphinxstylestrong{univers des possibles} d’une expérience aléatoire l’ensemble  \(\Omega\) des issues de cette expérience.
\end{sphinxadmonition}
\label{Rappels:example-4}
\begin{sphinxadmonition}{note}{Example 3}



\sphinxAtStartPar
Lorsque l’on joue à pile ou face avec une pièce de monnaie, l’expérience a deux issues possibles et \(\Omega = \{P,F\}\).
\end{sphinxadmonition}

\sphinxAtStartPar
L’univers des possibles n’est pas défini de manière unique, mais dépend de l’usage de l’experience. Par exemple, pour le lancer de deux dés, on peut être intéressé par :
\begin{itemize}
\item {} 
\sphinxAtStartPar
le résultat du lancer, dans ce cas \(\Omega = \{(1,1), (1,2), \cdots (6,6)\}\)

\item {} 
\sphinxAtStartPar
la somme des deux faces et \(\Omega = [\![2,12]\!]\)

\end{itemize}

\index{Evènement@\spxentry{Evènement}}\ignorespaces \label{Rappels:evenement}
\begin{sphinxadmonition}{note}{Definition 4 (Evènement)}



\sphinxAtStartPar
Etant donnée une expérience aléatoire, un \sphinxstylestrong{évènement} est une assertion vraie ou fausse suivant l’issue de l’expérience. C’est donc un sous\sphinxhyphen{}ensemble \(E\) de \(\Omega\).
\end{sphinxadmonition}
\label{Rappels:example-6}
\begin{sphinxadmonition}{note}{Example 4}


\begin{itemize}
\item {} 
\sphinxAtStartPar
Dans l’expérience du lancer de deux dés, on peut s’intéresser à l’évènement “la somme des deux faces est paire” ou encore  “la somme est supérieure à 7”.

\item {} 
\sphinxAtStartPar
Si l’expérience considérée concerne les jobs effectués sur une machine on peut considérer :

\end{itemize}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\Omega=\mathbb{N}\) et l’évènement “le nombre de jobs ne dépasse pas 10” : \(E=[\![0,10]\!]\)

\item {} 
\sphinxAtStartPar
\(\Omega=\mathbb{R}^*\) et  l’évènement “le job dure plus de 15 s” et \(E=]15,+\infty[\)

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
Il existe certains évènements particuliers :
\begin{itemize}
\item {} 
\sphinxAtStartPar
l’évènement dit certain : c’est l’univers des possibles (par exemple “la somme des deux faces d’un lancer de deux dés est inférieure ou égale à 12”)

\item {} 
\sphinxAtStartPar
l’évènement impossible (“la somme des deux faces d’un lancer de deux dés est supérieure ou égale à 20”)

\item {} 
\sphinxAtStartPar
l’évènement simple : tout singleton de \(\Omega\)

\item {} 
\sphinxAtStartPar
l’évènement composé : tout sous\sphinxhyphen{}ensemble de \(\Omega\) de cardinalité au moins égale à 2.

\end{itemize}


\subsection{Notation et opérations sur les évènements}
\label{\detokenize{Rappels:notation-et-operations-sur-les-evenements}}
\sphinxAtStartPar
Les évènements peuvent être interprétés soit d’un point de vue ensembliste (Diagrammes de Venn), soit de manière équivalente d’un point de vue probabiliste.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Notation}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstylestrong{Interprétation probabiliste}
\\
\hline
\sphinxAtStartPar
\(\omega\)
&
\sphinxAtStartPar
issue possible, évènement élémentaire
\\
\hline
\sphinxAtStartPar
\(A\)
&
\sphinxAtStartPar
évènement
\\
\hline
\sphinxAtStartPar
\(\omega\in A\)
&
\sphinxAtStartPar
\(\omega\) réalise \(A\)
\\
\hline
\sphinxAtStartPar
\(A\subset B\)
&
\sphinxAtStartPar
\(A\) implique \(B\)
\\
\hline
\sphinxAtStartPar
\(A\cup B\)
&
\sphinxAtStartPar
\(A\) ou \(B\)
\\
\hline
\sphinxAtStartPar
\(A\cap B\)
&
\sphinxAtStartPar
\(A\) et \(B\)
\\
\hline
\sphinxAtStartPar
\(\bar A\)
&
\sphinxAtStartPar
contraire de \(A\)
\\
\hline
\sphinxAtStartPar
\(\emptyset\)
&
\sphinxAtStartPar
évènement impossible
\\
\hline
\sphinxAtStartPar
\(\Omega\)
&
\sphinxAtStartPar
évènement certain
\\
\hline
\sphinxAtStartPar
\(A\cap B=\emptyset\)
&
\sphinxAtStartPar
\(A\) et \(B\) incompatibles
\\
\hline
\sphinxAtStartPar
\(A\setminus B = A\cap \bar B\)
&
\sphinxAtStartPar
\(A\) et pas \(B\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Probabilités}
\label{\detokenize{Rappels:probabilites}}

\subsection{Objectif}
\label{\detokenize{Rappels:objectif}}
\sphinxAtStartPar
L’objectif des probabilités est de donner une \sphinxstylestrong{mesure} à la chance qu’a un évènement de se réaliser lors d’une expérience aléatoire. Pour ce faire, on définit une fonction \(P:\Omega\rightarrow [0,1]\) vérifiant certains axiomes et propriétés.

\index{Tribu@\spxentry{Tribu}}\ignorespaces \label{Rappels:tribu}
\begin{sphinxadmonition}{note}{Definition 5 (Tribu)}



\sphinxAtStartPar
Soit \(T\) une famille d’évènements. Pour que \(T\) soit probabilisable, il faut que :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\emptyset\in T, \Omega\in T\)

\item {} 
\sphinxAtStartPar
Si \(A_i\) est une suite dans \(T\) alors \(\cup_iA_i\in T\) et \(\cap_iA_i\in T\)

\item {} 
\sphinxAtStartPar
Si \(A\in T\) alors \(\bar A\in T\)

\end{itemize}

\sphinxAtStartPar
\(T\) est une \sphinxstylestrong{tribu} et \((\Omega,T)\) est un \sphinxstylestrong{espace probabilisable}.
\end{sphinxadmonition}

\sphinxAtStartPar
En pratique, on choisit souvent la tribu engendrée par une famille de \(n\) évènements \(A_i\), qui est l’ensemble des parties de \(\Omega\) obtenues en effectuant l’union de \(k\) évènements \(A_i,i\in [\![1,n]\!]\).
\label{Rappels:example-8}
\begin{sphinxadmonition}{note}{Example 5}



\sphinxAtStartPar
Dans le cas du lancer d’un dé, \(\Omega = \{1,2,3,4,5,6\}\), et :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la tribu engendrée par la famille d’évènements \(\{\{1,3,5\},\{2,4,6\}\}\) est \(\{\emptyset,\{1,3,5\},\{2,4,6\},\Omega\}\).

\item {} 
\sphinxAtStartPar
la tribu engendrée par la famille d’évènements \(\{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}\) est l’ensemble des parties de \(\Omega\). Plus généralement, si \(\Omega\) est dénombrable, cette tribu est appelée \sphinxstylestrong{tribu discrète}.

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
On peut également s’intéresser, si \(\Omega=\mathbb R\), à la tribu engendrée par les ouverts de \(\mathbb{R}\), on parle alors de \sphinxstylestrong{tribu borélienne}.


\subsection{Probabilité}
\label{\detokenize{Rappels:probabilite}}
\begin{sphinxShadowBox}
\sphinxstylesidebartitle{A. Kolmogorov}

\sphinxAtStartPar
\sphinxincludegraphics{{kolmogorov}.jpeg}
\end{sphinxShadowBox}

\index{Probabilité@\spxentry{Probabilité}}\ignorespaces \label{Rappels:axiomKolmo}
\begin{sphinxadmonition}{note}{Axiom 1 (Axiomatique de Kolmogorov)}



\sphinxAtStartPar
On appelle \sphinxstylestrong{probabilité} sur \((\Omega,T)\) une application \(P\) de \(T\) dans {[}0,1{]} vérifiant :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\((\forall A\in T)\; P(A)\in[0,1]\)

\item {} 
\sphinxAtStartPar
\(P(\Omega)=1\)

\item {} 
\sphinxAtStartPar
Pour toute famille dénombrable \((A_i)\) d’évènements disjoints \(P(\displaystyle\bigcup_i A_i) = \displaystyle\sum_iP(A_i)\)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
\((\Omega,T,P)\) est un \sphinxstylestrong{espace probabilisé}.
\label{Rappels:property-10}
\begin{sphinxadmonition}{note}{Property 1}


\begin{itemize}
\item {} 
\sphinxAtStartPar
\(P(\emptyset)=0\)

\item {} 
\sphinxAtStartPar
\((\forall A)\; P(\bar A)=1-P(A)\)

\item {} 
\sphinxAtStartPar
\((\forall A,B)\; P(A\setminus B) = P(A)-P(A\bigcap B)\)

\item {} 
\sphinxAtStartPar
\((\forall A,B)\; P(A\bigcup B) = P(A)+P(B)-P(A\bigcap B)\)

\item {} 
\sphinxAtStartPar
\((\forall A,B)\) si \(A\subset B\) alors \(P(A)\leq P(B)\)

\item {} 
\sphinxAtStartPar
Pour toute famille dénombrable \((A_i)\) d’évènements quelconques \(P(\displaystyle\bigcup_i A_i) \leq \displaystyle\sum_iP(A_i)\)

\end{itemize}
\end{sphinxadmonition}


\subsection{Conditionnement}
\label{\detokenize{Rappels:conditionnement}}
\sphinxAtStartPar
Les probabilités \sphinxstylestrong{conditionnelles} intègrent une information supplémentaire sous la forme de l’observation de la réalisation d’un évènement donné.

\index{Probabilité@\spxentry{Probabilité}!conditionnelle@\spxentry{conditionnelle}}\ignorespaces \label{Rappels:definition-11}
\begin{sphinxadmonition}{note}{Definition 6 (Probabilité conditionnelle)}



\sphinxAtStartPar
Soit \(B\) un évènement de probabilité non nulle. On appelle \sphinxstylestrong{probabilité conditionnelle} de \(A\) sachant \(B\) le rapport

\sphinxAtStartPar
\(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\)
\end{sphinxadmonition}

\sphinxAtStartPar
\(P(A\mid B)\) représente la probabilité que \(A\) se réalise sachant que \(B\) est réalisé.

\sphinxAtStartPar
Remarquons que l’on peut écrire \(P(A\bigcap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\).
\label{Rappels:example-12}
\begin{sphinxadmonition}{note}{Example 6}



\sphinxAtStartPar
7\% des français sont atteints d’un cancer du poumon. 70\% des malades sont des fumeurs et 50\% des français fument. On recherche la probabilité d’être atteint d’un cancer du poumon lorsque l’on est fumeur.
L’évènement \(A\) est “avoir un cancer du poumon”, et \(B\) est “être fumeur”. D’après les données on a \(P(A)\)=0.07, \(P(B)\) = 0.5 et \(P(B\mid A)\) = 0.7.
On a alors \(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\) avec \(P(A\bigcap B)=P(B\mid A)P(A)\) d’où

\sphinxAtStartPar
\(P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B}\) = 0.098
\end{sphinxadmonition}


\subsection{Indépendance}
\label{\detokenize{Rappels:independance}}
\index{Indépendance@\spxentry{Indépendance}}\ignorespaces \label{Rappels:definition-13}
\begin{sphinxadmonition}{note}{Definition 7 (Indépendance)}



\sphinxAtStartPar
Deux évènements \(A\) et \(B\) sont dits \sphinxstylestrong{indépendants} si et seulement si \(P(A\mid B) = P(A)\).
\end{sphinxadmonition}

\sphinxAtStartPar
On a alors bien évidemment \(P(A\bigcap B) = P(A)P(B)\).
\label{Rappels:remark-14}
\begin{sphinxadmonition}{note}{Remark 1}



\sphinxAtStartPar
La notion d’indépendance est directement rattachée à \(P\) : \(A\) et \(B\) peuvent être indépendants pour une probabilité donnée, mais pas pour une autre.
\end{sphinxadmonition}

\sphinxAtStartPar
On peut généraliser la notion d’indépendance à une famille d’évènements \((A_i)_{i\in[\![1,n]\!]}\) : on dira que les \(A_i\) sont \sphinxstylestrong{mutuellement indépendants} si pour tout \(I\subset [\![1,n]\!]\)

\sphinxAtStartPar
\(P\left (\displaystyle\bigcap_{i\in I} A_i\right ) = \displaystyle\prod_{i\in I} P(A_i)\)

\sphinxAtStartPar
L’indépendance mutuelle est plus forte que l’indépendance deux à deux.
\label{Rappels:remark-15}
\begin{sphinxadmonition}{note}{Remark 2}



\sphinxAtStartPar
La notion d’indépendance n’est pas une notion purement ensembliste. Deux évènements peuvent être indépendants pour une loi de probabilité et pas pour une autre.
\end{sphinxadmonition}


\subsection{Théorème des probabilités totales}
\label{\detokenize{Rappels:theoreme-des-probabilites-totales}}\label{Rappels:theorem-16}
\begin{sphinxadmonition}{note}{Theorem 1}



\sphinxAtStartPar
Soit \(\{B_i\}\) un système complet d’évènements (qui forment donc une partition de \(\Omega\)). Pour tout évènement \(A\), on peut écrire

\sphinxAtStartPar
\(P(A) = \displaystyle\sum_i P(A\bigcap B_i) = \displaystyle\sum_i P(A| B_i) P(B_i)\)
\end{sphinxadmonition}


\subsection{Règle de Bayes}
\label{\detokenize{Rappels:regle-de-bayes}}
\index{Bayes@\spxentry{Bayes}!règle de@\spxentry{règle de}}\ignorespaces 
\sphinxAtStartPar
A partir de l’égalité \(P(A\bigcap B) = P(A|B)P(B)=P(B|A)P(A)\), on définit la règle de Bayes

\sphinxAtStartPar
\((\forall A,B)\quad P(B|A)=\frac{P(A|B)P(B)}{P(A)}\)

\sphinxAtStartPar
Si \(B_i\) est un système complet d’évènements, on a de plus

\sphinxAtStartPar
\(P(B_i|A)= \frac{P(A|B_i)P(B_i)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\displaystyle\sum_k P(A|B_k)P(B_k)}\)
\label{Rappels:example-17}
\begin{sphinxadmonition}{note}{Example 7}



\sphinxAtStartPar
Un fabricant de boulons a trois usines de fabrication situées à Amiens, Besançon et Clermont\sphinxhyphen{}Ferrand. Amiens fournit 25\% de la production, Besançon 20\% et Clermont\sphinxhyphen{}Ferrand 55\%. Les boulons de 5mm représentent 20\% des boulons produits à Amiens, 30\% à Besançon et 15\% à Clermont\sphinxhyphen{}Ferrand. On répond à la question suivante : sachant que le boulon acheté a une taille de 5mm, quelle est la probabilité qu’il soit produit à Clermont\sphinxhyphen{}Ferrand ?

\sphinxAtStartPar
On note \(B_1\) (respectivement \(B_2,B_3\)) l’évènement “Le boulon est produit à Amiens (resp. Besançon, Clermont\sphinxhyphen{}Ferrand)”. On note également \(A\) l’évènement “Le boulon fait 5mm”. On cherche donc

\sphinxAtStartPar
\(P(B_3|A) = \frac{P(A|B_3)P(B_3)}{P(A)}= \frac{0.15*0.55}{0.1925}=0.428\).
\end{sphinxadmonition}

\sphinxAtStartPar
On a calculé dans l’exemple une \sphinxstylestrong{probabilité a posteriori}, c’est à dire sachant une information supplémentaire (le boulon fait 5mm). La prise en compte de cette information modifie la valeur de la probabilité associée à \(B_3\). La théorie des probabilités au travers de l’approche bayésienne est adaptée pour prendre en compte toute information nouvelle.


\section{Variable aléatoire}
\label{\detokenize{Rappels:variable-aleatoire}}

\subsection{Concept de variable aléatoire}
\label{\detokenize{Rappels:concept-de-variable-aleatoire}}
\sphinxAtStartPar
Soit un espace probabilisé \((\Omega, T,P)\).

\index{Variable aléatoire@\spxentry{Variable aléatoire}}\ignorespaces \label{Rappels:definition-18}
\begin{sphinxadmonition}{note}{Definition 8 (Variable aléatoire)}



\sphinxAtStartPar
Une variable aléatoire est une application \(X:\Omega\rightarrow E\) (on prendra \(E=\mathbb R\))
\end{sphinxadmonition}

\sphinxAtStartPar
Pour obtenir la probabilité d’une valeur quelconque image par \(X\), il suffit de dénombrer les \(\omega\) qui réalisent cette valeur.
\label{Rappels:example-19}
\begin{sphinxadmonition}{note}{Example 8}



\sphinxAtStartPar
Si \(\Omega\) = (Pile,Face), on considère la loi de probabilité \(P\) telle que : \((\forall \omega\in\Omega)\; P(\omega)=\frac12\).
\(P(X=1)= P(\{Pile\}) = \frac12\).
\end{sphinxadmonition}

\sphinxAtStartPar
On dit que l’on transporte la loi de probabilité de \(\Omega\) sur \(E\) par l’application \(X\).

\sphinxAtStartPar
Les éléments de \(E\) sont les \sphinxstylestrong{réalisations} de la variable aléatoire.
\label{Rappels:example-20}
\begin{sphinxadmonition}{note}{Example 9}



\sphinxAtStartPar
Si l’expérience consiste à observer le résultat du tirage de deux dés à 6 faces, \(\Omega = \{(1,1), (1,2), \cdots (6,5), (6,6)\}\), on considère la loi de probabilité telle que \((\forall \omega\in\Omega)\; P(\omega)=\frac{1}{36}\).

\sphinxAtStartPar
Si l’application \(X\) réalise la somme des deux éléments de \(\omega\in\Omega\), alors on a par exemple \(P(X=3)= P(\{(1,2),(2,1)\}) = \frac{2}{36}\), ou encore \(P(X=5)= P(\{(1,4),(2,3),(3,2),(4,1)\}) = \frac{4}{36}\).
\end{sphinxadmonition}


\subsection{Variable aléatoire mesurable}
\label{\detokenize{Rappels:variable-aleatoire-mesurable}}
\sphinxAtStartPar
On définit sur \(E\) une tribu \(T'\).  \((E,T')\) est alors un espace probabilisable, et tout élément \(B\) de \(T'\) est un évènement. On note alors \(X^{-1}(B) = \{\omega\in\Omega,\; X(\omega)\in B\}\)

\index{Variable aléatoire@\spxentry{Variable aléatoire}!mesurable@\spxentry{mesurable}}\ignorespaces \label{Rappels:definition-21}
\begin{sphinxadmonition}{note}{Definition 9 (Variable aléatoire mesurable)}



\sphinxAtStartPar
Une variable aléatoire \(X\) est dite mesurable  si et seulement si : \((\forall B\in T')\; X^{-1}(B)\in T\)
\end{sphinxadmonition}

\sphinxAtStartPar
Dans les deux exemples précédents, on a par exemple \(X^{-1}(1)= \{Pile\}\) ou encore \(X^{-1}(3) = \{(1,2),(2,1)\}\) et \(P(X=3)=P(X^{-1}(3)) = \frac{2}{36}\).

\index{Distribution de probabilité@\spxentry{Distribution de probabilité}}\ignorespaces 
\index{Probabilité@\spxentry{Probabilité}!distribution de@\spxentry{distribution de}}\ignorespaces 
\sphinxAtStartPar
On note souvent \(P_X(B) = P(X^{-1}(B))=P(\{\omega / X(\omega)\in B\})\) et on l’appelle \sphinxstylestrong{probabilité image} de \(P\) par \(X\). En calculant la probabilité de chaque réalisation de la variable aléatoire \(X\), on peut en déduire la \sphinxstylestrong{loi de probabilité} (ou \sphinxstylestrong{distribution}) de \(X\).
\begin{itemize}
\item {} 
\sphinxAtStartPar
Pour une variable aléatoire discrète \(X\), la loi de probabilité est donc \(P_X(x_i)= P(X=x_i) = P(\{\omega / X(\omega)=x_i\})\). \(P_X\) est appelée \sphinxstylestrong{masse ponctuelle}.

\end{itemize}

\index{Probabilité@\spxentry{Probabilité}!densité de@\spxentry{densité de}}\ignorespaces 
\index{Densité de probabilité@\spxentry{Densité de probabilité}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
Pour une variable aléatoire continue \(X\), la loi de probabilité est donc \(f_X(x)dx = P(x\leq X\leq x+dx) = P(\{\omega /x\leq X(\omega)\leq x+dx\})\). \(f_X\) est appelée \sphinxstylestrong{densité de probabilité}.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{floor}
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{random}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k}{def} \PYG{n+nf}{tirage}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{d1}\PYG{o}{=}\PYG{n}{floor}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}   
    \PYG{n}{d2}\PYG{o}{=}\PYG{n}{floor}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}  
    \PYG{k}{return} \PYG{n}{d1}\PYG{o}{+}\PYG{n}{d2} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}          

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}
\PYG{n}{f} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10000}                       
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}        
    \PYG{n}{f}\PYG{p}{[}\PYG{n}{tirage}\PYG{p}{(}\PYG{p}{)} \PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{f}\PYG{o}{=}\PYG{n}{f}\PYG{o}{/}\PYG{n}{n}                      

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}   
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{vlines}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{f} \PYG{p}{)}   
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{n}{bottom}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loi de probabilité d}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{une variable aléatoire discrète}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{2b40be8386aeb598154a1505ed1c5096ec0556bb45e947d1dc4f76ee5f9fec04}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\index{Fonction de répartition@\spxentry{Fonction de répartition}}\ignorespaces \label{Rappels:definition-22}
\begin{sphinxadmonition}{note}{Definition 10 (Fonction de répartition)}



\sphinxAtStartPar
La fonction de répartition d’une variable aléatoire \(X\) est l’application \(F_X\) de \(\mathbb R\) dans {[}0,1{]} telle que \(F_X(x) = P(X\leq x)\).
\end{sphinxadmonition}

\sphinxAtStartPar
\(F_X\) est donc monotone croissante, continue à droite et on a en particulier :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(P(a\leq X\leq b) = F_X(b)-F_X(a)\)

\item {} 
\sphinxAtStartPar
\(P(X>x) = 1-P(X\leq x) = 1-F_X(x)\)

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{floor}
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{random}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k}{def} \PYG{n+nf}{tirage}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{d1}\PYG{o}{=}\PYG{n}{floor}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}   
    \PYG{n}{d2}\PYG{o}{=}\PYG{n}{floor}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}  
    \PYG{k}{return} \PYG{n}{d1}\PYG{o}{+}\PYG{n}{d2} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}          

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}
\PYG{n}{f} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10000}                       
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}        
    \PYG{n}{f}\PYG{p}{[}\PYG{n}{tirage}\PYG{p}{(}\PYG{p}{)} \PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{f}\PYG{o}{=}\PYG{n}{f}\PYG{o}{/}\PYG{n}{n}     

\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{fn} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}          
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}   
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{vlines}\PYG{p}{(} \PYG{n}{x}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{f} \PYG{p}{)}   
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{n}{bottom}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Distribution}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{fn}\PYG{p}{,} \PYG{n}{xmin}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{xmax}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,}
          \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{vlines}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ymin}\PYG{o}{=}\PYG{n}{fn}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ymax}\PYG{o}{=}\PYG{n}{fn}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
          \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(} \PYG{n}{bottom}\PYG{o}{=}\PYG{l+m+mi}{0} \PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{f8cb77e0cb4ee1d410b20d28e1a812f534de338ae185b2af4a99c78ba7175763}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
La notion de variable aléatoire est ainsi une formalisation de la notion de grandeur variant selon le résultat d’une expérience aléatoire. On peut alors préciser et formaliser la définition précédente.
\label{Rappels:definition-23}
\begin{sphinxadmonition}{note}{Definition 11 (Variable aléatoire)}



\sphinxAtStartPar
Une variable aléatoire est une application mesurable \(X:(\Omega,T,P) \rightarrow (E,T')\)
\end{sphinxadmonition}
\label{Rappels:remark-24}
\begin{sphinxadmonition}{note}{Remark 3}


\begin{itemize}
\item {} 
\sphinxAtStartPar
Si \(E=\mathbb N\), on parle de variable aléatoire (réelle) discrète

\item {} 
\sphinxAtStartPar
Si \(E=\mathbb R\), on parle de variable aléatoire (réelle) continue. \(T'\) est alors la tribu \sphinxstylestrong{borélienne}

\item {} 
\sphinxAtStartPar
Si \(E=\mathbb N^n\) ou \(E=\mathbb R^n\), on parle de \sphinxstylestrong{vecteur aléatoire} de dimension \(n\).

\end{itemize}
\end{sphinxadmonition}


\subsection{Caractéristiques des variables aléatoires}
\label{\detokenize{Rappels:caracteristiques-des-variables-aleatoires}}
\sphinxAtStartPar
Une loi de probabilité est caractérisée par un certain nombre de grandeurs :
\begin{itemize}
\item {} 
\sphinxAtStartPar
sa valeur centrale

\item {} 
\sphinxAtStartPar
sa dispersion

\item {} 
\sphinxAtStartPar
sa forme

\end{itemize}


\subsubsection{Espérance mathématique d’une variable aléatoire}
\label{\detokenize{Rappels:esperance-mathematique-d-une-variable-aleatoire}}
\index{Espérance@\spxentry{Espérance}}\ignorespaces \label{Rappels:definition-25}
\begin{sphinxadmonition}{note}{Definition 12 (Espérance)}



\sphinxAtStartPar
Soit \(X\) une variable aléatoire. On définit l’espérance mathématique de \(X\), et on note \(\mathbb E(X)\) par :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\mathbb E(X) = \mu_X = \displaystyle\sum_{x_i} x_iP(X=x_i)= \displaystyle\sum_{x_i} x_i P_X(x_i)\) si \(X\) est discrète et si la somme converge.

\item {} 
\sphinxAtStartPar
\(\mathbb E(X) = \mu_X =\int_x xdP(x) = \int_x x f_X(x) dx\) si \(X\) est continue et si l’intégrale converge.

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
\(\mathbb E(X)\) est la moyenne arithmétique (également notée \(\mu_X\)) des différentes valeurs prises par \(X\) pondérées par leur probabilité.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{rv\PYGZus{}discrete}
\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{]}
\PYG{n}{p} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}
\PYG{n}{distribution} \PYG{o}{=} \PYG{n}{rv\PYGZus{}discrete}\PYG{p}{(}\PYG{n}{values}\PYG{o}{=}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Espérance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{expect}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Espérance :  23.0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{rv\PYGZus{}continuous}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{exp}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mf}{3.5} 
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mf}{5.5} 
\PYG{k}{class} \PYG{n+nc}{distribution\PYGZus{}gen}\PYG{p}{(}\PYG{n}{rv\PYGZus{}continuous}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}pdf}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{distribution} \PYG{o}{=} \PYG{n}{distribution\PYGZus{}gen}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Espérance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{expect}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{lb}\PYG{o}{=}\PYG{n}{a}\PYG{p}{,} \PYG{n}{ub}\PYG{o}{=}\PYG{n}{b}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Espérance :  7.904816400226159
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
On dira que \(X\) est \sphinxstylestrong{centrée} si \(\mathbb{E}(X)=0\).
\label{Rappels:example-26}
\begin{sphinxadmonition}{note}{Example 10}



\sphinxAtStartPar
Pour l’expérience d’un lancer de dé à 6 faces  : \(\mathbb E(X) = \mu_X = \displaystyle\sum_{i=1}^6 i\frac16 = \frac72\)
\end{sphinxadmonition}
\label{Rappels:property-27}
\begin{sphinxadmonition}{note}{Property 2}


\begin{itemize}
\item {} 
\sphinxAtStartPar
\((\forall a\in\mathbb{R})\; \mathbb{E}(a)= a\)

\item {} 
\sphinxAtStartPar
\((\forall a\in\mathbb{R})\; \mathbb{E}(aX)= a\mathbb{E}(X)\)

\item {} 
\sphinxAtStartPar
\((\forall a\in\mathbb{R})\; \mathbb{E}(X+a) = \mathbb{E}(X) +a\)

\end{itemize}
\end{sphinxadmonition}


\subsubsection{Moment d’une fonction d’une variable aléatoire}
\label{\detokenize{Rappels:moment-d-une-fonction-d-une-variable-aleatoire}}
\index{Moment@\spxentry{Moment}}\ignorespaces 
\sphinxAtStartPar
Soit \(\phi\) l’application qui associe à toute variable aléatoire \(X\) la variable aléatoire \(Y=\phi(X)\).
\label{Rappels:definition-28}
\begin{sphinxadmonition}{note}{Definition 13 (Moment)}



\sphinxAtStartPar
Le moment  \(\mathbb{E}[\phi(X)]\) de la fonction \(\phi\) de la variable aléatoire \(X\) est égal à
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\mathbb{E}[\phi(X)] = \displaystyle\sum_{x_i} \phi(x_i)P_X(x_i)\) si \(X\) est discrète

\item {} 
\sphinxAtStartPar
\(\mathbb{E}[\phi(X)] = \int_x \phi(x) f_X(x)dx\) si \(X\) est continue

\end{itemize}
\end{sphinxadmonition}

\index{Moment@\spxentry{Moment}!d'ordre k@\spxentry{d'ordre k}}\ignorespaces \label{Rappels:definition-29}
\begin{sphinxadmonition}{note}{Definition 14 (Moment d’ordre \protect\(k\protect\))}



\sphinxAtStartPar
Le moment d’ordre \(k\) d’une variable aléatoire \(X\) est égal à :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\mathbb{E}(X^k) = \displaystyle\sum_{x_i} x_i^k P_X(x_i)\) si \(X\) est discrète

\item {} 
\sphinxAtStartPar
\( \mathbb{E}(X^k) = \int_x x^k f_X(x)dx\) si \(X\) est continue

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Le moment d’ordre \(k\) est donc un cas particulier avec \(Y=X^k\).
\label{Rappels:remark-30}
\begin{sphinxadmonition}{note}{Remark 4}



\sphinxAtStartPar
L’espérance \(\mathbb{E}(X)\) est le moment d’ordre 1.
\end{sphinxadmonition}

\index{Moment@\spxentry{Moment}!centré@\spxentry{centré}}\ignorespaces \label{Rappels:definition-31}
\begin{sphinxadmonition}{note}{Definition 15 (Moment centré d’ordre \protect\(k\protect\))}



\sphinxAtStartPar
On appelle moment centré d’ordre \(k\) la quantité \(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right]\)
\end{sphinxadmonition}

\sphinxAtStartPar
Ainsi :
\begin{itemize}
\item {} 
\sphinxAtStartPar
pour une variable aléatoire discrète \(X\), \(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \displaystyle\sum_{x_i} (x_i-\mathbb{E}(X))^k P_X(x_i) = \displaystyle\sum_{x_i} (x_i-\mu_X)^k P_X(x_i)\)

\item {} 
\sphinxAtStartPar
pour une variable aléatoire continue \(X\), \(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \int_x (x-\mathbb{E}(X))^k f_X(x)dx = \int_x (x-\mu_X)^k f_X(x)dx\)

\end{itemize}


\subsubsection{Variance d’une variable aléatoire}
\label{\detokenize{Rappels:variance-d-une-variable-aleatoire}}
\index{Variance@\spxentry{Variance}}\ignorespaces 
\index{Ecart\sphinxhyphen{}type@\spxentry{Ecart\sphinxhyphen{}type}}\ignorespaces 
\sphinxAtStartPar
Pour \(k\)=2, le moment centré d’ordre 2 est appelé la \sphinxstylestrong{variance} et est noté \(\mathbb{V}(X)\). La racine carrée de la variance est \sphinxstylestrong{l’écart type} et est noté \(\sigma_X\). On a donc \(\sigma_X^2=\mathbb{V}(X)\).
\label{Rappels:proposition-32}
\begin{sphinxadmonition}{note}{Proposition 1 (Formule de Koenig)}



\sphinxAtStartPar
\(\mathbb{V}(X) = \mathbb{E}(X^2)-\mu_X^2\)
\end{sphinxadmonition}

\sphinxAtStartPar
En effet, \(\mathbb{E}\left [(X-\mu_X)^2 \right] = \mathbb{E}\left [(X^2-2\mu_XX+\mu_X^2 \right] = \mathbb{E}(X^2)-2\mu_X\mathbb{E}(X)+\mu_X^2\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{rv\PYGZus{}discrete}

\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{]}
\PYG{n}{p} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}
\PYG{n}{distribution} \PYG{o}{=} \PYG{n}{rv\PYGZus{}discrete}\PYG{p}{(}\PYG{n}{values}\PYG{o}{=}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ecart\PYGZhy{}type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Variance :  61.0
Ecart\PYGZhy{}type :  7.810249675906654
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{rv\PYGZus{}continuous}
\PYG{k+kn}{from} \PYG{n+nn}{math} \PYG{k+kn}{import} \PYG{n}{exp}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mf}{3.5} 
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mf}{5.5} 
\PYG{k}{class} \PYG{n+nc}{distribution\PYGZus{}gen}\PYG{p}{(}\PYG{n}{rv\PYGZus{}continuous}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}pdf}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{6}\PYG{o}{*}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{distribution} \PYG{o}{=} \PYG{n}{distribution\PYGZus{}gen}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{distribution} \PYG{o}{=} \PYG{n}{distribution\PYGZus{}gen}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{o}{=}\PYG{n}{b}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Ecart\PYGZhy{}type : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{distribution}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Variance:  0.029595913310970445
Ecart\PYGZhy{}type :  0.1720346282321395
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\label{Rappels:property-33}
\begin{sphinxadmonition}{note}{Property 3}


\begin{itemize}
\item {} 
\sphinxAtStartPar
\((\forall a,b\in\mathbb{R})\; \mathbb{V}(aX+b)= a^2\mathbb{V}(X)\)

\item {} 
\sphinxAtStartPar
\((\forall a\in\mathbb{R})\; \mathbb{E}\left [(X-a)^2\right ] = \mathbb V(X) +(\mathbb{E}(X)-a)^2\) (théorème de Huygens)

\item {} 
\sphinxAtStartPar
\(\forall k>0\; P(|X-\mathbb{E}(X)|\geq k\sigma_X)\leq \frac{1}{k^2}\) (inégalité de Bienaymé\sphinxhyphen{}Tchebychev)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
On dira que la variable aléatoire \(X\) est \sphinxstylestrong{réduite} (ou \sphinxstylestrong{normée}) si \(\mathbb{V}(X)=1\).


\subsubsection{Moments d’ordre supérieur}
\label{\detokenize{Rappels:moments-d-ordre-superieur}}
\index{Skewness@\spxentry{Skewness}}\ignorespaces 
\index{Kurtosis@\spxentry{Kurtosis}}\ignorespaces 
\sphinxAtStartPar
On considère également souvent les moments d’ordre 3 (coefficient d’asymétrie ou skewness) et 4 (coefficient d’applatissement ou kurtosis).

\sphinxstepscope


\chapter{Elements de statistiques}
\label{\detokenize{elemstats:elements-de-statistiques}}\label{\detokenize{elemstats::doc}}
\sphinxAtStartPar
Dans l’expression “étude statistique”, il faut distinguer :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{les données statistiques} : suivant l’étude, plusieurs problèmes peuvent être posés :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Recueil des données (brutes) avec notamment le problème des sondages

\item {} 
\sphinxAtStartPar
Nature des données avec éventuellement la transformation des données brutes, notamment pour les séries chronologiques (série corrigée des variations saisonnières)

\item {} 
\sphinxAtStartPar
Organisation des données : il s’agit le plus souvent de résumer l’information par les techniques de la statistique descriptive

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{le modèle mathématique} : une analyse du phénomène étudié doit permettre de traduire les problèmes posés par l’étude dans un langage formel, celui des probabilités. Après avoir fait des choix, des hypothèses sur la loi de probabilité et sur les paramètres de cette loi, on s’efforce de se placer dans un modèle statistique dans lequel des outils théoriques permettent de résoudre un certain nombre de problèmes théoriques. Dans ce modèle théorique, il s’agit de donner une interprétation aux données expérimentales et, souvent, des hypothèses implificatrices de “même loi” et d’indépendance sont faites.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{l’analyse statistique} : l’utilisation d’outils statistiques adaptés au modèle retenu permet de faire l’interface entre les données statistiques et le modèle théorique choisi pour décrire le phénomène étudié.

\end{enumerate}

\sphinxAtStartPar
L’étude statistique peut alors se traduire sous diverses formes :
\begin{itemize}
\item {} 
\sphinxAtStartPar
préciser le modèle choisi, en estimant les paramètres intervenant dans celui\sphinxhyphen{}ci

\item {} 
\sphinxAtStartPar
juger la validité d’hypothèses faites sur ces paramètres qui se traduira non pas en ‘’confirmation d’hypothèses’’, mais en ‘’détecteur d’hypothèses fausses’’

\item {} 
\sphinxAtStartPar
juger l’adéquation du modèle retenu en termes de lois de probabilité avec la même réserve que ci\sphinxhyphen{}dessus

\end{itemize}

\sphinxAtStartPar
Les résultats théoriques devront être interprétés dans le contexte de l’étude en considérant que ces résultats ont été obtenus dans le cadre d’un modèle théorique précis, d’où la nécessité d’une analyse correcte et d’une bonne formalisation. De plus, il faudra prendre en compte les techniques utilisées, qui ne permettent de répondre qu’à des questions précises. Enfin, dans le cas d’une application pratique, il faudra garder à l’esprit que les conclusions auront des conséquences économiques (ou autres).


\section{Echantillon d’une variable aléatoire}
\label{\detokenize{elemstats:echantillon-d-une-variable-aleatoire}}

\subsection{Définition}
\label{\detokenize{elemstats:definition}}\label{elemstats:definition-0}
\begin{sphinxadmonition}{note}{Definition 16 (Echantillon)}



\sphinxAtStartPar
Soit une variable aléatoire \(X:(\Omega,\mathcal A,P)\mapsto \mathbb{R}\). On appelle \(n\)\sphinxhyphen{}échantillon de la variable aléatoire parente \(X\) la donnée de \(n\) variables aléatoires \(X_1\cdots X_n\), définies sur le même espace, indépendantes, ayant même loi que \(X\).
\end{sphinxadmonition}

\sphinxAtStartPar
On a donc pour tout \((x_1\cdots x_n)^T\in\mathbb{R}^n\)

\sphinxAtStartPar
\(P(X_1<x_1\cdots X_n<x_n)=P(X_1<x_1)\cdots P(X_n<x_n)=P(X<x_1)\cdots P(X<x_n)\)

\sphinxAtStartPar
On considère alors une expérience aléatoire \(\mathcal E\) décrite par l’intermédiaire de la variable aléatoire \(X\). Considérer un \(n\) échantillon de \(X\) consiste à supposer la possibilité de \(n\) répétitions de l’expérience \(\mathcal E\) dans des conditions identiques, sans interactions entre elles.

\sphinxAtStartPar
Chaque répétition conduit à l’observation d’une valeur prise par \(X\), d’où l’observation de \(n\) valeurs \(x_1\cdots x_n\) à la suite des \(n\) répétitions, considérées comme une valeur effectivement prise par le \(n\)\sphinxhyphen{}échantillon \((X_1\cdots X_n)\) de \(X\). Les valeurs \((x_1\cdots x_n)\)  relèvent de l’observation : ce sont les données statistiques recueillies à la suite des \(n\) expériences : elles sont appelées réalisation du \(n\)\sphinxhyphen{}échantillon.

\sphinxAtStartPar
A noter que les hypothèses de même loi et d’indépendance sont simplificatrices.


\subsection{Schéma de Bernoulli et modèle binomial}
\label{\detokenize{elemstats:schema-de-bernoulli-et-modele-binomial}}
\sphinxAtStartPar
Si \(\mathcal E\) n’a que deux éventualités possibles (réalisation ou non d’un évènement \(A\)), alors l’expérience peut être décrite par l’intermédiaire d’une variable aléatoire \(X\) (\(\mathbb{1}_A\), fonction indicatrice de \(A\)), de Bernoulli \(X:(\Omega,\mathcal A,P)\mapsto \{0,1\}\) avec \(P(X=1)=P(A)=p\in]0,1[\).

\sphinxAtStartPar
Si \(\mathcal E\) est répétée \(n\) fois dans des conditions identiques, sans interaction entre elles, on considère un \(n\)\sphinxhyphen{}échantillon \((X_1\cdots X_n)\) de variable aléatoire parente \(X\). Les valeurs prises par la variable aléatoire \(S_n=X_1+\cdots X_n\) représentent le nombre de réalisations de \(A\) à la suite des \(n\) répétitions. Une telle situation est dite relever du schéma de Bernoulli.
\label{elemstats:property-1}
\begin{sphinxadmonition}{note}{Property 4}



\sphinxAtStartPar
\(S_n:(\Omega,\mathcal A,P)\mapsto [\![0,n]\!]\) a une loi binomiale \(\mathcal{B}(n,p)\) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\forall k\in[\![0,n]\!]\; P(S_n=k)=\begin{pmatrix}n\\k\end{pmatrix} p^k (1-p)^{n-k}\)

\item {} 
\sphinxAtStartPar
\(\mathbb{E}(S_n)=np,\; \mathbb{V}(S_n)=np(1-p)\)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
En effet, d’après l’indépendance pour toute suite (\(\delta_1\cdots \delta_n\)) avec pour tout \(k\in[\![1,n]\!]\) \(\delta_k\in\{0,1\}\), on a :

\sphinxAtStartPar
\(P(X_1=\delta_1\cdots X_n=\delta_n) = \displaystyle\prod_{k=1}^n P(X_k=\delta_k) = \displaystyle\prod_{k=1}^n P(X=\delta_k)=p^{s_n}(1-p)^{(n-s_n)}\)

\sphinxAtStartPar
avec \(\delta_1+\cdots+ \delta_n=s_n\) , les variables aléatoires ayant même loi de Bernoulli que \(X\).

\sphinxAtStartPar
Le nombre de solutions de \(\delta_1+\cdots+ \delta_n=s_n\) avec \(s_n\in[\![0,n]\!]\) et \(\delta_k\in\{0,1\}\) est \(\begin{pmatrix}s_n\\n\end{pmatrix}\), d’où le résultat.

\sphinxAtStartPar
D’après la linéarité de l’espérance et l’égalité de Bienaymé, on a de plus
\(\mathbb{E}(S_n) = \displaystyle\sum_{k=1}^n \mathbb{E}(X_k)=n\mathbb{E}(X)=np\quad \mathbb{V}(S_n)=\displaystyle\sum_{k=1}^n \mathbb{V}(X_k)=n\mathbb{V}(X)=np(1-p)\)


\subsection{Moyenne et variances empiriques d’un \protect\(n\protect\)\sphinxhyphen{}échantillon}
\label{\detokenize{elemstats:moyenne-et-variances-empiriques-d-un-n-echantillon}}
\sphinxAtStartPar
Etant donné un \(n\)\sphinxhyphen{}échantillon \((X_1\cdots X_n)\) d’une variable aléatoire parente \(X\), on appelle :

\index{moyenne empirique@\spxentry{moyenne empirique}}\ignorespaces 
\index{variance empirique@\spxentry{variance empirique}}\ignorespaces \begin{itemize}
\item {} 
\sphinxAtStartPar
moyenne empirique du \(n\)\sphinxhyphen{}échantillon la variable aléatoire

\item {} \begin{equation*}
\begin{split}\bar{X}_n=\frac1n \displaystyle\sum_{k=1}^n X_k\end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
variance empirique biaisée du \(n\)\sphinxhyphen{}échantillon la variable aléatoire (Ne pas confondre avec la variable \(S_n\) du schéma de Bernoulli)

\end{itemize}
\begin{equation*}
\begin{split}S_n^2=\frac1n \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2=\frac1n \displaystyle\sum_{k=1}^n X_k^2 -\bar{X}_n^2\end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
variance empirique non biaisée du \(n\)\sphinxhyphen{}échantillon la variable aléatoire

\end{itemize}
\begin{equation*}
\begin{split}{S'}_n^2=\frac{1}{n-1} \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2\end{split}
\end{equation*}
\sphinxAtStartPar
On a bien sûr \((n-1){S'}_n^2=nS_n^2\).

\sphinxAtStartPar
Les valeurs prises par \(\bar{X}_n\) coïncident avec la moyenne expérimentale \(\bar{x}_n\) des données expérimentales \((x_1\cdots x_n)\), réalisation du \(n\)\sphinxhyphen{}échantillon. De même pour \(S_n^2\) pour la variance expérimentale.
\label{elemstats:property-2}
\begin{sphinxadmonition}{note}{Property 5}


\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbb{E}(\bar{X}_n)= \mathbb{E}(X)=m\; ;\; \mathbb{V}(\bar{X}_n) = \frac{\mathbb{V}(X)}{n}=\frac{\sigma^2}{n}\)

\item {} 
\sphinxAtStartPar
\(\mathbb{E}(S_n^2) = \frac{n-1}{n}\sigma^2\; ;\;  \mathbb{E}({S'}_n^2)=\sigma^2\)

\item {} 
\sphinxAtStartPar
Sous l’hypothèse de normalité, \(\mathbb{V}({S'}_n^2)=\frac{2\sigma^4}{n-1}\)

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
En effet :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Immédiat d’après la linéarité de l’espérance, l’égalité de Bienaymé et la propriété \(\mathbb{V}(\alpha X)=\alpha^2\mathbb{V}(X)\)

\item {} 
\sphinxAtStartPar
\((n-1){S'}_n^2=\displaystyle\sum_{k=1}^n X_k^2-n\bar{X_n^2}\) d’où

\end{enumerate}

\sphinxAtStartPar
\((n-1)\mathbb{E}({S'}_n^2)=\displaystyle\sum_{k=1}^n\mathbb{E}(X_k^2)-n\mathbb{E}(\bar{X_n^2})=n(\sigma^2+m^2)-n\left (\frac{\sigma^2}{n}+m^2 \right )\)
et le résultat.

\sphinxAtStartPar
Le dernier point est admis.


\subsection{Echantillons de variables aléatoires normales}
\label{\detokenize{elemstats:echantillons-de-variables-aleatoires-normales}}
\sphinxAtStartPar
Les lois de probabilité usuelles sont rappelées en fin de ce chapitre (\{ref\}`loisusuelles).


\subsubsection{Etude d’un \protect\(n\protect\)\sphinxhyphen{}échantillon}
\label{\detokenize{elemstats:etude-d-un-n-echantillon}}
\sphinxAtStartPar
Soit un \(n\)\sphinxhyphen{}échantillon \(X_1\cdots X_n\) de variable aléatoire parente \(X\) de loi \(\mathcal{N}(m,\sigma)\). On a les résultats suivants :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\sqrt{n} \frac{\bar{X}_n-m}{\sigma}\) suit une loi \(\mathcal{N}(0,1)\)

\item {} 
\sphinxAtStartPar
\(\frac{nS_n^2}{\sigma^2} = \frac{(n-1)S'^2_n}{\sigma^2}\) suit une loi \(\chi^2_{n-1}\)

\item {} 
\sphinxAtStartPar
les variables aléatoires \(\bar{X}_n\) et \(S_n^2\) sont indépendantes

\item {} 
\sphinxAtStartPar
\(T=\sqrt{n}\frac{\bar{X}_n-m}{S'_n}=\sqrt{n-1}\frac{\bar{X}_n-m}{S_n}\) suit une loi de Student à \(n-1\) degrés de liberté.

\end{enumerate}


\subsubsection{Etude de deux échantillons indépendants}
\label{\detokenize{elemstats:etude-de-deux-echantillons-independants}}
\sphinxAtStartPar
Soient un \(n\)\sphinxhyphen{}échantillon \(X_1\cdots X_n\) de \(X\) de loi \(\mathcal{N}(m_1,\sigma_1)\), un \(m\)\sphinxhyphen{}échantillon \(Y_1\cdots Y_m\) de \(Y\) de loi \(\mathcal{N}(m_2,\sigma_2)\), les échantillons étant indépendants. Avec des notations évidentes, on a les résultats suivants :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(F = \frac{\sigma_2^2 S'^2_n(X)}{\sigma_1^2 S'^2_m(Y)} = \frac{(m-1)n}{(n-1)m}\frac{\sigma_2^2S_n^2(X)}{\sigma_1^2S_m^2(Y)}\) admet une loi de Fisher\sphinxhyphen{}Snédécor FS(\(n-1\),\(m-1\))

\item {} 
\sphinxAtStartPar
\(T = \sqrt{\frac{(n+m-2)mn}{m+n}}\frac{(\bar{X}_n-\bar{Y}_m)-(m_1-m_2)}{\sqrt{nS_n^2(X)+mS_m^2(Y)}}\) admet, sous l’hypothèse \(\sigma_1=\sigma_2\), une loi de Student à \((n+m-2)\) degrés de liberté.

\end{itemize}
\label{elemstats:remark-3}
\begin{sphinxadmonition}{note}{Remark 5}



\sphinxAtStartPar
Sous l’hypothèse \(\sigma_1=\sigma_2=\sigma\) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\bar{X}_n-\bar{Y}_m\) suit une loi \(\mathcal{N}(m_1-m_2,\sigma\sqrt{\frac1n+\frac1m})\)

\item {} 
\sphinxAtStartPar
\(\frac{nS_n^2(X)}{\sigma^2}+\frac{mS_m^2(Y)}{\sigma^2}\) a une loi \(\chi^2_{n-1+m-1}\).

\end{itemize}
\end{sphinxadmonition}


\section{Loi des grands nombres}
\label{\detokenize{elemstats:loi-des-grands-nombres}}

\subsection{Inégalité de Tchebychev}
\label{\detokenize{elemstats:inegalite-de-tchebychev}}\label{elemstats:theorem-4}
\begin{sphinxadmonition}{note}{Theorem 2}



\sphinxAtStartPar
Soit une variable aléatoire \(X\) de moyenne \(m\) et d’écart\sphinxhyphen{}type \(\sigma\). Alors :

\sphinxAtStartPar
\((\forall t>0)\; P(|X-m|\geq t)\leq \frac{\sigma^2}{t^2}\quad\textrm{et}\quad (\forall u>0)\; P(\frac{|X-m|}{\sigma}\geq u)\leq \frac{1}{u^2}\)
\end{sphinxadmonition}

\sphinxAtStartPar
En effet :
Soit \(A=\left \{|X-m|\geq t\right \}\) et \(\mathbb{1}_A(\omega)\) = 1 si \(\omega\in A\), 0 sinon. Alors :

\sphinxAtStartPar
\((\forall \omega\in\Omega)\; |X(\omega)-m|^2\geq |X(\omega)-m|^2\mathbb{1}_A(\omega) \geq t^2\mathbb{1}_A(\omega)\)

\sphinxAtStartPar
L’espérance étant croissante et vérifiant \(\mathbb{E}(\mathbb{1}_A)=P(A)\), on a
\(\sigma^2=\mathbb{E}(|X-m|^2)\geq t^2P(A) = t^2P(|X-m|\geq t)\) et le résultat.
\label{elemstats:remark-5}
\begin{sphinxadmonition}{note}{Remark 6}



\sphinxAtStartPar
Ces inégalités, souvent très grossières et d’intéret essentiellement théorique, n’ont d’utilité que pour \(t>\sigma\) ou \(u>1\) (une probabilité est toujours inférieure à 1). La seconde donne un majorant de la probabilité d’observer des valeurs prises par \(X\) à l’extérieur de l’intervalle \([m-u\sigma,m+u\sigma]\)
\end{sphinxadmonition}


\subsection{Phénomène de régularité statistique}
\label{\detokenize{elemstats:phenomene-de-regularite-statistique}}
\sphinxAtStartPar
Considérons plusieurs séquences de 100 lancers d’une pièce de monnaie et notons, pour chaque séquence, la suite \((f_n)_{n\geq 1}\) des fréquences des piles obtenus. Un exemple de simulation avec \(p=0.4\) est proposé dans la figure suivante avec le code ayant servi à la produire.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from}  \PYG{n+nn}{random}  \PYG{k+kn}{import}  \PYG{n}{random}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k}{def} \PYG{n+nf}{experience}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mf}{0.4}
    \PYG{n}{f} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{n}{p}\PYG{p}{:}
            \PYG{n}{f} \PYG{o}{+}\PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{]}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{f}\PYG{o}{+}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        
    \PYG{n}{f} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{f}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{f}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{return}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{nb\PYGZus{}sequences} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nb\PYGZus{}sequences}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{f} \PYG{o}{=} \PYG{n}{experience}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{f}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}n\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}f\PYGZus{}n\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{n}{color} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{0cea81a801c984e24621e903cb2edc97e70a88d552ca3e93915f8d74de36b8f4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
La fluctuation de la fréquence est importante pour des petites valeurs de \(n\), puis elle s’atténue, pour se stabiliser autour d’une valeur voisine de \(p\).

\sphinxAtStartPar
Cette constatation expérimentale conduit aux remarques suivantes, qui sont précisées dans la suite dans le cadre théorique :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(f_n\) donne une idée de la valeur de \(p\) avec une plus ou moins grande précision

\item {} 
\sphinxAtStartPar
la probabilité apparaît comme une fréquence limite.

\end{itemize}


\subsection{Loi faible des grands nombres}
\label{\detokenize{elemstats:loi-faible-des-grands-nombres}}\label{elemstats:theorem-6}
\begin{sphinxadmonition}{note}{Theorem 3}



\sphinxAtStartPar
Soit \((X_n)_{n\geq 1}\) une suite de variables aléatoires indépendantes, identiquement distribuées (i.i.d) de même loi qu’une variable \(X\), admettant une moyenne \(m\) et un écart\sphinxhyphen{}type \(\sigma\). Si \((\bar{X}_n)_{n\geq 1}\) est la suite des moyennes empiriques associée à \((X_n)_{n\geq 1}\) alors

\sphinxAtStartPar
\((\forall t>0)\; \displaystyle\lim_{n\rightarrow\infty} P(|\bar{X}_n-m|\geq t) = 0\)

\sphinxAtStartPar
On dit que la suite converge en probatilité vers \(m\) et on note \(\bar{X}_n\xrightarrow[n\rightarrow\infty]{P} m\)
\end{sphinxadmonition}

\sphinxAtStartPar
C’est une conséquence immédiate de l’inégalité de Tchebychev : \(P(|\bar{X}_n-m|\geq t)\leq\frac{\sigma^2}{nt^2}\) puisque \(\mathbb{V}(\bar{X}_n)=\frac{\sigma^2}{n}\)

\sphinxAtStartPar
L’observation des valeurs prises par la moyenne empirique donne une bonne information sur la moyenne théorique \(m\) de \(X\). La précision, au sens ci\sphinxhyphen{}dessus, est d’autant meilleure que \(n\) est grand.


\subsection{Loi forte des grands nombres}
\label{\detokenize{elemstats:loi-forte-des-grands-nombres}}
\sphinxAtStartPar
avec les hypothèses précédentes, on peut montrer que

\sphinxAtStartPar
\(P(\{\omega\in\Omega, \displaystyle\lim_{n\rightarrow\infty} \bar{X}_n(\omega)=m\})=1\)

\sphinxAtStartPar
Sauf cas très improbable (avec probabilité nulle), la suite des réalisations \((\bar{x}_n)_{n\geq 1}\) des moyennes expérimentales des mesures converge vers la moyenne théorique \(m\). On dit que la suite \((\bar{X}_n)_{n\geq 1}\) converge presque sûrement vers \(m\) et on note \(\bar{X}_n\xrightarrow[n\rightarrow\infty]{p.s.} m\).
\label{elemstats:remark-7}
\begin{sphinxadmonition}{note}{Remark 7}



\sphinxAtStartPar
Si \(X=\mathbb{1}_A\) alors \(m=p=P(A)\) et la probabilité de l’évènement \(A\) apparaît comme une fréquence limite.
\end{sphinxadmonition}


\section{Approximation de \protect\(\mathcal{B}(n,p)\protect\) par la loi de Poisson \protect\(\mathcal P(\lambda)\protect\)}
\label{\detokenize{elemstats:approximation-de-mathcal-b-n-p-par-la-loi-de-poisson-mathcal-p-lambda}}

\subsection{Théorème d’analyse}
\label{\detokenize{elemstats:theoreme-d-analyse}}\label{elemstats:theorem-8}
\begin{sphinxadmonition}{note}{Theorem 4}



\sphinxAtStartPar
Si \(p\) est une fonction de \(n\) telle que \(\displaystyle\lim_{n\rightarrow\infty}np(n)=\lambda>0\), alors pour tout \(k\geq 0\)

\sphinxAtStartPar
\(\displaystyle\lim_{n\rightarrow\infty}\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k} = e^{-\lambda}\frac{\lambda^k}{k!}\)
\end{sphinxadmonition}

\sphinxAtStartPar
En effet

\sphinxAtStartPar
\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{n(n-1)\cdots (n-k+1)}{k!}p^k(1-p)^{n-k}\)

\sphinxAtStartPar
\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{(np)^k}{k!}\displaystyle\prod_{j=0}^k\left (1-\frac{j}{n}\right )(1-p)^{n-k}\)

\sphinxAtStartPar
et le résultat est démontré en remarquant que \(\displaystyle\lim_{n\rightarrow\infty} p(n)=0\).


\subsection{Application}
\label{\detokenize{elemstats:application}}
\sphinxAtStartPar
Soit \(S_n\) une variable aléatoire de loi \(\mathcal{B}(n,p)\). Lorsque \(n\) est grand (>50) et \(p\) petite (\(np\)<10), on peut approcher la loi de \(S_n\) par une loi de Poisson \(\mathcal P(np)\). On lit alors la valeur correspondante dans la table de la loi de Poisson, pour tout \(k\in[\![0,n]\!]\)
\(P(S_n=k)\approx e^{-\lambda}\frac{\lambda^k}{k!}\)

\sphinxAtStartPar
De plus, en remarquant que \(\Sigma_n=n-S_n\) suit \(\mathcal{B}(n,1-p)\), on a
\begin{equation*}
\begin{split}P(\Sigma_n=k)=P(S_n=n-k)=\begin{pmatrix}n\\p\end{pmatrix} p^{n-k}(1-p)^{k} \end{split}
\end{equation*}
\sphinxAtStartPar
et quand \(n\) est grand (>50) et \(p\) voisin de 1 (\(n(1-p)<10\)) on peut approcher la loi de \(\Sigma_n\) par une loi de Poisson \(\mathcal P(n(1-p))\).


\section{Théorème central limite}
\label{\detokenize{elemstats:theoreme-central-limite}}

\subsection{Le T.C.L.}
\label{\detokenize{elemstats:le-t-c-l}}\label{elemstats:theorem-9}
\begin{sphinxadmonition}{note}{Theorem 5}



\sphinxAtStartPar
Soit une suite \((X_n)_{n\geq 1}\) de variables aléatoires, i.i.d. de même loi qu’une variable parente \(X\), définies sur le même espace \((\Omega,\mathcal A,P)\). On considère la suite des moyennes empiriques \((X_n)_{n\geq 1}\) des \(n\)\sphinxhyphen{}échantillons \((X_1\cdots X_n)\).

\sphinxAtStartPar
Si \(X\) admet une moyenne \(m\) et un écart\sphinxhyphen{}type \(\sigma\), alors

\sphinxAtStartPar
\((\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\sqrt{n}\frac{\bar X_n-m}{\sigma} <x\right) = \phi(x)\)
où \( \phi(x)\) est la fonction de répartition de la loi normale centrée réduite \(\mathcal{N}(0,1)\).

\sphinxAtStartPar
On dit que \(\left (\sqrt{n}\frac{\bar X_n-m}{\sigma}\right )_{n\geq 1}\) converge en loi vers \(\mathcal{N}(0,1)\).
\end{sphinxadmonition}

\sphinxAtStartPar
La figure suivante illustre ce modèle dans le cas où la variable aléatoire parente \(X\) suit un schéma de Bernoulli avec \(P(X = 1)=0.1, P(X=0)=0.9\).

\sphinxAtStartPar
\sphinxincludegraphics{{tcl}.png}


\subsection{Commentaires}
\label{\detokenize{elemstats:commentaires}}
\sphinxAtStartPar
Pour mesurer une grandeur de valeur inconnue \(m\), il suffit d’une seule mesure lorsqu’il n’y a pas d’erreur expérimentale. Mais les mesures sont toujours entâchées d’erreur et une expérience ou mesure peut être modélisée par une variable aléatoire \(X\) dnot la moyenne théorique \(\mathbb{E}(X)\) est la valeur cherchée \(m\) si les mesures ne sont pas biaisées, c’est\sphinxhyphen{}à\sphinxhyphen{}dire affectées d’une erreur systématique.

\sphinxAtStartPar
Ayant effectué \(n\) mesures, on a une réalisation d’un \(n\)\sphinxhyphen{}échantillon de \(X\) et une valeur observée \(\bar x_n\) de la moyenne empirique \(\bar X_n\). On peut prendre cette valeur comme estimation de \(m\), l’écart \(|\bar x_n-m|\) étant une réalisation de \(|\bar X_n-m|\).
\begin{itemize}
\item {} 
\sphinxAtStartPar
La loi forte des grands nombres justifie cette estimation en supposant  \(\mathbb{E}(X)=m\)

\item {} 
\sphinxAtStartPar
L’inégalité de Tchebychev donne une idée grossière de l’écart en terme de probabilité

\item {} 
\sphinxAtStartPar
le théorème central limite donne une évaluation asymptotique de cet écart aléatoire

\end{itemize}

\sphinxAtStartPar
Dans la pratique, pour \(n\) grand, dans le cadre de ce théorème, on a l’approximation suivante :

\sphinxAtStartPar
\((\forall a<b)\;\;\;\; P\left (a\sqrt{n}\frac{\bar X_n-m}{\sigma} <b\right)\approx \phi(b)-\phi(a)\)


\subsection{Cas particulier : théorème de Moivre\sphinxhyphen{}Laplace}
\label{\detokenize{elemstats:cas-particulier-theoreme-de-moivre-laplace}}\label{elemstats:theorem-10}
\begin{sphinxadmonition}{note}{Theorem 6}



\sphinxAtStartPar
Soit \(X=\mathbb{1}_A\)  une variable aléatoire de Bernoulli avec \(P(A)=p\). Dans les conditions du théorème central limite la variable \(S_n=\displaystyle\sum_{k=1}^n X_k=n\bar X_n\) suit une loi binomiale \(\mathcal{B}(n,p)\) et

\sphinxAtStartPar
\( (\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\frac{S_n-np}{\sqrt{np(1-p)}} <x\right) = \phi(x)\)
\end{sphinxadmonition}

\sphinxAtStartPar
On peut donc approcher une loi binomiale par une loi normale.


\section{Modèles probabilistes usuels}
\label{\detokenize{elemstats:modeles-probabilistes-usuels}}\label{\detokenize{elemstats:loisusuelles}}
\sphinxAtStartPar
On donne ici un catalogue non exhaustif des principaux modèles probabilistes, et leurs principales propriétés. Une illustration graphique des lois correspondantes est proposée dans les figures suivantes.


\subsection{Lois discrètes}
\label{\detokenize{elemstats:lois-discretes}}
\sphinxAtStartPar
On considère une variable aléatoire \(X:(\Omega,\mathcal A,P)\mapsto \mathcal D\)


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Modèle
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathcal D}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{P(X=k)}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathbb{E}(X)}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathbb{V}(X)}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Utilisation
\\
\hline
\sphinxAtStartPar
Bernoulli
&
\sphinxAtStartPar
\(\{0,1\}\)
&
\sphinxAtStartPar
\(P(X=1)=p,P(X=0)=1-p=q\)
&
\sphinxAtStartPar
\(p\)
&
\sphinxAtStartPar
\(pq\)
&
\sphinxAtStartPar
Expérience ayant 2 éventualités possibles
\\
\hline
\sphinxAtStartPar
Binomiale \(\mathcal{B}(n,p) \)
&
\sphinxAtStartPar
\([\![0,n]\!]\)
&
\sphinxAtStartPar
\(\begin{pmatrix}n\\k\end{pmatrix}p^k q^{n-k}\)
&
\sphinxAtStartPar
\(np\)
&
\sphinxAtStartPar
\(npq\)
&
\sphinxAtStartPar
Tirage avec remise
\\
\hline
\sphinxAtStartPar
Hypergéométrique, \(\mathcal{H}(m,N,n), m<N\)
&
\sphinxAtStartPar
\([\![0,n]\!]\)
&
\sphinxAtStartPar
\(\frac{\begin{pmatrix}m\\k\end{pmatrix}\begin{pmatrix}N-m\\n-k\end{pmatrix}}{\begin{pmatrix}N\\n\end{pmatrix}}\)
&
\sphinxAtStartPar
\(n\frac{m}{M}\)
&
\sphinxAtStartPar
\(\frac{N-n}{N-1}n\frac{m}{N}\frac{N-m}{N}\)
&
\sphinxAtStartPar
Tirage sans remise
\\
\hline
\sphinxAtStartPar
Uniforme
&
\sphinxAtStartPar
\([\![1,n]\!]\)
&
\sphinxAtStartPar
\(\frac1n\)
&
\sphinxAtStartPar
\(\frac{n+1}{2}\)
&
\sphinxAtStartPar
\(\frac{n2-1}{12}\)
&
\sphinxAtStartPar
Equiprobabilité des résultats
\\
\hline
\sphinxAtStartPar
Poisson \(\mathcal{P}(\lambda), \lambda>0\)
&
\sphinxAtStartPar
\(\mathbb{N}\)
&
\sphinxAtStartPar
\(e^{-\lambda}\frac{\lambda^k}{k!}\)
&
\sphinxAtStartPar
\(\lambda\)
&
\sphinxAtStartPar
\(\lambda\)
&
\sphinxAtStartPar
Files d’attente, Evènements rares
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxincludegraphics{{discretes}.png}


\subsubsection{Modèle de Bernoulli}
\label{\detokenize{elemstats:modele-de-bernoulli}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{bernoulli}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10} \PYG{c+c1}{\PYGZsh{}nombre de répétitions de l\PYGZsq{}expérience}
\PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{c+c1}{\PYGZsh{} probabilité de succès}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{bernoulli}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{bernoulli}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  0.3
Variance:  0.21
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Loi binomiale}
\label{\detokenize{elemstats:loi-binomiale}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{binom}

\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10} 
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{7} 
\PYG{n}{p} \PYG{o}{=} \PYG{l+m+mf}{0.2} 

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{binom}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{binom}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{binom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{binom}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{n}\PYG{p}{,}\PYG{n}{p}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  2.0
Variance:  1.6
Densité de probabilité :  0.000786432
Fonction de répartition :  0.9999220736
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Loi hypergéométrique}
\label{\detokenize{elemstats:loi-hypergeometrique}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{hypergeom}

\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{2} 
\PYG{n}{M} \PYG{o}{=} \PYG{l+m+mi}{15} 
\PYG{n}{m} \PYG{o}{=} \PYG{l+m+mi}{9} 
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{5} 

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hypergeom}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hypergeom}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hypergeom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{M}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hypergeom}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{M}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  3.0
Variance:  0.8571428571428571
Densité de probabilité :  0.23976023976023975
Fonction de répartition :  0.28671328671328666
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Distribution de Poisson}
\label{\detokenize{elemstats:distribution-de-poisson}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{poisson}

\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{1} 
\PYG{n}{Lambda} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{3} 

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{poisson}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{poisson}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{poisson}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{poisson}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  0.6666666666666666
Variance:  0.6666666666666666
Densité de probabilité :  0.3422780793550613
Fonction de répartition :  0.8556951983876534
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Lois absolument continues}
\label{\detokenize{elemstats:lois-absolument-continues}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Modèle
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathcal D}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Densité
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathbb{E}(X)}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\boldsymbol{\mathbb{V}(X)}\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Utilisation
\\
\hline
\sphinxAtStartPar
Uniforme
&
\sphinxAtStartPar
\([a,b]\)
&
\sphinxAtStartPar
\(f(x)=\frac{1}{b-a}\mathbb{1}_{]a,b[}(x)\)
&
\sphinxAtStartPar
\(\frac{b+a}{2}\)
&
\sphinxAtStartPar
\(\frac{(b-a)^2}{12}\)
&
\sphinxAtStartPar
Pas d’a priori sur la distribution
\\
\hline
\sphinxAtStartPar
Exponentiel \(Exp(\lambda)\)\(\lambda>0\)
&
\sphinxAtStartPar
\(\mathbb{R}^+\)
&
\sphinxAtStartPar
\(f(x) =\lambda e^{-\lambda x} \mathbb{1}_{x>0}\)
&
\sphinxAtStartPar
\(\frac{1}{\lambda}\)
&
\sphinxAtStartPar
\(\frac{1}{\lambda^2}\)
&
\sphinxAtStartPar
Files d’attente, Durée de vie sans usure
\\
\hline
\sphinxAtStartPar
Pareto  \(\alpha>1,x_0>0\)
&
\sphinxAtStartPar
\([x_0,+\infty[\)
&
\sphinxAtStartPar
\(f(x)=\frac{\alpha-1}{x_0}\left (\frac{x_0}{x} \right )^\alpha \mathbb{1}_{x\geq x_0}\)
&
\sphinxAtStartPar
\(\frac{\alpha-1}{\alpha-2}x_0\) \(\alpha>2\)
&
\sphinxAtStartPar
\(\frac{(\alpha-1)x_0^2}{(\alpha-3)(\alpha-2)^2}\) \(\alpha>3\)
&
\sphinxAtStartPar
Revenu des ménages
\\
\hline
\sphinxAtStartPar
Normale \(\mathcal{N}(m,\sigma)\)
&
\sphinxAtStartPar
\(\mathbb{R}\)
&
\sphinxAtStartPar
\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-m)^2}{2\sigma^2}}\)
&
\sphinxAtStartPar
\(m\)
&
\sphinxAtStartPar
\(\sigma^2\)
&
\sphinxAtStartPar
voir T.C.L.
\\
\hline
\sphinxAtStartPar
Gamma \(\gamma(a,\lambda)\)\(a>0,\lambda>0\)
&
\sphinxAtStartPar
\((\mathbb{R}^+)^*\)
&
\sphinxAtStartPar
\(f(x) = \frac{\lambda^a}{\Gamma(a)}e^{-\lambda x}x^{a-1}\mathbb{1}_{x>0}\)
&
\sphinxAtStartPar
\(\frac{a}{\lambda}\)
&
\sphinxAtStartPar
\(\frac{a}{\lambda^2}\)
&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
Khi\sphinxhyphen{}deux \(\chi_n^2\) \(n\) degrés liberté
&
\sphinxAtStartPar
\(\mathbb{R}\)
&
\sphinxAtStartPar
\(f(x, k)=\frac{1}{2^\frac{k}{2}\Gamma(\frac{k}{2})} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}\)
&
\sphinxAtStartPar
\(n\)
&
\sphinxAtStartPar
\(2n\)
&
\sphinxAtStartPar
Test du khi\sphinxhyphen{}deux
\\
\hline
\sphinxAtStartPar
Student \(n\) degrés liberté
&
\sphinxAtStartPar
\(\mathbb{R}\)
&
\sphinxAtStartPar
\(f(x)=\frac{1}{\sqrt{\pi n}}\frac{\Gamma((n+1)/2)}{\Gamma(n/2)} \left (1+\frac{t^2}{n} \right )^{-\frac{n+1}{2}}\) t>0
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar
Test égalité moyenne
\\
\hline
\sphinxAtStartPar
Fisher\sphinxhyphen{}Snédécor \(n\) et \(m\) degrés liberté
&
\sphinxAtStartPar
\((\mathbb{R}^+)^*\)
&
\sphinxAtStartPar
\(\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}n^{\frac{n}{2}}m^{\frac{m}{2}}\frac{x^{\frac{n-2}{2}}}{(nx+m)^{\frac{n+m}{2}}}\)
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxincludegraphics{{continues}.png}


\subsubsection{Modèle uniforme}
\label{\detokenize{elemstats:modele-uniforme}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{uniform}
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mf}{2.5}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{1} 
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mi}{5} 
\PYG{n}{mean}\PYG{p}{,} \PYG{n}{var} \PYG{o}{=} \PYG{n}{uniform}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{a}\PYG{p}{,}\PYG{n}{scale}\PYG{o}{=}\PYG{n}{b}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Espérance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mean} \PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var} \PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{uniform}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{loc} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{b}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{uniform}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{loc} \PYG{o}{=} \PYG{n}{a}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{b}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Espérance:  3.0
Variance:  1.3333333333333333
Densité de probabilité :  0.25
Fonction de répartition :  0.375
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Loi normale}
\label{\detokenize{elemstats:loi-normale}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{norm}

\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mf}{1.3} 
\PYG{n}{m} \PYG{o}{=} \PYG{l+m+mi}{0} 
\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{1} 

\PYG{n}{mean}\PYG{p}{,}\PYG{n}{var} \PYG{o}{=} \PYG{n}{norm}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{loc} \PYG{o}{=} \PYG{n}{m}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{sigma}\PYG{p}{,} \PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)} 
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{norm}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{loc} \PYG{o}{=} \PYG{n}{m}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{norm}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{loc} \PYG{o}{=} \PYG{n}{m}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  0.0
Variance :  1.0
Densité de probabilité :  0.17136859204780736
Fonction de répartition :  0.9031995154143897
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Sous l’hypothèse de normalité, de nombreux outils statistiques sont disponibles. Souvent, l’hypothèse de normalité est justifiée par l’intermédiaire du théorème centrale limite. Des considérations, parfois abusives, permettent de se placer dans le cadre d’utilisation de ce théorème et de choisir un modèle normal alors qu’une étude des données statistiques met en défaut le choix de ce modèle (problème dit d’adéquation).
\label{elemstats:property-11}
\begin{sphinxadmonition}{note}{Property 6}



\sphinxAtStartPar
Si \(X\) est une variable aléatoire de loi \(\mathcal{N}(m,\sigma)\) alors la variable \(Z=\frac{X-m}{\sigma}\) est la variable centrée réduite associée, et suit une loi \(\mathcal{N}(0,1)\) dite aussi loi de Gauss\sphinxhyphen{}Laplace.
\end{sphinxadmonition}

\sphinxAtStartPar
La fonction de répartition de \(Z\) est \(\phi(Z) = P(Z<z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{-\frac{t^2}{2}}dt\), dont les valeurs peuvent être lues dans une table.
\label{elemstats:theorem-12}
\begin{sphinxadmonition}{note}{Theorem 7}



\sphinxAtStartPar
Soient \(X_1\) et \(X_2\) deux variables aléatoires indépendantes, de loi respective \(\mathcal{N}(m_1,\sigma_1)\) et \(\mathcal{N}(m_2,\sigma_2)\). Alors la variable aléatoire \(X=\alpha_1X_1+\alpha_2X_2\) admet une loi \(\mathcal{N}(m,\sigma)\) avec

\sphinxAtStartPar
\(m = \alpha_1 m_1+\alpha_2 m_2\quad \textrm{et}\quad \sigma_2^2 = \alpha_1^2 \sigma_1+\alpha_2^2 \sigma_2^2\)

\sphinxAtStartPar
En particulier, étant données \(n\) variables aléatoires \(X_1\cdots X_n\) i.i.d. de loi \(\mathcal{N}(m,\sigma)\), alors la variable aléatoire \(\bar X_n = \frac1n \displaystyle\sum_{k=1}^nX_k\) suit une loi normale \(\mathcal{N}(m,\sigma/\sqrt{n})\).
\end{sphinxadmonition}
\label{elemstats:remark-13}
\begin{sphinxadmonition}{note}{Remark 8}



\sphinxAtStartPar
Dans ce cas, \(\sqrt{n}\frac{\bar X_n-m}{\sigma}\) suit une loi \(\mathcal{N}(0,1)\).
\end{sphinxadmonition}


\subsubsection{Loi exponentielle}
\label{\detokenize{elemstats:loi-exponentielle}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{expon}

\PYG{n}{Lambda} \PYG{o}{=} \PYG{l+m+mf}{0.5} 
\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{1} 
\PYG{n}{mean}\PYG{p}{,}\PYG{n}{var} \PYG{o}{=} \PYG{n}{expon}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{n}{Lambda}\PYG{p}{,} \PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Espérance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{expon}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{expon}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Espérance :  0.5
Variance :  0.25
Densité de probabilité :  0.2706705664732254
Fonction de répartition :  0.8646647167633873
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
On parle de loi de probabilité sans mémoire car elle vérifie :
\( (\forall s,t\in(\mathbb{R}^+)^*\; P(X>s+t |X>t) = P(X>s)\)


\subsubsection{Distribution Gamma}
\label{\detokenize{elemstats:distribution-gamma}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{gamma}

\PYG{n}{x} \PYG{o}{=} \PYG{l+m+mi}{3} 
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{3} 
\PYG{n}{Lambda} \PYG{o}{=} \PYG{l+m+mf}{1.8} 

\PYG{n}{mean}\PYG{p}{,} \PYG{n}{var} \PYG{o}{=} \PYG{n}{gamma}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}  \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{Lambda}\PYG{p}{,} \PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,}  \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{scale} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{Lambda}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  1.6666666666666667
Variance :  0.925925925925926
Densité de probabilité :  0.11853315025792688
Fonction de répartition :  0.9052421318239862
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Les propriétés de cette loi reposent sur celles de la fonction \(\Gamma(a) = \int_0^{+\infty} x-{a-1}e^{-x}dx\), intégrale convergente pour tout \(a>0\).
\label{elemstats:theorem-14}
\begin{sphinxadmonition}{note}{Theorem 8}



\sphinxAtStartPar
Si \(X\) et \(Y\) sont des variables aléatoires indépendantes de loi respective \(\gamma(a,\lambda)\) et \(\gamma(b,\lambda)\), alors \(X=X_1+X_2\) est de loi \(\gamma(a+b,\lambda)\)
\end{sphinxadmonition}
\label{elemstats:theorem-15}
\begin{sphinxadmonition}{note}{Theorem 9}



\sphinxAtStartPar
Si \(X\) est de loi \(\mathcal{N}(0,1)\) alors la variable aléatoire \(Y=X^2\) admet une loi \(\gamma(\frac12,\frac12)\).

\sphinxAtStartPar
Etant données plus généralement \(n\) variables aléatoires i.i.d. de loi \(\mathcal{N}(m,\sigma)\), alors  la variable aléatoire \(V=\displaystyle\sum_{k=1}^n \left (\frac{X_k-m}{\sigma}\right )^2\) admet une loi \(\gamma(\frac{n}{2},\frac12)\). C’est la loi du khi\sphinxhyphen{}deux à \(n\) degrés de liberté.
\end{sphinxadmonition}


\subsubsection{Loi du Khi\sphinxhyphen{}deux}
\label{\detokenize{elemstats:loi-du-khi-deux}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{chi2}
\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{2}
\PYG{n}{mean}\PYG{p}{,} \PYG{n}{var} \PYG{o}{=} \PYG{n}{chi2}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{chi2}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{chi2}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  2.0
Variance :  4.0
Densité de probabilité :  0.11156508007421491
Fonction de répartition :  0.7768698398515702
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Loi de Student}
\label{\detokenize{elemstats:loi-de-student}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{t}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{n}{mean}\PYG{p}{,} \PYG{n}{var} \PYG{o}{=} \PYG{n}{t}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{t}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{t}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  0.0
Variance :  inf
Densité de probabilité :  0.027410122234342152
Fonction de répartition :  0.9522670168666454
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
L’utilisation pratique de cette loi est énoncée par le théorème suivant :
\label{elemstats:theorem-16}
\begin{sphinxadmonition}{note}{Theorem 10}



\sphinxAtStartPar
Soient deux variables aléatoires \(X\) et \(Y\) indépendantes, de loi respective \(\mathcal{N}(0,1)\) et \(\chi_n^2\). Alors la variable aléatoire \(T=\frac{X}{\sqrt{Y/n}}\) admet une loi de Student à \(n\) degrés de liberté.
\end{sphinxadmonition}


\subsubsection{Loi de Fisher\sphinxhyphen{}Snédécor}
\label{\detokenize{elemstats:loi-de-fisher-snedecor}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{f}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{m}\PYG{o}{=}\PYG{l+m+mi}{4}
\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{n}{mean}\PYG{p}{,} \PYG{n}{var} \PYG{o}{=} \PYG{n}{f}\PYG{o}{.}\PYG{n}{stats}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,}\PYG{n}{moments}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Moyenne : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}  \PYG{n}{mean}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{var}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Densité de probabilité : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{f}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{n}{m}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fonction de répartition : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{f}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{n}{m}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Moyenne :  2.0
Variance :  inf
Densité de probabilité :  0.06399999999999996
Fonction de répartition :  0.84
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
L’utilisation pratique de cette loi est énoncée par le théorème suivant :
\label{elemstats:theorem-17}
\begin{sphinxadmonition}{note}{Theorem 11}



\sphinxAtStartPar
Soient deux variables aléatoires \(X\) et \(Y\) indépendantes, de loi respective \(\chi_n^2\) et \(\chi_m^2\). Alors la variable aléatoire \(T=\frac{X/n}{Y/m}\) admet une loi de Fisher\sphinxhyphen{}Snédécor à \(n\) et \(m\) degrés de liberté.
\end{sphinxadmonition}

\sphinxstepscope


\chapter{Statistique descriptive}
\label{\detokenize{statsdescriptives:statistique-descriptive}}\label{\detokenize{statsdescriptives::doc}}

\section{Définitions}
\label{\detokenize{statsdescriptives:definitions}}
\sphinxAtStartPar
Dans la suite, nous nous intéressons à des unités statistiques ou individus statistiques ou unités d’observation (individus,  entreprises,  ménages, données abstraites…). Bien que le cas infini soit envisageable, nous nous restreignons ici à l’étude d’un nombre fini de ces unités. Un ou plusieurs caractères (ou variables) est mesuré sur chaque unité. Les variables sont désignées par simplicité par une lettre. Leurs valeurs possibles sont appelées modalités et l’ensemble des valeurs possibles ou des modalités est appelé le domaine. L’ensemble des individus statistiques forme la population.


\subsection{Typologie des variables}
\label{\detokenize{statsdescriptives:typologie-des-variables}}
\sphinxAtStartPar
La typologie des variables définit le type de problème statistique que l’on doit aborder :

\index{Variable@\spxentry{Variable}!qualitative@\spxentry{qualitative}}\ignorespaces \label{statsdescriptives:definition-0}
\begin{sphinxadmonition}{note}{Definition 17 (Variable qualitative)}



\sphinxAtStartPar
La variable est dite qualitative lorsque les modalités sont des catégories. Suivant qu’il existe une relation d’ordre sur les catégories, on distingue :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la variable qualitative nominale, si les modalités  ne peuvent pas être ordonnées

\item {} 
\sphinxAtStartPar
la variable qualitative ordinale, si les modalités peuvent être ordonnées

\end{itemize}
\end{sphinxadmonition}

\index{Variable@\spxentry{Variable}!quantitative@\spxentry{quantitative}}\ignorespaces \label{statsdescriptives:definition-1}
\begin{sphinxadmonition}{note}{Definition 18 (Variable quantitative)}



\sphinxAtStartPar
La variable est dite quantitative lorsque les modalités sont des valeurs numériques (scalaires ou vectorielles) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la variable est quantitative discrète si les modalités forment un ensemble dénombrable

\item {} 
\sphinxAtStartPar
la variable quantitative est continue si les modalités vivent dans un espace continu.

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Dans certains cas (l’âge par exemple), une variable d’un type (quantitative continue ici) peut être exprimée d’une autre manière pour des raisons pratiques de collecte ou de mesure. De même, les variables qualitatives ordinales peuvent être codées, par exemple selon une échelle de satisfaction.

\index{Statistique@\spxentry{Statistique}!série@\spxentry{série}}\ignorespaces \label{statsdescriptives:definition-2}
\begin{sphinxadmonition}{note}{Definition 19 (Série statistique)}



\sphinxAtStartPar
On appelle série statistique une suite de \(n\) valeurs prises par une variable \(X\) sur les unités d’observation, notées \(x_1\cdots x_n\).
\end{sphinxadmonition}


\subsection{Variable qualitative nominale}
\label{\detokenize{statsdescriptives:variable-qualitative-nominale}}
\index{Variable@\spxentry{Variable}!qualitative ; nominale@\spxentry{qualitative ; nominale}}\ignorespaces 
\sphinxAtStartPar
Une variable qualitative nominale a des valeurs distinctes qui ne peuvent pas être ordonnées. On note \(J\) le nombre de valeurs distinctes ou de modalités, notées \(x_1\cdots x_J\). On appelle effectif d’une modalité ou d’une valeur distincte le nombre de fois que cette modalité (ou valeur distincte) apparaît dans la série statistique. On note \(n_j\) l’effectif de la modalité \(x_j\). La fréquence d’une modalité \(j\) est  alors égale à \(f_j=\frac{n_j}{n}\).

\sphinxAtStartPar
Le tableau statistique d’une variable qualitative nominale peut être représenté par deux types de graphiques. Les effectifs sont représentés par un diagramme en tuyau d’orgue et les fréquences par un diagramme en secteurs. Pour ce dernier, si le nombre de modalités devient trop important, la représentation perd de son intérêt.

\sphinxAtStartPar
\sphinxincludegraphics{{baton}.png}


\subsection{Variable qualitative ordinale}
\label{\detokenize{statsdescriptives:variable-qualitative-ordinale}}
\index{Variable@\spxentry{Variable}!qualitative ; ordinale@\spxentry{qualitative ; ordinale}}\ignorespaces 
\sphinxAtStartPar
Le domaine peut être muni d’une relation d’ordre.  Les valeurs distinctes d’une variable ordinale peuvent donc être ordonnées \(x_1\leq x_2\cdots\leq  x_J\), à permutation près dans l’ordre croissant des indices. L’effectif cumulé \(N_j\) et la fréquence cumulée \(F_j\) des variables sont alors définis par
\((\forall j\in[\![1,J]\!])\quad N_j=\displaystyle\sum_{i=1}^j n_i\quad \textrm {et}\quad F_j=\displaystyle\sum_{i=1}^j f_i\)

\sphinxAtStartPar
Les fréquences et les effectifs (cumulés ou non) peuvent être représentés sous la forme d’un diagramme en tuyaux d’orgue.


\subsection{Variable quantitative discrète}
\label{\detokenize{statsdescriptives:variable-quantitative-discrete}}
\index{Variable@\spxentry{Variable}!quantitative ; discrète@\spxentry{quantitative ; discrète}}\ignorespaces 
\sphinxAtStartPar
Le domaine d’une telle variable est dénombrable. Comme pour les variables qualitatives ordinales, on peut calculer les effectifs (cumulés ou non) et les fréquences (cumulées ou non).

\sphinxAtStartPar
La répartition des valeurs de la variable peut être représentée par un diagramme en bâtonnets. Les fréquences cumulées  sont visualisées par la fonction de répartition de la variable , définie par

\sphinxAtStartPar
\(F(x) = \left \{
\begin{eqnarray}
0&\textrm{ si} &x<x_1\\
F_j &\textrm{ si}&  x\in[x_j,x_{j+1}[\\
1& \textrm{ si}&  x_J\leq x
\end{eqnarray}\right .\)

\sphinxAtStartPar
\sphinxincludegraphics{{baton2}.png}


\subsection{Variable quantitative continue}
\label{\detokenize{statsdescriptives:variable-quantitative-continue}}
\index{Variable@\spxentry{Variable}!quantitative ; continue@\spxentry{quantitative ; continue}}\ignorespaces 
\sphinxAtStartPar
Le domaine d’une  variable quantitative continue est infini et est assimilé à \(\mathbb{R}\) ou à un intervalle de \(\mathbb{R}\). Cependant, la mesure étant limitée en précision, on peut traiter ces variables comme des variables discrètes.

\sphinxAtStartPar
La représentation graphique de ces variables (et la construction du tableau statistique) passe par le regroupement des modalités ou valeurs en classes. Le tableau ainsi construit est souvent appelé distribution groupée. La classe \(j\) est l’ensemble des valeurs incluses dans \([c^-_j,c^+_j[\), où \(c^-_j\) et \(c^+_j\) sont les bornes inférieure et supérieure de la classe. Sur cet intervalle, on peut calculer la fréquence \(f_j\) de la classe, la fréquence cumulée, l’effectif \(n_j\)… La répartition en classes nécessite de définir a priori le nombre de classes \(J\) et l’amplitude \(a_j\) des intervalles. Si elles peuvent être définies de manière empirique, quelques règles permettent d’établir \(J\) et l’amplitude pour une série statistique de \(n\) observations. Par exemple :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(J=1+3.3log_{10}(n)\) (règle de Sturge)

\item {} 
\sphinxAtStartPar
\(J=2.5\sqrt[4\,]{n}\) (règle de Yule)

\end{itemize}

\sphinxAtStartPar
La représentation graphique se fait par exemple par histogramme.
Les histogrammes sont des représentations de la distribution des données, agrégées par intervalles. A partir de l’étendue des données, on subdivise l’intervalle en \(k\) bins, de tailles \(t_k\) non nécessairement identiques, et on compte le nombre d’individus \(n_k\) rentrant dans chaque bin. L’histogramme peut alors être :
\begin{itemize}
\item {} 
\sphinxAtStartPar
non normalisé : \(h_k = n_k\)

\item {} 
\sphinxAtStartPar
normalisé: \(h_k = n_k/t_k\)

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{loadtxt}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./data/data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Comptage des individus}
\PYG{k}{def} \PYG{n+nf}{count}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{findBin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n+nb}{bin} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{bins}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{left}\PYG{p}{,} \PYG{n}{right} \PYG{o}{=} \PYG{n+nb}{bin}
            \PYG{k}{if} \PYG{n}{left} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{x} \PYG{o+ow}{and} \PYG{n}{x} \PYG{o}{\PYGZlt{}} \PYG{n}{right}\PYG{p}{:}
                \PYG{k}{return} \PYG{n}{i}
        \PYG{k}{return} \PYG{k+kc}{None}
    
    \PYG{n}{count} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{bins}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{X}\PYG{p}{:}
        \PYG{n}{i} \PYG{o}{=} \PYG{n}{findBin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{i} \PYG{o}{!=} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{count}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}

    \PYG{k}{return} \PYG{n}{count}

        
\PYG{c+c1}{\PYGZsh{} Affichage de l\PYGZsq{}histogramme}
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}hist}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}  \PYG{n}{bin\PYGZus{}min}\PYG{p}{,} \PYG{n}{bin\PYGZus{}max}\PYG{p}{,} \PYG{n}{bin\PYGZus{}width}\PYG{p}{,}\PYG{n}{normed}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{bins} \PYG{o}{=}\PYG{p}{[} \PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{i}\PYG{o}{+}\PYG{n}{bin\PYGZus{}width}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{bin\PYGZus{}min}\PYG{p}{,} \PYG{n}{bin\PYGZus{}max}\PYG{p}{,} \PYG{n}{bin\PYGZus{}width}\PYG{p}{)} \PYG{p}{]}
    \PYG{n}{bin\PYGZus{}left} \PYG{o}{=} \PYG{p}{[} \PYG{n}{l} \PYG{k}{for} \PYG{n}{l}\PYG{p}{,} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n}{bins} \PYG{p}{]}
    \PYG{n}{bin\PYGZus{}widths} \PYG{o}{=} \PYG{p}{[} \PYG{n}{r}\PYG{o}{\PYGZhy{}}\PYG{n}{l}  \PYG{k}{for} \PYG{n}{l}\PYG{p}{,}\PYG{n}{r} \PYG{o+ow}{in} \PYG{n}{bins} \PYG{p}{]}
    \PYG{n}{bin\PYGZus{}height} \PYG{o}{=} \PYG{p}{[} 
        \PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)} \PYG{o}{/} \PYG{n}{w} \PYG{k}{if} \PYG{n}{normed} \PYG{k}{else} \PYG{n}{c} 
        \PYG{k}{for} \PYG{n}{c}\PYG{p}{,}\PYG{n}{w} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{count}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{bins}\PYG{p}{)}\PYG{p}{,} \PYG{n}{bin\PYGZus{}widths}\PYG{p}{)}
    \PYG{p}{]}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{bin\PYGZus{}left}\PYG{p}{,}\PYG{n}{width}\PYG{o}{=}\PYG{n}{bin\PYGZus{}width}\PYG{p}{,}\PYG{n}{height}\PYG{o}{=}\PYG{n}{bin\PYGZus{}height}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{bin\PYGZus{}min} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{bin\PYGZus{}max} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{subplot}\PYG{p}{,} \PYG{n}{binsize} \PYG{o+ow}{in} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{141}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+m+mi}{142}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{143}\PYG{p}{,} \PYG{l+m+mi}{80}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{144}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{title} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Taille des bins : }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{binsize}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{n}{subplot}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}
    \PYG{n}{plot\PYGZus{}hist}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{bin\PYGZus{}min}\PYG{p}{,} \PYG{n}{bin\PYGZus{}max}\PYG{p}{,} \PYG{n}{binsize}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{68753ea9f1244fce844333bff92b83bb3502b3dae3e8026b3dd5d3e6598c8a48}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Le choix de la largeur \(t\) des bins dépend des données, et par exemple on a :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Loi de Scott : \(t = \frac{3.5 \sigma}{Card(X)^{1/3}}\), où \(\sigma\) est l’écart type des données.

\item {} 
\sphinxAtStartPar
Loi de Freedman–Diaconis : \( t = \frac{2 IQR}{Card(X)^{1/3}}\), où \(IQR\) est la distance interquartile.

\end{itemize}
\label{statsdescriptives:remark-3}
\begin{sphinxadmonition}{note}{Remark 9}



\sphinxAtStartPar
Toutes les classes n’ont pas nécessairement la même amplitude
\end{sphinxadmonition}

\sphinxAtStartPar
Les effectifs (ou les fréquences) sont représenté(e)s par un histogramme. Si l’on s’intéresse à la représentation des effectifs (resp. des fréquences), la densité d’effectif \(h_j\) (resp. de fréquence \(d_j\)),  définie par \(h_j=\frac{n_j}{a_j}\) (resp. \(d_j=\frac{f_j}{a_j}\)), détermine la hauteur du rectangle représentant la classe \(j\). L’aire de l’histogramme est égale à l’effectif total \(n\) pour l’histogramme des effectifs, et à 1 pour l’histogramme des fréquences.

\sphinxAtStartPar
Comme dans le cas discret, la fonction de répartition peut être calculée de la manière suivante :

\sphinxAtStartPar
\(F(x) = \left \{
\begin{eqnarray}
0&\textrm{ si}& x<c^-_1\\
F_{j-1}+\frac{f_j}{c^+_j-c^-_j}(x-c^-_j) &\textrm{ si}& x\in[c^-_j,c^+_j[\\
1& \textrm{ si}&c^+_J\leq x
\end{eqnarray}\right .\)


\section{Pré\sphinxhyphen{}traitement des données}
\label{\detokenize{statsdescriptives:pre-traitement-des-donnees}}
\sphinxAtStartPar
Faire une analyse de données, c’est traiter un tableau de taille \(n\times d\) où \(n\) est le nombre d’individus et \(d\) le nombre de variables (caractères) mesurées sur ces individus. En raison de la colecte des données, des erreurs de mesure ou d’autres facteurs, ce tableau est parfois incomplet et il convient de le prétraiter pour pouvoir effectuer l’analyse.


\subsection{Points aberrants}
\label{\detokenize{statsdescriptives:points-aberrants}}
\sphinxAtStartPar
Une anomalie (ou point aberrant, ou outlier) est une observation (ou un sous\sphinxhyphen{}ensemble d’observations) qui semble incompatible avec le reste de l’ensemble de données.

\sphinxAtStartPar
S’il est parfois possible d’identifier graphiquement ces points aberrants à l’aide de boîtes à moustaches (voir {\hyperref[\detokenize{statsdescriptives:boxplot}]{\sphinxcrossref{\DUrole{std,std-ref}{Pour résumer}}}}), il existe une vaste littérature sur la détection d’anomalies qu’il n’est pas possible d’aborder ici. De plus, suivant le type de données manipulées (données séquentielles ou non), le type de méthode peut être différent. On mentionne donc ici quelques techniques simples :
\begin{itemize}
\item {} 
\sphinxAtStartPar
le détecteur de Hampel : on considère que \(x_i\) est un point aberrant si

\end{itemize}
\begin{equation*}
\begin{split}|x_i-x_{\frac12}|>3.MADM\end{split}
\end{equation*}
\sphinxAtStartPar
où \(MADM = 1.4826.|x_i-x_{\frac12}|_\frac12\), et où \(y_{\frac12}\) est la médiane des données \(y\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
la règle empirique de l’écart\sphinxhyphen{}type : on considère que \(x_i\) est un point aberrant si

\end{itemize}
\begin{equation*}
\begin{split}|x_i-\bar x|>3.\sigma\end{split}
\end{equation*}
\sphinxAtStartPar
où  \(\bar x\) (respectivement \(\sigma\)) est la moyenne (resp. l’écart\sphinxhyphen{}type ) des données.
\begin{itemize}
\item {} 
\sphinxAtStartPar
la méthode LOF (Local Outlier Factor) qui repose sur le concept de densité locale, où la localité est donnée par les \(k\) voisins les plus proches, dont la distance est utilisée pour estimer la densité. En comparant la densité locale d’un objet aux densités locales de ses voisins, il est possible d’identifier des régions de densité similaire et des points dont la densité est nettement inférieure à celle de leurs voisins. Ces derniers sont considérés comme des valeurs aberrantes. La densité locale est estimée par la distance typique à laquelle un point peut être atteint à partir de ses voisins.

\item {} 
\sphinxAtStartPar
la méthode COF (Connectivity based Outlier Factor) basée sur le même principe que LOF, à ceci près que l’estimation de densité est effectuée en utilisant le minimum de la somme des distances reliant tous les voisins d’un point donné.

\end{itemize}


\subsection{Données manquantes}
\label{\detokenize{statsdescriptives:donnees-manquantes}}
\sphinxAtStartPar
Lors de la collecte des données, il arrive que certaines d’entre elles ne soient pas disponibles ou enregistrées. On distingue trois types de données manquantes :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
les données manquant de manière complètement aléatoire :  la probabilité qu’une donnée soit manquante ne dépend pas des valeurs connues ni de la valeur manquante elle\sphinxhyphen{}même.

\item {} 
\sphinxAtStartPar
les données manquant de manière aléatoire :  la probabilité qu’une donnée soit manquante peut dépendre de valeurs connues (d’autres variables parmi les \(d\)), mais pas de la variable dont les valeurs sont manquantes.

\item {} 
\sphinxAtStartPar
les données manquant de manière non aléatoire : la probabilité qu’une donnée soit manquante dépend d’autres variables qui ont également des valeurs manquantes, ou elle dépend de la variable elle\sphinxhyphen{}même.

\end{enumerate}

\sphinxAtStartPar
Pour résoudre ce problème de données manquantes, dans la mesure où ces dernières ne sont pas trop nombreuses, on a recours à des techniques d’\sphinxstylestrong{imputation}.

\sphinxAtStartPar
Dans le cas d’une imputation simple (une seule donnée manquante), on peut par exemple remplacer la valeur manquante dans une colonne \(j\in[\![1,p]\!]\) par :
\begin{itemize}
\item {} 
\sphinxAtStartPar
une valeur fixe

\item {} 
\sphinxAtStartPar
une statistique sur la colonne \(j\) (la plus petite ou la plus grande valeur, la moyenne de la colonne, la valeur la plus fréquente…)

\item {} 
\sphinxAtStartPar
une valeur issue des \(k\) plus proches voisins de la ligne du tableau où la valeur en position \(j\) est manquante

\item {} 
\sphinxAtStartPar
une valeur calculée par régression (voir chapitre correspondant) sur l’ensemble du tableau

\item {} 
\sphinxAtStartPar
la valeur précédente (ou suivante) dans le cas où la colonne est une série ordonnée ou temporelle.

\end{itemize}

\sphinxAtStartPar
Dans le cas d’une imputation multiple, où un sous\sphinxhyphen{}ensemble de valeurs doit être comblé, on peut adopter la stratégie suivante :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Effectuer une imputation simple pour toutes les valeurs manquantes de l’ensemble de données.

\item {} 
\sphinxAtStartPar
Remettre les valeurs manquantes d’une variable \(j\in[\![1,p]\!]\) à “manquante”.

\item {} 
\sphinxAtStartPar
Former un modèle pour prédire les valeurs manquantes de \(j\) en utilisant les valeurs disponibles de la variable \(j\) en tant que variable dépendante et les autres variables de l’ensemble de données comme indépendantes.

\item {} 
\sphinxAtStartPar
Prédire les valeurs manquantes dans la colonne \(j\) en utilisant le modèle entraîné à l’étape 3.

\item {} 
\sphinxAtStartPar
Répéter les étapes 2 à 4 pour toutes les autres colonnes présentant des valeurs manquantes.

\item {} 
\sphinxAtStartPar
Répéter l’étape 2\sphinxhyphen{}5 jusqu’à convergence (ou un nombre maximal d’itérations)

\item {} 
\sphinxAtStartPar
Répéter les étapes 1\sphinxhyphen{}6 plusieurs fois avec différentes initialisations de nombres aléatoires pour créer différentes versions de l’ensemble de données complet/imputé.

\end{enumerate}


\subsection{Transformation des données qualitatives}
\label{\detokenize{statsdescriptives:transformation-des-donnees-qualitatives}}
\sphinxAtStartPar
Pour pouvoir être traitées numériquement, les données qualitatives doivent être transformées. Plusieurs techniques existent parmi lesquelles :
\begin{itemize}
\item {} 
\sphinxAtStartPar
pour le cas des variables ordinales : on utilise le rang pour encoder les modalités de la variable. Par exemple, pour un niveau de diplomation Brevet\(<\)Bac\(<\)Licence\(<\)Master\(<\)Doctorat, on codera Licence par 3 et Doctorat par 5.

\item {} 
\sphinxAtStartPar
le one\sphinxhyphen{}hot encoding : pour une variable qualitative présentant \(J\) modalités, on construit un vecteur de taille \(J\) dont les composantes sont toutes nulles sauf la \(J\)\sphinxhyphen{}ème qui vaut 1. Par exemple, si \(J\)=3, on construit 1 vecteur de taille 3, et pour un individu ayant la modalité 2, on le code en (0 1 0). Lorsque \(J\) est élevé, on se retrouve avec un jeu de données volumineux.

\item {} 
\sphinxAtStartPar
les méthodes de plongement (embedding) : utilisées principalement en apprentissage profond (Deep learning) pour le traitement du langage naturel, ces classes de méthodes construisent une représentation de chaque modalité d’une variable qualitative en un vecteur numérique de taille fixe et choisie. Pour le mot “rouge” de la variable “couleur”, par exemple, l’encodage peut par exemple être représenté par le vecteur (0.31 0.57 0.12). En pratique, le calcul de ces représentations s’effectue classiquement par l’entraînement d’un réseau de neurones ayant pour entrée uniquement les variables qualitatives. Tout d’abord, un encodage one\sphinxhyphen{}hot est appliqué à la variable afin d’être mise en entrée du réseau, qui n’accepte que les entrées numériques. La sortie d’une des couches cachées du réseau constitue alors le vecteur recherché. On concatène ensuite ce vecteur aux données initiales, utilisées dans l’ajustement du modèle final.

\end{itemize}


\subsection{Normalisation}
\label{\detokenize{statsdescriptives:normalisation}}
\sphinxAtStartPar
Il arrive que les données collectées ne soient pas du même ordre de grandeur, notamment en raison des unités de mesure (un individu mesuré par sa taille en millimètres et son poids en tonnes par exemple). Cette différence de valeur absolue introduit un biais dans l’analyse des données ({\hyperref[\detokenize{statsdescriptives:biais}]{\sphinxcrossref{\DUrole{std,std-ref}{figure 1}}}}) qu’il convient de corriger. C’est le processus de normalisation des données.

\sphinxAtStartPar
Pour une colonne \(j\in[\![1,p]\!]\), on dispose de \(n\) valeurs \(x_{ij},i\in[\![1,n]\!]\). On note : \(x_{min} = \displaystyle\min_{i\in[\![1,n]\!]}x_{ij}\), \(x_{max} = \displaystyle\max_{i\in[\![1,n]\!]}x_{ij}\),   \(\bar x_j\) la moyenne des \(x_{ij}\), \(\sigma_j\) leur écart\sphinxhyphen{}type, \(x_\frac14, x_\frac12\) et \(x_\frac34\) les premier, deuxième et troisième quartiles. On distingue alors classiquement trois types de normalisation :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
la normalisation min\sphinxhyphen{}max : \(x_{ij} = \frac{x_{ij}-x_{min}}{x_{max}-x_{min}}\)

\item {} 
\sphinxAtStartPar
la normalisation standard : \(x_{ij}=\frac{x_{ij}-\bar x_j}{\sigma_j}\)

\item {} 
\sphinxAtStartPar
la normalisation robuste : \(x_{ij}=\frac{x_{ij}-x_\frac12}{x_\frac34-x_\frac14}\)

\end{enumerate}

\sphinxAtStartPar
La normalisation standard dépend de la présence de points aberrants (qui affectent la moyenne).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{normK}.png}
\caption{Effet de la normalisation sur un algorithme de classification (voir chapitre correspondant). En haut un jeu de données avec deux nuages de points allongés selon l’axe des \(x\), certainement en raison d’une différence d’échelle entre les unités de mesure de \(x\) et \(y\). Au milieu une classification par \(k\)\sphinxhyphen{}moyennes, \(k\)=2 sans normalisation, en utilisant la distance euclidienne. Les deux classes sont séparées suivant l’axe des \(x\), ne reflétant pas la répartition naturelle des points. En bas, après normalisation, les deux nuages de points sont correctement séparés}\label{\detokenize{statsdescriptives:biais}}\end{figure}


\section{Statistique descriptive univariée}
\label{\detokenize{statsdescriptives:statistique-descriptive-univariee}}
\index{Statistique@\spxentry{Statistique}!univariée@\spxentry{univariée}}\ignorespaces 
\sphinxAtStartPar
La statistique descriptive univariée consiste à étudier un ensemble d’unités d’observations, lorsque celles\sphinxhyphen{}ci sont décrites par une seule variable.

\sphinxAtStartPar
Soit donc \(X\) une variable et \(x_j,j\in [\![1,n]\!]\) l’ensemble des valeurs prises par cette variable, \(n_i\) étant le nombre de fois où la valeur \(x_i\) est prise. \(X\) peut être qualitative ou quantitative, les paramètres de description décrits dans la suite s’appliqueront à l’une de ces natures ou au deux.


\subsection{Paramètres de position}
\label{\detokenize{statsdescriptives:parametres-de-position}}
\sphinxAtStartPar
Plusieurs paramètres permettent de décrire la position “la plus représentative” d’une variable :
\label{statsdescriptives:definition-4}
\begin{sphinxadmonition}{note}{Definition 20 (Mode)}



\sphinxAtStartPar
Le mode est la valeur distincte correspondant à l’effectif le plus élevé. Il est noté \(x_M\).
\end{sphinxadmonition}

\sphinxAtStartPar
Le mode peut être calculé pour tout type de variable, n’est pas nécessairement unique. Lorsqu’une variable continue est découpée en classes, il est possible de définir une classe modale (classe correspondant à l’effectif le plus élevé)
\label{statsdescriptives:definition-5}
\begin{sphinxadmonition}{note}{Definition 21 (Moyennes)}



\sphinxAtStartPar
Les moyennes ne peuvent être définies que sur des variables quantitatives. Plusieurs moyennes peuvent être calculées, parmi lesquelles :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la moyenne \sphinxstylestrong{arithmétique}  \(\bar{x} = \frac{1}{n}{\displaystyle\sum_{i=1}^nx_i}=  \frac{1}{n}{\displaystyle\sum_{i=1}^J n_ix_i}\). C’est le moment à l’origine d’ordre 1.

\item {} 
\sphinxAtStartPar
la moyenne \sphinxstylestrong{géométrique} : si les \(x_i\) sont positifs, la moyenne géométrique est la quantité \(G=\left (\displaystyle\prod_{i=1}^n x_i\right )^\frac{1}{n}\). C’est donc l’exponentielle de la moyenne arithmétique des logarithmes des valeurs observées.

\item {} 
\sphinxAtStartPar
la moyenne \sphinxstylestrong{harmonique} : si les \(x_i\) sont positifs, la moyenne harmonique est définie par \(H=\frac{n}{\displaystyle\sum_{i=1}^J 1/x_i}\)

\item {} 
\sphinxAtStartPar
la moyenne \sphinxstylestrong{pondérée} : dans certains cas, on n’accorde pas la même importance à toutes les observations (fiabilité, confiance…). La moyenne pondérée est alors définie par
\(\bar{x}_w= \frac{\displaystyle\sum_{i=1}^n w_ix_i}{\displaystyle\sum_{i=1}^n w_i}\)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Dans le cas où \(\forall i,w_i=1/n\), la moyenne pondérée est la moyenne arithmétique. De plus, dans tous les cas, on peut montrer que \(H\leq G\leq \bar{x}\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{loadtxt}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./data/data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}


\PYG{k}{def} \PYG{n+nf}{ArithmeticMean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} calculable directement avec np.mean(X)}
    \PYG{k}{return} \PYG{n+nb}{float}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{GeometricMean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mi}{1} 
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{*}\PYG{o}{=}\PYG{n}{X}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} 
    \PYG{k}{return} \PYG{n+nb}{float}\PYG{p}{(}\PYG{n}{p}\PYG{o}{*}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{HarmonicMean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{s} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{X}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} 
    \PYG{k}{return} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)} \PYG{o}{/} \PYG{n}{s}

\PYG{k}{def} \PYG{n+nf}{WeightedMean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Exemples de poids}
    \PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{average}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{weights}\PYG{o}{=}\PYG{n}{w}\PYG{p}{)}


\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{font.size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{16}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{|}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{method}\PYG{p}{,} \PYG{n}{style}\PYG{p}{,} \PYG{n}{title} \PYG{o+ow}{in} \PYG{p}{(}\PYG{p}{(}\PYG{n}{ArithmeticMean}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Arithmétique}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{GeometricMean}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Géométrique}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
                             \PYG{p}{(}\PYG{n}{HarmonicMean}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Harmonique}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{WeightedMean}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pondérée}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{m}\PYG{o}{=}\PYG{n}{method}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{method}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{m}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{p}{,}\PYG{n}{m}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{style}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{n}{title}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
ArithmeticMean  :  1316.3086347078017
GeometricMean  :  1258.4787575642572
HarmonicMean  :  1198.219210728503
WeightedMean  :  1334.7912420657199
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{2804fa9a53fac2c66af4fbfaabb6bf50e03c3e6bc0da88aab5fe60d0addf6a53}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\label{statsdescriptives:definition-6}
\begin{sphinxadmonition}{note}{Definition 22 (Médiane)}



\sphinxAtStartPar
La médiane, notée \(x_\frac{1}{2}\) est la valeur centrale de la série statistique triée par ordre croissant.
\end{sphinxadmonition}

\sphinxAtStartPar
En d’autres termes, c’est la valeur de la série triée telle qu’au moins 50\% des effectifs soient inférieurs à \(x_\frac{1}{2}\). Elle peut être calculée sur des variables quantitatives ou qualitatives ordinales (dans le cas où des échelles de valeur ont été définies).
\label{statsdescriptives:definition-7}
\begin{sphinxadmonition}{note}{Definition 23 (Quantiles)}



\sphinxAtStartPar
Le quantile d’ordre \(p\) est défini par \(x_p=F^{-1}(p)\), où \(F\) est la fonction de répartition.
\end{sphinxadmonition}

\sphinxAtStartPar
La notion de quantile généralise la notion de médiane. Si la fonction de répartition était continue et strictement croissante, la définition de \(x_p\) serait unique. Or \(F\) est discontinue et définie par paliers et les valeurs de quantiles varient suivant par exemple l’utilisation ou non d’une méthode d’interpolation de \(F\). Pour calculer \(x_p\), on peut par exemple considérer que si \(np\) est pair,
\(x_p=\frac{x_{np}+x_{np+1}}{2}\)
on remarque alors que la médiane est le quantile d’ordre \(\frac{1}{2}\)
et sinon
\(x_p=x_{\lceil{np}\rceil}\)
En particulier, un quartile est chacune des 3 valeurs qui divisent les données triées en 4 parts égales, de sorte que chaque partie représente 1/4 de l’échantillon de population. On note \(Q_i\) le \(i^e\) quartile.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{loadtxt}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./data/data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{font.size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{16}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{|}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    
\PYG{k}{for} \PYG{n}{q}\PYG{p}{,} \PYG{n}{style}  \PYG{o+ow}{in} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+m+mi}{75}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{m}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}
    \PYG{n+nb}{print} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{quartile }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{q}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{m}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{p}{,}\PYG{n}{m}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{style}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{n}{q}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
quartile  25  :  905.9190521240237
quartile  50  :  1399.66320800781
quartile  75  :  1626.326538085935
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{e2dcdea760a58a0bc06ee4e52de94d5c9add4fc4e0d97921134b4fe12767c888}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Paramètres de dispersion}
\label{\detokenize{statsdescriptives:parametres-de-dispersion}}
\sphinxAtStartPar
Il est très souvent utile d’apprécier la dispersion des mesures autour du paramètre de position. Pour cela, sur des variables quantitatives uniquement, plusieurs outils sont disponibles :
\label{statsdescriptives:definition-8}
\begin{sphinxadmonition}{note}{Definition 24 (Etendue)}



\sphinxAtStartPar
L’étendue est la simple différence entre la plus grande et la plus petite valeur observée.
\end{sphinxadmonition}
\label{statsdescriptives:definition-9}
\begin{sphinxadmonition}{note}{Definition 25 (Déviation maximale)}



\sphinxAtStartPar
La déviation maximale est définie par
\( maxdev(X) = max \{ |x_i - \bar{x}| \,|\, i\in[\![1,n]\!]\}\)
\end{sphinxadmonition}
\label{statsdescriptives:definition-10}
\begin{sphinxadmonition}{note}{Definition 26 (Déviation moyenne absolue)}



\sphinxAtStartPar
La déviation moyenne absolue est définie par
\( mad(X) = \frac{1}{n} \displaystyle\sum_{i=1}^n |x_i - \bar{x}|\)
\end{sphinxadmonition}
\label{statsdescriptives:definition-11}
\begin{sphinxadmonition}{note}{Definition 27 (Distance interquartile)}



\sphinxAtStartPar
La distance interquartile \(Q_3-Q_1\) est la différence entre le troisième et le premier quartile. C’est une statistique robuste aux points aberrants.
\end{sphinxadmonition}
\label{statsdescriptives:definition-12}
\begin{sphinxadmonition}{note}{Definition 28 (Variance)}



\sphinxAtStartPar
La variance est la somme des carrés des écarts à la moyenne, normalisée par le nombre d’observations
\(\sigma^2 = \frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )^2\)
\end{sphinxadmonition}

\sphinxAtStartPar
Cette variance est dite biaisée. La variance non biaisée est obtenue en divisant non pas par \(n\), mais par \(n-1\).
\label{statsdescriptives:definition-13}
\begin{sphinxadmonition}{note}{Definition 29 (Ecart type)}



\sphinxAtStartPar
L’écart type est la racine carrée de la variance.
\end{sphinxadmonition}
\label{statsdescriptives:definition-14}
\begin{sphinxadmonition}{note}{Definition 30 (Ecart moyen absolu)}



\sphinxAtStartPar
L’écart moyen absolu est la somme des valeurs absolues des écarts à la moyenne divisée par le nombre d’observations.
\end{sphinxadmonition}

\sphinxAtStartPar
Notons qu’il s’agit de la distance \(L_1\) du vecteur des observations au vecteur composé de la valeur moyenne, divisé par le nombre d’observations. La variance est la distance \(L_2\) entre ces deux vecteurs. Lorsque la distance est calculée par rapport au vecteur composé de la valeur médiane, on parle d’écart médian absolu.

\sphinxAtStartPar
\sphinxincludegraphics{{dispersion}.png}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{loadtxt}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./data/data.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{k}{def} \PYG{n+nf}{max\PYGZus{}dev}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{k}{return} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{m}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{X}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{mad}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{k}{return} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{m}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{X}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{float}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{sigma}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{math}\PYG{o}{.}\PYG{n}{pow}\PYG{p}{(}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{m}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{X}\PYG{p}{)} \PYG{o}{/} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{IQR}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{l+m+mi}{75}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{rcParams}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{font.size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{16}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.01}\PYG{p}{]}\PYG{o}{*}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{|}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{method}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,}\PYG{n}{style}\PYG{p}{,}  \PYG{o+ow}{in} \PYG{p}{(}\PYG{p}{(}\PYG{n}{max\PYGZus{}dev}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{mad}\PYG{p}{,}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{sigma}\PYG{p}{,}\PYG{l+m+mf}{0.7}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{n}{IQR}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{method}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
    \PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{method}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{m}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{+/\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{s}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{p}{,}\PYG{n}{m}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{,}\PYG{n}{m}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{style}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{n}{method}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{o}{+}\PYG{n}{s}\PYG{p}{,}\PYG{n}{m}\PYG{o}{+}\PYG{n}{s}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{style}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{m}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{,}\PYG{n}{m}\PYG{o}{+}\PYG{n}{s}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{pos}\PYG{p}{,}\PYG{n}{pos}\PYG{p}{]}\PYG{p}{,}\PYG{n}{style}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
max\PYGZus{}dev  :  1316.3086347078017 +/\PYGZhy{} 738.0729570890783
mad  :  1316.3086347078017 +/\PYGZhy{} 327.4656915004233
sigma  :  1316.3086347078017 +/\PYGZhy{} 374.5723639541368
IQR  :  1316.3086347078017 +/\PYGZhy{} 720.4074859619113
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{6809a268f52e2a602de0d33de2c2d15f89d5a5a8b8b70550ee2f772ca674c7a2}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Paramètres de forme}
\label{\detokenize{statsdescriptives:parametres-de-forme}}
\sphinxAtStartPar
Les paramètres de forme sont souvent calculés en référence à la forme de la loi normale, pour évaluer la symétrie, l’aplatissement ou la dérive par rapport à cette loi.
\label{statsdescriptives:definition-15}
\begin{sphinxadmonition}{note}{Definition 31 (Skewness)}


\begin{equation*}
\begin{split}g_1 = \frac{m_3}{\sigma^3}\end{split}
\end{equation*}\end{sphinxadmonition}

\sphinxAtStartPar
Le skewness est également appelé coefficient d’asymétrie de Fisher.
\label{statsdescriptives:definition-16}
\begin{sphinxadmonition}{note}{Definition 32 (Kurtosis)}


\begin{equation*}
\begin{split}K=\frac{m_4}{m_2^2}\end{split}
\end{equation*}\end{sphinxadmonition}

\sphinxAtStartPar
\(K\) permet de mesurer l’aplatissement.
\label{statsdescriptives:definition-17}
\begin{sphinxadmonition}{note}{Definition 33 (Coefficient d’asymétrie de Yule)}


\begin{equation*}
\begin{split}A_Y = \frac{x_{3/4}+x_{1/4}-2x_{1/2}}{x_{3/4}-x_{1/4}}\end{split}
\end{equation*}\end{sphinxadmonition}

\sphinxAtStartPar
Ce coefficient est fondé sur les positions de trois quartiles (le premier, la médiane et le troisième) et est normalisé par la distance interquartile.
\label{statsdescriptives:definition-18}
\begin{sphinxadmonition}{note}{Definition 34 (Coefficient d’asymétrie de Pearson)}


\begin{equation*}
\begin{split}A_P = \frac{\bar{x}-x_M}{\sigma}\end{split}
\end{equation*}\end{sphinxadmonition}

\sphinxAtStartPar
Ce coefficient est fondé sur la comparaison de la moyenne et du mode, et est normalisé par l’écart type.

\sphinxAtStartPar
Tous les coefficients d’asymétrie ont des propriétés similaires : ils sont nuls si la distribution est symétrique, négatifs si la distribution est allongée à gauche (left asymmetry), et positifs si la distribution est allongée à droite (right asymmetry).

\sphinxAtStartPar
On peut aussi chercher à mesurer l’aplatissement (ou kurtosis) d’une distribution de mesure. Dans ce cas, on utilise le coefficient d’aplatissement de Pearson ou de Fisher, respectivement donnés par
\(\beta_2=\frac{m_4}{\sigma^4}\quad\textrm{et}\quad g_2=\beta_2-3\)

\sphinxAtStartPar
Une distribution est alors dite :
\begin{itemize}
\item {} 
\sphinxAtStartPar
mésokurtique si \(g_2\) est proche de 0

\item {} 
\sphinxAtStartPar
leptokurtique si \(g_2>0\) (queues plus longues et distribution plus pointue)

\item {} 
\sphinxAtStartPar
platykyrtique si \(g_2<0\) (queues plus courtes et distribution arrondie).

\end{itemize}


\subsubsection{Pour résumer}
\label{\detokenize{statsdescriptives:pour-resumer}}\label{\detokenize{statsdescriptives:boxplot}}
\sphinxAtStartPar
Les principales statistiques d’une série statistique peuvent être résumées dans des \sphinxstylestrong{boîtes à moustache}, qui permettent de voir sur un même graphique :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la médiane

\item {} 
\sphinxAtStartPar
une boîte entre les premier et le troisième quartile

\item {} 
\sphinxAtStartPar
l’étendue

\item {} 
\sphinxAtStartPar
les points aberrants.

\end{itemize}

\sphinxAtStartPar
Ce mode de représentation consiste à dessiner une boîte dont les extrémités dépendent du premier et du troisième quartiles \(Q_1\) et \(Q_3\) , en ajoutant une barre à l’intérieur
matérialisant le second quartile  \(Q_2\) (la valeur médiane de l’échantillon). A cette boîte, on ajoute des “moustaches” dont les extrémités dépendent :
\begin{itemize}
\item {} 
\sphinxAtStartPar
soit des valeurs extrémales prises par l’échantillon (minimum et maximum);

\item {} 
\sphinxAtStartPar
soit de la plus petite et de la plus grande valeur de l’échantillon appartenant à l’intervalle \([Q_1 -\delta, Q_3+\delta ]\). La grandeur \(\delta\) est une mesure de la dispersion des données. Généralement, on utilise \(\delta = 1.5(Q_3-Q_1)\).

\end{itemize}

\sphinxAtStartPar
Les valeurs de l’ échantillon en dehors des moustaches sont parfois matérialisées par des points et sont alors considérées comme les points aberrants de l’échantillon.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{k}{def} \PYG{n+nf}{annotate\PYGZus{}boxplot}\PYG{p}{(}\PYG{n}{bpdict}\PYG{p}{,} \PYG{n}{annotate\PYGZus{}params}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                     \PYG{n}{x\PYGZus{}offset}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{x\PYGZus{}loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
                     \PYG{n}{text\PYGZus{}offset\PYGZus{}x}\PYG{o}{=}\PYG{l+m+mi}{35}\PYG{p}{,}
                     \PYG{n}{text\PYGZus{}offset\PYGZus{}y}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}

    \PYG{k}{if} \PYG{n}{annotate\PYGZus{}params} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{annotate\PYGZus{}params} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{xytext}\PYG{o}{=}\PYG{p}{(}\PYG{n}{text\PYGZus{}offset\PYGZus{}x}\PYG{p}{,} \PYG{n}{text\PYGZus{}offset\PYGZus{}y}\PYG{p}{)}\PYG{p}{,} \PYG{n}{textcoords}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{offset points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{arrowprops}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{arrowstyle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZgt{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Médiane}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}loc} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{x\PYGZus{}offset}\PYG{p}{,} \PYG{n}{bpdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medians}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{x\PYGZus{}loc}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}ydata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{annotate\PYGZus{}params}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}Q\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}loc} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{x\PYGZus{}offset}\PYG{p}{,} \PYG{n}{bpdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{boxes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{x\PYGZus{}loc}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}ydata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{annotate\PYGZus{}params}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}Q\PYGZus{}3\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}loc} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{x\PYGZus{}offset}\PYG{p}{,} \PYG{n}{bpdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{boxes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{x\PYGZus{}loc}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}ydata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{annotate\PYGZus{}params}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}Q\PYGZus{}1\PYGZhy{}1.5(Q\PYGZus{}3\PYGZhy{}Q\PYGZus{}1)\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}loc} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{x\PYGZus{}offset}\PYG{p}{,} \PYG{n}{bpdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{caps}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{x\PYGZus{}loc}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}ydata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{annotate\PYGZus{}params}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}Q\PYGZus{}3+1.5(Q\PYGZus{}3\PYGZhy{}Q\PYGZus{}1)\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x\PYGZus{}loc} \PYG{o}{+} \PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{x\PYGZus{}offset}\PYG{p}{,} \PYG{n}{bpdict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{caps}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{(}\PYG{n}{x\PYGZus{}loc}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}ydata}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{annotate\PYGZus{}params}\PYG{p}{)}


\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Données}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{bpdict} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{grid}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}\PYG{n}{whis}\PYG{o}{=}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{n}{return\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dict}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{annotate\PYGZus{}boxplot}\PYG{p}{(}\PYG{n}{bpdict}\PYG{p}{,} \PYG{n}{x\PYGZus{}loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{fb54cd244877b2f8954d547e22f2771224f0e621141faef4d25862f5da2427f4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{La description ne fait pas tout…}
\label{\detokenize{statsdescriptives:la-description-ne-fait-pas-tout}}
\sphinxAtStartPar
La description d’un ensemble de valeurx \(x_j\) par la moyenne, la variance, voire le comportement linéaire (coefficient de corrélation, voir plus loin) peut ne pas suffire à comprendre la distribution des données. Un exemple classique (analyse bivariée, section suivante) est le quartet d’Anscombe (figure ci\sphinxhyphen{}dessous), constitué de quatre ensembles de points  \((x,y)\in\mathbb{R}^2\) de même propriétés statistiques (moyenne, variance, coefficient de régression linéaire) mais qui sont distribués de manière totalement différente dans le plan.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}
\PYG{n}{y1} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{8.04}\PYG{p}{,} \PYG{l+m+mf}{6.95}\PYG{p}{,} \PYG{l+m+mf}{7.58}\PYG{p}{,} \PYG{l+m+mf}{8.81}\PYG{p}{,} \PYG{l+m+mf}{8.33}\PYG{p}{,} \PYG{l+m+mf}{9.96}\PYG{p}{,} \PYG{l+m+mf}{7.24}\PYG{p}{,} \PYG{l+m+mf}{4.26}\PYG{p}{,} \PYG{l+m+mf}{10.84}\PYG{p}{,} \PYG{l+m+mf}{4.82}\PYG{p}{,} \PYG{l+m+mf}{5.68}\PYG{p}{]}
\PYG{n}{y2} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{9.14}\PYG{p}{,} \PYG{l+m+mf}{8.14}\PYG{p}{,} \PYG{l+m+mf}{8.74}\PYG{p}{,} \PYG{l+m+mf}{8.77}\PYG{p}{,} \PYG{l+m+mf}{9.26}\PYG{p}{,} \PYG{l+m+mf}{8.10}\PYG{p}{,} \PYG{l+m+mf}{6.13}\PYG{p}{,} \PYG{l+m+mf}{3.10}\PYG{p}{,} \PYG{l+m+mf}{9.13}\PYG{p}{,} \PYG{l+m+mf}{7.26}\PYG{p}{,} \PYG{l+m+mf}{4.74}\PYG{p}{]}
\PYG{n}{y3} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{7.46}\PYG{p}{,} \PYG{l+m+mf}{6.77}\PYG{p}{,} \PYG{l+m+mf}{12.74}\PYG{p}{,} \PYG{l+m+mf}{7.11}\PYG{p}{,} \PYG{l+m+mf}{7.81}\PYG{p}{,} \PYG{l+m+mf}{8.84}\PYG{p}{,} \PYG{l+m+mf}{6.08}\PYG{p}{,} \PYG{l+m+mf}{5.39}\PYG{p}{,} \PYG{l+m+mf}{8.15}\PYG{p}{,} \PYG{l+m+mf}{6.42}\PYG{p}{,} \PYG{l+m+mf}{5.73}\PYG{p}{]}
\PYG{n}{x4} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{19}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{]}
\PYG{n}{y4} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{6.58}\PYG{p}{,} \PYG{l+m+mf}{5.76}\PYG{p}{,} \PYG{l+m+mf}{7.71}\PYG{p}{,} \PYG{l+m+mf}{8.84}\PYG{p}{,} \PYG{l+m+mf}{8.47}\PYG{p}{,} \PYG{l+m+mf}{7.04}\PYG{p}{,} \PYG{l+m+mf}{5.25}\PYG{p}{,} \PYG{l+m+mf}{12.50}\PYG{p}{,} \PYG{l+m+mf}{5.56}\PYG{p}{,} \PYG{l+m+mf}{7.91}\PYG{p}{,} \PYG{l+m+mf}{6.89}\PYG{p}{]}

\PYG{n}{datasets} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y1}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y2}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y3}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{4.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{(}\PYG{n}{x4}\PYG{p}{,} \PYG{n}{y4}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{sharex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{sharey}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,}
                        \PYG{n}{gridspec\PYGZus{}kw}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wspace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.08}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hspace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.18}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{xlim}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ylim}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{14}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax}\PYG{p}{,} \PYG{p}{(}\PYG{n}{label}\PYG{p}{,} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{,} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{p1}\PYG{p}{,} \PYG{n}{p0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{polyfit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{deg}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} slope, intercept}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{axline}\PYG{p}{(}\PYG{n}{xy1}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{p0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{slope}\PYG{o}{=}\PYG{n}{p1}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

    \PYG{n}{stats} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{bar x\PYGZdl{} = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}
             \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}}\PYG{l+s+se}{\PYGZbs{}\PYGZbs{}}\PYG{l+s+s1}{sigma\PYGZdl{} = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}
             \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}r\PYGZdl{} = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{corrcoef}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{l+m+mf}{0.95}\PYG{p}{,} \PYG{l+m+mf}{0.07}\PYG{p}{,} \PYG{n}{stats}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{,} 
            \PYG{n}{transform}\PYG{o}{=}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{transAxes}\PYG{p}{,} \PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{f4a58c4b2213da08a87d1a9464c3353ff17b764178a2fc7ef335d138cbc5e5f9}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Statistique descriptive bivariée}
\label{\detokenize{statsdescriptives:statistique-descriptive-bivariee}}
\sphinxAtStartPar
On s’intéresse à deux variables \(x\) et \(y\), mesurées sur les \(n\) unités d’observation. La série statistique est alors une suite de \(n\) couples \((x_i,y_i)\) des valeurs prises par les deux variables sur chaque individu.


\subsection{Cas de deux variables quantitatives}
\label{\detokenize{statsdescriptives:cas-de-deux-variables-quantitatives}}
\sphinxAtStartPar
Le couple est un couple de valeurs numériques. C’est donc un point dans le plan \(\mathbb{R}^2\). Les variables \(x\) et \(y\) peuvent être analysées séparément, en opérant une statistique univariée sur chacune de ces variables. Les paramètres calculés (de position, de dispersion…) sont dits marginaux. Cependant, il est intéressant d’étudier le lien entre ces deux variables, par l’intermédiaire des valeurs des couples. On définit pour cela un certain nombre d’outils :
\label{statsdescriptives:definition-19}
\begin{sphinxadmonition}{note}{Definition 35 (Covariance)}



\sphinxAtStartPar
La covariance de \(x\) et \(y\) est définie par :
\(\sigma_{xy}=\frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )\left (y_i-\bar{y}\right )\)
\end{sphinxadmonition}

\index{Corrélation@\spxentry{Corrélation}!coefficient@\spxentry{coefficient}}\ignorespaces 
\index{Détermination@\spxentry{Détermination}!coefficient@\spxentry{coefficient}}\ignorespaces \label{statsdescriptives:definition-20}
\begin{sphinxadmonition}{note}{Definition 36 (Coefficient de corrélation)}



\sphinxAtStartPar
Le coefficient de corrélation  de deux variables \(x\) et \(y\) est défini par
\(r_{xy}=\frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}\).
Le coefficient de détermination est le carré du coefficient de corrélation.
\end{sphinxadmonition}

\sphinxAtStartPar
Le coefficient de corrélation est donc la covariance normalisée par les écarts types marginaux des variables. Il mesure la dépendance linéaire entre \(x\) et \(y\). Il est compris dans l’intervalle {[}\sphinxhyphen{}1,1{]} est est positif (resp. négatif) si les points sont alignés le long d’une droite croissante (resp. décroissante), d’autant plus grand en valeur absolue que la dépendance linéaire est vérifiée. Dans le cas où le coefficient est nul, il n’existe pas de dépendance linéaire.

\sphinxAtStartPar
Pour connaître plus précisément la relation linéaire qui lie \(x\) et \(y\), on effectue une régression linéaire en calculant par exemple la droite de régression : si \(y=a+bx\), il est facile de montrer que
\(b=\frac{\sigma_{xy}}{\sigma_x^2}\quad\textrm{et}\quad a=\bar{y}-b\bar{x}\)

\sphinxAtStartPar
et la droite de régression s’écrit \(y-\bar{y}=\frac{\sigma_{xy}}{\sigma_x^2}\left ( x-\bar{x}\right )\).

\sphinxAtStartPar
A partir de cette droite, on peut calculer les valeurs ajustées, obtenues à partir de la droite de régression : \(y^*_i=a+bx_i\). Ce sont les valeurs théoriques des \(y_i\) et les résidus \(e_i=y_i-y_i^*\) représentent la partie inexpliquée des \(y_i\) par la droite de régression (ceux là même que l’on essaye de minimiser par la méthode des moindres carrés). Nous reviendrons dans le chapitre sur la régression sur l’analyse de ces résidus.


\subsection{Cas de deux variables qualitatives}
\label{\detokenize{statsdescriptives:cas-de-deux-variables-qualitatives}}
\sphinxAtStartPar
Le couple est un couple de valeurs \((x_i,y_i)\) où \(x_i\) et \(y_i\) prennent comme valeurs des modalités qualitatives. Notons \(x_1\cdots x_J\) et \(y_1\cdots y_K\) les valeurs distinctes prises.

\sphinxAtStartPar
Les données peuvent être regroupées sous la forme d’un \sphinxstylestrong{tableau de contingence} prenant la forme suivante :

\index{Tableau@\spxentry{Tableau}!contingence@\spxentry{contingence}}\ignorespaces 
\index{Contingence@\spxentry{Contingence}!tableau@\spxentry{tableau}}\ignorespaces 
\sphinxAtStartPar
\(\begin{array}{c|ccccc|c}
&y_1&\cdots&y_k&\cdots&y_K&total\\
\hline
x_1&n_{11}&\cdots&n_{1k}&\cdots&n_{1K}&n_{1.}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
x_j&n_{j1}&\cdots&n_{jk}&\cdots&n_{jK}&n_{j.}\\
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
x_J&n_{J1}&\cdots&n_{Jk}&\cdots&n_{JK}&n_{J.}\\
\hline
total&n_{.1}&\cdots&n_{.k}&\cdots&n_{.K}&n\\
\end{array}
\)

\sphinxAtStartPar
où \(n_{j.}\) (resp \(n_{.k}\) )sont les effectifs marginaux représentant le nombre de fois où \(x_j\) (resp. \(y_k\)) apparaît, et \(n_{jk}\) le nombre d’apparition du couple \((x_j,y_k)\).

\sphinxAtStartPar
Le tableau des fréquences \(f_{jk}\) s’obtient en divisant tous les effectifs par la taille \(n\) dans ce tableau.

\sphinxAtStartPar
Un tel tableau s’interprète toujours en comparant les fréquences en lignes ou les fréquences en colonnes (profils lignes ou colonnes), définies  respectivement par
\(f_k^{(j)}= \frac{n_{jk}}{n_{j.}}=\frac{f_{jk}}{f_{j.}}\quad\textrm{ et }\quad f_j^{(k)}= \frac{n_{jk}}{n_{.k}}=\frac{f_{jk}}{f_{.k}}\)

\sphinxAtStartPar
Si l’on cherche un lien entre les variables, on construit un tableau d’effectifs théoriques qui représente la situation où les variables ne sont pas liées (indépendance). Ce tableau est constitué des effectifs
\(n_{jk}^*=\frac{n_{j.}n_{.k}}{n}\)
Les effectifs observés \(n_{jk}\) ont les mêmes marges que les \(n_{jk}^*\), et les écarts à l’indépendance sont calculés par la différence \(e_{jk}=n_{jk}-n_{jk}^*\)

\index{Khi\sphinxhyphen{}deux@\spxentry{Khi\sphinxhyphen{}deux}}\ignorespaces 
\sphinxAtStartPar
La dépendance du tableau se mesure au moyen du khi\sphinxhyphen{}deux défini par
\(\chi^2_{obs}= \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{e_{jk}^2}{n_{jk}^*}\)
qui peut être normalisé pour ne plus dépendre du nombre d’observations :
\(\phi^2=\frac{\chi^2_{obs}}{n}\)

\sphinxAtStartPar
La construction du tableau des effectifs théoriques et sa comparaison au tableau des observations permet dans un premier temps de mettre en évidence les associations significatives entre modalités des deux variables. Pour cela, on calcule la contribution au \(\chi^2\) des modalités \(j\) et \(k\) :
\begin{equation*}
\begin{split}\frac{1}{\chi^2_{obs}}\frac{\left (n_{jk}-n_{j.}n_{.k}\right )^2}{n_{jk}^*}\end{split}
\end{equation*}
\sphinxAtStartPar
Le signe de la différence \(n_{jk}-n_{jk}^*\) indique alors s’il y a une association positive ou négative entre les modalités \(j\) et \(k\).

\sphinxAtStartPar
Plus généralement, le \(\chi^2_{obs}\) est un indicateur de liaison entre les variables.  Dans le cas où \(\chi^2_{obs}=0\), il y a indépendance. Pour rechercher la borne supérieure du khi\sphinxhyphen{}deux et voir dans quel cas elle est atteinte, on développe le carré et on obtient
\begin{equation*}
\begin{split}\chi^2_{obs} = n\left [\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}^2}{n_{j.}n_{.k}} -1\right ]\end{split}
\end{equation*}
\sphinxAtStartPar
Comme \(\frac{n_{jk}}{n_{.k}}\leq 1\) on a \( \frac{n_{jk}^2}{n_{j.}n_{.k}} \leq \frac{n_{jk}}{n_{.k}}\) d’où
\begin{equation*}
\begin{split}\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{n_{jk}^2}{n_{j.}n_{.k}}\leq \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}}{n_{.k}} = \displaystyle\sum_{k=1}^K \frac{\displaystyle\sum_{j=1}^J n_{jk}}{n_{.k}}=\displaystyle\sum_{k=1}^K \frac{n_{.k}}{n_{.k}}=1\end{split}
\end{equation*}
\sphinxAtStartPar
d’où \(\chi^2_{obs}\leq n(K-1)\). On pourrait de même montrer que \(\chi^2_{obs}\leq n(J-1)\) et donc \(\phi^2\leq min(J-1,K-1)\).

\sphinxAtStartPar
La borne est atteinte dans le cas de la dépendance fonctionnelle (si \(\forall j \frac{n_{jk}}{n_{j.}}=1\), i.e. il n’existe qu’une case non nulle dans chaque ligne.)

\sphinxAtStartPar
A partir de ce khi\sphinxhyphen{}deux normalisé, on calcule finalement plusieurs coefficients permettant de mesurer l’indépendance, et parmi ceux\sphinxhyphen{}ci citons :
\begin{itemize}
\item {} 
\sphinxAtStartPar
le coefficient de Cramer:
\(V=\sqrt{\frac{\phi^2}{min(J-1,K-1)}}\)

\item {} 
\sphinxAtStartPar
le coefficient de contingence de Pearson :
\(C = \sqrt{\frac{\phi^2}{\phi^2 + 1}}\)

\item {} 
\sphinxAtStartPar
le coefficient de Tschuprow :
\(T = \sqrt{\frac{\phi^2}{\sqrt{(K-1)(J-1)}}}\)

\end{itemize}

\sphinxAtStartPar
Ces coefficients sont tous compris entre 0 (indépendance) et 1 (dépendance fonctionnelle). Pour estimer à partir de quelle valeur la dépendance fonctionnelle est significative, on procède de la manière suivante : si les \(n\) observations étaient prélevées dans une population où les variables sont indépendantes, on recherche les valeurs probables de \(\chi^2_{obs}\).

\sphinxAtStartPar
En s’appuyant sur la loi multinomiale et le test du \(\chi^2\), on montre que \(\chi^2_{obs}\) est une réalisation d’une variable aléatoire \(Z\) suivant approximativement une loi \(\chi^2_{(K-1)(J-1)}\).
\label{statsdescriptives:remark-21}
\begin{sphinxadmonition}{note}{Remark 10}



\sphinxAtStartPar
Soient \(U_1\ldots U_p\) \(p\) variables i.i.d de loi normale centrée réduite. On appelle loi du \(\chi^2\) à \(p\) degrés de liberté la loi de la variable \(\displaystyle\sum_{i=1}^pU_i^2\).
\end{sphinxadmonition}

\sphinxAtStartPar
En effet, les \(e_{jk}\) sont liées par \((K-1)(J-1)\) relations linéaires puisqu’on estime les probabilités de réalisation de \(x_j\) et \(y_k\) respectivement par \(n_{j,.}/n\) et \(n_{.k}/n\). Il suffit alors de fixer un risque d’erreur \(\alpha\) (une valeur qui, s’il y avait indépendance, n’aurait qu’une probabilité faible d’être dépassée), et on rejette l’hypothèse d’indépendance si \(\chi^2_{obs}\)  est supérieur à la valeur critique qu’une variable \(\chi^2_{(K-1)(J-1)}\) a une probabilité \(\alpha\) de dépasser.
L’espérance d’un \(\chi^2_{(K-1)(J-1)}\) étant égale à son degré de liberté, \(\chi^2_{obs}\) est d’autant plus grand que le nombre de modalités \(J\) et/ou \(K\) est grand.

\sphinxAtStartPar
D’autres indices existent, qui ne dépendent pas de \(\chi^2_{obs}\), comme par exemple

\sphinxAtStartPar
\(\begin{equation} G^2 = 2\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J n_{jk} ln \left (\frac{ n_{jk}}{ n^*_{jk}} \right )\end{equation}\)

\sphinxAtStartPar
qui sous l’hypothèse d’indépendance suit une loi \(\chi^2_{(K-1)(J-1)}\).


\subsection{Cas d’une variable quantitative et d’une variable qualitative}
\label{\detokenize{statsdescriptives:cas-d-une-variable-quantitative-et-d-une-variable-qualitative}}
\sphinxAtStartPar
On s’intéresse ici au cas où les modalités \(x_i\) sont qualitatives, et où \(y\) est une variable quantitative, dont les modalités sont des réalisations d’une variable aléatoire \(Y\).
Le rapport de corrélation théorique entre \(x\) et \(Y\) est défini par
\begin{equation*}
\begin{split}\eta^2_{Y\mid x} = \frac{\sigma^2_{\mathbb{E}_{Y\mid x}}}{\sigma^2_Y}\end{split}
\end{equation*}
\sphinxAtStartPar
Si \(n_j\) est le nombre d’observations de la modalité \(x_j,j\in[\![1\,J]\!]\), \(y_{ij}\) la valeur de \(Y\) du \(i^e\) individu de la modalité \(j\), \(\bar{y}_1\ldots \bar{y}_J\) sont les moyennes de \(Y\) pour ces modalités et \(\bar{y}\) la moyenne totale sur les \(n\) individus, le rapport de corrélation empirique est défini par
\begin{equation*}
\begin{split}e^2 = \frac{\frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2}{\sigma^2_y}\end{split}
\end{equation*}
\sphinxAtStartPar
La quantité

\sphinxAtStartPar
\(\sigma^2_\cap = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\sigma_j^2\)

\sphinxAtStartPar
avec \(\sigma_j^2 =  \frac{1}{n_j}\displaystyle\sum_{i=1}^{n_j}\left (y_{ij}-\bar{y}_j \right )^2\),  est appelée variance intra groupe (ou intra classe), et donne une idée de la variabilité à l’intérieur de chaque modalité.
La quantité
\(\sigma_\cup = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2\)
est la variance inter groupes (ou inter classes), et mesure la variabilité entre les différentes modalités.

\sphinxAtStartPar
Le théorème de décomposition de la variance (ou théorème de Huygens) affirme que la variance totale \(\sigma^2_y\), calculée sans distinction de modalité s’écrit :
\(\sigma^2_y = \sigma^2_\cap + \sigma^2_\cup\)

\sphinxAtStartPar
De ces définitions, on a alors :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(e^2=0\) si toutes les moyennes de \(Y\) sont égales, d’où l’absence de dépendance en moyenne

\item {} 
\sphinxAtStartPar
\(e^2=1\) si tous les individus d’une modalité de \(x\) ont même valeur de \(Y\) et ceci pour chaque modalité

\item {} 
\sphinxAtStartPar
\(e^2\) permet de comprendre, via le théorème de Huygens,  quelle variation est prédominante dans la variance totale. Ainsi par exemple, si la variable quantitative est la note d’un élève à un examen, et la variable qualitative son assiduité au cours correspondant, la variabilité entre les notes obtenues dans toute la promotion dépend de deux
facteurs : le fait que les étudiants assistent ou pas aux cours, et le fait qu’à assiduité
égale (i.e. à l’intérieur d’une même modalité d’assiduité) les étudiants n’ont pas le même niveau. \(e^2\)  permet alors de savoir lequel de ces deux facteurs est prédominant
pour expliquer la variabilité des notes dans toute la promotion.

\end{itemize}

\sphinxAtStartPar
Pour déterminer à partir de quelle valeur \(e^2\) est significatif, on compare donc \(\sigma^2_\cap\) à \(\sigma^2_\cup\). On peut montrer que si le rapport de corrélation théorique est nul, alors la variable \(\frac{\left (\frac{e^2}{J-1}\right )}{\left (\frac{1-e^2}{n-J}\right )}\) suit une loi de Fisher Snedecor, en supposant que les distributions conditionnelles de \(Y\) pour chaque modalité de \(X\) sont gaussiennes, de même espérance et de même variance.
\label{statsdescriptives:remark-22}
\begin{sphinxadmonition}{note}{Remark 11}



\sphinxAtStartPar
Soient \(U\) et \(V\) deux variables aléatoires indépendantes suivant respectivement des lois \(\chi^2_n\) et \(\chi^2_p\). On définit la loi de Fisher Snedecor par \(F(n,p)=\frac{U/n}{V/P}\)) \(F(J-1,n-J)\)
\end{sphinxadmonition}


\section{Vers une analyse multivariée}
\label{\detokenize{statsdescriptives:vers-une-analyse-multivariee}}
\sphinxAtStartPar
Bien évidemment, dans la majorité des cas, un individu sera décrit par \(p\geq 2\) variables. Si certains algorithmes de statistique descriptive multidimensionnelle sont abordés dans ce cours, il est néanmoins possible d’avoir une première approche exploratoire de ce cas.


\subsection{Matrices de covariance et de corrélation}
\label{\detokenize{statsdescriptives:matrices-de-covariance-et-de-correlation}}
\sphinxAtStartPar
La première idée, lorsque l’on a observé \(d\) variables sur \(n\) individus, est de calculer les \(d\) variances de ces variables, et les \(\frac{p(p-1)}{2}\) covariances. Ces mesures sont regroupées dans une matrice \(p\times p\), symétrique, semi définie positive, appelée matrice de variance\sphinxhyphen{}covariance (ou matrice des covariances), et classiquement notée \(\boldsymbol\Sigma\).

\sphinxAtStartPar
De même, on peut former la matrice des corrélations entre les variables, à diagonale unité et symétrique. La matrice résultante, notée \(\mathbf R\), est également semi définie positive et sa représentation graphique en fausses couleurs permet d’apprécier les dépendances linéaires entre variables.

\sphinxAtStartPar
\sphinxincludegraphics{{batiments}.png}

\sphinxAtStartPar
Dans le cas de variables qualitatives, les coefficients de corrélation peuvent être remplacés par les coefficients de Cramer, de Tschuprow…


\subsection{Tableaux de nuages}
\label{\detokenize{statsdescriptives:tableaux-de-nuages}}
\sphinxAtStartPar
On peut proposer à partir de là des représentations entre sous\sphinxhyphen{}ensembles de variables. La figure suivante propose un exemple de tels tableaux, parfois appelés splom (Scatter PLOt Matrix) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la partie triangulaire supérieure représente les nuages de points de couples de variables

\item {} 
\sphinxAtStartPar
la diagonale représente les histogrammes des variables

\item {} 
\sphinxAtStartPar
la partie trianglaire inférieure donne le coefficient de corrélation entre les deux variables, et une estimation de la densité de la distribution 2D des données

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{batiments2}.png}


\subsection{Tableaux de Burt}
\label{\detokenize{statsdescriptives:tableaux-de-burt}}
\sphinxAtStartPar
Le tableau de Burt est une généralisation particulière de la table de contingence dans le cas où l’on étudie simultanément \(p\) variables qualitatives \(X_1\ldots X_p\). Notons \(c_j\) le nombre de modalités de \(X_j\) et posons \(c=\displaystyle\sum_{j=1}^p c_j\).

\index{Tableau@\spxentry{Tableau}!Burt@\spxentry{Burt}}\ignorespaces 
\index{Burt@\spxentry{Burt}!tableau@\spxentry{tableau}}\ignorespaces 
\sphinxAtStartPar
Le tableau de Burt est une matrice carrée symétrique de taille \(c\), constituée de \(p^2\) sous\sphinxhyphen{}matrices. Chacune des \(p\) sous\sphinxhyphen{}matrices diagonales est relative à l’une des \(p\) variables, la \(j^e\) étant carrée de taille \(c_j\), diagonale, et de coefficients diagonaux les effectifs marginaux de \(X_j\). La sous\sphinxhyphen{}matrice dans le bloc \((k,l)\) du tableau, \(k\neq l\), est la table de contingence des variables \(X_k\) et \(X_l\).

\sphinxstepscope


\chapter{Sélection de variables}
\label{\detokenize{selection:selection-de-variables}}\label{\detokenize{selection::doc}}
\sphinxAtStartPar
On s’intéresse ici à \(n\) individus  \(\mathbf x_i, i\in[\![1,n]\!]\) décrits par \(d\) variables quantitatives ou caractéristiques (features), \(x_i\in \mathbb{R}^d\). Avec l’avènement des Big Data, et la généralisation des capteurs, \(d\) peut être très grand (plusieurs milliers), et analyser telles quelles les données brutes devient difficile d’un point de vue calculatoire et interprétation. De plus, il est rare que les caractéristiques soient totalement utiles et indépendantes.

\sphinxAtStartPar
Une étape souvent utilisée en analyse de données consiste donc à prétraiter cet espace, par exemple pour :
\begin{itemize}
\item {} 
\sphinxAtStartPar
le transformer en un format compatible avec des algorithmes qui seront utilisés

\item {} 
\sphinxAtStartPar
réduire la complexité temporelle des algorithmes qui seront utilisés

\item {} 
\sphinxAtStartPar
réduire la complexité spatiale du problème traité

\item {} 
\sphinxAtStartPar
découpler des variables et chercher les dépendances

\item {} 
\sphinxAtStartPar
introduire des a priori, ou des propriétés importantes pour les algorithmes (données centrées normées, descripteurs épars…)

\item {} 
\sphinxAtStartPar
permettre une interprétation plus intuitive et/ou graphique ({\hyperref[\detokenize{selection:tsne}]{\sphinxcrossref{\DUrole{std,std-ref}{figure 2}}}})

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{tsne}.png}
\caption{Exemple de réduction de dimension (source: Maaten \& Hinton, 2008). Des images 28\(\times\) 28 de chiffres manuscrits sont représentées par un vecteur de 784 valeurs, puis transformés en vecteurs de \(\mathbb{R}^2\) pour les projeter dans le plan. La méthode utilisée permet d’optimiser la transformation de sorte à ce que les images représentant le même chiffre soient regroupées dans des nuages compacts.}\label{\detokenize{selection:tsne}}\end{figure}

\sphinxAtStartPar
Deux stratégies peuvent alors être utilisées :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
sélectionner un sous\sphinxhyphen{}ensemble des variables initiales comme descripteurs des individus

\item {} 
\sphinxAtStartPar
calculer de nouveaux descripteurs à partir des variables initiales.

\end{enumerate}

\sphinxAtStartPar
Nous nous intéressons ici à la première approche, la seconde (extraction de caractéristiques) étant abordée pour une approche linéaire dans le chapitre sur l’analyse en composantes principales.
\label{selection:remark-0}
\begin{sphinxadmonition}{note}{Remark 12}



\sphinxAtStartPar
Les méthodes d’extraction de caractéristiques peuvent être soit linéaires (on recherche des combinaisons linéaires des variables initiales  permettant d’optimiser un cerrtain critère), ou non linéaires (on parle également de manifold learning)
\end{sphinxadmonition}


\section{Définitions}
\label{\detokenize{selection:definitions}}
\sphinxAtStartPar
La sélection de caractéristiques consiste à choisir parmi les \(d\) descripteurs d’un ensemble d’individus \(\mathbf x_i,i\in[\![1,n]\!]\), un sous\sphinxhyphen{}ensemble de  \(t<d\)  caractéristiques jugées “les plus pertinentes”, les \(d-t\) restantes étant ignorées.

\sphinxAtStartPar
On note \(F = \left (f_1\cdots f_d\right )\) les \(d\) caractéristiques.  On note \(Perf\) une fonction qui permet d’évaluer un sous\sphinxhyphen{}ensemble de caractéristiques, et on suppose que \(Perf\) atteint son maximum pour le meilleur sous\sphinxhyphen{}ensemble de caractéristiques (“le plus pertinent”). Le problème de sélection se formule donc comme un problème d’optimisation
\begin{equation*}
\begin{split}\hat{F} = Arg\displaystyle\max_{U\subset F} Perf(U)\end{split}
\end{equation*}
\sphinxAtStartPar
le cardinal \(|\hat{F|}\) de \(\hat{F}\) étant soit contrôlé par l’utilisateur, soit défini par l’algorithme de sélection.

\sphinxAtStartPar
On distingue alors trois stratégies :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|\hat{F|}\) est défini par l’utilisateur et l’optimisation s’effectue sur tous les sous\sphinxhyphen{}ensembles ayant ce cardinal

\item {} 
\sphinxAtStartPar
On connaît une mesure minimale de performance \(\gamma\)  et la sélection recherche le plus petit sous\sphinxhyphen{}ensemble \(U\) dont la performance \(Perf(U)\) est supérieure ou égale à \(\gamma\)

\item {} 
\sphinxAtStartPar
On cherche un compromis entre l’amélioration de la performance \(Perf(U)\) et la réduction de la taille du sous ensemble.

\end{itemize}

\sphinxAtStartPar
La mesure de pertinence d’une caractéristique est donc au centre des algorithmes de sélection. Plusieurs définitions sont possibles, et nous dirons ici  qu’une caractéristique \(f_i\) est :
\begin{itemize}
\item {} 
\sphinxAtStartPar
pertinente si son absence entraîne une détérioration significative de la performance de l’algorithme utilisé en aval (classification ou régression)

\item {} 
\sphinxAtStartPar
peu pertinente si elle n’est pas pertinente et s’il existe un sous\sphinxhyphen{}ensemble \(U\) tel que la performance de \(U\cup\{f_i\}\) est significativement meilleure que la peformance de \(U\)

\item {} 
\sphinxAtStartPar
non pertinente, si elle ne rentre pas dans les deux premières définitions. En général, ces caractéristiques sont supprimées.

\end{itemize}


\section{Caractéristiques des méthodes de sélection}
\label{\detokenize{selection:caracteristiques-des-methodes-de-selection}}
\sphinxAtStartPar
Une méthode de sélection basée sur l’optimisation de \(Perf\) utilise généralement trois étapes. Les  deux dernières sont itérées jusqu’à un test d’arrêt.


\subsection{Initialisation}
\label{\detokenize{selection:initialisation}}
\sphinxAtStartPar
L’initialisation consiste à choisir l’ensemble de départ des caractéristiques. Il peut s’agir de l’ensemble vide, de \(F\) tout entier, ou un sous\sphinxhyphen{}ensemble quelconque \(U\subset F\).


\subsection{Exploration des sous\sphinxhyphen{}ensembles}
\label{\detokenize{selection:exploration-des-sous-ensembles}}
\sphinxAtStartPar
A partir de cette initialisation, les stratégies d’exploration des sous\sphinxhyphen{}ensembles de caractéristiques se déclinent en trois catégories :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
génération exhaustive : tous les sous\sphinxhyphen{}ensembles de caractéristiques sont évalués. Si elle garantit de trouver la valeur optimale, cette méthode n’est que peu applicable dès que \(|F|\) devient important (\(2^{|F|}\) sous\sphinxhyphen{}ensembles possibles)

\item {} 
\sphinxAtStartPar
génération heuristique : une génération itérative est effectuée, chaque itération permettant de sélectionner ou de rejeter une ou plusieurs caractéristiques. La génération peut être ascendante (ajout de caractéristiques à partir de l’ensemble vide), descendante (suppression de caractéristiques à partir de \(F\)), ou mixte.

\item {} 
\sphinxAtStartPar
génération stochastique : pour un ensemble de données et une initialisation définie, une stratégie de recherche heuristique retourne toujours le même sous\sphinxhyphen{}ensemble, ce qui la rend très sensible au changement
de l’ensemble de données. La génération stochastique génère aléatoirement un nombre fini de sous\sphinxhyphen{}ensembles de caractéristiques afin de sélectionner le meilleur. La convergence est sous\sphinxhyphen{}optimale mais peut s’avérer préférable dans des algorithmes d’apprentissage, par exemple pour éviter le phénomène de surapprentissage.

\end{enumerate}


\subsection{Evaluation des sous\sphinxhyphen{}ensembles}
\label{\detokenize{selection:evaluation-des-sous-ensembles}}

\subsubsection{Filtres}
\label{\detokenize{selection:filtres}}
\sphinxAtStartPar
Le critère d’évaluation utilisé évalue la pertinence d’une caractéristique selon des mesures
qui reposent sur les propriétés de données d’apprentissage.

\sphinxAtStartPar
Pour \(n\) exemples  \(\mathbf x_i, i\in[\![1,n]\!]\) , on note \(\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d\)  une donnée d’apprentissage (la \(j^e\) caractéristique \(f_j\) ayant donc pour valeur \(x_{ij}\)) , d’étiquette \(y_i\) (en classification ou régression). Les méthodes de type filtres calculent un score pour évaluer le degré de pertinence de chacune des caractéristiques \(f_i\) , parmi lesquelles on peut citer
\begin{itemize}
\item {} 
\sphinxAtStartPar
Le critère de corrélation, utilisé en classification binaire

\end{itemize}
\begin{equation*}
\begin{split}C_i =\frac{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )\left (y_{k} -\mu_k\right )}{\sqrt{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )^2\displaystyle\sum_{k=1}^n\left (y_{k} -\mu_k\right )^2}}\end{split}
\end{equation*}
\sphinxAtStartPar
où \(\mu_i\) (resp. \(\mu_k\)) est la moyenne de la caractéristique \(f_i\) observée sur \(\mathbf x_1\cdots \mathbf x_n\) (resp. moyenne des étiquettes)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Le critère de Fisher,  qui permet de mesurer dans un problème de classification à \(C\) classes le degré de séparabilité des classes à l’aide
d’une caractéristique donnée

\end{itemize}
\begin{equation*}
\begin{split}F_i = \frac{\displaystyle\sum_{c=1}^C n_c\left (\mu_c^i-\mu_i \right )^2}{\displaystyle\sum_{c=1}^C n_c(\Sigma_c^i)^2}\end{split}
\end{equation*}
\sphinxAtStartPar
où \(n_c, \mu_c^i\) et \(\Sigma_c^i\) sont l’effectif, la moyenne et l’écart\sphinxhyphen{}type de la caractéristique  \(f_i\) dans la classe \(c\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
l’information mutuelle

\end{itemize}
\begin{equation*}
\begin{split}I(i) = \displaystyle\sum_{\mathbf x_i} \displaystyle\sum_{y}P(X=\mathbf x_i,Y=y)log\left ( \frac{P(X=\mathbf x_i,Y=y)}{P(X=\mathbf x_i)P(Y=y)}\right )\end{split}
\end{equation*}
\sphinxAtStartPar
qui mesure la dépendance entre les distributions de deux populations. Ici \(X\) et \(Y\) sont deux variables aléatoires dont les réalisations sont les valeurs de \(f_i\) et des étiquettes de classes. Les probabilités sont estimées de manière fréquentiste.

\sphinxAtStartPar
Dans l’exemple suivant, on choisit de garder \(|\hat{F|}=2\) descripteurs, en contrôlant la pertinence par l’information mutuelle en classification.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}iris}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{SelectKBest}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{load\PYGZus{}iris}\PYG{p}{(}\PYG{n}{return\PYGZus{}X\PYGZus{}y}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taille des données avant : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{SelectKBest}\PYG{p}{(}\PYG{n}{mutual\PYGZus{}info\PYGZus{}classif}\PYG{p}{,}\PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{X2} \PYG{o}{=} \PYG{n}{s}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taille des données après : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X2}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variables sélectionnées : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{.}\PYG{n}{get\PYGZus{}support}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Taille des données avant :  (150, 4)
Taille des données après :  (150, 2)
Variables sélectionnées :  [False False  True  True]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Méthodes enveloppantes}
\label{\detokenize{selection:methodes-enveloppantes}}
\sphinxAtStartPar
Le principal inconvénient des approches précédentes est le fait qu’elles ignorent l’influence des caractéristiques sélectionnées sur la performance de l’algorithme à utiliser par la suite. Les méthodes de type enveloppantes (wrappers)  évaluent un sous\sphinxhyphen{}ensemble de caractéristiques par sa performance
de classification en utilisant un algorithme d’apprentissage.  Les sous\sphinxhyphen{}ensembles de caractéristiques sélectionnés par cette méthode sont bien adaptés à l’algorithme de classification utilisé, mais ils ne sont pas nécessairement pour un autre. De plus, la complexité de l’algorithme d’apprentissage rend ces méthodes coûteuses.

\sphinxAtStartPar
Les principales différences entre les filtres et les méthodes enveloppantes pour la sélection des caractéristiques sont les suivantes :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Les filtres mesurent la pertinence des caractéristiques par leur corrélation avec la variable dépendante, tandis que les méthodes enveloppantes mesurent l’utilité d’un sous\sphinxhyphen{}ensemble de caractéristiques en entraînant un modèle sur celles\sphinxhyphen{}ci.

\item {} 
\sphinxAtStartPar
Les filtres sont beaucoup plus rapides que les méthodes enveloppantes car elles n’impliquent pas l’apprentissage des modèles. D’un autre côté, les méthodes enveloppantes sont également très coûteuses en termes de calcul.

\item {} 
\sphinxAtStartPar
Les filtres utilisent des méthodes statistiques pour l’évaluation d’un sous\sphinxhyphen{}ensemble de caractéristiques, tandis que les méthodes enveloppantes utilisent la validation croisée.

\item {} 
\sphinxAtStartPar
Les filtres peuvent échouer à trouver le meilleur sous\sphinxhyphen{}ensemble de caractéristiques dans de nombreuses occasions, mais les méthodes enveloppantes peuvent toujours fournir le meilleur sous\sphinxhyphen{}ensemble de caractéristiques.

\item {} 
\sphinxAtStartPar
L’utilisation d’un sous\sphinxhyphen{}ensemble de caractéristiques à partir des méthodes enveloppantes amène plus facilement au phénomène de surapprentissage

\end{itemize}
\label{selection:remark-1}
\begin{sphinxadmonition}{note}{Remark 13}



\sphinxAtStartPar
Les wrappers sélectionnent les caractéristiques en se fondant sur une estimation du risque réel.
\end{sphinxadmonition}


\subsubsection{Méthodes intégrées}
\label{\detokenize{selection:methodes-integrees}}
\sphinxAtStartPar
Les méthodes intégrées incluent la sélection de variables lors du processus d’apprentissage. Un tel mécanisme intégré pour la sélection des caractéristiques peut être trouvé, par
exemple, dans les algorithmes de type SVM,  AdaBoost  ou dans les
arbres de décision.


\section{Quelques méthodes de sélection}
\label{\detokenize{selection:quelques-methodes-de-selection}}

\subsection{Suppression des descripteurs à variance faible}
\label{\detokenize{selection:suppression-des-descripteurs-a-variance-faible}}
\sphinxAtStartPar
Une première idée simple consiste à supprimer les descripteurs ayant une faible variance, ces derniers n’étant pas discriminants dans la définition des individus.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{VarianceThreshold}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn} \PYG{k+kn}{import} \PYG{n}{datasets}

\PYG{n}{iris} \PYG{o}{=} \PYG{n}{datasets}\PYG{o}{.}\PYG{n}{load\PYGZus{}iris}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{iris}\PYG{o}{.}\PYG{n}{data}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{iris}\PYG{o}{.}\PYG{n}{target}

\PYG{n}{v} \PYG{o}{=} \PYG{n}{VarianceThreshold}\PYG{p}{(}\PYG{n}{threshold}\PYG{o}{=}\PYG{l+m+mf}{.5}\PYG{p}{)}
\PYG{n}{X2} \PYG{o}{=} \PYG{n}{v}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Avant sélection, }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Après sélection, }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X2}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variables sélectionnées : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{v}\PYG{o}{.}\PYG{n}{get\PYGZus{}support}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Avant sélection,  (150, 4)
Après sélection,  (150, 3)
Variables sélectionnées :  [ True False  True  True]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Algorithmes de sélection séquentielle}
\label{\detokenize{selection:algorithmes-de-selection-sequentielle}}
\sphinxAtStartPar
Les algorithmes SFS (Sequential Forward Selection, {\hyperref[\detokenize{selection:SFS}]{\sphinxcrossref{Algorithm 1}}}) et SBS (Sequential Backward Selection, {\hyperref[\detokenize{selection:SFS}]{\sphinxcrossref{Algorithm 1}}}\sphinxhyphen{}rouge) ont été les premiers à être proposés. Ils utilisent des approches heuristiques de recherche en partant, pour la première, d’un ensemble de caractéristiques vide et pour la seconde de  \(F\) tout entier.
\label{selection:SFS}
\begin{sphinxadmonition}{note}{Algorithm 1 (Algorithmes SFS et SBS)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(F = \left (f_1\cdots f_d\right )\), taille de l’ensemble final  \(T\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} \(\hat{F}\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\hat{F}\leftarrow \emptyset\quad\) (\(\hat{F}\leftarrow F\))

\item {} 
\sphinxAtStartPar
Pour \(i=1\) à \( T\quad\) (\(i=1\) à \(d-T\))
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Pour \(j=1\) à \( |{F}|\quad\) (\(j=1\) à \(|\hat{F}|\))
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
Evaluer \(\{f_j\}\cup \hat{F}\quad\) (\(\hat{F}\setminus \{f_j\}\))

\end{enumerate}

\item {} 
\sphinxAtStartPar
\(f_{max}\) = meilleure caractéristique \(\quad\) (\(f_{min}\)=moins bonne caractéristique)

\item {} 
\sphinxAtStartPar
\(\hat{F}\leftarrow\hat{F}\cup\{f_{max}\}, F=F\setminus f_{max}\quad\) (\(\hat{F}\setminus\hat{F}f_{min}\))

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
L’étape d’évaluation utilise des données d’apprentissage : une heuristique évalue, sur un critère de performance, l’intérêt d’ajouter (ou de supprimer) le descripteur \(f_i\).

\sphinxAtStartPar
Des variantes autour de ces algorithmes simples ont été proposées depuis et par exemple :
\begin{itemize}
\item {} 
\sphinxAtStartPar
il est possible à chaque itération d’inclure (ou d’exclure) un sous\sphinxhyphen{}ensemble de caractéristiques, plutôt qu’une seule (méthodes GSFS et GSBS)

\item {} 
\sphinxAtStartPar
on peut appliquer \(p\) fois SFS puis \(q\) fois SBS, de manière itérative, avec \(p,q\) des paramètres qui peuvent évoluer au cours des itérations (algorithme SFFS et SFBS)

\end{itemize}

\sphinxAtStartPar
Dans l’exemple suivant, l’heuristique choisie est l’algorithme des 3 plus proches voisins et la mesure de performance sous\sphinxhyphen{}jacente est la mesure de validation croisée.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{SequentialFeatureSelector}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{KNeighborsClassifier}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}iris}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{load\PYGZus{}iris}\PYG{p}{(}\PYG{n}{return\PYGZus{}X\PYGZus{}y}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{knn} \PYG{o}{=} \PYG{n}{KNeighborsClassifier}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{sfs} \PYG{o}{=} \PYG{n}{SequentialFeatureSelector}\PYG{p}{(}\PYG{n}{knn}\PYG{p}{,} \PYG{n}{n\PYGZus{}features\PYGZus{}to\PYGZus{}select}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{sfs}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taille des données avant sélection}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taille des données après sélection}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{sfs}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variables sélectionnées : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sfs}\PYG{o}{.}\PYG{n}{get\PYGZus{}support}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Taille des données avant sélection (150, 4)
Taille des données après sélection (150, 3)
Variables sélectionnées :  [ True False  True  True]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Algorithme Focus}
\label{\detokenize{selection:algorithme-focus}}
\sphinxAtStartPar
L’algorithme de filtrage Focus ({\hyperref[\detokenize{selection:FOCUS}]{\sphinxcrossref{Algorithm 2}}}\}) repose sur une recherche exhaustive sur \(F\) pour trouver le sous\sphinxhyphen{}ensemble le plus performant de taille optimale.
\label{selection:FOCUS}
\begin{sphinxadmonition}{note}{Algorithm 2 (Algorithme FOCUS)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\) , taille de l’ensemble final  \(T\), seuil \(\epsilon\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} \(\hat{F}\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\hat{F}\leftarrow \emptyset\)

\item {} 
\sphinxAtStartPar
Pour \(i=1\) à \( T\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
chaque sous\sphinxhyphen{}ensemble \(S_i\) de taille \(i\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
Si Inconsistance(A,\(S_i\))<\(\epsilon\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiv}{enumv}{}{.}%
\item {} 
\sphinxAtStartPar
\(\hat{F}\leftarrow S_i\)

\item {} 
\sphinxAtStartPar
Retourner \(\hat{F}\)

\end{enumerate}

\end{enumerate}

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}


\subsection{Algorithme relief}
\label{\detokenize{selection:algorithme-relief}}
\sphinxAtStartPar
La méthode relief en classification binaire ({\hyperref[\detokenize{selection:relief}]{\sphinxcrossref{Algorithm 3}}}), propose de calculer une mesure globale de la pertinence des caractéristiques en accumulant la différence des distances entre des exemples d’apprentissage choisis aléatoirement et leurs plus proches voisins de la même classe et de l’autre classe.
\label{selection:relief}
\begin{sphinxadmonition}{note}{Algorithm 3 (Algorithme Relief)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\) , nombre d’itérations \(T\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} \(w\in\mathbb{R}^d\) un vecteur de poids des caractéristiques, \(w_i\in[-1,1],i\in[\![1,d]\!]\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Pour \(i=1\) à \( d\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
\(w_i\leftarrow 0\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour \(i=1\) à \( T\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Choisir aléatoirement un exemple \(\mathbf x_k\)

\item {} 
\sphinxAtStartPar
Chercher deux plus proches voisins de \(\mathbf x_k\), l’un (\(\mathbf x_p\)) dans sa  classe, l’autre (\(\mathbf x_q\)) dans l’autre classe

\item {} 
\sphinxAtStartPar
Pour \(j=1\) à \(d\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(w_j\leftarrow w_j+\frac{1}{nT}\left (|x_{kj} -x_{qj}|-|x_{kj} -x_{pj}| \right )\)

\end{enumerate}

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}


\subsection{Méthode SAC}
\label{\detokenize{selection:methode-sac}}
\sphinxAtStartPar
L’algorithme SAC (Selection Adaptative de Caractéristiques)  construit un ensemble de classifieurs (ou de régresseurs) \((M_1\cdots M_d)\) appris sur chacun des descripteurs et sélectionne les meilleurs par discrimination linéaire de Fisher. Pour ce faire, l’algorithme construit un vecteur dont les éléments sont les performances \(Perf(M_i)\) des modèles \(M_i\), triés par ordre décroissant. Deux moyennes \(m_1(i)\) et \(m_2(i)\) sont calculées, qui représentent les deux moyennes de performance d’apprentissage qui ont une valeur respectivement plus grande (plus petite) que la performance du modèle \(M_i\) :
\begin{equation*}
\begin{split}m_1(i) = \frac{1}{i}\displaystyle\sum_{j=1}^i Perf (M_j)\textrm{ et } m_2(i) = \frac{1}{d-i}\displaystyle\sum_{j=i+1}^d Perf (M_j)\end{split}
\end{equation*}
\sphinxAtStartPar
Deux variances des performances  \(v_1^2(i)\) et \( v_2^2(i)\) sont alors calculées à partir de ces moyennes, et le sous\sphinxhyphen{}ensemble de caractéristiques sélectionné est celui qui maximise le discriminant de Fisher
\begin{equation*}
\begin{split}\frac{|m_1(i)-m_2(i)|}{v_1^2(i)+v_2^2(i)}\end{split}
\end{equation*}

\subsection{Algorithme RFE}
\label{\detokenize{selection:algorithme-rfe}}
\sphinxAtStartPar
L’algorithme RLE (Recusrive Feature Elimination) trie les descripteurs en analysant, localement, la sensibilité de la performance.
Étant donné un prédicteur \(f\) qui attribue des poids aux caractéristiques (par exemple, les coefficients d’un modèle linéaire), l’objectif de l’algorithme est de sélectionner les caractéristiques en considérant de manière récursive des ensembles de caractéristiques de plus en plus petits. Tout d’abord, le prédicteur \(f\) est entraîné sur l’ensemble initial de caractéristiques et l’importance de chaque caractéristique est calculée par un algorithme dédié (critère de Gini, entropie…). Les caractéristiques les moins importantes sont éliminées de l’ensemble actuel de caractéristiques. Cette procédure est répétée de manière récursive sur l’ensemble élagué jusqu’à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{RFE} 
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}iris}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{tree} \PYG{k+kn}{import} \PYG{n}{DecisionTreeClassifier}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{load\PYGZus{}iris}\PYG{p}{(}\PYG{n}{return\PYGZus{}X\PYGZus{}y}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{estimator} \PYG{o}{=} \PYG{n}{DecisionTreeClassifier}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{RFE}\PYG{p}{(}\PYG{n}{estimator}\PYG{p}{,} \PYG{n}{n\PYGZus{}features\PYGZus{}to\PYGZus{}select}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{s}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taille des données avant sélection}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variables sélectionnées : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{.}\PYG{n}{get\PYGZus{}support}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Classement des variables : }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{s}\PYG{o}{.}\PYG{n}{ranking\PYGZus{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Taille des données avant sélection (150, 4)
Variables sélectionnées :  [False False  True  True]
Classement des variables :  [2 3 1 1]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope

\sphinxAtStartPar
Les méthodes factorielles ont pour but de traiter et visualiser des données multidimensionnelles. La prise en compte simultanée de l’ensemble des variables est un problème difficile, rendu parfois plus simple car l’information apportée par les variables est redondante. Les méthodes factorielles visent alors à exploiter cette redondance pour tenter de remplacer les variables initiales par un nombre réduit de nouvelles variables, conservant au mieux l’information initiale.

\sphinxAtStartPar
Les principales méthodes de ce type incluent l’analyse factorielle des correspondances, l’analyse des correspondances multiples, l’analyse factorielle d’un tableau de distance (pour les tableaux de proximité) ou encore l’analyse factorielle discriminante. Ces méthodes sont proposées en annexe de ce cours.

\sphinxAtStartPar
Nous nous intéressons ici à une méthode de réduction de dimension linéaire sur données quantitatives, l’analyse en composantes principales.


\chapter{Analyse en composantes principales}
\label{\detokenize{acp:analyse-en-composantes-principales}}\label{\detokenize{acp::doc}}
\index{ACP@\spxentry{ACP}}\ignorespaces 
\index{Analyse@\spxentry{Analyse}!composantes principales@\spxentry{composantes principales}}\ignorespaces 
\sphinxAtStartPar
Pour les données quantitatives, l’Analyse en Composantes Principales (ACP) est l’une des méthodes les plus utilisées. Elle considère que les nouvelles variables sont des combinaisons linéaires des variables initiales, non corrélées.

\sphinxAtStartPar
\sphinxincludegraphics{{acpintro}.png}

\sphinxAtStartPar
Dans la suite, les données seront des tableaux \(n\times d\) de variables quantitatives, une ligne étant un individu, et les colonnes décrivant les paramètres mesurés. Les observations de \(d\) variables sur \(n\) individus sont donc rassemblées dans une matrice \({\bf X}\in\mathcal{M}_{n,d}(\mathbb{R})\) .  On notera \(x^j\) la j\sphinxhyphen{}ème variable, identifiée par la j\sphinxhyphen{}ème colonne \({\bf X_{\bullet,j}}\) de \({\bf X}\), et \(\mathbf{e_i}\) le i\sphinxhyphen{}ème individu (i.e. \({\bf X_{i,\bullet}^T}\)).


\section{Principe de la méthode}
\label{\detokenize{acp:principe-de-la-methode}}

\section{Pré\sphinxhyphen{}traitement du tableau}
\label{\detokenize{acp:pre-traitement-du-tableau}}
\sphinxAtStartPar
En analyse en composantes principales, on raisonne souvent sur des variables centrées et/ou réduites.


\subsection{Données centrées}
\label{\detokenize{acp:donnees-centrees}}
\sphinxAtStartPar
Notons \(\mathbf{g} = \left ( \bar{x}^1\cdots \bar{x}^d\right )\) le vecteur des moyennes arithmétiques de chaque variable (centre de gravité) :

\sphinxAtStartPar
\(\mathbf{g}={\bf X^TD\mathbf{1}}\)

\sphinxAtStartPar
où \({\bf D}\) est une matrice diagonale de poids,  chaque \(d_{ii}\) donnant l’importance de l’individu \(i\) dans les données (le plus souvent \({\bf D}=\frac{1}{n}{ \mathbb{I}}\)),  et \(\mathbf{1}\) est le vecteur de \(\mathbb{R}^n\) dont toutes les composantes sont égales à 1. Le tableau \({\bf Y}={\bf X}-\mathbf{1}\mathbf{g}^T=({ \mathbb{I}}-\mathbf{1}\mathbf{1}^T{\bf D}){\bf X}\) est le tableau centré associé à \({\bf X}\).


\subsection{Données réduites}
\label{\detokenize{acp:donnees-reduites}}
\sphinxAtStartPar
La matrice de variance/covariance des données centrées est égale à
\({\bf V} = {\bf X^TDX} - \mathbf{g}\mathbf{g^T} = {\bf Y^TDY}\).

\sphinxAtStartPar
Si on note \({\bf D_{1/\sigma}}\) la matrice diagonale des inverses des écarts\sphinxhyphen{}types des variables, alors  \({\bf Z}={\bf YD_{1/\sigma}}\)
est la matrice des données centrées réduites. La matrice \({\bf R}={\bf D_{1/\sigma}VD_{1/\sigma}}={\bf Z^TDZ}\)
est la matrice de corrélation des données et résume la structure des dépendances linéaires entre les \(d\) variables.


\section{Projection des individus sur un sous\sphinxhyphen{}espace}
\label{\detokenize{acp:projection-des-individus-sur-un-sous-espace}}
\sphinxAtStartPar
Le principe de la méthode est d’obtenir une représentation approchée du nuage des \(n\) individus dans un sous\sphinxhyphen{}espace \(F_k\) de dimension faible. Ceci s’effectue par un mécanisme de projection.

\sphinxAtStartPar
Le choix de l’espace de projection est dicté par le critère suivant, qui revient à déformer le moins possible les distances en projection : le sous\sphinxhyphen{}espace de dimension \(k\) recherché est tel que la moyenne des carrés des distances entre projections soit la plus grande possible. En définissant l’inertie d’un nuage de points comme la moyenne pondérée des carrés des distances au centre de gravité, le critère revient alors à maximiser l’inertie du nuage projeté sur \(F_k\).

\sphinxAtStartPar
Soit \({\bf P}\) la projection orthogonale sur \(F_k\). Le nuage de points projeté est associé au tableau \({\bf XP^T}\) puisque chaque individu \(\mathbf{e_i}\) se projette sur \(F_k\) selon un vecteur colonne \(\mathbf{Pe_i}\) ou ligne \(\mathbf{e_i P^T}\).

\sphinxAtStartPar
La matrice de variance du tableau \({\bf XP^T}\) est, dans le cas où les variables sont centrées :
\({\bf (XP^T)^TD(XP^T) }= {\bf PVP^T}\).
L’inertie du nuage projeté est donc égale à \(Tr({\bf PVP^TM})\), où \({\bf M}\) est une matrice symétrique définie positive de taille \(d\), définissant la distance entre deux individus

\sphinxAtStartPar
\(d^2(\mathbf{e_i},\mathbf{e_j}) = (\mathbf{e_i}-\mathbf{e_j})^T{\bf M}(\mathbf{e_i}-\mathbf{e_j})\)

\sphinxAtStartPar
Mais

\sphinxAtStartPar
\(\begin{eqnarray*}
Tr({\bf PVP^TM})&=&Tr({\bf PVMP})\quad \textrm{car }{\bf P^TM}={\bf MP}\\
&=& Tr({\bf VMP^2})\quad \textrm{car }Tr({\bf AB})=Tr({\bf BA})\\
&=&Tr({\bf VMP})\quad \textrm{car } P\textrm{ est une projection}
\end{eqnarray*}\)

\sphinxAtStartPar
Le problème posé est donc de trouver la projection \({\bf P}\), de rang \(k\) maximisant \(Tr({\bf VMP})\). La projection \({\bf P}\) réalisant cette optimisation donnera alors \(F_k\).

\sphinxAtStartPar
L’analyse en composantes principales consiste alors, de manière itérative, à chercher un sous\sphinxhyphen{}espace de dimension 1 d’inertie maximale, puis le sous\sphinxhyphen{}espace de dimension 1 orthogonal au précédent d’inertie maximale et ainsi de suite. Elle s’appuie sur le résultat suivant :
\label{acp:theorem-0}
\begin{sphinxadmonition}{note}{Theorem 12}



\sphinxAtStartPar
Soit \(F_k\) un sous\sphinxhyphen{}espace portant l’inertie maximale. Alors le sous\sphinxhyphen{}espace de dimension \(k+1\) portant l’inertie maximale est la somme directe de \(F_k\) et de la droite orthogonale à \(F_k\) portant l’inertie maximale.
\end{sphinxadmonition}


\section{Elements principaux}
\label{\detokenize{acp:elements-principaux}}
\index{ACP@\spxentry{ACP}!axes principaux@\spxentry{axes principaux}}\ignorespaces 
\index{Composantes principales@\spxentry{Composantes principales}}\ignorespaces 

\subsection{Axes principaux}
\label{\detokenize{acp:axes-principaux}}\label{\detokenize{acp:index-3}}
\sphinxAtStartPar
Rechercher un sous\sphinxhyphen{}espace de dimension 1 d’inertie maximale revient à rechercher une droite de \(\mathbb{R}^n\) passant par le centre de gravité des données \(\mathbf{g}\) maximisant l’inertie du nuage projeté sur cet axe. Soit \(\mathbf{a}\) un vecteur directeur de cette droite. La projection orthogonale sur la droite est définie par la matrice de projection

\sphinxAtStartPar
\(\mathbf P=\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\)

\sphinxAtStartPar
L’inertie du nuage projeté sur \(Lin(\mathbf{a})\) vaut alors
\(\begin{eqnarray*}
Tr({\bf VMP})&=&Tr\left ({\bf VM}\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\right )\\
&=& \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr({\bf VM}\mathbf{a}\mathbf{a^T}{\bf M})\\
&=& \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr(\mathbf{a^T}{\bf MVM}\mathbf{a})\quad \text{car } Tr(\mathbf{AB})=Tr(\mathbf{BA})\\
&=& \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}\mathbf{a^T}{\bf MVM}\mathbf{a}\quad \text{car } \mathbf{a^T}{\bf MVM}\mathbf{a}\in\mathbb{R}
\end{eqnarray*}\)

\sphinxAtStartPar
La matrice \({\bf MVM}\) est la matrice d’inertie du nuage (égale à la matrice de variance\sphinxhyphen{}covariance si \({\bf M}=\mathbb I\)).  Maximiser cette quantité revient à annuler sa dérivée par rapport à \(\mathbf{a}\) d’où :

\sphinxAtStartPar
\(
\frac{d}{d\mathbf{a}}\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}}=\frac{(\mathbf{a^T}{\bf M}\mathbf{a})2{\bf MVM}\mathbf{a}-(\mathbf{a^T}{\bf MVM}\mathbf{a})2{\bf M}\mathbf{a}}{(\mathbf{a^T}{\bf M}\mathbf{a})^2}
\)
et donc

\sphinxAtStartPar
\({\bf MVM}\mathbf{a}=\left (\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}} \right ){\bf M}\mathbf{a}\)

\sphinxAtStartPar
soit \({\bf VM}\mathbf{a}=\lambda \mathbf{a}\) car \({\bf M}\) est de rang plein. Donc \(\mathbf{a}\) est vecteur propre de \({\bf VM}\), et \(\lambda\) est la plus grande des valeurs propres de \({\bf VM}\). Or \({\bf M}\) est symétrique, elle est diagonalisable sur une base de vecteurs propres orthonormés et on a le résultat suivant :
\label{acp:theorem-1}
\begin{sphinxadmonition}{note}{Theorem 13}



\sphinxAtStartPar
Le sous\sphinxhyphen{}espace \(F_k\) de dimension \(k\) portant l’inertie maximale est engendré par les \(k\) premiers vecteurs propres de \({\bf VM}\)
\end{sphinxadmonition}

\sphinxAtStartPar
Les droites portées par ces vecteurs propres sont les axes principaux. Dans la suite on supposera \(\mathbf{a}\) \(\mathbf M\)\sphinxhyphen{}normé.


\subsection{Facteurs principaux}
\label{\detokenize{acp:facteurs-principaux}}
\index{ACP@\spxentry{ACP}!Facteurs principaux@\spxentry{Facteurs principaux}}\ignorespaces 
\index{Facteurs principaux@\spxentry{Facteurs principaux}}\ignorespaces 
\sphinxAtStartPar
On associe à  \(Lin(\mathbf{a})\) la forme linéaire \(\mathbf{u}\), coordonnée orthogonale sur l’axe \(Lin(\mathbf{a})\). Le vecteur \(\mathbf{u}\) définit une combinaison linéaire des variables descriptives \(x^1\cdots x^d\). A l’axe principal \(\mathbf{a}\) est associé le facteur principal \(\mathbf{u}=\mathbf{Ma}\). Puisque \(\mathbf{a}\) est vecteur propre de \({\bf VM}\), on peut alors écrire

\sphinxAtStartPar
\({\bf MVM}\mathbf{a}=\lambda {\bf M}\mathbf{a}\)

\sphinxAtStartPar
et donc \({\bf MV}\mathbf{u}=\lambda \mathbf{u}\).  Les facteurs principaux sont donc les vecteurs propres de \({\bf MV}\)


\subsection{Composantes principales}
\label{\detokenize{acp:composantes-principales}}
\sphinxAtStartPar
Les composantes principales sont les éléments de \(\mathbb{R}^n\) définis par \(\mathbf{c_i}=\mathbf{Xu_i}\). Ce sont donc les vecteurs coordonnées des projections orthogonales des individus sur les axes propres \(\mathbf{a_i}\).  Ce sont donc les combinaisons linéaires des \(x^1\cdots x^p\) de variance maximale sous la contrainte \(\mathbf{u_i}^T{\bf M}\mathbf{u_i}=1\), et cette variance est égale à la valeur propre \(\lambda_i\) associée à \(\mathbf{a_i}\).

\sphinxAtStartPar
En pratique, l’analyse en composantes principales consiste à calculer les \(\mathbf{u}\) par diagonalisation de \({\bf MV}\), puis à calculer les \(\mathbf{c}=\mathbf{Xu}\). Le calcul explicite des vecteurs propres \(\mathbf{a}\) n’a que peu d’intérêt.


\subsection{Reconstitution}
\label{\detokenize{acp:reconstitution}}
\sphinxAtStartPar
Il est possible de reconstituer le tableau \({\bf X}\) centré des données (ou une approximation par une matrice de rang \(k\)) en utilisant les composantes. En effet, puisque \(\mathbf{Xu_j}=\mathbf{c_j}\) on a

\sphinxAtStartPar
\({\bf X}\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}} = \displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)

\sphinxAtStartPar
Mais \(\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}}=\mathbb{I}\)  car les \(\mathbf{u_j}\) sont orthonormés pour la métrique \({\bf M^{-1}}\) donc

\sphinxAtStartPar
\({\bf X}=\displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)
et si l’on s’intéresse à l’approximation de \({\bf X}\) on ne somme que les \(k\) premiers termes.

\sphinxAtStartPar
A noter que lorsque \({\bf M}=\mathbb{I}, {\bf X}= \displaystyle\sum_j\sqrt{\lambda_j}\mathbf{z_j}\mathbf{v_j^T}\) où les \(\mathbf{z_j}\) sont les vecteurs propres unitaires de \({\bf XX^T}\) et les \(\mathbf{v_j}\) les vecteurs propres unitaires de \({\bf X^TX}\) (décomposition dite en valeurs singulières).


\section{Interprétation des résultats}
\label{\detokenize{acp:interpretation-des-resultats}}

\section{Quelle dimension pour \protect\(F_k\protect\) ?}
\label{\detokenize{acp:quelle-dimension-pour-f-k}}
\sphinxAtStartPar
Le but premier de l’ACP est de réduire la dimension pour permettre une visualisation efficace des données, tout en préservant l’information (ici représentée par la variance du nuage de points).  Il faut donc se doter d’outils permettant de répondre à la question : quelle dimension pour \(F_k\) ? Il n’y a pas de réponse théorique satisfaisante, l’essentiel étant d’avoir une représentation suffisamment expressive pour permettre une interprétation correcte du nuage de points.
En préambule, il convient de remarquer que la réduction de dimension ne sera possible que si les variables \(x^1\cdots x^d\) ne sont pas indépendantes.


\subsection{Critère théorique}
\label{\detokenize{acp:critere-theorique}}
\sphinxAtStartPar
On détermine ici si les valeurs propres sont significativement différentes entre elles à partir d’un certain rang : si la réponse est négative on conserve les
premières valeurs propres.

\sphinxAtStartPar
On fait l’hypothèse que les \(n\) individus proviennent d’un tirage aléatoire dans une population gaussienne  où \(\lambda_{k+1}=\cdots =\lambda_{d}\). Si l’hypothèse est vérifiée, la moyenne arithmétique \(\alpha\) des \(d-k\) dernières valeurs propres et leur moyenne géométrique \(\gamma\) sont peu différentes. On admet que :

\sphinxAtStartPar
\(c=\left ( n-\frac{2p+11}{6}\right )(d-k) ln\frac{\alpha}{\gamma}\)
suit une loi du \(\chi^2\) de degré de liberté \(\frac{(d-k+2)(d-k-1)}{2}\) et on rejette l’hypothèse d’égalité des \(d-k\) valeurs propres si \(c\) est trop grand.


\subsection{Pourcentage d’inertie}
\label{\detokenize{acp:pourcentage-d-inertie}}
\sphinxAtStartPar
Le critère couramment utilisé est le pourcentage d’inertie totale expliquée, qui s’exprime sur les \(k\) premiers axes par :
\begin{equation*}
\begin{split}\frac{\displaystyle\sum_{j=1}^k \lambda_j}{\displaystyle\sum_{j=1}^d \lambda_j}\end{split}
\end{equation*}
\sphinxAtStartPar
Un seuil par exemple de 90\% d’inertie totale expliquée donne une valeur de \(k\) correspondante. Attention cependant, le pourcentage d’inertie doit faire intervenir le nombre de variables initiales.

\sphinxAtStartPar
\sphinxincludegraphics{{scree}.png}


\subsection{Mesures locales}
\label{\detokenize{acp:mesures-locales}}
\sphinxAtStartPar
Le pourcentage d’inertie expliquée est un critère global qui doit être complété par d’autres considérations. Supposons que le plan \(F_2\) explique une part importante d’inertie, et que, en projection sur ce plan, deux individus soient très proches. Cette proximité peut être illusoire si les deux individus se trouvent éloignés dans l’orthogonal de \(F_2\). Pour prendre en compte ce phénomène, il faut envisager pour chaque individu \(\mathbf{e_i}\) la qualité de sa représentation, souvent exprimée par le \sphinxstylestrong{cosinus de l’angle entre le plan principal et le vecteur \(\mathbf{e_i}\)}. Si ce cosinus est grand, \(\mathbf{e_i}\) est voisin du plan, on peut  alors examiner la position de sa projection sur le plan par rapport à d’autres points.

\sphinxAtStartPar
Dans la figure suivante, \({\bf e_i} \) et \({\bf e_j}\) se projettent sur \(F_2\) en \({\bf p}\) mais sont éloignés dans \(F_2^\perp\).

\sphinxAtStartPar
\sphinxincludegraphics{{proj}.png}


\subsection{Critères empiriques}
\label{\detokenize{acp:criteres-empiriques}}
\index{Kaiser@\spxentry{Kaiser}!critère@\spxentry{critère}}\ignorespaces 
\sphinxAtStartPar
Lorsqu’on travaille sur données centrées réduites, on retient les composantes principales correspondant à des valeurs propres supérieures à 1 (critère de Kaiser) : en effet les composantes principales \(c_j\) étant des combinaisons linéaires des \(z-j\) de variance maximale \(V(c_j)=\lambda\), seules les composantes de variance supérieure à celle des variables initiales présentent un intérêt.


\section{Interprétation des résultats : exemple}
\label{\detokenize{acp:interpretation-des-resultats-exemple}}
\sphinxAtStartPar
Une analyse en composantes principales est réalisée sur un jeu de données composé de \(d\)=9 indicateurs de qualité pour \(n\)=329 villes américaines. Les paragraphes suivants sont illustrés par ces données.


\subsection{Corrélation variables\sphinxhyphen{}facteurs}
\label{\detokenize{acp:correlation-variables-facteurs}}
\sphinxAtStartPar
Pour donner du sens aux composantes principales \(\mathbf{c}\), il faut les relier aux variables initiales \(x^j\) en calculant les coefficients de corrélation linéaire  \(r(\mathbf{c},x^j)\) et en seuillant ces coefficients en valeur absolue.

\sphinxAtStartPar
Lorsque l’on travaille sur des données centrées réduites (métrique \(\mathbf D_{1/\sigma}\)), le calcul de \(r(\mathbf{c},x^j)\) se réduit à

\sphinxAtStartPar
\(r(\mathbf{c},x^j)=\frac{\mathbf{c}^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}\)

\sphinxAtStartPar
Or \(\mathbf{c}=Z\mathbf{u}\) où \(\mathbf{u}\), facteur principal associé à \(\mathbf{c}\), est vecteur propre de la matrice de corrélation \(\mathbf R\) associé à \(\lambda\). Donc

\sphinxAtStartPar
\(r(\mathbf{c},x^j)=\frac{\mathbf{u}^T\mathbf Z^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}=\frac{(\mathbf{z^j})^T\mathbf D\mathbf Z\mathbf{u}}{\sqrt{\lambda}}\)

\sphinxAtStartPar
\((\mathbf{z^j})^T\mathbf D\mathbf Z\) est la \(j^e\) ligne de \(\mathbf Z^T\mathbf D\mathbf Z=\mathbf R\) donc \((\mathbf{z^j})^T\mathbf D \mathbf Z \mathbf{u}\) est la \(j^e\) composante de \(\mathbf R\mathbf{u}=\lambda \mathbf{u}\) d’où

\sphinxAtStartPar
\(r(\mathbf{c},x^j)=\sqrt{\lambda}u_j\)

\sphinxAtStartPar
Ces calculs s’effectuent pour chaque composante principale. Pour un couple de composantes principales \(\mathbf{c_1}\) et \(\mathbf{c_2}\) par exemple on représente fréquemment les corrélations sur une figure appelée « cercle des corrélations» où chaque variable \(x^j\) est repérée par un point d’abscisse \(r(\mathbf{c_1},x^j)\) et d’ordonnée \(r(\mathbf{c_2},x^j)\).

\sphinxAtStartPar
\sphinxincludegraphics{{cercle}.png}
\label{acp:remark-2}
\begin{sphinxadmonition}{note}{Remark 14}



\sphinxAtStartPar
Attention de ne pas interpréter des proximités entre points variables, si ceux\sphinxhyphen{}ci ne sont pas proches de la circonférence.
\end{sphinxadmonition}

\sphinxAtStartPar
Notons que dans le cas de la métrique \(D_{1/\sigma}\), le cercle des corrélations est la projection de l’ensemble des variables centrées\sphinxhyphen{}réduites sur le sous\sphinxhyphen{}espace engendré par \(\mathbf{c_1},\mathbf{c_2}\). En ce sens, le cercle de corrélation est le pendant, dans l’espace des variables, de la projection des individus sur le premier plan principal.


\section{Positionnement des individus}
\label{\detokenize{acp:positionnement-des-individus}}
\sphinxAtStartPar
Dire que \(\mathbf{c_1}\) est très corrélée à \(x^j\) signifie que les individus ayant une forte coordonnée positive sur l’axe 1 sont caractérisés par une valeur de \(x^j\) nettement supérieure à la moyenne.

\sphinxAtStartPar
Il est très utile aussi de calculer pour chaque axe la contribution apportée par les divers individus à cet axe. Si \(c_{ki}\) est la valeur de la composante \(k\) pour le \(i^e\) individu, alors par construction
\begin{equation*}
\begin{split}\displaystyle\sum_{i=1}^np_ic_{ki}^2=\lambda_k\end{split}
\end{equation*}
\sphinxAtStartPar
où \(p_i\) est le poids de l’individu \(i\). On appelle alors contribution de l’individu \(i\) à la composante \(\mathbf{c_k}\) la quantité \(\frac{p_ic_{ki}^2}{\lambda_k}\). Dans le cas où le poids est différent de \(1/n\) (certains individus sont “plus importants” que d’autres), la contribution est riche d’interprétation. Dans le cas contraire, elle n’apporte rien de plus que les coordonnées de l’individu.

\sphinxAtStartPar
On peut alors positionner les individus sur les sous\sphinxhyphen{}espaces des premières composantes principales (plans factoriels). La figure suivante présente le positionnement de 329 villes américaines, où les 9 variables de qualité de vie précédentes ont été mesurées. Par soucis de lisibilité, seul les villes qui contribuent le plus à la création de la première composante principale ont leurs noms inscrits.

\sphinxAtStartPar
\sphinxincludegraphics{{individus}.png}

\sphinxAtStartPar
On peut également superposer les deux informations précédentes pour corréler le positionnement des villes selon les variables originales. La figure suivante présente les 329 villes précédentes, plongées dans \(F_3\), les  anciennes variables étant matérialisées par des vecteurs dont la direction et la norme indiquent à quel point chaque variable contribue aux 3 premières composantes principales.

\sphinxAtStartPar
\sphinxincludegraphics{{biplot}.png}

\sphinxAtStartPar
Il n’est pas souhaitable, et ceci surtout pour les premières composantes,  qu’un individu ait une contribution excessive car cela serait un facteur d’instabilité, le fait de retirer cet individu modifiant profondément le résultat de l’analyse. Si ce cas se produisait il y aurait intérêt à effectuer l’analyse en éliminant cet individu puis en le mettant en élément supplémentaire, s’il ne s’agit pas d’une donnée erronée qui a été ainsi mise en évidence.


\section{Facteur de taille, facteur de forme}
\label{\detokenize{acp:facteur-de-taille-facteur-de-forme}}
\sphinxAtStartPar
Le théorème de Frobenius stipule qu’une matrice symétrique n’ayant que des termes positifs admet un premier vecteur propre dont toutes les composantes sont de même signe. Si ce signe est positif, la première composante est alors corrélée positivement avec toutes les variables et les individus sont rangés sur l’axe 1 par valeurs croissantes de l’ensemble des variables. Si de plus les corrélations entre variables sont du même ordre de grandeur, la première composante principale est proportionnelle à la moyenne des variables initiales. Cette première composante définit alors un facteur de taille.

\sphinxAtStartPar
La deuxième composante principale différencie alors des individus de “taille” semblable, on l’appelle souvent facteur de forme.


\section{Ajout de variable et ou d’individu}
\label{\detokenize{acp:ajout-de-variable-et-ou-d-individu}}
\sphinxAtStartPar
Toutes les interprétations précédentes expliquent les résultats à l’aide des données initiales, qui ont permis de les calculer. On risque alors de prendre pour une propriété intrinsèque des données un simple artefact de la méthode (par exemple il existe de fortes corrélations entre la première composante principale et certaines variables, puisque \(\mathbf{c_1}\) maximise \(\sum_j r^2(\mathbf{c},x^j)\)).

\sphinxAtStartPar
En revanche une forte corrélation entre une composante principale et une variable qui n’a pas servi à l’analyse sera significative. D’où la pratique courante de partager en deux groupes l’ensemble des variables : d’une part les variables actives qui servent à déterminer les axes principaux, d’autre part les variables passives ou supplémentaires que l’on relie a posteriori aux composantes principales. On distingue alors les variables supplémentaires suivant leur type, numérique (à placer dans les cercles de corrélation) ou qualitative (donnée d’une partition des \(n\) individus en \(k\) classes).


\section{Exemple}
\label{\detokenize{acp:exemple}}
\sphinxAtStartPar
On étudie les consommations annuelles en 1972, exprimées en devises, de 8 denrées alimentaires (les variables), les individus étant 8 catégories socio\sphinxhyphen{}professionnelles (CSP) . Les données sont des moyennes par CSP :

\sphinxAtStartPar
\(
\begin{array}{|c|cccccccc|}
\hline
  &PAO  &PAA  &VIO& VIA&  POT&  LEC &RAI& PLP\\
\hline
AGRI  &167  &1  &163& 23& 41  &8& 6 &6\\
SAAG  &162& 2 &141& 12  &40 &12&  4&  15\\
PRIN  &119& 6 &69 &56 &39&  5 &13 &41\\
CSUP  &87 &11 &63 &111& 27& 3 &18 &39\\
CMOY  &103  &5  &68 &77 &32&  4 &11 &30\\
EMPL  &111  &4  &72 &66 &34&  6 &10 &28\\
OUVR  &130  &3  &76 &52 &43&  7 &7  &16\\
INAC  &138  &7  &117  &74&  53& 8 &12 &20\\
\hline
\end{array}
\)

\sphinxAtStartPar
avec les notations suivantes :

\sphinxAtStartPar
AGRI = Exploitants agricoles, SAAG= Salariés agricoles,   PRIN = Professions indépendantes, CSUP = Cadres supérieurs, CMOY= Cadres moyens, EMPL= Employés, OUVR = Ouvriers, INAC = Inactifs.

\sphinxAtStartPar
et

\sphinxAtStartPar
PAO = Pain ordinaire, PAA = Autre pain, VIO = Vin ordinaire, VIA=Autre vin, POT= Pommes de terre, LEC=Légumes secs, RAI=Raisin de table, PLP= Plats préparés.

\sphinxAtStartPar
La matrice de corrélation des variables est alors

\sphinxAtStartPar
\(\begin{pmatrix}
   1.0000   &  -.7737    & 0.9262    & -.9058    & 0.6564  &   0.8886   &  -.8334  &   -.8558\\
    -.7737    & 1.0000    & -.6040    & 0.9044    & -.3329    & -.6734    & 0.9588    & 0.7712\\
   0.9262    & -.6040    & 1.0000    & -.7502    & 0.5171    & 0.7917   &  -.6690     &-.8280\\
  -.9058    & 0.9044    & -.7502    & 1.0000    & -.4186    & -.8386    & 0.9239     &0.7198\\
    0.6564   &  -.3329    & 0.5171    & -.4186    & 1.0000   &  0.6029   &  -.4099    & -.5540\\
  0.8886   &  -.6734    & 0.7917   &  -.8386    & 0.6029   &  1.0000   &  -.8245    & -.7509\\
  -.8334    & 0.9588    & -.6690    & 0.9239    & -.4099    & -.8245    & 1.0000     &0.8344\\
   -.8558    & 0.7712   &  -.8280   &  0.7198   &  -.5540    & -.7509  &   0.8344    & 1.0000\\
\end{pmatrix}\)

\sphinxAtStartPar
et son analyse spectrale donne

\sphinxAtStartPar
\(\begin{array}{|c||c|c|c|}
\hline
                      &    \textrm{Valeur propre}  &      \textrm{Variance expliquée}  &  \textrm{Variance cumulative expliquée}\\
\hline
                     1  &  6.20794684      &      0.7760  &      0.7760\\
                     2   & 0.87968139      &      0.1100    &    0.8860\\
                     3    &0.41596112    &        0.0520      &  0.9379\\
                     4    &0.30645467    &        0.0383      &  0.9763\\
                     5    &0.16844150    &        0.0211      &  0.9973\\
                     6    &0.01806771    &       0.0023       & 0.9996\\
                     7    &0.00344677    &       0.0004       & 1.0000\\
                     8    &0.00000000        &              0.0000      &  1.0000\\
\hline
\end{array}\)

\sphinxAtStartPar
Le critère de Kaiser  conduit à sélectionner un seul axe, qui retient 77\% de l’inertie totale. L’axe 2 retenant 11\% de l’inertie, il peut être  intéressant de le rajouter à l’étude pour expliquer près de 90\% de la variance des données. Les suivantes représentent les variables et les individus dans le plan des deux premiers vecteurs propres.

\sphinxAtStartPar
\sphinxincludegraphics{{ex1}.png}

\sphinxAtStartPar
L’interprétation de ce plan se fait séquentiellement, pour chaque axe et chaque nuage de points, en regardant les contributions à la formation des axes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Axe 1 :

\end{itemize}

\sphinxAtStartPar
1\sphinxhyphen{} \sphinxstylestrong{Variables} :  les variables contribuant le plus à la formation de l’axe 1 sont celles dont les coordonnées sur cet axe sont proches de 1 en valeur absolue.
PAA et VIO sont très proches de la contribution moyenne, on les intègre donc dans l’interprétation de l’axe si elles vont dans le sens de l’interprétation que l’on peut en faire, sans elles. L’axe 1 oppose les individus consommant du pain ordinaire, des légumes secs (et éventuellement du vin ordinaire) à ceux qui consomment du raisin, du vin (éventuellement du pain) plus sophistiqué et des plats préparés. L’axe 1, et donc la première composante principale, mesure la répartition entre aliments ordinaires bon marché et aliments plus recherchés.

\sphinxAtStartPar
Toutes les variables sont bien représentées sur l’axe (la qualité de représentation est égale à la coordonnée au carré). D’un point de vue graphique, une variable bien représentée est proche du bord du cercle des corrélation et à proximité de l’axe. La première composante principale explique donc correctement tous les types de consommations alimentaires.

\sphinxAtStartPar
2\sphinxhyphen{} \sphinxstylestrong{Individus} : de même, les individus contribuant le plus à la formation de l’axe 1 sont ceux dont les coordonnées sur cet axe sont les plus élevées en valeur absolue. Le premier axe met donc en opposition les agriculteurs et les cadres supérieurs quant à leurs habitudes alimentaires. Les autres catégories socio\sphinxhyphen{}professionnelles, assez bien représentées sur l’axe à l’exception des inactifs (cf. contributions des individus sur l’axe 1), s’échelonnent suivant la hiérarchie habituelle. Elles sont bien expliquées par l’axe.
\begin{itemize}
\item {} 
\sphinxAtStartPar
Axe 2 :

\end{itemize}

\sphinxAtStartPar
1\sphinxhyphen{} \sphinxstylestrong{Variables} : L’axe 2 est défini par les variables POT et PAA. Compte tenu de la différence de contribution existant entre ces deux variables, de la contribution élevée de POT (55\%), et de la qualité de représentation moyenne de PAA, la deuxième composante principale peut être considérée comme essentiellement liée à la consommation de pommes de terre. Les variables, à l’exception de POT et de PAA (dans une moindre mesure) sont assez mal représentées sur l’axe. La deuxième composante principale n’explique donc qu’un aspect très particulier de la consommation alimentaire.

\sphinxAtStartPar
2\sphinxhyphen{} \sphinxstylestrong{Individus} : Pour repérer les individus ayant une contribution significative, on compare les coordonnées des individus sur l’axe 2, à la racine de la deuxième valeur propre  =0,94, le signe donnant le sens de contribution.

\sphinxAtStartPar
L’axe 1 reflète donc l’opposition qui existe entre les catégories socio\sphinxhyphen{}professionnelles dans leur alimentation, opposant les CSP modestes qui consomment des produits basiques aux catégories favorisées qui consomment des produits plus recherchés. L’axe 2 reflète quant à lui la particularité des inactifs quant à leur alimentation, fortement composée de pommes de terre (un retour aux données d’origine vient confirmer cette conclusion).


\section{Implémentation}
\label{\detokenize{acp:implementation}}
\sphinxAtStartPar
De nombreuses librairies Python permettent d’utiliser facilement l’ACP, notamment \sphinxhref{https://scikit-learn.org/stable/}{scikit\sphinxhyphen{}learn} qui propose une méthode \sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca\#sklearn.decomposition.PCA}{PCA}.

\sphinxAtStartPar
Nous proposons ici d’implémenter entièrement l’ACP, pour bien comprendre les mécanismes de cette approche.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd} 

\PYG{n}{pd}\PYG{o}{.}\PYG{n}{options}\PYG{o}{.}\PYG{n}{mode}\PYG{o}{.}\PYG{n}{chained\PYGZus{}assignment} \PYG{o}{=} \PYG{k+kc}{None}

\PYG{c+c1}{\PYGZsh{} Données}
\PYG{n}{vins} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./data/vins.csv}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{header}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\PYG{n}{cat\PYGZus{}vins} \PYG{o}{=} \PYG{n}{vins}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:} \PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{vins}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{vins}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{n}\PYG{p}{,}\PYG{n}{d} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{ind} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{variables} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} a}\PYG{l+s+s1}{lcool}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acide malique}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cendres}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcalinité}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magnésium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{phénols}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{,} 
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flavonoïdes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{non flavanoïdes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{proanthocyanidines}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{couleur}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{teinte}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OD280/OD315}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{proline}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Affichage d\PYGZsq{}un tableau}
\PYG{k}{def} \PYG{n+nf}{print\PYGZus{}tab} \PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{tab}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{*} \PYG{l+m+mi}{12} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{c} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{r} \PYG{o}{+}\PYG{o}{=} \PYG{n}{c}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{r} \PYG{o}{+}\PYG{o}{=} \PYG{n}{c}\PYG{p}{[}\PYG{n}{d} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{r} \PYG{o}{+}\PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{  }\PYG{l+s+si}{\PYGZpc{}8.8s}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{ind}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{r} \PYG{o}{+}\PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s2}{ }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}
        \PYG{n}{r} \PYG{o}{+}\PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{d} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{return} \PYG{n}{r}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Préparation des données}
\label{\detokenize{acp:preparation-des-donnees}}

\subsection{Données centrées}
\label{\detokenize{acp:id1}}
\sphinxAtStartPar
\(g=X^TD{\bf 1}\) = Vecteur des moyennes arithmétiques de chaque variable

\sphinxAtStartPar
\(D=\frac{1}{n}I\) = Matrice diagonale de poids, chaque \(d_{ii}\) donnant l’importance de l’individu \(i\) dans les données

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Xt} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{D} \PYG{o}{=} \PYG{l+m+mf}{1.}\PYG{o}{/}\PYG{n}{n} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{un} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{n} \PYG{o}{*} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{g} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{Xt}\PYG{p}{,} \PYG{n}{D}\PYG{p}{)}\PYG{p}{,} \PYG{n}{un}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g = }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{g}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
g = 
 [[1.30006180e+01]
 [2.33634831e+00]
 [2.36651685e+00]
 [1.94949438e+01]
 [9.97415730e+01]
 [2.29511236e+00]
 [2.02926966e+00]
 [3.61853933e\PYGZhy{}01]
 [1.59089888e+00]
 [5.05808988e+00]
 [9.57449438e\PYGZhy{}01]
 [2.61168539e+00]
 [7.46893258e+02]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\(Y = X - {\bf 1} g^T = (I - {\bf 11}^TD)X\) = Tableau centré associé à \(X\)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{gt} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{g}\PYG{p}{)}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{X} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{un}\PYG{p}{,} \PYG{n}{gt}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Données réduites}
\label{\detokenize{acp:id2}}
\sphinxAtStartPar
\(V=X^TDX-gg^T=Y^TDY\) = Matrice de variance/covariance.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Yt} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}
\PYG{n}{V} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{Yt}\PYG{p}{,} \PYG{n}{D}\PYG{p}{)}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
\(Z = Y D_{1/\sigma}\) = Matrice des données centrées réduites

\sphinxAtStartPar
\(R = D_{1/\sigma}VD_{1/\sigma} = Z^T  D  Z\) = Matrice (symétrique) de variance/covariance des données centrées réduites.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sigma} \PYG{o}{=} \PYG{n}{seq} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{Xt}\PYG{p}{]}
\PYG{n}{i\PYGZus{}sigma} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{o}{/}\PYG{n}{s} \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sigma}\PYG{p}{]}
\PYG{n}{D\PYGZus{}sigma} \PYG{o}{=} \PYG{n}{i\PYGZus{}sigma} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}
\PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{D\PYGZus{}sigma}\PYG{p}{)}
\PYG{n}{R} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{D\PYGZus{}sigma}\PYG{p}{,} \PYG{n}{V}\PYG{p}{)}\PYG{p}{,} \PYG{n}{D\PYGZus{}sigma}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Matrice de corrélation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{dadc8d6f0827a8670b00d3dd4b51016465ea4e90b225ace4cfd49e35483f74ba}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Inertie du nuage de points}
\label{\detokenize{acp:inertie-du-nuage-de-points}}
\sphinxAtStartPar
\(M\) est une matrice symétrique définie positive correspondant à la métrique
\begin{itemize}
\item {} 
\sphinxAtStartPar
Si \(M=D^2_{1/\sigma}\) on calcule \( \frac{1}{n}\displaystyle\sum_{i=1}^n (e_i - g)^T M (e_i-g) = \frac{1}{n}\displaystyle\sum_{i=1}^n (y_i)^T M y_i = Tr(VM)\)

\item {} 
\sphinxAtStartPar
Si \(M=I\) on calcule \( \frac{1}{n}\displaystyle\sum_{i=1}^n (z_i)^T M z_i = Tr(RM)\)

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{calcul\PYGZus{}inertie\PYGZus{}somme} \PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{inertie} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{inertie} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} 
    \PYG{k}{return} \PYG{n}{inertie} \PYG{o}{/} \PYG{n}{n}
    
\PYG{k}{def} \PYG{n+nf}{calcul\PYGZus{}inertie\PYGZus{}trace} \PYG{p}{(}\PYG{n}{V}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{trace} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{V}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Si les données sont centrées mais pas encore réduites  on travaille avec Y et V}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{D\PYGZus{}sigma}\PYG{p}{,} \PYG{n}{D\PYGZus{}sigma}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{calcul\PYGZus{}inertie\PYGZus{}somme}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{calcul\PYGZus{}inertie\PYGZus{}trace}\PYG{p}{(}\PYG{n}{V}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Si les données sont centrées réduites, on travaille avec Z et R}
\PYG{n}{M} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{calcul\PYGZus{}inertie\PYGZus{}somme}\PYG{p}{(}\PYG{n}{Z}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{calcul\PYGZus{}inertie\PYGZus{}trace}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{n}{M}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
12.999999999999995
13.000000000000004
12.999999999999998
13.000000000000004
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Analyse spectrale}
\label{\detokenize{acp:analyse-spectrale}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eigenvalues}\PYG{p}{,}\PYG{n}{eigenvectors} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{eig}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}
\PYG{n}{eigenvalues} \PYG{o}{=} \PYG{n+nb}{sorted}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{,} \PYG{n}{reverse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{u} \PYG{o}{=} \PYG{p}{[}\PYG{n}{eigenvectors}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Calcul des composantes principales}
\label{\detokenize{acp:calcul-des-composantes-principales}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{c} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{c}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{matmul} \PYG{p}{(}\PYG{n}{Z}\PYG{p}{,} \PYG{n}{u}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Pourcentage d’inertie expliquée par un axe}
\label{\detokenize{acp:pourcentage-d-inertie-expliquee-par-un-axe}}
\sphinxAtStartPar
Pourcentage d’inertie cumulée expliquée par les \(k\) premiers axes : \(\frac{\displaystyle\sum_{j=1}^k\lambda_j}{\displaystyle\sum_{j=1}^d\lambda_j}\)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{i\PYGZus{}lambda} \PYG{o}{=} \PYG{p}{[}\PYG{n}{l}\PYG{o}{/}\PYG{n}{d} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n}{eigenvalues}\PYG{p}{]}
\PYG{n}{i\PYGZus{}cum} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{)}\PYG{o}{/}\PYG{n}{d}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{d}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{i\PYGZus{}lambda}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Valeurs propres}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{d}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{i\PYGZus{}cum}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+s1}{e variance expliquée}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{2dad4ba32c4808dbcf57b41305c7abe36a143e9f223385008bc71d67354fc559}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Critère de Kaiser}
\label{\detokenize{acp:critere-de-kaiser}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nb\PYGZus{}l} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{)}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{On retient }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{nb\PYGZus{}l}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ axes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
On retient 3 axes
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Analyse des résultats}
\label{\detokenize{acp:analyse-des-resultats}}

\subsection{Corrélation variables/facteurs}
\label{\detokenize{acp:id3}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{r} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{eigenvalues}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{o}{*} \PYG{n}{u}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Cercle des corrélations pour un couple de composantes principales}
\label{\detokenize{acp:cercle-des-correlations-pour-un-couple-de-composantes-principales}}
\sphinxAtStartPar
Pour \(c_1\) et \(c_2\), chaque variable \(x_j\) est repérée par un point d’abscisse \(r(c_1,x^j)\) et d’ordonnée \(r(c_2, x_j)\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{i1} \PYG{o}{=} \PYG{n}{i\PYGZus{}lambda}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{100}
\PYG{n}{i2} \PYG{o}{=} \PYG{n}{i\PYGZus{}lambda}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{100}
\PYG{n}{i3} \PYG{o}{=} \PYG{n}{i\PYGZus{}lambda}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{100}
\PYG{n}{i12} \PYG{o}{=} \PYG{n}{i1} \PYG{o}{+} \PYG{n}{i2}
\PYG{n}{i13} \PYG{o}{=} \PYG{n}{i1} \PYG{o}{+} \PYG{n}{i3}
\PYG{n}{i23} \PYG{o}{=} \PYG{n}{i2} \PYG{o}{+} \PYG{n}{i3}


\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{131}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP1/CP2 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i12} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i1} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 2 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i2} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{facecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{variables}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{132}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP1/CP3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i13} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i1} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i3} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{facecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{variables}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{133}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP2/CP3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i23} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i2} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i3} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{facecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{variables}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{r}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{890895d5f844f44dec5929957163b9bab5ada18fafe8471de9dd2494e8c4585a}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Contribution des variables}
\label{\detokenize{acp:contribution-des-variables}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{contributions\PYGZus{}variables} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range} \PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{line} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range} \PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{line}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{contributions\PYGZus{}variables}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{line}\PYG{p}{)}
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{print\PYGZus{}tab} \PYG{p}{(}\PYG{n}{d}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{variables}\PYG{p}{,} \PYG{n}{contributions\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
  \PYGZpc{} alcool	0.02 	0.23 	0.04 	0.00 	0.07 	0.05 	0.00 	0.00 	0.16 	0.07 	0.26 	0.05 	0.04
  acide ma	0.06 	0.05 	0.01 	0.29 	0.00 	0.29 	0.18 	0.00 	0.00 	0.01 	0.01 	0.01 	0.10
   cendres	0.00 	0.10 	0.39 	0.05 	0.02 	0.02 	0.02 	0.02 	0.03 	0.00 	0.09 	0.25 	0.00
  alcalini	0.06 	0.00 	0.37 	0.00 	0.00 	0.01 	0.08 	0.01 	0.18 	0.00 	0.04 	0.23 	0.00
  magnésiu	0.02 	0.09 	0.02 	0.12 	0.53 	0.00 	0.10 	0.00 	0.02 	0.00 	0.07 	0.01 	0.00
   phénols	0.16 	0.00 	0.02 	0.04 	0.02 	0.01 	0.00 	0.22 	0.16 	0.09 	0.08 	0.09 	0.10
  flavonoï	0.18 	0.00 	0.02 	0.02 	0.01 	0.00 	0.00 	0.69 	0.04 	0.00 	0.00 	0.00 	0.03
  non flav	0.09 	0.00 	0.03 	0.04 	0.25 	0.07 	0.35 	0.01 	0.05 	0.00 	0.04 	0.01 	0.05
  proantho	0.10 	0.00 	0.02 	0.16 	0.02 	0.28 	0.14 	0.01 	0.14 	0.01 	0.04 	0.06 	0.02
   couleur	0.01 	0.28 	0.02 	0.00 	0.01 	0.18 	0.05 	0.00 	0.00 	0.37 	0.00 	0.00 	0.08
    teinte	0.09 	0.08 	0.01 	0.18 	0.03 	0.01 	0.05 	0.01 	0.19 	0.07 	0.01 	0.00 	0.27
  OD280/OD	0.14 	0.03 	0.03 	0.03 	0.01 	0.07 	0.00 	0.02 	0.01 	0.36 	0.02 	0.00 	0.27
   proline	0.08 	0.13 	0.02 	0.05 	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.33 	0.29 	0.03
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Représentation des individus}
\label{\detokenize{acp:representation-des-individus}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{131}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP1/CP2 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i12} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i1} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 2 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i2} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{ind}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{132}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP1/CP3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i13} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i1} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i3} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{ind}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{133}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CP2/CP3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i23} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 1 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i2} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Axe 3 (}\PYG{l+s+si}{\PYGZpc{}.2f}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}} \PYG{n}{i3} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{} d}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{inertie)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{ind}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{9d89ee34ba73aa750b9b3fd9ca37bf3233ac94e9bf5e9a867c5763d97b7e1bcd}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Contribution des individus}
\label{\detokenize{acp:contribution-des-individus}}
\sphinxAtStartPar
\(\frac{p_ic_{ki}^2}{\lambda_k}\)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{contributions\PYGZus{}individus} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range} \PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{line} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range} \PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{val} \PYG{o}{=} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{transpose}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{n} \PYG{o}{*} \PYG{n}{eigenvalues}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} 
        \PYG{n}{line}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{val}\PYG{p}{)}
    \PYG{n}{contributions\PYGZus{}individus}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{line}\PYG{p}{)}
    
\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{print\PYGZus{}tab} \PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{contributions\PYGZus{}individus}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.06
         1	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
         2	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
         3	0.02 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
         4	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         5	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         6	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.06
         7	0.01 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
         8	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
         9	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        10	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.01 	0.02 	0.00
        11	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        12	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        13	0.01 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.05 	0.00
        14	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        15	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.03
        16	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        17	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01
        18	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.03
        19	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
        20	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04 	0.01 	0.01
        21	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
        22	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.04
        23	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01
        24	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        25	0.00 	0.00 	0.06 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.01
        27	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00
        28	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        29	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
        30	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.01
        31	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.04 	0.00
        32	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        33	0.00 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        34	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        35	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        36	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.04 	0.00
        37	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        38	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        39	0.01 	0.01 	0.00 	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.01 	0.05 	0.00
        40	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        41	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
        42	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00
        43	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        44	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01
        45	0.00 	0.01 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        46	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        47	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        48	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        49	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
        50	0.01 	0.00 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.02
        51	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00
        52	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.01
        53	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        54	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        55	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        56	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        57	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        58	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        59	0.00 	0.02 	0.08 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.00 	0.00 	0.00 	0.05 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
        61	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.00 	0.02
        62	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04 	0.00 	0.01
        63	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11
        64	0.00 	0.01 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.06
        65	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.03
        66	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        67	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
        68	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.00 	0.03 	0.02 	0.02 	0.00 	0.00
        69	0.00 	0.00 	0.01 	0.01 	0.12 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00 	0.00 	0.01
        70	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02
        71	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.05 	0.00 	0.03 	0.02 	0.03
        72	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.02 	0.00 	0.01
        73	0.01 	0.00 	0.04 	0.01 	0.03 	0.00 	0.00 	0.00 	0.02 	0.00 	0.03 	0.11 	0.00
        74	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.02 	0.01 	0.00 	0.01 	0.04 	0.00
        75	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.05
        76	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        77	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        78	0.00 	0.00 	0.01 	0.01 	0.04 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        79	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02
        80	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02
        81	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        82	0.00 	0.01 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        83	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
        84	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.04 	0.05 	0.04
        85	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        86	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
        87	0.00 	0.01 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        88	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        89	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        90	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        91	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        92	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.03
        93	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01
        94	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00 	0.09
        95	0.01 	0.00 	0.00 	0.01 	0.09 	0.02 	0.07 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
        96	0.00 	0.00 	0.01 	0.01 	0.07 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.00
        97	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        98	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.05
        99	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00 	0.01 	0.08
       100	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.02
       101	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
       102	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
       103	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00
       104	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.03
       105	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.11
       106	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       107	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.05
       108	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04
       109	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.04 	0.01 	0.01
       110	0.00 	0.00 	0.00 	0.07 	0.03 	0.02 	0.03 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00
       112	0.00 	0.00 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01
       113	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01
       114	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
       115	0.00 	0.03 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.01 	0.01 	0.00 	0.00 	0.10
       116	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04
       117	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
       118	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       119	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       120	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00
       121	0.00 	0.00 	0.11 	0.00 	0.00 	0.01 	0.00 	0.04 	0.06 	0.02 	0.00 	0.00 	0.00
       122	0.00 	0.00 	0.04 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       123	0.00 	0.00 	0.00 	0.08 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       124	0.00 	0.00 	0.01 	0.09 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00
       125	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       126	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01 	0.01 	0.00 	0.01
       127	0.00 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       128	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       129	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
       130	0.00 	0.00 	0.01 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       131	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
       132	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00
       133	0.01 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.06
       134	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
       135	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       136	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.04 	0.00 	0.01
       137	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       139	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00
       140	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
       141	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.05
       142	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       143	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       144	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.01 	0.00
       145	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.03
       146	0.02 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
       147	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       148	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02
       150	0.01 	0.01 	0.00 	0.00 	0.03 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01
       151	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
       152	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       153	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       154	0.01 	0.00 	0.01 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04
       155	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       156	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       157	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.06 	0.01
       158	0.00 	0.03 	0.01 	0.01 	0.01 	0.09 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.01
       159	0.00 	0.01 	0.00 	0.00 	0.01 	0.07 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
       160	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.02
       161	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       162	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       163	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       164	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       165	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
       166	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.04
       167	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.02
       168	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
       169	0.01 	0.02 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       171	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       172	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       173	0.01 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       174	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       175	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00
       176	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       177	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Tableau des cosinus carrés}
\label{\detokenize{acp:tableau-des-cosinus-carres}}
\sphinxAtStartPar
\(\frac{c_{ki}^2}{\displaystyle\sum_{j=1}^d c_{ji}^2}\)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cosinus\PYGZus{}carres} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{c} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{line} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} on prend la représentation de l\PYGZsq{}individu i sur chacune des composantes}
    \PYG{n}{tot} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{o}{*}\PYG{n}{x} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{line}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{*}\PYG{n}{c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{/}\PYG{n}{tot}\PYG{p}{)}
    \PYG{n}{cosinus\PYGZus{}carres}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{line}\PYG{p}{)}

\PYG{n+nb}{print} \PYG{p}{(}\PYG{n}{print\PYGZus{}tab} \PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{n}{d}\PYG{p}{,} \PYG{n}{ind}\PYG{p}{,} \PYG{n}{cosinus\PYGZus{}carres}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.69 	0.13 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.02 	0.03 	0.01 	0.07
         1	0.43 	0.01 	0.36 	0.01 	0.01 	0.08 	0.00 	0.00 	0.09 	0.01 	0.01 	0.00 	0.00
         2	0.57 	0.10 	0.09 	0.05 	0.01 	0.03 	0.02 	0.00 	0.01 	0.00 	0.13 	0.01 	0.00
         3	0.60 	0.32 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00
         4	0.14 	0.11 	0.58 	0.02 	0.01 	0.02 	0.03 	0.00 	0.02 	0.01 	0.02 	0.04 	0.00
         5	0.60 	0.29 	0.03 	0.02 	0.03 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
         6	0.52 	0.12 	0.08 	0.00 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.02 	0.02 	0.09
         7	0.38 	0.23 	0.00 	0.13 	0.00 	0.18 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.05
         8	0.51 	0.07 	0.25 	0.00 	0.06 	0.00 	0.00 	0.00 	0.02 	0.03 	0.03 	0.02 	0.00
         9	0.71 	0.06 	0.09 	0.01 	0.02 	0.00 	0.07 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00
        10	0.73 	0.10 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.09 	0.00 	0.02 	0.03 	0.00
        11	0.39 	0.05 	0.18 	0.10 	0.07 	0.04 	0.02 	0.00 	0.12 	0.00 	0.03 	0.00 	0.01
        12	0.53 	0.05 	0.09 	0.02 	0.17 	0.01 	0.01 	0.00 	0.03 	0.00 	0.09 	0.00 	0.00
        13	0.50 	0.05 	0.06 	0.00 	0.17 	0.02 	0.09 	0.00 	0.01 	0.02 	0.00 	0.06 	0.00
        14	0.65 	0.15 	0.06 	0.00 	0.04 	0.02 	0.04 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00
        15	0.45 	0.24 	0.00 	0.18 	0.02 	0.02 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.05
        16	0.39 	0.44 	0.06 	0.07 	0.00 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00
        17	0.39 	0.29 	0.07 	0.13 	0.02 	0.01 	0.00 	0.03 	0.00 	0.01 	0.02 	0.01 	0.02
        18	0.54 	0.27 	0.01 	0.04 	0.06 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.04 	0.03
        19	0.45 	0.12 	0.00 	0.02 	0.08 	0.20 	0.00 	0.00 	0.03 	0.01 	0.00 	0.07 	0.00
        20	0.70 	0.04 	0.01 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00 	0.02 	0.13 	0.02 	0.01
        21	0.19 	0.01 	0.14 	0.17 	0.02 	0.24 	0.01 	0.02 	0.00 	0.08 	0.05 	0.07 	0.00
        22	0.74 	0.00 	0.01 	0.00 	0.02 	0.12 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.08
        23	0.50 	0.05 	0.00 	0.03 	0.03 	0.11 	0.08 	0.02 	0.01 	0.04 	0.09 	0.00 	0.04
        24	0.49 	0.02 	0.12 	0.00 	0.05 	0.13 	0.06 	0.01 	0.02 	0.02 	0.00 	0.02 	0.07
        25	0.05 	0.05 	0.77 	0.09 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.44 	0.07 	0.00 	0.01 	0.18 	0.05 	0.03 	0.00 	0.12 	0.01 	0.08 	0.00 	0.03
        27	0.26 	0.00 	0.32 	0.04 	0.02 	0.06 	0.02 	0.00 	0.01 	0.02 	0.09 	0.13 	0.02
        28	0.51 	0.05 	0.21 	0.06 	0.07 	0.04 	0.00 	0.00 	0.00 	0.00 	0.02 	0.03 	0.00
        29	0.65 	0.00 	0.15 	0.01 	0.03 	0.01 	0.00 	0.03 	0.03 	0.00 	0.00 	0.00 	0.08
        30	0.49 	0.12 	0.15 	0.01 	0.03 	0.03 	0.01 	0.00 	0.09 	0.03 	0.02 	0.01 	0.01
        31	0.59 	0.18 	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.02 	0.00 	0.05 	0.10 	0.00
        32	0.52 	0.00 	0.01 	0.11 	0.08 	0.00 	0.19 	0.01 	0.05 	0.01 	0.00 	0.01 	0.01
        33	0.23 	0.17 	0.09 	0.35 	0.00 	0.01 	0.06 	0.00 	0.02 	0.00 	0.03 	0.03 	0.00
        34	0.42 	0.10 	0.05 	0.24 	0.00 	0.11 	0.01 	0.00 	0.01 	0.01 	0.03 	0.01 	0.01
        35	0.75 	0.01 	0.04 	0.02 	0.01 	0.01 	0.09 	0.00 	0.03 	0.01 	0.01 	0.01 	0.02
        36	0.28 	0.06 	0.03 	0.24 	0.01 	0.06 	0.00 	0.00 	0.13 	0.00 	0.01 	0.17 	0.01
        37	0.35 	0.00 	0.00 	0.25 	0.03 	0.05 	0.05 	0.00 	0.00 	0.04 	0.20 	0.00 	0.03
        38	0.37 	0.10 	0.34 	0.10 	0.00 	0.03 	0.00 	0.02 	0.00 	0.01 	0.01 	0.01 	0.01
        39	0.34 	0.17 	0.01 	0.08 	0.09 	0.13 	0.05 	0.00 	0.03 	0.00 	0.03 	0.08 	0.00
        40	0.67 	0.06 	0.00 	0.02 	0.02 	0.05 	0.03 	0.00 	0.05 	0.00 	0.07 	0.01 	0.01
        41	0.08 	0.01 	0.11 	0.30 	0.02 	0.30 	0.00 	0.03 	0.02 	0.00 	0.01 	0.12 	0.00
        42	0.67 	0.09 	0.01 	0.02 	0.01 	0.05 	0.07 	0.00 	0.06 	0.00 	0.00 	0.01 	0.00
        43	0.05 	0.03 	0.01 	0.47 	0.02 	0.24 	0.08 	0.01 	0.07 	0.00 	0.02 	0.00 	0.00
        44	0.66 	0.00 	0.06 	0.09 	0.03 	0.02 	0.00 	0.00 	0.06 	0.00 	0.01 	0.03 	0.03
        45	0.14 	0.34 	0.00 	0.05 	0.00 	0.39 	0.00 	0.00 	0.01 	0.00 	0.03 	0.04 	0.00
        46	0.56 	0.11 	0.02 	0.15 	0.02 	0.08 	0.04 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        47	0.70 	0.04 	0.12 	0.09 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
        48	0.58 	0.22 	0.00 	0.01 	0.02 	0.04 	0.01 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        49	0.57 	0.24 	0.03 	0.00 	0.02 	0.04 	0.00 	0.02 	0.00 	0.04 	0.00 	0.02 	0.01
        50	0.53 	0.00 	0.20 	0.06 	0.00 	0.08 	0.00 	0.00 	0.01 	0.01 	0.08 	0.01 	0.02
        51	0.68 	0.05 	0.00 	0.00 	0.05 	0.00 	0.01 	0.00 	0.10 	0.01 	0.08 	0.03 	0.00
        52	0.68 	0.14 	0.02 	0.00 	0.02 	0.00 	0.00 	0.00 	0.11 	0.00 	0.01 	0.00 	0.01
        53	0.45 	0.32 	0.01 	0.13 	0.03 	0.01 	0.02 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        54	0.56 	0.13 	0.11 	0.01 	0.09 	0.02 	0.02 	0.01 	0.01 	0.00 	0.01 	0.00 	0.03
        55	0.61 	0.18 	0.03 	0.01 	0.08 	0.03 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00
        56	0.66 	0.18 	0.03 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.08 	0.00 	0.01
        57	0.56 	0.17 	0.01 	0.03 	0.06 	0.01 	0.01 	0.00 	0.04 	0.00 	0.08 	0.00 	0.02
        58	0.66 	0.20 	0.01 	0.00 	0.00 	0.01 	0.05 	0.00 	0.03 	0.02 	0.01 	0.00 	0.00
        59	0.03 	0.28 	0.61 	0.03 	0.01 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.14 	0.11 	0.04 	0.49 	0.06 	0.00 	0.07 	0.00 	0.07 	0.01 	0.00 	0.00 	0.02
        61	0.29 	0.06 	0.22 	0.18 	0.01 	0.04 	0.01 	0.01 	0.08 	0.00 	0.07 	0.00 	0.03
        62	0.00 	0.17 	0.35 	0.16 	0.02 	0.04 	0.04 	0.00 	0.04 	0.00 	0.16 	0.01 	0.01
        63	0.33 	0.29 	0.00 	0.04 	0.00 	0.05 	0.09 	0.01 	0.01 	0.00 	0.03 	0.00 	0.15
        64	0.03 	0.32 	0.04 	0.40 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.07 	0.10
        65	0.17 	0.12 	0.07 	0.10 	0.00 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.14 	0.10
        66	0.28 	0.20 	0.23 	0.11 	0.03 	0.09 	0.01 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01
        67	0.00 	0.54 	0.11 	0.00 	0.03 	0.00 	0.17 	0.00 	0.00 	0.11 	0.01 	0.02 	0.00
        68	0.06 	0.00 	0.04 	0.53 	0.03 	0.01 	0.01 	0.01 	0.13 	0.09 	0.09 	0.00 	0.00
        69	0.13 	0.06 	0.05 	0.04 	0.57 	0.01 	0.07 	0.01 	0.04 	0.02 	0.00 	0.00 	0.01
        70	0.29 	0.09 	0.05 	0.16 	0.11 	0.01 	0.00 	0.02 	0.06 	0.02 	0.14 	0.01 	0.05
        71	0.18 	0.06 	0.25 	0.00 	0.04 	0.01 	0.14 	0.01 	0.15 	0.01 	0.09 	0.04 	0.03
        72	0.08 	0.17 	0.00 	0.00 	0.00 	0.04 	0.38 	0.01 	0.16 	0.00 	0.12 	0.00 	0.02
        73	0.23 	0.00 	0.39 	0.03 	0.14 	0.00 	0.01 	0.00 	0.04 	0.00 	0.04 	0.11 	0.00
        74	0.30 	0.15 	0.02 	0.00 	0.09 	0.00 	0.17 	0.09 	0.03 	0.02 	0.02 	0.10 	0.01
        75	0.06 	0.50 	0.20 	0.06 	0.04 	0.00 	0.01 	0.01 	0.00 	0.03 	0.00 	0.00 	0.07
        76	0.01 	0.35 	0.46 	0.00 	0.00 	0.02 	0.03 	0.00 	0.04 	0.00 	0.05 	0.01 	0.02
        77	0.26 	0.24 	0.01 	0.06 	0.14 	0.08 	0.11 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        78	0.09 	0.03 	0.08 	0.05 	0.37 	0.14 	0.22 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        79	0.02 	0.13 	0.38 	0.11 	0.00 	0.05 	0.17 	0.00 	0.01 	0.00 	0.08 	0.01 	0.04
        80	0.04 	0.83 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.03 	0.00 	0.03
        81	0.25 	0.48 	0.03 	0.02 	0.02 	0.00 	0.05 	0.01 	0.08 	0.03 	0.02 	0.00 	0.00
        82	0.02 	0.49 	0.15 	0.09 	0.09 	0.00 	0.04 	0.01 	0.08 	0.00 	0.02 	0.00 	0.00
        83	0.61 	0.00 	0.02 	0.05 	0.12 	0.00 	0.13 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00
        84	0.07 	0.22 	0.04 	0.03 	0.08 	0.08 	0.07 	0.01 	0.03 	0.00 	0.15 	0.16 	0.07
        85	0.10 	0.64 	0.01 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.04 	0.06 	0.00
        86	0.07 	0.56 	0.07 	0.12 	0.01 	0.01 	0.01 	0.00 	0.13 	0.00 	0.00 	0.00 	0.03
        87	0.02 	0.38 	0.36 	0.07 	0.02 	0.00 	0.02 	0.01 	0.05 	0.04 	0.01 	0.01 	0.00
        88	0.17 	0.45 	0.13 	0.02 	0.07 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11 	0.01 	0.01
        89	0.03 	0.60 	0.06 	0.00 	0.15 	0.00 	0.05 	0.01 	0.02 	0.00 	0.01 	0.03 	0.04
        90	0.20 	0.49 	0.00 	0.02 	0.09 	0.03 	0.06 	0.00 	0.00 	0.00 	0.04 	0.05 	0.00
        91	0.28 	0.39 	0.07 	0.04 	0.03 	0.06 	0.00 	0.00 	0.02 	0.02 	0.02 	0.02 	0.04
        92	0.36 	0.23 	0.00 	0.03 	0.13 	0.08 	0.03 	0.02 	0.03 	0.02 	0.00 	0.02 	0.05
        93	0.06 	0.60 	0.00 	0.18 	0.01 	0.03 	0.00 	0.01 	0.00 	0.01 	0.00 	0.07 	0.02
        94	0.09 	0.47 	0.00 	0.01 	0.08 	0.01 	0.05 	0.04 	0.08 	0.00 	0.00 	0.00 	0.15
        95	0.20 	0.00 	0.02 	0.03 	0.44 	0.06 	0.22 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        96	0.02 	0.01 	0.10 	0.09 	0.65 	0.03 	0.01 	0.01 	0.00 	0.00 	0.06 	0.03 	0.00
        97	0.11 	0.67 	0.12 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.05
        98	0.42 	0.17 	0.00 	0.07 	0.00 	0.09 	0.06 	0.01 	0.05 	0.03 	0.00 	0.02 	0.07
        99	0.12 	0.29 	0.03 	0.09 	0.03 	0.02 	0.27 	0.00 	0.02 	0.00 	0.00 	0.02 	0.09
       100	0.06 	0.50 	0.22 	0.00 	0.03 	0.01 	0.01 	0.00 	0.01 	0.04 	0.00 	0.08 	0.03
       101	0.03 	0.56 	0.24 	0.01 	0.01 	0.00 	0.02 	0.00 	0.07 	0.00 	0.00 	0.00 	0.05
       102	0.01 	0.27 	0.20 	0.09 	0.02 	0.09 	0.05 	0.01 	0.18 	0.00 	0.02 	0.00 	0.07
       103	0.04 	0.74 	0.06 	0.02 	0.00 	0.02 	0.00 	0.01 	0.02 	0.03 	0.01 	0.05 	0.00
       104	0.02 	0.64 	0.03 	0.03 	0.01 	0.01 	0.03 	0.00 	0.02 	0.04 	0.00 	0.08 	0.08
       105	0.25 	0.23 	0.07 	0.01 	0.08 	0.00 	0.09 	0.02 	0.01 	0.02 	0.05 	0.00 	0.16
       106	0.02 	0.70 	0.03 	0.02 	0.03 	0.01 	0.01 	0.02 	0.01 	0.05 	0.02 	0.01 	0.06
       107	0.37 	0.26 	0.01 	0.00 	0.05 	0.05 	0.00 	0.06 	0.07 	0.01 	0.00 	0.01 	0.12
       108	0.00 	0.60 	0.02 	0.09 	0.01 	0.14 	0.01 	0.00 	0.01 	0.00 	0.04 	0.00 	0.07
       109	0.22 	0.19 	0.28 	0.03 	0.01 	0.06 	0.01 	0.00 	0.03 	0.00 	0.14 	0.03 	0.01
       110	0.07 	0.07 	0.00 	0.44 	0.18 	0.08 	0.10 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.01 	0.55 	0.00 	0.13 	0.01 	0.07 	0.11 	0.00 	0.01 	0.05 	0.05 	0.00 	0.00
       112	0.12 	0.04 	0.29 	0.21 	0.04 	0.02 	0.09 	0.01 	0.05 	0.04 	0.04 	0.03 	0.02
       113	0.02 	0.53 	0.12 	0.07 	0.01 	0.10 	0.03 	0.00 	0.06 	0.01 	0.02 	0.00 	0.03
       114	0.03 	0.43 	0.20 	0.00 	0.08 	0.00 	0.11 	0.00 	0.11 	0.00 	0.02 	0.00 	0.02
       115	0.01 	0.61 	0.07 	0.03 	0.04 	0.04 	0.08 	0.00 	0.02 	0.02 	0.00 	0.00 	0.07
       116	0.01 	0.80 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00 	0.01 	0.00 	0.02 	0.07
       117	0.00 	0.57 	0.07 	0.01 	0.18 	0.00 	0.00 	0.01 	0.02 	0.00 	0.10 	0.00 	0.03
       118	0.44 	0.12 	0.27 	0.06 	0.02 	0.05 	0.00 	0.01 	0.03 	0.01 	0.00 	0.01 	0.00
       119	0.03 	0.54 	0.01 	0.20 	0.01 	0.04 	0.10 	0.00 	0.00 	0.00 	0.03 	0.01 	0.02
       120	0.07 	0.25 	0.16 	0.18 	0.02 	0.00 	0.01 	0.00 	0.24 	0.00 	0.05 	0.02 	0.01
       121	0.05 	0.00 	0.75 	0.00 	0.00 	0.02 	0.01 	0.06 	0.08 	0.02 	0.00 	0.00 	0.00
       122	0.10 	0.03 	0.63 	0.09 	0.01 	0.10 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       123	0.01 	0.02 	0.01 	0.75 	0.00 	0.13 	0.03 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00
       124	0.05 	0.10 	0.11 	0.69 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00
       125	0.00 	0.60 	0.03 	0.15 	0.01 	0.00 	0.04 	0.01 	0.10 	0.00 	0.03 	0.02 	0.01
       126	0.00 	0.21 	0.06 	0.21 	0.02 	0.12 	0.09 	0.06 	0.13 	0.04 	0.04 	0.00 	0.03
       127	0.15 	0.09 	0.66 	0.00 	0.02 	0.04 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00
       128	0.03 	0.47 	0.21 	0.06 	0.00 	0.06 	0.02 	0.03 	0.01 	0.05 	0.02 	0.00 	0.05
       129	0.34 	0.14 	0.06 	0.23 	0.03 	0.10 	0.01 	0.00 	0.01 	0.01 	0.06 	0.02 	0.00
       130	0.14 	0.00 	0.11 	0.17 	0.41 	0.00 	0.05 	0.01 	0.01 	0.08 	0.00 	0.02 	0.00
       131	0.53 	0.01 	0.05 	0.01 	0.17 	0.05 	0.08 	0.02 	0.00 	0.01 	0.01 	0.03 	0.02
       132	0.63 	0.01 	0.00 	0.01 	0.09 	0.00 	0.19 	0.03 	0.02 	0.02 	0.01 	0.00 	0.00
       133	0.39 	0.01 	0.02 	0.00 	0.25 	0.09 	0.09 	0.00 	0.01 	0.04 	0.01 	0.00 	0.09
       134	0.44 	0.02 	0.10 	0.06 	0.09 	0.16 	0.01 	0.02 	0.06 	0.04 	0.01 	0.00 	0.01
       135	0.68 	0.01 	0.09 	0.04 	0.04 	0.05 	0.04 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00
       136	0.78 	0.00 	0.00 	0.00 	0.01 	0.06 	0.04 	0.00 	0.00 	0.02 	0.07 	0.00 	0.01
       137	0.67 	0.02 	0.13 	0.01 	0.01 	0.03 	0.09 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.74 	0.01 	0.08 	0.00 	0.09 	0.01 	0.05 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
       139	0.59 	0.01 	0.16 	0.06 	0.02 	0.02 	0.00 	0.06 	0.01 	0.01 	0.03 	0.03 	0.00
       140	0.71 	0.01 	0.03 	0.07 	0.03 	0.05 	0.00 	0.01 	0.02 	0.00 	0.00 	0.02 	0.05
       141	0.56 	0.01 	0.10 	0.01 	0.01 	0.09 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.09
       142	0.65 	0.02 	0.07 	0.08 	0.02 	0.11 	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.01
       143	0.51 	0.02 	0.01 	0.04 	0.04 	0.22 	0.09 	0.01 	0.01 	0.01 	0.02 	0.00 	0.02
       144	0.37 	0.10 	0.12 	0.01 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.08 	0.02 	0.00
       145	0.63 	0.03 	0.07 	0.01 	0.04 	0.02 	0.03 	0.00 	0.01 	0.07 	0.01 	0.04 	0.05
       146	0.70 	0.02 	0.08 	0.05 	0.00 	0.10 	0.00 	0.00 	0.02 	0.02 	0.00 	0.01 	0.00
       147	0.77 	0.10 	0.00 	0.07 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       148	0.68 	0.21 	0.02 	0.03 	0.01 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.51 	0.25 	0.02 	0.01 	0.13 	0.00 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02
       150	0.28 	0.29 	0.01 	0.00 	0.26 	0.00 	0.08 	0.03 	0.01 	0.00 	0.00 	0.02 	0.01
       151	0.34 	0.22 	0.01 	0.01 	0.18 	0.05 	0.14 	0.01 	0.00 	0.02 	0.00 	0.01 	0.01
       152	0.21 	0.15 	0.12 	0.00 	0.20 	0.06 	0.15 	0.00 	0.00 	0.07 	0.01 	0.01 	0.01
       153	0.41 	0.25 	0.05 	0.02 	0.01 	0.18 	0.06 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       154	0.53 	0.01 	0.10 	0.03 	0.01 	0.25 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.05
       155	0.63 	0.16 	0.00 	0.08 	0.02 	0.00 	0.10 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
       156	0.53 	0.24 	0.04 	0.11 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03 	0.00
       157	0.60 	0.09 	0.13 	0.01 	0.02 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.09 	0.01
       158	0.04 	0.42 	0.05 	0.03 	0.03 	0.36 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00
       159	0.13 	0.29 	0.01 	0.03 	0.05 	0.42 	0.03 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00
       160	0.72 	0.04 	0.00 	0.07 	0.01 	0.01 	0.00 	0.01 	0.09 	0.00 	0.00 	0.02 	0.03
       161	0.53 	0.15 	0.00 	0.14 	0.01 	0.07 	0.03 	0.02 	0.00 	0.00 	0.03 	0.01 	0.01
       162	0.73 	0.03 	0.06 	0.07 	0.01 	0.01 	0.07 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01
       163	0.70 	0.05 	0.08 	0.01 	0.06 	0.05 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01
       164	0.60 	0.17 	0.07 	0.00 	0.01 	0.02 	0.07 	0.00 	0.04 	0.02 	0.01 	0.00 	0.00
       165	0.77 	0.05 	0.01 	0.02 	0.03 	0.01 	0.02 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00
       166	0.38 	0.44 	0.01 	0.00 	0.01 	0.02 	0.00 	0.00 	0.03 	0.04 	0.00 	0.00 	0.05
       167	0.60 	0.11 	0.10 	0.01 	0.01 	0.01 	0.03 	0.00 	0.00 	0.08 	0.02 	0.00 	0.03
       168	0.40 	0.36 	0.05 	0.01 	0.01 	0.03 	0.05 	0.00 	0.07 	0.00 	0.00 	0.00 	0.00
       169	0.30 	0.36 	0.11 	0.02 	0.07 	0.05 	0.05 	0.01 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.82 	0.01 	0.06 	0.00 	0.03 	0.01 	0.01 	0.00 	0.03 	0.00 	0.03 	0.00 	0.00
       171	0.70 	0.04 	0.09 	0.00 	0.01 	0.04 	0.05 	0.00 	0.02 	0.03 	0.00 	0.00 	0.00
       172	0.43 	0.34 	0.06 	0.00 	0.03 	0.03 	0.05 	0.00 	0.01 	0.00 	0.01 	0.03 	0.01
       173	0.56 	0.24 	0.01 	0.06 	0.02 	0.06 	0.05 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       174	0.64 	0.29 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.01 	0.00
       175	0.38 	0.41 	0.05 	0.01 	0.09 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
       176	0.38 	0.36 	0.02 	0.03 	0.04 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.01 	0.02
       177	0.49 	0.36 	0.05 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Régression}
\label{\detokenize{regression:regression}}\label{\detokenize{regression::doc}}
\index{Régression@\spxentry{Régression}}\ignorespaces 
\sphinxAtStartPar
On s’intéresse ici à l’explication d’une variable (aléatoire) \(Y\) (la variable expliquée) par une (ou plusieurs) variable(s) aléatoire(s) \(X_j\) (prédicteurs, ou variables explicatives).


\section{Régression simple}
\label{\detokenize{regression:regression-simple}}
\sphinxAtStartPar
On dispose de \(n\) couples de variables quantitatives \((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\) constituant un échantillon d’observations indépendantes de \((X,Y)\) et on cherche une relation statistique pouvant exister entre \(Y\) et \(X\).
On rappelle ici quelques résultats élémentaires sur la régression linéaire simple.


\subsection{Modèle théorique}
\label{\detokenize{regression:modele-theorique}}
\sphinxAtStartPar
Théoriquement, on cherche une fonction \(f\) telle que \(f(X)\) soit aussi proche que possible de \(Y\). Par proximité, on entend ici au sens des moindres carrés, et donc on cherche \(f\) telle que \(\mathbb{E}\left ( (Y-f(X))^2\right )\) soit minimale. On sait alors que la fonction \(f\) qui satisfait cette propriété est :

\sphinxAtStartPar
\(f(X) = \mathbb{E}(Y\mid X)\)
\label{regression:definition-0}
\begin{sphinxadmonition}{note}{Definition 37 (Fonction de régression)}



\sphinxAtStartPar
La fonction \(x\mapsto \mathbb{E}(Y\mid X=x)\) est la fonction de régression de \(Y\) en \(X\).
\end{sphinxadmonition}

\sphinxAtStartPar
La qualité de l’approximation est mesurée par le rapport de corrélation.

\index{Corrélation@\spxentry{Corrélation}!rapport@\spxentry{rapport}}\ignorespaces \label{regression:definition-1}
\begin{sphinxadmonition}{note}{Definition 38 (Rapport de corrélation)}



\sphinxAtStartPar
Le rapport de corrélation entre deux variables aléatoires \(X\) et \(Y\) est défini par le rapport entre la variation expliquée et la variation totale :

\sphinxAtStartPar
\(\eta_{Y\mid X}^2 = \frac{\sigma_{\mathbb{E}(Y\mid X)}^2}{\sigma_Y^2}\)
\end{sphinxadmonition}

\sphinxAtStartPar
En pratique, \(Y\) est approchée par \(Y=\mathbb{E}(Y\mid X)+\varepsilon\), où \(\varepsilon\) est un résidu aléatoire de moyenne nulle, non corrélé à \(X\) et à \(\mathbb{E}(Y\mid X)\) et tel que \(\sigma_\varepsilon^2= (1-\eta_{Y\mid X}^2)\sigma_Y^2\).

\sphinxAtStartPar
Le cadre le plus utilisé est celui de la régression linéaire, c’est\sphinxhyphen{}à\sphinxhyphen{}dire lorsque \(Y=a+bX+\varepsilon\) et donc \(\mathbb{E}(Y\mid X)=a+bX\), ce qui est le cas lorsque \((X,Y)\) est un couple de variables aléatoires gaussiennes.

\sphinxAtStartPar
Puisque \(\mathbb{E}(\varepsilon)=0\), la droite de régression passe par le point \((\mathbb{E}(X),\mathbb{E}(Y))\). Ainsi

\sphinxAtStartPar
\(Y-\mathbb{E}(Y)=b(X-\mathbb{E}(X))+\varepsilon\)

\sphinxAtStartPar
En multipliant par \(X-\mathbb{E}(X)\) et en prenant l’espérance, on trouve à gauche la covariance de \((X,Y)\) et à droite la variance de \(X\), soit

\sphinxAtStartPar
\(\begin{array}{ccll}
\sigma_{XY}&=& b\sigma_X^2+\mathbb{E}(\varepsilon(X-\mathbb{E}(X)))&\\
&=& b\sigma_X^2 + \sigma_{\varepsilon X}&[\mathbb{E}(\varepsilon)=0]\\ 
&=& b\sigma_X^2 &[X\text{ et } \varepsilon\text{ non corrélés}]\\ 
\end{array}
\)

\sphinxAtStartPar
d’où
\(b = \frac{\sigma_{XY}}{\sigma_X^2} = r_{XY}\frac{\sigma_Y}{\sigma_X}\)

\sphinxAtStartPar
L’équation de la droite de régression est donc finalement

\sphinxAtStartPar
\(Y-\mathbb{E}(Y)=r_{XY}\frac{\sigma_Y}{\sigma_X}(X-\mathbb{E}(X))+\varepsilon\)

\sphinxAtStartPar
En calculant la variance des deux termes, et puisque \(\varepsilon\) et \(X\) ne sont pas corrélés, on trouve

\sphinxAtStartPar
\(r_{XY}^2 = \eta_{Y\mid X}^2\)


\subsection{Ajustement aux données}
\label{\detokenize{regression:ajustement-aux-donnees}}
\sphinxAtStartPar
On cherche ici à ajuster le modèle linéaire théorique aux \(n\) couples d’observations indépendantes \((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\). Il s’agit donc de trouver \(a,b\) ainsi que la variance du résidu \(\varepsilon\).

\sphinxAtStartPar
La méthode la plus classique est la méthode des moindres carrés : on cherche à ajuster au nuage de points  \((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\) une droite d’équation \(y^*=\alpha +\beta x\) de sorte à minimiser

\sphinxAtStartPar
\(\displaystyle\sum_{i=1}^n (y_i^*-y_i)^2 = \displaystyle\sum_{i=1}^n (\alpha + \beta x_i-y_i)^2\)

\sphinxAtStartPar
En annulant le gradient de cette fonction à deux variables \((\alpha,\beta)\), on trouve facilement

\sphinxAtStartPar
\(\beta = \frac{\sigma_{xy}}{\sigma_x^2} = r_{xy}\frac{\sigma_y}{\sigma_x}\)

\sphinxAtStartPar
de sorte que \(y^* = \bar y + r_{xy}\frac{\sigma_y}{\sigma_x}(x-\bar x)\).

\sphinxAtStartPar
La droite de régression linéaire passe donc par le centre de masse du nuage de points.
\label{regression:remark-2}
\begin{sphinxadmonition}{note}{Remark 15}



\sphinxAtStartPar
les \(x_i\) et \(y_i\) étant des réalisations de variables aléatoires, tous les termes de l’équation de la droite de régression linéaire le sont également.
\end{sphinxadmonition}
\label{regression:remark-3}
\begin{sphinxadmonition}{note}{Remark 16}



\sphinxAtStartPar
On peut montrer que \(\alpha\), \(\beta\) et \(y^*\) sont des estimateurs sans biais de \(a\), \(b\) et \(\mathbb{E}(Y\mid X)\).
\end{sphinxadmonition}

\sphinxAtStartPar
La figure suivante illustre la régression linéaire d’un ensemble de points, décomposé en un ensemble d’apprentissage (bleu) sur lequel la droite de régression a été apprise et un ensemble de test (vert) sur lequel les valeurs ont été prédites (magenta).

\sphinxAtStartPar
\sphinxincludegraphics{{regressionlin}.png}


\section{Régression multiple}
\label{\detokenize{regression:regression-multiple}}
\index{Régression@\spxentry{Régression}!multiple@\spxentry{multiple}}\ignorespaces 

\subsection{Ajustement linéaire d’un ensemble d’observations}
\label{\detokenize{regression:ajustement-lineaire-d-un-ensemble-d-observations}}\label{\detokenize{regression:index-2}}
\sphinxAtStartPar
La régression multiple généralise la régression simple au cas de \(p\geq 2\) prédicteurs quantitatifs (ou variables explicatives). Ici on considère un échantillon de \(n\) individus, sur lesquels \(p+1\) variables sont mesurées : une variable à expliquer \(\mathbf Y = (y_1\cdots y_n)^T\in\mathbb{R}^n\) et \(p\) variables explicatives \(\mathbf X_i\) linéairement indépendantes, mais possiblement en relation.

\sphinxAtStartPar
On cherche

\sphinxAtStartPar
\(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\)

\sphinxAtStartPar
proche de \(\mathbf Y\) au sens des moindres carrés. \(\mathbf{1}\) est le vecteur de \(\mathbb{R}^n\) dont toutes les composantes valent 1.

\sphinxAtStartPar
En notant
\(X = \begin{pmatrix}\mathbf{1} & \mathbf{X_1}\cdots \mathbf{X_p}\end{pmatrix}\in\mathcal{M}_{n,p+1}(\mathbb{R})\quad\text{et}\quad \boldsymbol{\beta}=(\beta_0\cdots \beta_p)^T
\in\mathbb{R}^{p+1}\)

\sphinxAtStartPar
on a \(\mathbf Y^*=\mathbf X\boldsymbol \beta\).

\sphinxAtStartPar
\(\mathbf Y^*\) est par définition des moindres carrés la projection de \(\mathbf Y\) sur \(Im(\mathbf X)\), soit (Voir cours analyse numérique) :

\sphinxAtStartPar
\(\mathbf Y^* = \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)

\sphinxAtStartPar
et donc

\sphinxAtStartPar
\(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)

\sphinxAtStartPar
et on a donc les paramètres de la régression multiple.
\label{regression:remark-4}
\begin{sphinxadmonition}{note}{Remark 17}



\sphinxAtStartPar
Dans le cas où la métrique utilisée est définie par une matrice symétrique définie positive \(D\) de taille \(p\), alors

\sphinxAtStartPar
\(\boldsymbol\beta = (\mathbf X^T\mathbf D \mathbf X)^{-1}\mathbf X^T \mathbf D \mathbf Y\)
\end{sphinxadmonition}


\subsection{Modèle}
\label{\detokenize{regression:modele}}
\sphinxAtStartPar
On suppose que les \(\mathbf X_i\) et \(\mathbf Y\) sont \(n\) réalisations indépendantes de \(p+1\) variables aléatoires \(\chi_i\) et \(\omega\). De même qu’en régression simple, la recherche de la meilleure approximation de \(\omega\) par une fonction des \(\chi_i\) amène à \(\mathbb{E}(\omega\mid \chi_1\cdots \chi_p)\) et l’hypothèse de régression multiple est

\sphinxAtStartPar
\(\mathbb{E}(\omega\mid \chi_0\cdots \chi_p) = b_0+\displaystyle\sum_{i=1}^p b_i\chi_i+\varepsilon\)

\sphinxAtStartPar
avec \(\mathbb{E}(\varepsilon)=0, \sigma_\varepsilon=\sigma^2\) et \(\varepsilon\) non corrélée aux \(\chi_i\).

\sphinxAtStartPar
On peut montrer que \(\boldsymbol\beta\) est un estimateur sans biais du vecteur aléatoire  \((b_0\cdots b_p)\), et en est la meilleure approximation. De plus, la meilleure estimation sans biais de la variance \(\sigma^2\) est

\sphinxAtStartPar
\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1}\)


\section{Modèle linéaire généralisé}
\label{\detokenize{regression:modele-lineaire-generalise}}

\subsection{Position du problème}
\label{\detokenize{regression:position-du-probleme}}
\sphinxAtStartPar
Dans le cas le plus général, on ne cherche pas à expliquer une seule variable mais \(k\in\mathbb{N}\), obtenues par répétitions de l’expérience, les \(\mathbf X_j\) restant identiques : pour \(i\in[\![1,k]\!]\) \(\mathbf{Y}_i\in\mathbb{R}^n\) est la \(i^e\) observation.


\subsection{Solution à partir des données}
\label{\detokenize{regression:solution-a-partir-des-donnees}}
\sphinxAtStartPar
Le modèle fait l’hypothèse que le centre de gravité \(\mathbf g\) des \(k\) observations se situe dans \(Im(\mathbf X)\), soit \(\mathbf g = \mathbf X \boldsymbol \beta\)
La plupart du temps, on ne connaît cependant qu’une seule des \(k\) observations \(\mathbf Y\), et le problème revient à approximer le mieux possible \(\mathbf g\) en ne connaissant que \(\mathbf Y\).

\sphinxAtStartPar
Cette approximation \(\mathbf g^*\) s’exprime comme la projection orthogonale de \(\mathbf Y\) sur \(Im(\mathbf X)\), selon une métrique \(\mathbf M\), à choisir de sorte que \(\mathbf g^*\) soit la plus proche possible de \(\mathbf g\). Dit autrement, en répétant la projection avec \(\mathbf Y_1\cdots \mathbf Y_k\), les \(k\) approximations \(g^*_i=\mathbf X (\mathbf X^T\mathbf M\mathbf X)^{-1} \mathbf X^T \mathbf M \mathbf Y_i, i\in[\![1,k]\!]\) doivent être le plus concentrées possible autour de \(\mathbf g\).

\sphinxAtStartPar
Ceci revient donc à trouver \(\mathbf M\) de sorte à ce que l’inertie du nuage des \(\mathbf g_i^*\) soit minimale. On montre (théorème de Gauss\sphinxhyphen{}Markov généralisé) que \(\mathbf M=\mathbf V^{-1}\), où \(\mathbf V\) est la matrice de variance\sphinxhyphen{}covariance du nuage des \(\mathbf Y_i\). Ainsi, pour une seule observation, on en déduit

\sphinxAtStartPar
\(\begin{eqnarray*}
\mathbf g^*&=&\mathbf X(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y\\
\boldsymbol \beta&=&(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y
\end{eqnarray*}\)


\subsection{Modèle}
\label{\detokenize{regression:id1}}
\sphinxAtStartPar
En ayant une infinité d’observations, on approche le modèle probabiliste. On suppose que \(\mathbf Y\) est une réalisation d’un vecteur aléatoire d’espérance \(\mathbf X\mathbf b\) et de matrice de variance\sphinxhyphen{}covariance \(\boldsymbol\Sigma\). Le modèle s’écrit alors  \(\mathbf Y=\mathbf X\mathbf b+\varepsilon\), avec \(\varepsilon\) centré de variance \(\boldsymbol\Sigma\), et le problème est donc d’estimer \(\mathbf b\). On montre que  \(\mathbf b = (\mathbf X^T \boldsymbol\Sigma^{-1}\mathbf X)^{-1}\mathbf X^T\boldsymbol\Sigma^{-1}\mathbf Y\), appelé estimation des moindres carrés généralisés est, sous des hypothèses larges, l’estimation de variance minimale de \(\mathbf b\).


\section{Modèles régularisés}
\label{\detokenize{regression:modeles-regularises}}
\sphinxAtStartPar
On peut montrer que l’estimateur des moindres carrés est de variance minimale parmi les estimateurs linéaires sans biais. Cependant, la variance aboutit dans certains cas à des erreurs de prédiction importantes. Dans ce cas, on cherche des estimateurs de variance plus petite quitte à avoir un (léger) biais. Pour ce faire, on peut supprimer l’effet de certaines variables explicatives ce qui revient à leur attribuer un poids nul.
Par ailleurs, dans le cas où \(p\) est grand, l’interprétation des résultats obtenus est parfois complexe. Ainsi, on pourra préférer un modèle estimé avec moins de variables explicatives afin de privilégier l’interprétation plutôt que la précision.

\sphinxAtStartPar
Dans cette section, on s’intéresse à des méthodes permettant de produire des estimateurs dont les valeurs sont d’amplitudes réduites. On parle de modèles parcimonieux lorsque des variables ont des coefficients nuls.


\subsection{Régression Ridge}
\label{\detokenize{regression:regression-ridge}}
\index{Régression@\spxentry{Régression}!Ridge@\spxentry{Ridge}}\ignorespaces 
\index{Ridge regression@\spxentry{Ridge regression}}\ignorespaces 
\sphinxAtStartPar
Dans l’approche moindres carrés linéaires classique, on cherche \(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\) proche de \(\mathbf Y\) au sens de la minimisation de \(\|\mathbf Y^*-\mathbf Y\|^2 \). On cherche donc \(\boldsymbol\beta_{mc}\in\mathbb{R}^{p+1}\) tel que :

\sphinxAtStartPar
\(\boldsymbol\beta_{mc} = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2\right ]\)

\sphinxAtStartPar
Dans l’approche Ridge regression (ou régression de Tikhonov), on pénalise l’amplitude des coefficients \(\beta_j\). Pour ce faire, on pose \(\boldsymbol\beta_{\setminus 0}\) le vecteur des \(p\) dernières composantes de \(\boldsymbol\beta\) et on cherche le vecteur \(\boldsymbol\beta_r\) tel que

\sphinxAtStartPar
\(\boldsymbol\beta_r = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_2\right ]\)

\sphinxAtStartPar
Le réel positif \(\lambda\), pondérant  \(\| \boldsymbol\beta_{\setminus 0}\|^2_2\) appelée fonction de pénalité, permet de réguler l’importance du second terme sur la minimisation. Un \(\lambda\) grand impose à la minimisation d’avoir une amplitude faible des coefficients \(\beta_j,j\in[\![1,p]\!]\), et une variance faible de l’estimateur de \(\boldsymbol\beta\).

\sphinxAtStartPar
Contrairement à la régression linéaire multiple classique où les variables ne sont pas nécessairement normalisées, ici il est nécessaire de réduire les variables explicatives. En pratique on les centre également, et dans ce cas :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
la première composante de \(\boldsymbol\beta_r\) est prise égale à la moyenne empirique des \(y_i\) avant centrage

\item {} 
\sphinxAtStartPar
les \(p\) autres composantes de \(\boldsymbol\beta_r\)  sont obtenues par minimisation :

\end{enumerate}

\sphinxAtStartPar
\(\hat{\boldsymbol \beta}_r = arg\displaystyle\min_{\mathbf v\in\mathbb{R}^{p}} \left ((\mathbf Y-\mathbf X\mathbf v)^T(\mathbf Y-\mathbf X\mathbf v) + \lambda \mathbf v^T\mathbf v\right )\)

\sphinxAtStartPar
dont la solution analytique est \((\mathbf X^T\mathbf X + \lambda \mathbb{I})^{-1}\mathbf X^T\mathbf Y\).

\sphinxAtStartPar
Le choix de \(\lambda\) n’est pas évident. La solution la plus simple consiste à prendre plusieurs valeurs, à tester les solutions proposées par ces valeurs et à retenir le \(\lambda\) ayant obtenu le meilleur score (par exemple la précision sur un ensemble de test). De manière moins expérimentale, il existe des algorithmes (basés sur la décomposition en valeurs singulières) permettant de choisir une ‘’bonne’’ valeur de paramètre.


\subsection{Régression Lasso}
\label{\detokenize{regression:regression-lasso}}
\index{Régression@\spxentry{Régression}!Lasso@\spxentry{Lasso}}\ignorespaces 
\index{Lasso@\spxentry{Lasso}}\ignorespaces 
\sphinxAtStartPar
La régression Lasso (Least Absolute Shrinkage and Selection Operator) est, dans son principe, très proche de la régression Ridge, la seule différence résidant dans la norme utilisée dans la fonction de pénalité : on cherche \(\boldsymbol\beta\) minimisant

\sphinxAtStartPar
\(\boldsymbol\beta_l = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_1\right ]\)

\sphinxAtStartPar
Contrairement à la régression Ridge, il n’y a pas de solution analytique (la norme \(\ell_1\) rend la fonction non différentiable) et on doit donc recourir à des méthodes de résolution numérique. Lorsque \(\lambda\) est grand, la minimisation force la fonction de pénalité à être petite : étant donné que cette dernière est une somme de valeurs absolues, la minimisation impose à certains coefficients \(\beta_j,j\in[\![1,p]\!]\) d’être nuls. On parle alors de régression parcimonieuse (et la régression peut donc être vue comme une méthode de sélection de variables).

\sphinxAtStartPar
Quand \(p>n\), la méthode ne sélectionne que \(n\) variables. De plus, si plusieurs variables sont corrélées entre elles, Lasso ignore toutes sauf une. Et, pire, même si \(n>p\), et s’il y a de fortes corrélations entre les variables explicatives, on trouve empiriquement que Ridge donne de meilleurs résultats que Lasso.


\subsection{Régression Elasticnet}
\label{\detokenize{regression:regression-elasticnet}}
\index{Régression@\spxentry{Régression}!Elasticnet@\spxentry{Elasticnet}}\ignorespaces 
\index{Elasticnet@\spxentry{Elasticnet}}\ignorespaces 
\sphinxAtStartPar
On suppose ici que \(\mathbf X\) est centré réduit, et \(\mathbf Y\) est centré (donc \(\beta_0=0\)). La régression Elasticnet est un mélange de Ridge et Lasso : on cherche \(\boldsymbol\beta_e\) tel que

\sphinxAtStartPar
\(\boldsymbol\beta_e = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p}}\left [\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda_1 \| \boldsymbol\beta\|^2_1 + \lambda_2 \| \boldsymbol\beta\|^2_2\right ]\)

\sphinxAtStartPar
En notant \(\lambda =\lambda_1+\lambda_2\) et \( \alpha = \lambda_1/\lambda\) on minimise alors

\sphinxAtStartPar
\(\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda(\alpha \| \boldsymbol\beta\|^2_1 + (1-\alpha) \| \boldsymbol\beta\|^2_2)\)

\sphinxAtStartPar
On montre alors que la solution de la régression Elasticnet peut être obtenue à l’aide de la solution de la régression Lasso.
\label{regression:property-5}
\begin{sphinxadmonition}{note}{Property 7}



\sphinxAtStartPar
Soit \(\mathbf X\in\mathcal{M}_{np}(\mathbb R)\) la matrice des variables explicatives, et \(\mathbf Y\in\mathbb{R}^n\) le vecteur des valeurs de la variable expliquée. Soient \(\lambda_1,\lambda_2\in\mathbb{R}^+\). On pose

\sphinxAtStartPar
\(\mathbf X^*\in\mathcal{M}_{(n+p)p}(\mathbb R) = \frac{1}{\sqrt{1+\lambda_2}}\begin{pmatrix}\mathbf X\\\sqrt{\lambda_2 }\mathbb{I}\end{pmatrix}\quad\text{et}\quad \mathbf Y^*=\begin{pmatrix}\mathbf Y\\0\end{pmatrix}\)

\sphinxAtStartPar
et on note \(\gamma=\lambda_1/(\lambda_1+\lambda_2)\).

\sphinxAtStartPar
Alors la fonction objectif de la régression Elasticnet s’écrit \(\|\mathbf Y^*-\mathbf X^*\boldsymbol\beta^*\|_2^2+\gamma\|\boldsymbol\beta^*\|_1\).
Si \(\hat{\boldsymbol\beta}\) minimise cette fonction, alors l’estimateur naïf de la régression Elasticnet est

\sphinxAtStartPar
\(\boldsymbol\beta_e = \frac{1}{\sqrt{1+\lambda_2}}\hat{\boldsymbol\beta}\)
\end{sphinxadmonition}

\sphinxAtStartPar
Puisque \(\mathbf X^*\) est de rang \(p\), la solution peut sélectionner \(p\) variables contrairement à la régression Lasso.

\sphinxAtStartPar
En pratique, cet estimateur naïf ne donne satisfaction que lorsqu’il est proche de \(\boldsymbol\beta_r\) ou de \(\boldsymbol\beta_l\). On retient généralement l’estimateur rééchelonné \((1+\lambda_2)\boldsymbol\beta_e = \sqrt{1+\lambda_2}\hat{\boldsymbol\beta}\) (Elasticnet peut être vu comme un Lasso où la matrice de variance\sphinxhyphen{}covariance est proche de la matrice Identité, et on montre que le facteur \(1+\lambda_2\) intervient alors).

\sphinxAtStartPar
La figure suivante compare les différentes méthodes de régression sur la fonction

\sphinxAtStartPar
\(f(x) = x-\frac35 x^2+\frac15x^3 + 18sin(x)\)

\sphinxAtStartPar
avec \(p=8\) et \(n=20\). Les \(n=20\) points  échantillonnés sur la courbe \(y=f(x)\) sont utilisés pour faire la régression sur l’intervalle {[}\sphinxhyphen{}10,10{]}.

\sphinxAtStartPar
\sphinxincludegraphics{{comparregression}.png}


\section{Régression logistique}
\label{\detokenize{regression:regression-logistique}}
\index{Régression@\spxentry{Régression}!logistique@\spxentry{logistique}}\ignorespaces 
\sphinxAtStartPar
Dans les sections précédentes, nous n’avons pas abordé les cas où les prédicteurs exhibent des dépendances non linéaires ou lorsque la variable à prédire n’est pas quantitative.

\sphinxAtStartPar
La régression logistique est un modèle linéaire généralisé utilisé pour prédire une variable binaire, ou catégorielle, à partir de prédicteurs quantitatifs ou catégoriels.


\subsection{Régression logistique binaire}
\label{\detokenize{regression:regression-logistique-binaire}}
\sphinxAtStartPar
Dans un premier temps, la variable à prédire est binaire : elle ne prend donc que deux valeurs 0/1 (ou \sphinxhyphen{}1/1). Dans le chapitre suivant, nous étudierons des algorithmes permettant d’aborder ce problème sous un angle classification. Ici, nous nous intéressons à une modélisation probabiliste, permettant notamment de prendre en compte le bruit dans les données.


\subsubsection{Modèle}
\label{\detokenize{regression:id2}}
\sphinxAtStartPar
On recherche une distribution conditionnelle \(P(Y|X)\) de la variable à prédire sachant les prédicteurs. Si le problème est en 0/1, alors \(Y\) est une variable indicatrice et on a \(P(Y=1)=\mathbb{E}(Y)\) et \(P(Y=1|X=x)=\mathbb{E}(Y|X=x)\). La probabilité conditionnelle est donc l’espérance conditionnelle de l’indicatrice.

\sphinxAtStartPar
Supposons que \(P(Y=1|X=x)=p(x,\boldsymbol\theta)\) avec \(p\) fonction paramétrée par \(\boldsymbol\theta\). On suppose également que les observations sont indépendantes. La vraisemblance est alors donnée par

\sphinxAtStartPar
\(\displaystyle\prod_{i=1}^n P(Y=y_i|X=x_i) = \displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)
\label{regression:remark-6}
\begin{sphinxadmonition}{note}{Remark 18}



\sphinxAtStartPar
Pour \(n\) tirages d’une variable de Bernoulli dont la probabilité de succès est constante et vaut \(p\), la vraisemblance est \(\displaystyle\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}\). Cette vraisemblance est maximisée lorsque
\(p=n^{-1}\displaystyle\sum_{i=1}^n y_i\).
\end{sphinxadmonition}

\sphinxAtStartPar
En notant \(p_i=p(x_i,\boldsymbol\theta)\), maximiser la vraisemblance sans contrainte amène à la solution non informative \(p_i=1\) si \(y_i=1\) et 0 sinon. Si l’on essaye d’ajouter des contraintes (relations entre les \(p_i\)), alors l’estimation du maximum de vraisemblance devient difficile.

\sphinxAtStartPar
Ici le modèle  \(p_i=p(x_i,\boldsymbol\theta)\) suppose que si \(p\) est continue, alors des valeurs proches de \(x_i\) amènent à des valeurs proches de \(p_i\). En supposant \(p\) connue comme fonction de \(\boldsymbol\theta\), la vraisemblance est une fonction de \(\boldsymbol\theta\) et on peut estimer ce paramètre en maximisant la vraisemblance.


\subsubsection{Régression logistique}
\label{\detokenize{regression:id3}}
\sphinxAtStartPar
On recherche un ‘’bon’’ modèle pour \(p\) :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
On peut dans un premier temps supposer que \(p(\mathbf x)\) est une fonction linéaire de \(\mathbf x\). Les fonctions linéaires étant non bornées, elles ne peuvent modéliser des probabilités.

\item {} 
\sphinxAtStartPar
On peut alors supposer que \(log\ p(\mathbf x)\) est une fonction linéaire de \(\mathbf x\). Là aussi, la fonction logarithme est non bornée supérieurement, et ne peut modéliser une probabilité.

\item {} 
\sphinxAtStartPar
Partant de cette idée, on borne le logarithme en utilisant la transformation logistique (ou logit) \(log\frac{p(\mathbf x)}{1-p(\mathbf x)}\). Etant donné un événement ayant une probabilité \(p\) de réussir, le rapport \(p/(1-p)\) est appelé la côte de l’événement (rapport de la probabilité qu’il se produise sur celle qu’il ne se produise pas. Si vous avez \(p\)=3/4 de chances de réussir à votre examen de permis, cotre côte est \(p/(1-p)=\frac{3/4}{1/4}\)=3 contre un). On peut alors supposer que cette fonction de \(p\) est linéaire en \(\mathbf x\).

\end{enumerate}

\sphinxAtStartPar
Le modèle de régression logistique s’écrit alors formellement

\sphinxAtStartPar
\(logit(p(\mathbf x)) = log \frac{p(\mathbf x)}{1-p(\mathbf x)} = \beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\)

\sphinxAtStartPar
En résolvant par rapport à \(p\) on trouve alors

\sphinxAtStartPar
\(p(\mathbf x,\boldsymbol\theta) = \frac{e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}{1+e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}=\frac{1}{1+e^{-(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x)}}\quad\text{avec }\boldsymbol\theta=(\beta_0,\boldsymbol\beta)^T\)

\sphinxAtStartPar
Pour minimiser les erreurs de prédiction, on doit prédire \(Y=1\) si \(p\geq 0.5\), soit \(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\geq 0\) et \(Y=0\) sinon. La régression logistique est donc un classifieur linéaire, dont la frontière de décision est justement l’hyperplan \(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x= 0\). On peut montrer que la distance de \(\mathbf x\) à cet hyperplan est \(\beta_0/\|\boldsymbol\beta\| + \mathbf x^T\boldsymbol\beta/\|\boldsymbol\beta\|\). Les probabilités d’appartenance de \(\mathbf x\) aux classes décroissent donc d’autant plus vite que \(\|\boldsymbol\beta\|\) est grand.

\sphinxAtStartPar
Dans la figure suivante, la probabilité d’appartenance à la classe 1 (points rouges) est donnée en fausses couleurs.

\sphinxAtStartPar
\sphinxincludegraphics{{regression}.png}


\subsection{Régression logistique à plusieurs classes}
\label{\detokenize{regression:regression-logistique-a-plusieurs-classes}}
\sphinxAtStartPar
Dans ce cas, \(Y\) peut prendre \(k\) valeurs. Le modèle reste le même, chaque classe \(c\in[\![0,k-1]\!]\) ayant son jeu de paramètres \(\boldsymbol\theta_c=(\beta^c_0,\boldsymbol\beta^c)^T\). Les probabilités conditionnelles prédites sont alors

\sphinxAtStartPar
\((\forall c\in[\![0,k-1]\!])\;\;P(Y=c|X=\mathbf x) = \frac{e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}{1+e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}\)


\subsection{Interprétation}
\label{\detokenize{regression:interpretation}}
\sphinxAtStartPar
Si \(\mathbf x=\mathbf 0\), alors \(p(\mathbf x)=\frac{1}{1+e^{-\beta_0}}\). L’ordonnée à l’origine fixe donc le taux d’événements “de base”.

\sphinxAtStartPar
Supposons \(\boldsymbol\beta\in\mathbb{R}\) (l’interprétation sera la même dans le cas général). Considérons l’effet sur la probabilité d’un évènement du changement de \(x\in\mathbb{R}\) d’une unité, passant de \(x_0\) à \(x_0+1\). Alors :

\sphinxAtStartPar
\(logit(p(x_0+1))-logit(p(x_0)) = \beta_0+\beta(x_0+1)-(\beta_0+\beta(x_0)) = \beta\)
et en utilisant la définition de la fonction logit :

\sphinxAtStartPar
\(\begin{eqnarray*}
log \frac{p( x_0+1)}{1-p(x_0+1)}-log \frac{p( x_0)}{1-p(x_0)} &=& \beta\\
log \left  [\frac{\frac{p( x_0+1)}{1-p(x_0+1)}}{\frac{p( x_0)}{1-p(x_0)}} \right ]&=& \beta\\
\end{eqnarray*}\)

\sphinxAtStartPar
En notant OR (Odds Ratio, ou rapport de côte) le terme en argument du log, et en prenant l’exponentielle, on trouve \(OR=e^\beta\). Le coefficient \(\beta\) est donc tel que \(e^\beta\) est le rapport de côte pour un changement unitaire de l’entrée \(x\). Si \(x\) est incrémenté de deux unités, alors le rapport de côte est de \(e^{2\beta}=(e^\beta)^2\), que l’on généralise facilement au cas d’un changement de \(n\) unités à OR=\((e^\beta)^n\).

\sphinxAtStartPar
Dans le cas où \(\boldsymbol\beta\) est un vecteur, sa ième composante est une estimation du changement de la probabilité d’un évènement correspondant à une augmentation d’une unité de la ième composante de \(\mathbf x\), les autres composantes étant constantes.


\subsection{Estimation des coefficients de la régression logistique}
\label{\detokenize{regression:estimation-des-coefficients-de-la-regression-logistique}}
\sphinxAtStartPar
D’après le modèle probabiliste, la distribution associée à la régression logistique est la loi binomiale. Pour \(n\) échantillons \((x_i,y_i),i\in[\![1,n]\!]\), la vraisemblance s’écrit

\sphinxAtStartPar
\(\displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)

\sphinxAtStartPar
Pour estimer les paramètres \(\beta_0\) et \(\boldsymbol\beta\) à partir des données, on maximise cette vraisemblance. On prend son logarithme, on calcule son gradient et on en déduit un système d’équations à résoudre. Cette approche amène à des calculs complexes, la formulation analytique n’étant pas simple, et une approximation numérique est en pratique mise en oeuvre pour trouver l’optimal.


\section{Analyse des résultats d’une régression}
\label{\detokenize{regression:analyse-des-resultats-d-une-regression}}

\subsection{Etude des résidus}
\label{\detokenize{regression:etude-des-residus}}
\sphinxAtStartPar
L’étude des résidus \( y_i- y^*_i\) permet de repérer les observations aberrantes ou au contraire qui jouent un rôle fondamental dans la détermination de la régression. Elle permet également de vérifier que  le modèle linéaire est justifié.

\sphinxAtStartPar
Comme \(\mathbf Y = \mathbf Y -\mathbf X\boldsymbol \beta +\mathbf X\boldsymbol \beta\) , où \(\mathbf Y-\mathbf X\boldsymbol \beta \) est orthogonal à \(\mathbf X\boldsymbol \beta\), la matrice de variance des résidus s’écrit

\sphinxAtStartPar
\(\begin{eqnarray*}
\mathbb{V}(\mathbf Y) &=& \mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+\mathbb{V}(\mathbf X\boldsymbol \beta)\\
\sigma^2 \mathbf{I} &=&\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+ \sigma^2 \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\\ 
\text {soit }\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)&=&\sigma^2(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T)
\end{eqnarray*}
\)

\sphinxAtStartPar
et les résidus sont donc en général corrélés entre eux.
\label{regression:remark-7}
\begin{sphinxadmonition}{note}{Remark 19}



\sphinxAtStartPar
\(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\) est la projection orthogonale sur \(Im(\mathbf X)^\perp\)
\end{sphinxadmonition}

\sphinxAtStartPar
Si \(p_i\) est le \(i^e\) terme diagonal du projecteur \(\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\), alors

\sphinxAtStartPar
\(\mathbb{V}( y_i- y^*_i) = (1-p_i)\sigma^2\)

\sphinxAtStartPar
d’où l’estimation de la variance du résidu \(\hat{\mathbb{V}}(y_i-y^*_i) = (1-p_i)\hat{\sigma}^2\).

\sphinxAtStartPar
Si le modèle linéaire est justifié, alors la distribution des résidus suit approximativement une loi normale. Un test statistique (par exemple le test de Jarque\sphinxhyphen{}Berra) viendra confirmer ou infirmer l’hypothèse selon laquelle la distribution peut être considérée comme telle.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxAtStartPar
\sphinxincludegraphics{{nuagelin}.png}
&
\sphinxAtStartPar
\sphinxincludegraphics{{nuagepaslin}.png}
\\
\hline
\sphinxAtStartPar
\sphinxincludegraphics{{reslin}.png}
&
\sphinxAtStartPar
\sphinxincludegraphics{{respaslin}.png}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\index{Résidu@\spxentry{Résidu}!studentisé@\spxentry{studentisé}}\ignorespaces \label{regression:definition-8}
\begin{sphinxadmonition}{note}{Definition 39 (Résidu studentisé)}



\sphinxAtStartPar
On appelle résidu studentisé la quantité \(\frac{y_i-y^*_i}{\hat{\sigma}\sqrt{1-hp}}\)
\end{sphinxadmonition}

\sphinxAtStartPar
Lorsque \(n\) est grand, ces résidus doivent être compris dans l’intervalle {[}\sphinxhyphen{}2,2{]}.

\sphinxAtStartPar
Un fort résidu peut indiquer une valeur aberrante, mais la réciproque n’est pas vraie. Il est donc nécessaire d’étudier l’influence de chaque observation sur les résultats.


\subsection{Influence des observations}
\label{\detokenize{regression:influence-des-observations}}
\sphinxAtStartPar
Pour étudier l’influence des observations sur la prédiction, deux approches sont possibles (et complémentaires) :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
étudier l’influence d’une observation sur sa propre prédiction. On calcule le résidu prédit \(y_i-y_{\bar{i}}^*\), où \(y_{\bar{i}}^*\) est la prévision obtenue avec les \(n-1\) autres observations que \(y_i\). Il est facile de montrer que ce résidu vaut \(\frac{y_i-y_i^*}{1-p_i}\)

\end{enumerate}
\label{regression:remark-9}
\begin{sphinxadmonition}{note}{Remark 20}



\sphinxAtStartPar
Il convient de rester prudent lorsque \(p_i\) est grand, et la quantité
\(\displaystyle\sum_{i=1}^n  \frac{(y_i-y_i^*)^2}{(1-p_i)^2}\)
est une mesure du pouvoir prédictif du modèle.
\end{sphinxadmonition}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
étudier l’influence d’une observation sur les estimations des paramètres de la régression \(\beta_i\). On peut par exemple calculer une distance, dite de Cook, entre \(\boldsymbol \beta\) et \(\boldsymbol \beta_{\bar{i}}\) :

\end{enumerate}
\begin{equation*}
\begin{split}d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}}) = \frac{(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})^T\mathbf X^T \mathbf X(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})}{\hat{\sigma}^2(p+1)}=\frac{\|\mathbf Y^*-\mathbf Y_{\bar{i}}^*\|^2}{\hat{\sigma}^2(p+1)}\end{split}
\end{equation*}
\sphinxAtStartPar
où \(\mathbf Y_{\bar{i}}^*=\mathbf X\boldsymbol\beta_{\bar{i}}\). Si \(d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}})>1\), alors en général l’observation \(i\) a une influence anormale.


\subsection{Stabilité des coefficients de régression}
\label{\detokenize{regression:stabilite-des-coefficients-de-regression}}
\sphinxAtStartPar
La source principale d’instabilité dans l’estimation des paramètres de régression réside dans le fait que les variables explicatives sont très corrélées entre elles. Comme \(\mathbb{V}(\boldsymbol \beta)=\sigma^2(\mathbf X^T\mathbf X)^{-1}\) alors si les \(\mathbf X_i\) sont corrélés, la matrice \(\mathbf X^T\mathbf X\) est mal conditionnée. Dans ce cas, les paramètres sont estimés avec imprécision et les prédictions sont entâchées d’erreur. Il est donc essentiel de mesurer les colinéarités entre prédicteurs. Par simplicité (sans que cela nuise à la généralité), on suppose ici que les variables sont centrées et réduites : \((\mathbf X^T\mathbf X)\) est donc une matrice de taille \(p\) (le fait de centrer les données supprime la constante) et \(\boldsymbol\beta\in\mathbb{R}^p\). Ainsi \((\mathbf X^T\mathbf X)=n\mathbf R\) où \(\mathbf R\) est la matrice de corrélation entre les prédicteurs.

\sphinxAtStartPar
Deux stratégies sont classiquement proposées :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Facteur d’inflation de la variance : on a \(\mathbb{V}(\boldsymbol \beta) = \sigma^2\frac{\mathbf{R}^{-1}}{n}\) et \(\sigma^2_{\beta_j} = \frac{\sigma^2}{n}(\mathbf{R}^{-1})_{jj}\). Or le \(j^e\) terme de \(\mathbf{R}^{-1}\) est \(\frac{1}{1-R^2_j}\) où \(R^2_j\) est le carré du coefficient de corrélation multiple de \(\mathbf X_j\) et des \(p-1\) autres variables explicatives. Ce terme est le facteur d’inflation de la variance. La moyenne de ces \(p\) termes est parfois utilisée comme indice global de colinéarité multiple.

\end{enumerate}
\label{regression:remark-10}
\begin{sphinxadmonition}{note}{Remark 21}



\sphinxAtStartPar
Si les variables explicatives sont orthogonales, la régression multiple revient à \(p\) régressions simples.
\end{sphinxadmonition}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
La factorisation spectrale de \(\mathbf R\) s’écrit \(\mathbf R = \mathbf U\boldsymbol\Lambda \mathbf U^T\). Donc \(\mathbf{R}^{-1}=\mathbf U\Lambda^{-1}\mathbf U^T\) et la variance de \(\beta_j\) s’écrit

\end{enumerate}

\sphinxAtStartPar
\(\mathbb{V}(\beta_j) = \frac{\sigma^2}{n}\displaystyle\sum_{i=1}^p \frac{u_{ji}^2}{\lambda_i}\)

\sphinxAtStartPar
et dépend donc des inverses des valeurs propres de \(\mathbf R\). Dans le cas où les prédicteurs sont fortement corrélés, les dernières valeurs propres sont proches de 0 ce qui entraîne l’instabilité des paramètres de régression.

\sphinxAtStartPar
Pour améliorer la stabilité des paramètres de régression, on peut alors :
\begin{itemize}
\item {} 
\sphinxAtStartPar
rejeter certains termes de la somme précédente, par exemple en remplaçant les \(p\) prédicteurs par leurs \(p\) composantes principales (Ceci revient à effectuer \(p\) régressions simples).

\item {} 
\sphinxAtStartPar
régulariser la régression en utilisant des approche de type Ridge regression.

\end{itemize}


\section{Sélection des variables}
\label{\detokenize{regression:selection-des-variables}}
\sphinxAtStartPar
Plutôt que d’expliquer \(\mathbf Y\) par l’ensemble des prédicteurs, on peut chercher un sous\sphinxhyphen{}ensemble de ces \(p\) variables permettant d’obtenir quasiment le même résultat (régression). Nous avons déjà abordé la sélection de variables dans un chapitre précédent.


\section{Exemple}
\label{\detokenize{regression:exemple}}

\subsection{Données}
\label{\detokenize{regression:donnees}}
\sphinxAtStartPar
On s’intéresse aux données suivantes et on cherche s’il existe une relation entre la production \(Y\) et les deux variables prédictives \(X_1\) et \(X_2\).


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Usine
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Travail (h) \(X_1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Capital (machines/h) \(X_2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Production (\(10^2\) T)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
1100
&
\sphinxAtStartPar
300
&
\sphinxAtStartPar
60
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
1200
&
\sphinxAtStartPar
400
&
\sphinxAtStartPar
120
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
1430
&
\sphinxAtStartPar
420
&
\sphinxAtStartPar
190
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
1500
&
\sphinxAtStartPar
400
&
\sphinxAtStartPar
250
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
1520
&
\sphinxAtStartPar
510
&
\sphinxAtStartPar
300
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
1620
&
\sphinxAtStartPar
590
&
\sphinxAtStartPar
360
\\
\hline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
1800
&
\sphinxAtStartPar
600
&
\sphinxAtStartPar
380
\\
\hline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
1820
&
\sphinxAtStartPar
630
&
\sphinxAtStartPar
430
\\
\hline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
1800
&
\sphinxAtStartPar
610
&
\sphinxAtStartPar
440
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Modèle}
\label{\detokenize{regression:id4}}
\sphinxAtStartPar
On fait l’hypothèse d’un modèle linéaire

\sphinxAtStartPar
\(y = \beta_0+\beta_1 X_1 + \beta_2 X_2+\varepsilon = \mathbf X \boldsymbol\beta+\boldsymbol\varepsilon\)

\sphinxAtStartPar
On a alors \(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y = \begin{pmatrix} -437.714\\0.336\\0.410\end{pmatrix}\) et l’équation du modèle linéaire (hyperplan) aux moindres carrés est

\sphinxAtStartPar
\(y = -437.714+0.336 X_1+0.41X_2\)

\sphinxAtStartPar
De plus
\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1} = \frac{3194}{6} = 639\)

\sphinxAtStartPar
de sorte que la covariance des paramètres de régression vaut

\sphinxAtStartPar
\(\hat{\sigma}^2 (\mathbf X^T\mathbf X)^{-1} = \begin{pmatrix} 3355.56 & -4.152 & 6.184\\-4.152 & 0.008 & -0.016 \\ 6.184 & -0.016 & 0.038\end{pmatrix}\)

\sphinxAtStartPar
Dans la figure suivante, les points au\sphinxhyphen{}dessus du plan regresseur sont en bleu, les autres en vert.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxAtStartPar
\sphinxincludegraphics{{plan}.png}
&
\sphinxAtStartPar
\sphinxincludegraphics{{plan2}.png}
\\
\hline
\sphinxAtStartPar
Un point de vue…
&
\sphinxAtStartPar
Un autre
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{Quelques méthodes de classification}
\label{\detokenize{clustering:quelques-methodes-de-classification}}\label{\detokenize{clustering::doc}}

\section{Introduction}
\label{\detokenize{clustering:introduction}}
\sphinxAtStartPar
La classification automatique a pour but d’obtenir une représentation simplifiée des données initiales. Elle consiste à organiser un ensemble de données en classes homogènes ou classes naturelles.

\sphinxAtStartPar
Une définition formelle de la classification, qui puisse servir de base à un processus automatisé, amène à se poser les questions suivantes :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Comment les objets à classer sont\sphinxhyphen{}ils définis ?

\item {} 
\sphinxAtStartPar
Comment définir la notion de ressemblance entre objets ?

\item {} 
\sphinxAtStartPar
Qu’est\sphinxhyphen{}ce qu’une classe ?

\item {} 
\sphinxAtStartPar
Comment sont structurées les classes ?

\item {} 
\sphinxAtStartPar
Comment juger une classification par rapport à une autre ?

\end{itemize}

\sphinxAtStartPar
Pour effectuer cette classification, deux démarches sont généralement utilisées :
\begin{itemize}
\item {} 
\sphinxAtStartPar
on regroupe en classes les objets qui partagent certaines caractéristiques.

\item {} 
\sphinxAtStartPar
on regroupe en classes les objets qui possèdent des caractéristiques proches. C’est cette approche qui est étudiée ici

\end{itemize}


\section{Structures de classification}
\label{\detokenize{clustering:structures-de-classification}}

\subsection{Partition}
\label{\detokenize{clustering:partition}}
\index{Partition@\spxentry{Partition}}\ignorespaces \label{clustering:definition-0}
\begin{sphinxadmonition}{note}{Definition 40 (Partition)}



\sphinxAtStartPar
\(\Omega\) étant un ensemble fini, un ensemble \(P =(P_1 ,P_2 ,\cdots  P_g )\) de parties non vides de   \(\Omega\) est une partition si :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\((\forall k\neq l) P_k \cap P_l=\emptyset\)

\item {} 
\sphinxAtStartPar
\(\displaystyle\cup_{i=1}^gP_i=\Omega\)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Dans un ensemble  \(\Omega\) partitionné en \(g\) classes, chaque élément de l’ensemble appartient à une classe et une seule. Une manière pratique de décrire cette partition \(P\) consiste à lui associer la matrice de classification \({\bf C}=(c_{ij}), i\in [\![1,n]\!], j\in [\![1,g]\!]\), avec \(c_{ij}=1\) si l’individu \(i\) appartient à \(P_j\), et \(c_{ij}=0\) sinon. Dans le cas où l’on accepte qu’un individu appartienne à plusieurs classes (avec des degrés d’appartenance), on autorise \(c_{ij}\) à couvrir l’intervalle {[}0,1{]} et on parle alors de classification floue.


\subsection{Hiérarchie indicée}
\label{\detokenize{clustering:hierarchie-indicee}}
\index{Hiérarchie@\spxentry{Hiérarchie}}\ignorespaces \label{clustering:definition-1}
\begin{sphinxadmonition}{note}{Definition 41 (Hiérarchie)}



\sphinxAtStartPar
\(\Omega\) étant un ensemble fini, un ensemble \(H\) de parties non vides de \(\Omega\) est une hiérarchie sur \(\Omega\) si :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\Omega \in H\)

\item {} 
\sphinxAtStartPar
\((\forall x\in \Omega) \{x\}\in H\)

\item {} 
\sphinxAtStartPar
\((\forall h,h'\in H) h\cap h'=\emptyset\) ou \(h\subset h'\) ou \(h'\subset h\)

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Une hiérarchie est souvent représentée par l’intermédiaire d’un indice, fonction \(i\) de \(H\) dans \(\mathbb{R}^+\), strictement croissante vis à vis de l’inclusion et de noyau l’ensemble des singletons de \(\Omega\).


\subsection{Partition et hiérarchie}
\label{\detokenize{clustering:partition-et-hierarchie}}
\sphinxAtStartPar
Si \(P =(P_1 \cdots,P_g)\) est une partition de \(\Omega\), l’ensemble \(H\) formé des classes \(P_k\) de \(P\), des singletons de   \(\Omega\) et de l’ensemble  \(\Omega\) lui\sphinxhyphen{}même forme une hiérarchie. Remarquons qu’inversement, il est possible d’associer à chaque niveau d’une hiérarchie indicée une partition. Une hiérarchie indicée correspond donc à un ensemble de partitions emboîtées.


\section{Objectifs de la classification}
\label{\detokenize{clustering:objectifs-de-la-classification}}

\subsection{Difficultés de caractériser les objectifs}
\label{\detokenize{clustering:difficultes-de-caracteriser-les-objectifs}}
\sphinxAtStartPar
L’objectif de la classification automatique est l’organisation en classes homogènes des éléments d’un ensemble  \(\Omega\). Pour définir cette notion de classes homogènes, on utilise le plus souvent une mesure de similarité (ou de dissimilarité) sur  \(\Omega\). Par exemple, on peut imposer à un couple quelconque d’individus d’une même classe d’être plus “proches” que n’importe quel couple formé par un individu de la classe et un individu d’une autre classe. En pratique, cet objectif est inutilisable, et plusieurs démarches sont alors utilisées pour remplacer cet objectif trop difficile à atteindre.


\subsection{Démarche numérique}
\label{\detokenize{clustering:demarche-numerique}}

\subsubsection{Partition}
\label{\detokenize{clustering:id1}}
\sphinxAtStartPar
On remplace cette condition trop exigeante par une fonction numérique (critère) qui mesure la qualité d’homogénéité d’une partition. Le problème peut paraître alors très simple. En effet, par exemple, dans le cas de la recherche d’une partition, il suffit de chercher parmi l’ensemble fini de toutes les partitions celle qui optimise le critère numérique. Malheureusement, le nombre de ces partitions étant très grand, leur énumération est impossible dans un temps raisonnable.
Le nombre de partitions en \(g\) classes d’un ensemble à \(n\) éléments, que l’on note \(S_n^g\) est le nombre de Stirling de deuxième espèce. En posant \(S_0^0=1\) et pour tout \(n>0\), \(S_n^0=S_0^n=0\), il peut être calculé par récurrence grâce à la relation \(S_n^g=S_{n-1}^{g-1}+gS_{n-1}^g\). On peut montrer que
\begin{equation*}
\begin{split}S_n^g = \frac{1}{g!}\displaystyle\sum_{i=1}^g (-1)^{g-i}\begin{pmatrix}g\\ i \end{pmatrix}i^n\end{split}
\end{equation*}
\sphinxAtStartPar
et donc \(S_n^g\sim \frac{g^n}{g!}\) lorsque \(n\rightarrow\infty\). En pratique, sur un ordinateur calculant \(10^6\) partitions par seconde, il faut 126 000 ans pour calculer l’ensemble des partitions d’un ensemble à \(n=25\) éléments.

\sphinxAtStartPar
On utilise alors des heuristiques qui donnent, non pas la meilleure solution, mais une “bonne solution”, proche de la solution optimale. On parle alors d’optimisation locale. Lorsqu’il existe une structure d’ordre sur l’ensemble  \(\Omega\) et que celle\sphinxhyphen{}ci doit être respectée par la partition, il existe un algorithme de programmation dynamique (algorithme de Fisher), qui fournit la solution optimale.


\subsubsection{Hiérarchie}
\label{\detokenize{clustering:hierarchie}}
\sphinxAtStartPar
Dans le cas d’une hiérarchie, on cherche à obtenir des classes d’autant plus homogènes qu’elles sont situées dans le bas de la hiérarchie. La définition d’un critère est moins facile. Nous verrons qu’il est possible de le faire en utilisant la
notion d’ultramétrique (ultramétrique optimale).


\subsection{Démarche algorithmique}
\label{\detokenize{clustering:demarche-algorithmique}}
\sphinxAtStartPar
Il s’agit cette fois de définir directement un algorithme qui construit des classes homogènes en tenant compte de la mesure de similarité. Il est relativement facile de proposer de tels algorithmes, le problème est de pouvoir vérifier que les résultats fournis sont intéressants et répondent au problème posé. En réalité, cette démarche rejoint assez souvent la précédente.


\subsection{Mesure de dissimilarité et distance}
\label{\detokenize{clustering:mesure-de-dissimilarite-et-distance}}
\sphinxAtStartPar
Les algorithmes de classification dépendent d’une métrique qui définit implicitement la forme des classes qui seront calculées. Si la distance euclidienne suppose une isotropie dans les axes (et donc une représentation sphérique des classes), d’autres distances ou indices de dissimilarité peuvent être utilisés.


\subsubsection{Indice de dissimilarité}
\label{\detokenize{clustering:indice-de-dissimilarite}}
\sphinxAtStartPar
On se place dans \(\mathbb R^d\), et on considère \(n\) individus à classer \({\bf x_1}\ldots {\bf x_n}\).
\label{clustering:definition-2}
\begin{sphinxadmonition}{note}{Definition 42 (Dissimilarité \sphinxhyphen{} ultramétrique)}



\sphinxAtStartPar
Une mesure de dissimilarité \(\delta\) est une fonction de

\sphinxAtStartPar
\(
 \delta: \begin{array}{ccc}
\mathbb{R}^d\times\mathbb{R}^d &\rightarrow &\mathbb{R}^+\\
(\mathbf x_i,\mathbf x_j)&\mapsto & \delta_{ij} = \delta(\mathbf x_i,\mathbf x_j)
\end{array}
\)

\sphinxAtStartPar
vérifiant :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\((\forall i,j\in[\![1, n]\!])\ \delta_{ij}=\delta_{ji}\)

\item {} 
\sphinxAtStartPar
\((\forall i\in[\![1, n]\!])\ \delta_{ii}= 0\)

\end{itemize}

\sphinxAtStartPar
Si l’inégalité triangulaire \(\delta_{ij}\leq \delta_{ik}+\delta_{kj}\) est de plus vérifiée pour tout \(i,j,k\), alors \(\delta\) est une distance.

\sphinxAtStartPar
Si enfin l’inégalité ultramétrique  \(\delta_{ij}\leq max(\delta_{ik}+\delta_{jk})\) est  vérifiée pour tout \(i,j,k\), \(\delta\) est une ultramétrique.
\end{sphinxadmonition}

\sphinxAtStartPar
A partir des mesures de dissimilarité, on déduit des mesures de similarité \(s_{ij}\) le passage de l’une à l’autre se faisant par exemple par \(\delta_{ij} = s_{max}-s_{ij}\).


\subsubsection{Cas de variables qualitatives}
\label{\detokenize{clustering:cas-de-variables-qualitatives}}
\sphinxAtStartPar
On suppose que les \(d\) composantes des \({\bf x_i}\) sont qualitatives, et on se limite ici au cas de variables bimodales.
Étant donnés \({\bf x_i}=\begin{pmatrix} x_i^1\ldots x_i^d\end{pmatrix}\) et \({\bf x_j}\), on note :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(a_{ij}\) le nombre de co\sphinxhyphen{}occurences entre les individus \(i\) et \(j\)

\item {} 
\sphinxAtStartPar
\(b_{ij}\) le nombre de co\sphinxhyphen{}absences entre les individus \(i\) et \(j\)

\item {} 
\sphinxAtStartPar
\(c_{ij}\) le nombre d’attributs présents chez \(i\) et absents chez \(j\)

\item {} 
\sphinxAtStartPar
\(d_{ij}\) le nombre d’attributs absents chez \(i\) et présents chez \(j\)

\end{itemize}

\sphinxAtStartPar
les mesures suivantes sont des exemples de dissimilarité :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \sqrt{b_{ij}+c_{ij}}\) {[}distance “euclidienne” binaire{]}

\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \frac{(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\) {[}différence binaire de taille{]}

\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \frac{(b_{ij}c_{ij})}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\) {[}différence binaire de motif{]}

\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \frac{(a_{ij}+b_{ij}+c_{ij}+d_{ij})(b_{ij}+c_{ij})-(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\) {[}différence binaire de forme{]}

\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{4(a_{ij}+b_{ij}+c_{ij}+d_{ij})}\) {[}dissimilarité binaire de variance{]}

\item {} 
\sphinxAtStartPar
\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{2a_{ij}+b_{ij}+c_{ij}}\) {[}dissimilarité binaire de Lance et Williams{]}

\end{itemize}


\subsubsection{Cas de variables quantitatives}
\label{\detokenize{clustering:cas-de-variables-quantitatives}}
\sphinxAtStartPar
Dans le cas de variables quantitatives, les normes  \(L_p\) :

\sphinxAtStartPar
\(\|{\bf x_i}\|_p=\left (\displaystyle\sum_{j=1}^d|x_i^j|^p\right ) ^\frac{1}{p}\)

\sphinxAtStartPar
sont classiquement utilisées, et par exemple
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(p=1\) : \(\|{\bf x_i}-{\bf x_j}\|_1=\displaystyle\sum_{k=1}^d|x_i^k-x_j^k|\) est la norme \(L_1\) (ou city block).

\item {} 
\sphinxAtStartPar
\(p=2\) : \(\|{\bf x_i}-{\bf x_j}\|_2=\sqrt{\displaystyle\sum_{k=1}^d(x_i^k-x_j^k)^2}\) est la norme \(L_2\) (ou norme euclidienne).

\item {} 
\sphinxAtStartPar
“\(p=\infty\)” : \(\|{\bf x_i}-{\bf x_j}\|_\infty = \displaystyle\max_{1\leq k\leq d}\{|x_i^k-x_j^k|\}\) est la norme du max (ou norme de Tchebychev)

\end{itemize}

\sphinxAtStartPar
Si les variables ne sont pas normalisées, on peut utiliser la distance de Mahalanobis

\sphinxAtStartPar
\(\delta_{ij} = \displaystyle\sum_{k=1}^d\displaystyle\sum_{l=1}^dw_{kl}(x_i^k-x_j^k)(x_i^l-x_j^l)\)

\sphinxAtStartPar
où la matrice des \(w_{kl}\) est l’inverse de la matrice de covariance empirique. Cette distance élimine également les corrélations entre variables.

\sphinxAtStartPar
Enfin, on peut utiliser une métrique issue du coefficient de corrélation, dite distance de Pearson : \(\delta_{ij} =\sqrt{1-r^2_{ij}}\), avec

\sphinxAtStartPar
\(r^2_{ij} = \frac{\left (\displaystyle\sum_{k=1}^d (x_i^k-\bar{x_i})(x_j^k-\bar{x_j})\right )^2}{\displaystyle\sum_{k=1}^d(x_i^k-\bar{x_i})^2\displaystyle\sum_{k=1}^d(x_j^k-\bar{x_j})^2}\)


\subsubsection{Variables de comptage}
\label{\detokenize{clustering:variables-de-comptage}}
\sphinxAtStartPar
Dans le cas particulier de variables de comptage (\(x_i^k\) effectif de la classe \(k\) pour l’individu \(i\)), une mesure naturelle de dissimilarité entre \({\bf x_i}\) et \({\bf x_j}\) est le \(\chi^2\) du tableau de contingence 2\(\times d\) associé.


\subsubsection{Quelle mesure choisir ?}
\label{\detokenize{clustering:quelle-mesure-choisir}}
\sphinxAtStartPar
Une réflexion  sur le type de dissimilarité à choisir est nécessaire. Il est en particulier intéressant de répondre aux questions suivantes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
de quelles variables initiales (qualitatives et/ou quantitatives) doit dépendre la dissimilarité?

\item {} 
\sphinxAtStartPar
est\sphinxhyphen{}il souhaitable (et possible) d’obtenir des variables pertinentes supplémentaires? Si oui par mesure ? par analyse linéaire (ACP,…) ou non linéaire (manifold learning) ?

\item {} 
\sphinxAtStartPar
quelles doivent être les importances relatives des diverses variables retenues dans la constitution de la dissimilarité ?

\end{itemize}


\section{Classification ascendante hiérarchique}
\label{\detokenize{clustering:classification-ascendante-hierarchique}}
\sphinxAtStartPar
L’objectif est de construire une hiérarchie indicée d’un ensemble \(\Omega\) sur lequel on connaît une mesure de dissimilarité \(\delta\) telle que les points les plus proches soient regroupés dans les classes de plus petit indice. La hiérarchie est alors construite en appliquant itérativement ce principe, et l’arbre obtenu sur l’ensemble des itérations est appelé un dendrogramme.

\sphinxAtStartPar
Il existe essentiellement
deux approches :
\begin{itemize}
\item {} 
\sphinxAtStartPar
la classification descendante : on divise \(\Omega\) en classes, puis on recommence sur chacune de ces classes itérativement jusqu’à ce que les classes soient réduites à des singletons.

\item {} 
\sphinxAtStartPar
la classification ascendante : cette fois on part de la partition de \(\Omega\)  où chaque classe est un singleton. On procède alors par fusions successives des classes jusqu’à obtenir une seule classe, c’est\sphinxhyphen{}à \sphinxhyphen{}dire l’ensemble  \(\Omega\) lui\sphinxhyphen{}même. Nous insistons sur ce type de classification dans la suite.

\end{itemize}


\subsection{Algorithme}
\label{\detokenize{clustering:algorithme}}
\index{Clustering hiérarchique@\spxentry{Clustering hiérarchique}}\ignorespaces 

\subsubsection{Construction de la hiérarchie}
\label{\detokenize{clustering:construction-de-la-hierarchie}}\label{\detokenize{clustering:index-2}}
\sphinxAtStartPar
\(\Omega\)  étant l’ensemble à classifier et \(\delta\) une mesure de dissimilarité sur cet ensemble, on définit à partir de \(\delta\) une  distance \(D\) entre les parties de  \(\Omega\). Cette distance est en réalité une mesure de dissimilarité qui ne vérifie pas nécessairement toutes les propriétés d’une distance sur l’ensemble des parties de \(\Omega\). En général, \(D\) est appelé critère d’agrégation.
L’algorithme est alors le suivant :
\label{clustering:algorithm-3}
\begin{sphinxadmonition}{note}{Algorithm 4 (Algorithme de clustering hiérarchique ascendant)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} Les éléments de \(\Omega\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} Une hiérarchie
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialisation : partition des singletons

\item {} 
\sphinxAtStartPar
Calcul des distances entre classes.

\item {} 
\sphinxAtStartPar
Tant que le nombre de classes est \(>\)1
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Regroupement des 2 classes les plus proches au sens de \(D\)

\item {} 
\sphinxAtStartPar
Calcul des distances entre la nouvelle classe et les anciennes classes non regroupées.

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
Il est facile de montrer que l’ensemble des classes définies au cours de cet algorithme forme une hiérarchie.


\subsubsection{Construction de l’indice}
\label{\detokenize{clustering:construction-de-l-indice}}
\sphinxAtStartPar
Après avoir défini une hiérarchie, il est nécessaire de lui associer un indice. Pour les classes du bas de la hiérarchie, c’est\sphinxhyphen{}à\sphinxhyphen{}dire les singletons, cet indice est nécessairement la valeur 0. Pour les autres classes, cet indice est généralement
défini en associant à chacune des classes construites au cours de l’algorithme la distance \(D\) qui séparait les deux classes fusionnées pour former cette nouvelle classe. Pour que cette définition conduise bien à un indice, il est nécessaire que
les indices obtenus soient strictement croissants avec le niveau de la hiérarchie. Plusieurs difficultés peuvent alors apparaître :
\begin{itemize}
\item {} 
\sphinxAtStartPar
pour certains critères d’agrégation, l’indice ainsi défini n’est pas nécessairement croissant. On parle alors d’inversion. Par exemple, si les données sont formées par trois points du plan situés au sommet d’un triangle équilatéral de côté 1 et si on prend comme distance \(D\) entre classes la distance entre les centres de gravité, on obtient une inversion.

\item {} 
\sphinxAtStartPar
lorsqu’il y a égalité de l’indice pour plusieurs niveaux emboîtés, il suffit de filtrer la hiérarchie, c’est\sphinxhyphen{}à\sphinxhyphen{}dire conserver une seule classe qui regroupe toutes les classes emboîtées ayant le même indice.

\end{itemize}


\subsection{Critères d’agrégation}
\label{\detokenize{clustering:criteres-d-agregation}}
\sphinxAtStartPar
Il existe de nombreux critères d’agrégation, mais les plus utilisés sont les suivants :
\begin{itemize}
\item {} 
\sphinxAtStartPar
critère du lien commun : \(D_{min}(A,B)=\displaystyle\min_{i\in A,j\in B}\delta_{ij}\)

\item {} 
\sphinxAtStartPar
critère du lien maximum: \(D_{max}(A,B)=\displaystyle\max_{i\in A,j\in B}\delta_{ij}\)

\item {} 
\sphinxAtStartPar
critère du lien moyen : \(D_{moy}(A,B)=\frac{\displaystyle\sum_{i\in A}\displaystyle\sum_{j\in B}\delta_{ij}}{|A||B|}\)

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{agreg}.png}


\subsection{Formule de récurrence de Lance et Williams}
\label{\detokenize{clustering:formule-de-recurrence-de-lance-et-williams}}
\sphinxAtStartPar
Pour les trois critères d’agrégation précédents, il existe des relations de simplification du calcul des distances entre classes essentielles pour la mise en place pratique de l’algorithme de classification ascendante :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(D_{min}(A,B\cup C)=min(D_{min}(A,B),D_{min}(A,C))\)

\item {} 
\sphinxAtStartPar
\(D_{max}(A,B\cup C)=max(D_{max}(A,B),D_{min}(A,C))\)

\item {} 
\sphinxAtStartPar
\(D_{moy}(A,B\cup C)=\frac{|B|D_{moy}(A,B)+|C|D_{moy}(A,C)}{|B|+|C|}\)

\end{itemize}


\subsection{Critère de Ward}
\label{\detokenize{clustering:critere-de-ward}}
\index{Ward@\spxentry{Ward}!critère@\spxentry{critère}}\ignorespaces 
\sphinxAtStartPar
Lorsque l’ensemble  \(\Omega\) à classifier est mesuré par \(d\) variables quantitatives, il est possible de lui associer un nuage de points pondérés dans \(\mathbb{R}^d\) muni de la distance euclidienne. Généralement, les pondérations seront toutes égales à 1. Le critère d’agrégation le plus utilisé dans cette situation est alors le critère d’inertie de Ward :

\sphinxAtStartPar
\(D(A,B)=\frac{p_Ap_B}{p_A+p_B}\|({\bf g}(A),{\bf g}(B))\|_2^2\)

\sphinxAtStartPar
où \(p_E\) représente la somme des pondérations des éléments d’une classe \(E\) et \({\bf g}(E)\) est le centre de gravité d’une classe \(E\).


\subsection{Propriétés d’optimalité}
\label{\detokenize{clustering:proprietes-d-optimalite}}
\sphinxAtStartPar
La notion de hiérarchie indicée est équivalente à la notion d’ultramétrique. La classification hiérarchique ascendante transforme donc la mesure de dissimilarité \(d\) initiale en une mesure de dissimilarité \(\delta\) qui possède la propriété d’être une ultramétrique.

\sphinxAtStartPar
Le problème de la classification hiérarchique peut donc également se poser en ces termes : trouver l’ultramétrique \(\delta^*\) la plus proche de \(\delta\). Il reste à munir l’espace des mesures de dissimilarité sur  \(\Omega\) d’une distance. On pourra utiliser, par exemple :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}(\delta_{ij}-\delta^*_{ij})^2\)

\item {} 
\sphinxAtStartPar
\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}|\delta_{ij}-\delta^*_{ij}|\)

\end{itemize}


\subsection{Critère d’arrêt et partition}
\label{\detokenize{clustering:critere-d-arret-et-partition}}
\index{Dendrogramme@\spxentry{Dendrogramme}}\ignorespaces 
\sphinxAtStartPar
L’ensemble des itérations peut être visualisé sous la forme d’un arbre, appelé dendrogramme. La figure suivante présente un exemple de dendrogramme en clustering hiérarchique descendant sur \(X = \{a, b, c, d, e\}\). La distance \(D\) n’est pas reportée

\sphinxAtStartPar
\sphinxincludegraphics{{dendro1}.png}

\sphinxAtStartPar
Le critère d’arrêt permet de déterminer la partition  de \(X\) la plus appropriée. Ici encore, plusieurs choix sont possibles :
\begin{itemize}
\item {} 
\sphinxAtStartPar
en fixant a priori un nombre de classes

\item {} 
\sphinxAtStartPar
en fixant une borne supérieure \(r\) pour \(D\), et en stoppant les itérations dès que les distances calculées par les liens dépassent \(r\). A noter que \(r\) peut être également calculé par \(r=\alpha max\{\delta(x,y),x,y\in X\}\) (critère dit “scale distance upper bound”).

\item {} 
\sphinxAtStartPar
en coupant le dendrogramme au saut de distance \(D\) maximal.

\end{itemize}

\sphinxAtStartPar
\sphinxincludegraphics{{dendro2}.png}


\subsection{Utilisation des méthodes}
\label{\detokenize{clustering:utilisation-des-methodes}}
\sphinxAtStartPar
La première difficulté est le choix de la mesure de dissimilarité sur  \(\Omega\) et du critère d’agrégation. Généralement, lorsque l’on dispose de variables quantitatives, le critère conseillé est le critère d’inertie. Ensuite, il est souvent nécessaire de disposer d’outils d’aide à l’interprétation et d’outils permettant de diminuer le nombre de niveaux de hiérarchie. Il est d’autre part conseillé d’utiliser conjointement d’autres méthodes d’analyse des données comme l’Analyse en Composantes Principales.


\subsection{Exemple}
\label{\detokenize{clustering:exemple}}
\sphinxAtStartPar
On étudie ici un jeu de données correspondant aux achats dans un supermarché. On cherche à caractériser les comportements des acheteurs en fonction de leurs revenus

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/Mall\PYGZus{}Customers.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   CustomerID   Genre  Age  Annual Income (k\PYGZdl{})  Spending Score (1\PYGZhy{}100)
0           1    Male   19                  15                      39
1           2    Male   21                  15                      81
2           3  Female   20                  16                       6
3           4  Female   23                  16                      77
4           5  Female   31                  17                      40
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
On affiche les données

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Score/Revenu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Revenu annuel (k\PYGZdl{})}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Score d}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{achat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Annual Income (k\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Spending Score (1\PYGZhy{}100)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Distribution des âges et des scores d}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{achat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Age}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Score d}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{achat}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Spending Score (1\PYGZhy{}100)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{40af125764252cecf7df0609a0d09aea389d627bca442ec7639bad8eb392434d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
L’objectif est de trouver des catégories de population ayant les mêmes comportements d’achat. Le nombre de classes étant inconnu, la classification héararchique va permettre de donner des indications sur le nombre de groupes.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{cluster}\PYG{n+nn}{.}\PYG{n+nn}{hierarchy} \PYG{k}{as} \PYG{n+nn}{sch}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dendrogramme}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Clients}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Indice}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hlines}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{190}\PYG{p}{,}\PYG{n}{xmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{xmax}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{,}\PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{linestyles}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+m+mi}{900}\PYG{p}{,}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{220}\PYG{p}{,}\PYG{n}{s}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cut}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{dendrogram} \PYG{o}{=} \PYG{n}{sch}\PYG{o}{.}\PYG{n}{dendrogram}\PYG{p}{(}\PYG{n}{sch}\PYG{o}{.}\PYG{n}{linkage}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{method} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{78d4a9a45386b64f29780bb351aae8389a5aa0d02e0f3b4e0007c6466783e532}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
On projette ensuite le résultat de la classification

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{AgglomerativeClustering}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{AgglomerativeClustering}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{metric} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{euclidean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linkage} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{y\PYGZus{}model} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Radins}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prudents}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Riches}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dépensiers modestes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{n}{y\PYGZus{}model} \PYG{o}{==} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s} \PYG{o}{=} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magenta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Conscients}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Classification}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{revenu annuel (k\PYGZdl{})}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Score (1\PYGZhy{}100)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{14}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{85f8f2ed254f16dc77c66f71965aac224a56a48f2667c2a76e212b3e1629d6ea}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Recherche de partitions}
\label{\detokenize{clustering:recherche-de-partitions}}

\subsection{Méthode des centres mobiles}
\label{\detokenize{clustering:methode-des-centres-mobiles}}
\index{Centres mobiles@\spxentry{Centres mobiles}}\ignorespaces 
\index{K\sphinxhyphen{}means@\spxentry{K\sphinxhyphen{}means}}\ignorespaces 
\sphinxAtStartPar
La méthode des centres mobiles est encore connue sous le nom de méthode de réallocation\sphinxhyphen{}centrage ou des k\sphinxhyphen{}means lorsque l’ensemble à classifier est mesuré par \(d\) variables. Ici, \(\Omega \in \mathbb{R}^d\) est muni de sa distance euclidienne \(\delta\). Pour simplifier la présentation, les pondérations des individus seront toutes égales à 1, mais la généralisation à des pondérations quelconques ne pose aucun problème.


\subsubsection{Algorithme}
\label{\detokenize{clustering:id2}}
\sphinxAtStartPar
L’algorithme des centres\sphinxhyphen{}mobiles peut se définir ainsi :
\label{clustering:algorithm-4}
\begin{sphinxadmonition}{note}{Algorithm 5 (Algorithme des centres mobiles)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(\Omega\),\(g\), métrique

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} Une partition de \(\Omega\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialisation : tirage au hasard de \(g\) points de  \(\Omega\) (centres initiaux des \(g\) classes)

\item {} 
\sphinxAtStartPar
Tant que (non convergence)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Étape E : Construction de la partition en affectant chaque point de \(\Omega\) à la classe dont il est le plus près du centre (en cas d’égalité, l’affectation se fait à la classe de plus petit indice).

\item {} 
\sphinxAtStartPar
Étape M : Les centres de gravité de la partition qui vient d’être calculée deviennent les nouveaux centres

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
\sphinxincludegraphics{{kmeans1}.png}

\sphinxAtStartPar
L’initialisation des centres de classe étant aléatoire, il convient de répliquer l’algorithme plusieurs fois et de, par exemple, retenir la partition majoritaire. La figure suivante présente deux résultats des k\sphinxhyphen{}means, sur un même jeu de données (5 classes, 50 points par classes), avec une initialisation aléatoire différente.

\sphinxAtStartPar
\sphinxincludegraphics{{kmeans2}.png}


\subsubsection{Critère et convergence}
\label{\detokenize{clustering:critere-et-convergence}}
\sphinxAtStartPar
La qualité d’un couple partition\sphinxhyphen{}centres est mesurée par la somme des inerties des classes par rapport à leur centre. On peut montrer qu’à chacune des deux étapes de l’algorithme, on améliore ce critère.


\subsubsection{Lien avec la méthode de Ward}
\label{\detokenize{clustering:lien-avec-la-methode-de-ward}}
\sphinxAtStartPar
La méthode des centres mobiles et la méthode de Ward optimisent toutes deux, à leur façon, le critère d’inertie intra\sphinxhyphen{}classe. Cette situation conduit à proposer des stratégies utilisant les deux approches comme, par exemple :
\begin{itemize}
\item {} 
\sphinxAtStartPar
appliquer les centres\sphinxhyphen{}mobiles pour regrouper l’ensemble initial en un nombre “important” de classes

\item {} 
\sphinxAtStartPar
appliquer la méthode de Ward en partant de ces classes

\item {} 
\sphinxAtStartPar
rechercher quelques “bons” niveaux de la hiérarchie

\item {} 
\sphinxAtStartPar
éventuellement, appliquer de nouveau la méthode des centres\sphinxhyphen{}mobiles sur les partitions obtenues pour améliorer encore leur critère.

\end{itemize}


\subsection{Généralisation : les nuées dynamiques}
\label{\detokenize{clustering:generalisation-les-nuees-dynamiques}}
\index{Nuées dynamiques@\spxentry{Nuées dynamiques}}\ignorespaces 
\sphinxAtStartPar
L’idée de base consiste à remplacer les centres   qui étaient des éléments de \(\mathbb{R}^d\) jouant le rôle de représentant ou encore de noyau de la classe par des éléments de nature très diverse adaptés au problème que l’on cherche à résoudre.


\subsubsection{Formalisation}
\label{\detokenize{clustering:formalisation}}
\sphinxAtStartPar
On note \(L=\{\lambda_i\}\) l’ensemble des noyaux, \(D:\Omega\times L\rightarrow \mathbb{R}^+\) une mesure de ressemblance entre éléments de \(\Omega\) et de \(L\). L’objectif est alors de trouver la partition en \(g\) classes (\(g\) fixé a priori) de \(\Omega\) minimisant le critère \(\displaystyle\sum_{k}\displaystyle\sum_{x\in P_k}D(x,\lambda_k)\)

\sphinxAtStartPar
Cette minimisation est réalisée de façon alternée, comme pour les centres mobiles.


\subsubsection{Choix du nombre de classes}
\label{\detokenize{clustering:choix-du-nombre-de-classes}}
\sphinxAtStartPar
En général, le critère n’est pas indépendant du nombre de classes. Par exemple, le critère de l’inertie s’annule pour la partition triviale pour laquelle chaque point forme une classe. Il s’agit donc de la meilleure partition. Il est donc
nécessaire de fixer a priori le nombre de classes. Pour résoudre ce problème très difficile, plusieurs solutions sont utilisées :
\begin{itemize}
\item {} 
\sphinxAtStartPar
on a une idée du nombre de classes désirées

\item {} 
\sphinxAtStartPar
on recherche la meilleure partition pour plusieurs nombres de classes et on étudie la décroissance du critère en fonction du nombre de classes (méthode du coude)

\item {} 
\sphinxAtStartPar
on définit une fonction \(f(\Omega)\) qui rend le critère indépendant du nombre de classes

\item {} 
\sphinxAtStartPar
on ajoute des contraintes supplémentaires (nombre d’individus par classe, volume d’une classe…). C’est l’option retenue par la méthode Isodata

\item {} 
\sphinxAtStartPar
on effectue des tests statistiques sur les classes

\end{itemize}


\subsection{Quelques variantes}
\label{\detokenize{clustering:quelques-variantes}}

\subsubsection{K\sphinxhyphen{}means++}
\label{\detokenize{clustering:k-means}}
\sphinxAtStartPar
Plutôt que d’initialiser les centres de manière aléatoire, l’algorithme K\sphinxhyphen{}means++ propose de partitionner \(\Omega=\{\mathbf x_1\cdots \mathbf x_n\}\) selon l’algorithme suivant :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Tirer uniformément le premier centre de classe \(c_1\) dans \(\Omega\)\textbackslash{}

\item {} 
\sphinxAtStartPar
Pour \(i\in[\![2,g]\!]\), choisir \(\mathbf{c_i}\) à partir de \(\mathbf x_i\) selon la probabilité \(D(\mathbf{x}_i)^2\) / \(\displaystyle\sum\limits_{j=1}^{m}{D(\mathbf{x}_j)}^2\) où  \(D(\mathbf{x}_i)\) est la distance entre \(\mathbf{x}_i\) et le centre de classe le plus proche déjà choisi. Ceci assure de tirer des centres de classe éloignés avec forte probabilité.

\end{enumerate}


\subsubsection{Accélération des k\sphinxhyphen{}means}
\label{\detokenize{clustering:acceleration-des-k-means}}
\sphinxAtStartPar
L’algorithme original peut être amélioré de manière significative en évitant les calculs de distances non nécessaires. En exploitant l’inégalité triangulaire, et en conservant les bornes inférieures et supérieures des distances entre les points et les centres de classe, l’algorithme correspondant est performant, y compris pour de grandes valeurs de \(k\) ({\hyperref[\detokenize{clustering:km}]{\sphinxcrossref{Algorithm 6}}})
\label{clustering:km}
\begin{sphinxadmonition}{note}{Algorithm 6 (Accélération des k\sphinxhyphen{}means)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(\Omega, g\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} \(P\) une partition de \(X\) en \(g\) classes
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialisation : tirage au hasard de \(g\) points \(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)

\item {} 
\sphinxAtStartPar
Pour \(\mathbf x\in \Omega,\mathbf c\in C\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
\(l(\mathbf x,\mathbf c)=0\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \Omega\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Affecter \(\mathbf x\) à la classe du centre le plus proche : \(\mathbf c(x) = Arg \displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)

\item {} 
\sphinxAtStartPar
A chaque calcul de \(\delta(\mathbf x,\mathbf c)\),\( l(\mathbf x,\mathbf c)=\delta(\mathbf x,\mathbf c)\)

\item {} 
\sphinxAtStartPar
\(u(\mathbf x,\mathbf c)=\displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Tant que (non convergence)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf c,\mathbf {c'}\in C\) calculer \(\delta (\mathbf c,\mathbf {c'})\)

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf c\) \(s(c)= \frac{1}{2}\displaystyle\min_{\mathbf {c'}\neq \mathbf c} \delta(\mathbf c,\mathbf {c'})\)

\item {} 
\sphinxAtStartPar
Identifier les \(\mathbf x\) tels que \(u(\mathbf x)\leq s(\mathbf c(\mathbf x))\)

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \Omega,\mathbf c\in C\) tels que \(\mathbf c\neq \mathbf c(\mathbf x)\) et \(u(\mathbf x)>l(\mathbf x,\mathbf c)\) et \(u(\mathbf x)>\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
Si \(r(\mathbf x)\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiv}{enumv}{}{.}%
\item {} 
\sphinxAtStartPar
Calculer \(\delta(\mathbf c(\mathbf x),\mathbf x)\)

\item {} 
\sphinxAtStartPar
\(r(\mathbf x)=Faux\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Sinon
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiv}{enumv}{}{.}%
\item {} 
\sphinxAtStartPar
\(\delta(\mathbf c(\mathbf x),\mathbf x)=u(\mathbf x)\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Si \(\delta(\mathbf c(\mathbf x),\mathbf x)>l(\mathbf x,\mathbf c)\)  ou \(\delta(\mathbf c(\mathbf x),\mathbf x)>\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiv}{enumv}{}{.}%
\item {} 
\sphinxAtStartPar
Calculer \(\delta(\mathbf x,\mathbf c)\)

\item {} 
\sphinxAtStartPar
Si \(\delta(\mathbf x,\mathbf c)<\delta(\mathbf c(\mathbf x),\mathbf x)\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumv}{enumvi}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbf c(\mathbf x)= \mathbf c\)

\end{enumerate}

\end{enumerate}

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf c\in C\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbf m(\mathbf c)\) : centre de masse des points de \(\Omega\) plus proches de \(\mathbf c\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \Omega,\mathbf c\in C\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(l(\mathbf x,\mathbf c)=max\left (l(\mathbf x,\mathbf c)-\delta(\mathbf m(\mathbf c),\mathbf c),0 \right )\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \Omega\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(u(\mathbf x)=u(\mathbf x)+\delta(\mathbf m(\mathbf c(\mathbf x)),\mathbf c(\mathbf x))\)

\item {} 
\sphinxAtStartPar
\(r(\mathbf x)=Vrai\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf c\in C\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbf c = \mathbf m(\mathbf c)\)

\end{enumerate}

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}


\subsubsection{k\sphinxhyphen{}means à mini batchs}
\label{\detokenize{clustering:k-means-a-mini-batchs}}
\sphinxAtStartPar
Il est également possible d’appliquer une optimisation par mini\sphinxhyphen{}batchs dans l’algorithme des k\sphinxhyphen{}means ({\hyperref[\detokenize{clustering:kmbatch}]{\sphinxcrossref{Algorithm 7}}}).
\label{clustering:kmbatch}
\begin{sphinxadmonition}{note}{Algorithm 7 (Accélération des k\sphinxhyphen{}means)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(\Omega, g\), \(b\) taille des batchs

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} \(P\) une partition de \(X\) en \(g\) classes
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialisation : tirage au hasard de \(g\) points \(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)

\item {} 
\sphinxAtStartPar
\(\mathbf v=0\in\mathbb{R}^g\)

\item {} 
\sphinxAtStartPar
Tant que non convergence
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathcal{B}\leftarrow\) batch de \(b\) exemples tirés de \(X\)

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \mathcal{B}\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
Affecter \(\mathbf x\) à la classe du centre le plus proche \(\mathbf T(\mathbf x)\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Pour tout \(\mathbf x\in \mathcal{B}\)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumiii}{enumiv}{}{.}%
\item {} 
\sphinxAtStartPar
\(\mathbf c = \mathbf T(\mathbf x)\)

\item {} 
\sphinxAtStartPar
\(v_c = v_c + 1\)

\item {} 
\sphinxAtStartPar
\(\eta = \frac{1}{v_c}\)

\item {} 
\sphinxAtStartPar
\(\mathbf c = (1-\eta)\mathbf c + \eta \mathbf x\)

\end{enumerate}

\end{enumerate}

\end{enumerate}
\end{sphinxadmonition}


\subsection{Exemple}
\label{\detokenize{clustering:id3}}
\sphinxAtStartPar
On génère des données

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}blobs}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{nb\PYGZus{}classes} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{center} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}
        \PYG{p}{[}\PYG{p}{[} \PYG{l+m+mi}{3}\PYG{p}{,}  \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1} \PYG{p}{,}  \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}  \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{cluster\PYGZus{}std} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    

\PYG{n}{X}\PYG{p}{,} \PYG{n}{y} \PYG{o}{=} \PYG{n}{make\PYGZus{}blobs}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}\PYG{n}{centers}\PYG{o}{=}\PYG{n}{center}\PYG{p}{,}\PYG{n}{cluster\PYGZus{}std} \PYG{o}{=} \PYG{n}{cluster\PYGZus{}std}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelbottom}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelleft}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{d2f13817276b12842d3ef7f2a6c79d8ba48f2b22ac3edf3ae55bb2e97113f22f}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Puis on applique l’algorithme des \(k\)\sphinxhyphen{}means.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{n}{nb\PYGZus{}classes}\PYG{p}{,}\PYG{n}{n\PYGZus{}init}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{y}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{rainbow}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Vraies classes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelbottom}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelleft}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{n}{model}\PYG{o}{.}\PYG{n}{labels\PYGZus{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{rainbow}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{K means à }\PYG{l+s+si}{\PYGZob{}0:d\PYGZcb{}}\PYG{l+s+s2}{ classes}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{nb\PYGZus{}classes}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelbottom}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{labelleft}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{282a71541470bf599f673324fd8333f803863bae5ab287f44588409753ee6b0a}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Modèles de mélange}
\label{\detokenize{clustering:modeles-de-melange}}
\sphinxAtStartPar
Les modèles de mélange supposent que les données proviennent d’un mélange de distributions (généralement gaussiennes), et l’objectif est alors d’estimer les paramètres du modèle de mélange en maximisant la fonction de vraisemblance pour les données.
L’optimisation directe de la fonction de vraisemblance dans ce cas n’est pas une tâche simple, en raison des contraintes nécessaires sur les paramètres et de la nature complexe de la fonction de vraisemblance, qui présente généralement un grand nombre de maxima locaux et de points de selle. Une méthode courante pour estimer les paramètres du modèle de mélange est l’algorithme EM.


\subsection{Définition}
\label{\detokenize{clustering:definition}}
\sphinxAtStartPar
Soient \(\mathcal S = \{\mathbf X_1\cdots X_n\}\) \(n\) vecteurs aléatoires i.i.d. à valeur dans \(\mathcal X\subset \mathbb{R}^d\) , chaque \(\mathbf X_i\) étant distribué selon
\begin{equation*}
\begin{split}g(\mathbf x|\boldsymbol \theta) = \displaystyle\sum_{i=1}^K w_i\Phi_i(\mathbf x)\end{split}
\end{equation*}
\sphinxAtStartPar
où \(\Phi_i,i\in[\![1,K]\!]\) sont des densités de probabilité sur \(\mathcal X\) et les \(w_i\) sont des poids positifs, sommant à 1. \(g\) peut être interprétée comme suit : soit \(Z\) une variable aléatoire discrète prenant les valeurs \(i\in[\![1,K]\!]\) avec probabilité \(w_i\), et soit \(\mathbf X\) un vecteur aléatoire dont la distribution conditionnelle, étant donnée \(Z=z\) est \(\Phi_z\). Alors
\begin{equation*}
\begin{split}\Phi_{Z,\mathbf X}(z,\mathbf x) = \Phi_Z(z)\Phi_{\mathbf X|Z}(\mathbf x,z) = w_z(\mathbf x)\end{split}
\end{equation*}
\sphinxAtStartPar
et la distribution marginale de \(\mathbf X\) est calculée en sommant sur \(z\) les probabilités jointes.

\sphinxAtStartPar
Un vecteur aléatoire \(\mathbf X\) suivant \(g\) peut donc être simulé d’abord en tirant \(Z\) suivant \(P(Z=z)=w_z,z\in[\![1,K]\!]\), puis en tirant \(\mathbf X\) suivant \(\Phi_Z\). La famille \(\mathcal S\) ne contenant que les \(\mathbf X_i\), les \(Z_i\) sont des variables latentes, interprétées comme les étiquettes cachées des classes auxquelles les \(\mathbf X_i\) appartiennent.

\sphinxAtStartPar
Typiquement, les \(\Phi_k\) sont des lois paramétriques. Classiquement ce sont des lois gaussiennes \(\mathcal N(\boldsymbol \mu_k,\boldsymbol \Sigma_k)\) et donc en rassemblant tous les paramètres des lois, incluant les \(w_k\), dans un vecteur de paramètre \(\boldsymbol \theta = (\mu_k,\boldsymbol \Sigma_k,w_k,k\in[\!1,K]\!])\), on peut écrire
\begin{equation*}
\begin{split}g(s|\boldsymbol \theta) = \prod_{i=1}^n g(\mathbf x_i|\boldsymbol \theta) = \prod_{i=1}^n \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k)\end{split}
\end{equation*}
\sphinxAtStartPar
où \(s=(\mathbf x_1\cdots \mathbf x_n)\) dénote une réalisation de \(\mathcal S\).

\sphinxAtStartPar
On estime alors \(\boldsymbol\theta\) en maximisant la log vraisemblance
\begin{equation*}
\begin{split}\ell(\boldsymbol\theta|s) = \displaystyle\sum_{i=1}^n ln(g(\mathbf x_i|\boldsymbol \theta)) = \displaystyle\sum_{i=1}^n ln \left ( \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k) \right )\end{split}
\end{equation*}
\sphinxAtStartPar
ce qui est en général complexe, la fonction \(\ell\) admettant de nombreux extrema locaux.


\subsection{Algorithme EM}
\label{\detokenize{clustering:algorithme-em}}
\sphinxAtStartPar
Plutôt que d’optimiser \(\ell\) directement depuis les données \(s\), l’algorithme EM ({\hyperref[\detokenize{clustering:EM}]{\sphinxcrossref{Algorithm 8}}}) augmente d’abord les données des variables latentes (les étiquettes \(\mathbf z=(z_1\cdots z_n)\) des classes). L’idée est que \(s\) est uniquement la partie observée des données aléatoires \((\mathcal S,\mathbf Z)\) générées d’abord en tirant \(Z\) suivant \(P(Z=z)\), puis en tirant \(\mathbf X\) suivant \(\Phi_z\), de sorte à avoir
\begin{equation*}
\begin{split}g(s,z|\boldsymbol \theta) = \displaystyle\prod_{i=1}^n w_{z_i} \Phi_{z_i}(\mathbf x_i)\end{split}
\end{equation*}
\sphinxAtStartPar
Ainsi, la log vraisemblance des données complètes, en général plus facile à optimiser, est
\begin{equation*}
\begin{split}\bar\ell(\boldsymbol\theta|s,z) =\displaystyle\sum{i=1}^n ln(w_{z_i} \Phi_{z_i}(\mathbf x_i))\end{split}
\end{equation*}
\sphinxAtStartPar
Cependant, les \(z\) ne sont pas observées et \(\bar\ell\) ne peut être évaluée. Dans l’étape E de l’algorithme EM, \(\bar\ell\) est remplacée par \(\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\), où l’indice \(p\) indique que \(\mathbf Z\) est distribuée selon la distribution conditionnelle de \(\mathbf Z\) étant donnée \(\mathcal S=s\), soit
\begin{equation*}
\begin{split}p(z)=g(z|s,\boldsymbol \theta) \propto g(s,z|\boldsymbol \theta)\end{split}
\end{equation*}\label{clustering:remark-7}
\begin{sphinxadmonition}{note}{Remark 22}



\sphinxAtStartPar
\(p(z)\) est de la forme \(p_1(z_1)\cdots p_n(z_n)\) de telle sorte que, étant donné \(\mathcal S=s\), les composantes de \(\mathbf Z\) sont deux à deux indépendantes.
\end{sphinxadmonition}
\label{clustering:EM}
\begin{sphinxadmonition}{note}{Algorithm 8 (Algorithmes EM)}



\sphinxAtStartPar
\sphinxstylestrong{Entrée :} \(s,\boldsymbol\theta^{(0)}\)

\sphinxAtStartPar
\sphinxstylestrong{Sortie :} Approximation de la log vraisemblance maximale
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\(i=1\)

\item {} 
\sphinxAtStartPar
Tant que (not stop)
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Etape E : Trouver \(p^{(i)}(z) = g(s|s,\boldsymbol\theta^{(i-1)})\) et \(Q^{(i)}(\boldsymbol\theta)=\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\)

\item {} 
\sphinxAtStartPar
Etape M : \(\boldsymbol\theta^{(i)} = Arg \displaystyle\max_{\boldsymbol\theta} Q^{(i)}(\boldsymbol\theta)\)

\item {} 
\sphinxAtStartPar
\(i = i+1\)

\end{enumerate}

\item {} 
\sphinxAtStartPar
Retourner \(\boldsymbol\theta{(i)}\)

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
Dans l’{\hyperref[\detokenize{clustering:EM}]{\sphinxcrossref{Algorithm 8}}}, un critère d’arrêt est par exemple
\begin{equation*}
\begin{split}\frac{\ell(\boldsymbol\theta{(i)}|s)-\ell(\boldsymbol\theta^{(i-1)}|s)}{\ell(\boldsymbol\theta{(i)}|s)}<\epsilon\end{split}
\end{equation*}
\sphinxAtStartPar
Sous certaines conditions, la suite des \(\ell(\boldsymbol\theta{(i)}|s)\) converge vers un maximum local de la log vraisemblance \(\ell\). La convergence vers le maximum global dépend bien sûr du choix de \(\boldsymbol\theta^{(0)}\), de sorte qu’une stratégie possible est d’exécuter plusieurs fois l’algorithme avec des initialisations différentes.

\sphinxAtStartPar
Dans le cas d’un mélange gaussien, \(\Phi_k=\mathcal N(\boldsymbol\mu_k,\boldsymbol\Sigma_k),k\in[\![1,K]\!]\). Si \(\boldsymbol\theta^{(i-1)}\) est le vecteur optimal à l’itération courante, constitué des poids \(w_k^{(i-1)}\), des vecteurs moyenne \((\boldsymbol\mu_k)^{(i-1)}\) et des matrices de covariances \((\boldsymbol\Sigma_k)^{(i-1)}\), alors on détermine \(p^{(i)}\), la distribution de \(\mathbf Z\) conditionnelement à \(\mathcal S=s\), pour le paramètre \(\boldsymbol\theta^{(i-1)}\). Puisque les composantes de \(\mathbf Z\) étant donné \(\mathcal S=s\) sont indépendantes, il suffit de spécifier la distribution discrète \(p_j^{(i)}\) de chaque \(Z_j\), étant données l’observation \(\mathbf X_j=\mathbf x_j\), calculée à l’aide de la forule de Bayes
\begin{equation*}
\begin{split}p_j^{(i)}(k)\propto w_k^{(i-1)}\Phi_k(\mathbf x_j|\boldsymbol\mu_k^{(i-1)},\boldsymbol\Sigma_k^{(i-1)}),k\in[\![1,K]\!]\end{split}
\end{equation*}
\sphinxAtStartPar
Alors
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Pour l’étape E

\end{enumerate}
\begin{equation*}
\begin{split}Q^{(i)}(\boldsymbol\theta) = \mathbb{E}_p \displaystyle\sum_{j=1}^n \left (ln w_{z_j} + ln \Phi_{z_j}(\mathbf x_j|\boldsymbol\mu_{Z_j},\boldsymbol\Sigma_{Z_j}) \right )\end{split}
\end{equation*}
\sphinxAtStartPar
où les \(Z_j\) sont indépendants et distribués selon \(p_j^{(i)}\).
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
Pour l’étape M, on maximise

\end{enumerate}
\begin{equation*}
\begin{split} \displaystyle\sum_{j=1}^n\displaystyle\sum_{k=1}^K p_j^{(i)}(k)\left (ln w_k + ln \Phi_k(\mathbf x_j|\boldsymbol\mu_{k},\boldsymbol\Sigma_{k})\right )\end{split}
\end{equation*}
\sphinxAtStartPar
sous la contrainte \(\displaystyle\sum_{k=1}^K w_k=1\). En utilisant une relxation lagrangienne, et le fait que \(\displaystyle\sum_{k=1}^K p_j^{(i)}(k)=1\) on trouve pour tout \(k\in[\![1,K]\!]\)
\begin{equation*}
\begin{split}w_k = \frac1n\displaystyle\sum_{j=1}^n p_j^{(i)}(k)\end{split}
\end{equation*}\begin{equation*}
\begin{split}\boldsymbol\mu_k = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) \mathbf x_j}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\end{split}
\end{equation*}\begin{equation*}
\begin{split}\boldsymbol\Sigma_{k} = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) (\mathbf x_j-\boldsymbol\mu_k)(\mathbf x_j-\boldsymbol\mu_k)^T}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{multivariate\PYGZus{}normal}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{genfromtxt}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/mixture.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{delimiter}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{K} \PYG{o}{=} \PYG{l+m+mi}{3}
\PYG{n}{n}\PYG{p}{,} \PYG{n}{d} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}

\PYG{c+c1}{\PYGZsh{} Paramètres initiaux}
\PYG{n}{W} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} poids}
\PYG{n}{M}  \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Moyennes}
\PYG{n}{C} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Co}

\PYG{n}{C}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{C}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{p} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:} 

    \PYG{c+c1}{\PYGZsh{} Etape E}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}  
        \PYG{n}{mvn} \PYG{o}{=} \PYG{n}{multivariate\PYGZus{}normal}\PYG{p}{(} \PYG{n}{M}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{C}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{p}{)}
        \PYG{n}{p}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{W}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{*}\PYG{n}{mvn}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Etape M}
    \PYG{n}{p} \PYG{o}{=} \PYG{n}{p}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}   
    \PYG{n}{W} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{K}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{M}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{p}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{T}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{p}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{xm} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{\PYGZhy{}} \PYG{n}{M}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{C}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{xm} \PYG{o}{@} \PYG{p}{(}\PYG{n}{xm}\PYG{o}{*}\PYG{n}{p}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{o}{/}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{p}\PYG{p}{[}\PYG{n}{k}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}


\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{X}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{c} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{v}\PYG{p}{,} \PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{eigh}\PYG{p}{(}\PYG{n}{C}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{v} \PYG{o}{=} \PYG{l+m+mf}{2.0} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mf}{2.0}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}
    \PYG{n}{u} \PYG{o}{=} \PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{angle} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arctan}\PYG{p}{(}\PYG{n}{u}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{/} \PYG{n}{u}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{angle} \PYG{o}{=} \PYG{l+m+mf}{180.0} \PYG{o}{*} \PYG{n}{angle} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}  
    \PYG{n}{ell} \PYG{o}{=} \PYG{n}{matplotlib}\PYG{o}{.}\PYG{n}{patches}\PYG{o}{.}\PYG{n}{Ellipse}\PYG{p}{(}\PYG{n}{M}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{v}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{v}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{angle}\PYG{o}{=}\PYG{l+m+mf}{180.0} \PYG{o}{+} \PYG{n}{angle}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{c}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ell}\PYG{o}{.}\PYG{n}{set\PYGZus{}clip\PYGZus{}box}\PYG{p}{(}\PYG{n}{fig}\PYG{o}{.}\PYG{n}{bbox}\PYG{p}{)}
    \PYG{n}{ell}\PYG{o}{.}\PYG{n}{set\PYGZus{}alpha}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{)}
    \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}artist}\PYG{p}{(}\PYG{n}{ell}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{e82489a2b59d3c7873ab58b0cc2164a3c6ec3d3afb14eb796120c6bc39ad98d5}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\part{Annexes}

\sphinxstepscope


\chapter{Analyse Factorielle des correspondances}
\label{\detokenize{afc:analyse-factorielle-des-correspondances}}\label{\detokenize{afc::doc}}
\index{Analyse@\spxentry{Analyse}!factorielle des correspondances@\spxentry{factorielle des correspondances}}\ignorespaces 
\sphinxAtStartPar
On cherche à expliquer la liaison entre deux variables qualitatives \(X\) et \(Y\), caractérisées par un ensemble de couples de modalités \((x_i,y_i)\). On note \(x_1\cdots x_J\) et \(y_1\cdots y_K\) les modalités distinctes de \(X\) et \(Y\) respectivement.

\sphinxAtStartPar
Plus précisément, l’analyse factorielle des correspondances (AFC) vise à définir un modèle statistique permettant de fournir des paramètres dont la représentation graphique illustre les correspondances entre les modalités de ces variables. Dans sa version “analyse de données”, l’AFC cherche à réduire la dimension des données en effectuant la décomposition factorielle des nuages de points associés aux profils lignes et aux profils colonnes du tableau de contingence croisant les modalités des deux variables (L’AFC est une double ACP sur les deux tableaux de profils). On aborde à la fin du chapitre une modélisation statistique de l’AFC, en supposant que les fréquences d’observation correspondent à l’observation d’une probabilité théorique, dont la distribution modélise le tableau de contingence des deux variables.


\section{Notations}
\label{\detokenize{afc:notations}}
\sphinxAtStartPar
Le tableau de contingence \({\bf T}\) entre les \(X\) et \(Y\), vu comme une matrice,  est défini par

\index{Tableau@\spxentry{Tableau}!contingence@\spxentry{contingence}}\ignorespaces 
\index{Contingence@\spxentry{Contingence}!tableau@\spxentry{tableau}}\ignorespaces 

\begin{savenotes}\sphinxattablestart
\centering
\phantomsection\label{\detokenize{afc:index-2}}\nobreak
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(y_1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\cdots\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(y_k\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\cdots\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(y_K\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
total
\\
\hline
\sphinxAtStartPar
\(x_1\)
&
\sphinxAtStartPar
\(n_{11}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{1k}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{1K}\)
&
\sphinxAtStartPar
\(n_{1.}\)
\\
\hline
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
\\
\hline
\sphinxAtStartPar
\(x_j\)
&
\sphinxAtStartPar
\(n_{j1}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{jk}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{jK}\)
&
\sphinxAtStartPar
\(n_{j.}\)
\\
\hline
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
&
\sphinxAtStartPar
\(\vdots\)
\\
\hline
\sphinxAtStartPar
\(x_J\)
&
\sphinxAtStartPar
\(n_{J1}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{Jk}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{JK}\)
&
\sphinxAtStartPar
\(n_{J.}\)
\\
\hline
\sphinxAtStartPar
total
&
\sphinxAtStartPar
\(n_{.1}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{.k}\)
&
\sphinxAtStartPar
\(\cdots\)
&
\sphinxAtStartPar
\(n_{.K}\)
&
\sphinxAtStartPar
\(n\)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
où \(n_{j.}\) (resp \(n_{.k}\) )sont les effectifs marginaux représentant le nombre de fois où \(x_j\) (resp. \(y_k\)) apparaît, et \(n_{jk}\) le nombre d’apparitions du couple \((x_j,y_k)\).

\sphinxAtStartPar
Les fréquences conjointes \(f_{jk}=\frac{n_{jk}}{n}\) et les fréquences marginales sont stockées dans des vecteurs \({\bf g_J}=\begin{pmatrix}f_{1.}\ldots f_{J.} \end{pmatrix}^T\) et \({\bf g_K}=\begin{pmatrix}f_{.1}\ldots f_{.K} \end{pmatrix}^T\).

\sphinxAtStartPar
On note aussi \({\bf D_J}=diag\left (f_{1.}\ldots f_{J.}\right )\) et \({\bf D_K}=diag\left (f_{.1}\ldots f_{.K} \right )\).

\sphinxAtStartPar
Dans le tableau de contingence \(\mathbf T\), on lit le \(j^e\) profil ligne \([\frac{n_{j1}}{n_{j.}}\ldots \frac{n_{jK}}{n_{j.}}]\), considéré comme un vecteur de \(\mathbb{R}^K\) et le \(k^e\) profil colonne \([\frac{n_{1k}}{n_{.k}}\ldots \frac{n_{Jk}}{n_{.k}}]\) considéré comme un vecteur de \(\mathbb{R}^J\). Ces profils sont rangés dans des matrices de profils lignes \({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R})\) et de colonnes \({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\) définies par \({\bf A}=\frac{1}{n}{\bf T^TD_J^{-1}}\textrm{  et  } {\bf B}=\frac{1}{n}{\bf T D_K^{-1}}\)


\section{Double ACP}
\label{\detokenize{afc:double-acp}}
\sphinxAtStartPar
L’analyse factorielle des correspondances peut être considérée comme le résultat d’une double ACP :
\begin{itemize}
\item {} 
\sphinxAtStartPar
une effectuée sur les profils colonnes dans \(\mathbb{R}^J\)

\item {} 
\sphinxAtStartPar
une effectuée sur les profils lignes dans \(\mathbb{R}^K\)

\end{itemize}

\sphinxAtStartPar
relativement à la métrique du \(\chi^2\) de matrice \({\bf D_K^{-1}}\) pour l’analyse en lignes et \({\bf D_J^{-1}}\) pour l’analyse en colonnes.

\sphinxAtStartPar
Ainsi, par exemple, la distance entre deux modalités \(x_l\) et \(x_p\) de \(X\) est donnée par :

\sphinxAtStartPar
\(\Vert {\bf A_{.l}}-{\bf A_{.p}}\Vert^2_{{\bf D_K^{-1}}} = \displaystyle\sum_{i=1}^K\frac{1}{f_{.i}}\left (A_{i,l}-A_{i,p} \right )^2\)

\sphinxAtStartPar
où \({\bf A_{.l}}\) est la \(l^e\) colonne de \({\bf A}\). La métrique du \(\chi^2\) introduit les inverses
des fréquences marginales des modalités de \(Y\) comme pondérations des écarts
entre éléments de deux profils relatifs à \(X\) (et réciproquement). Elle attribue
donc plus de poids aux écarts correspondants à des modalités de faible effectif
(rares) pour \(Y\).


\section{ACP dans \protect\(\mathbb{R}^J\protect\)}
\label{\detokenize{afc:acp-dans-mathbb-r-j}}
\sphinxAtStartPar
L’ACP sur les profils colonnes est réalisée en recherchant les éléments propres de \({\bf BA}\), symétrique par rapport à la métrique \({\bf D_J^{-1}}\) et semi définie positive. On note \(\bf U\) la matrice des vecteurs propres. Cette ACP fournit une représentation des modalités de \(Y\), réalisée au moyen des lignes de la matrice des composantes principales \({\bf C_K}={\bf B^TD_J^{-1}U}\).


\section{ACP dans \protect\(\mathbb{R}^K\protect\)}
\label{\detokenize{afc:acp-dans-mathbb-r-k}}
\sphinxAtStartPar
L’ACP sur les profils lignes est réalisée en recherchant les éléments propres de \({\bf AB}\), symétrique par rapport à la métrique \({\bf D_K^{-1}}\) et semi définie positive. On note \(\bf V\) la matrice des vecteurs propres. Cette ACP fournit une représentation des modalités de \(X\), réalisée au moyen des lignes de la matrice des composantes principales \({\bf C_J}={\bf A^TD_K^{-1}V}\).

\sphinxAtStartPar
Puisque \(\bf U\) contient les vecteurs propres de \({\bf BA}\) et \(\bf V\) ceux de \({\bf AB}\), il suffit de réaliser en fait une seule ACP, les résultats de l’autre s’en déduisant simplement : si \(\bf \Lambda\) est la matrice des valeurs propres (hors \(\lambda_0=0\)) communes aux deux ACP :

\sphinxAtStartPar
\({\bf V} ={\bf AU\Lambda^{-\frac{1}{2}}}\textrm {  et  } {\bf U} ={\bf BV\Lambda^{-\frac{1}{2}}}\)
Alors

\sphinxAtStartPar
\({\bf C_K}={\bf B^TD_J^{-1}U }= {\bf B^TD_J^{-1}BV\Lambda^{-\frac{1}{2}}} = {\bf D_K^{-1}ABV \Lambda^{-\frac{1}{2}}} =  {\bf D_K^{-1}V \Lambda^{\frac{1}{2}}}\)
et

\sphinxAtStartPar
\({\bf C_J}={\bf A^TD_K^{-1}V}= {\bf D_J^{-1}U \Lambda^{\frac{1}{2}}}\)
d’où

\sphinxAtStartPar
\({\bf C_K}={\bf B^TC_J\Lambda^{-\frac{1}{2}}} \textrm{    et    } {\bf C_J}={\bf A^TC_K\Lambda^{-\frac{1}{2}}}\)
\label{afc:remark-0}
\begin{sphinxadmonition}{note}{Remark 23}



\sphinxAtStartPar
Soit deux matrices \({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R)}\)  et \({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\). Les valeurs propres non nulles de \({\bf AB}\) et \({\bf BA}\) sont identiques avec le même degré de multiplicité. De plus, si \(\bf u\) est vecteur propre de \({\bf BA}\) associé à la valeur propre \(\lambda\neq 0\), alors \({\bf v} = {\bf Au} \) est vecteur propre de \({\bf AB}\) associé à la même valeur
propre.
\end{sphinxadmonition}


\section{Représentation graphique}
\label{\detokenize{afc:representation-graphique}}
\sphinxAtStartPar
La décomposition de \(\mathbf T/n\) donne :

\sphinxAtStartPar
\(\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}} = \displaystyle\sum_{i=0}^{min(J-1,K-1)}\sqrt{\lambda_i}\frac{u^i_jv^i_k}{f_{j.}f_{.k}}\)
Cette quantité est appelée taux de liaison entre les modalités \(j\) et \(k\). En se limitant au rang \(q\) on obtient pour chaque couple de modalité \((j,k)\) de \(\mathbf T\) une approximation de son écart relatif à l’indépendance, comme produit scalaire des deux vecteurs \(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{j.}}u_j\) et \(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{.k}}v_k\), termes génériques des matrices \({\bf D_J^{-1}U\Lambda^{\frac{1}{4}}}\) et \({\bf D_K^{-1}V\Lambda^{\frac{1}{4}}}\)

\sphinxAtStartPar
La représentation graphique de ces vecteurs (par exemple avec \(q=2\)), appelée biplot, donne la correspondance entre les deux modalités \(x_j\) et \(y_k\). Lorsque ces deux modalités, éloignées de l’origine, sont
voisines (resp. opposées), leur produit scalaire est de valeur absolue importante ; leur cellule conjointe contribue alors fortement et de manière positive
(resp. négative) à la dépendance entre les deux variables. L’analyse factorielle des correspondances apparaît ainsi comme la meilleure reconstitution des fréquences \(f_{jk}\), ou encore la meilleure représentation des écarts relatifs à l’indépendance.


\section{Interprétation}
\label{\detokenize{afc:interpretation}}
\sphinxAtStartPar
Les qualités de représentation dans la dimension choisie et les contributions
des modalités de \(X\) ou de \(Y\) se déduisent facilement de celles de l’ACP. Ces
quantités sont utilisées à la fois pour choisir la dimension de l’analyse factorielle des correspondances  et pour interpréter ses résultats dans la dimension choisie.


\section{Inertie et test d’indépendance}
\label{\detokenize{afc:inertie-et-test-d-independance}}
\sphinxAtStartPar
En analyse en composantes principales centrée réduite, l’inertie totale du nuage de points est  égale au nombre de variables. En AFC,  l’inertie totale du nuage des profils lignes est  égale à l’inertie totale du nuage des profils colonnes, égale au \(\chi^2\) d’indépendance entre les deux variables qualitatives.\\
La valeur de l’inertie est donc un indicateur de la dispersion des nuages de points et une mesure de liaison entre les deux variables qualitatives,  appelée mesure d’écart à l’indépendance.


\section{Interprétation des valeurs propres}
\label{\detokenize{afc:interpretation-des-valeurs-propres}}
\sphinxAtStartPar
Les valeurs propres des ACP renseignent sur la dispersion des nuages de profils lignes et colonnes :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Une valeur propre proche de 1 indique une dichotomie parfaite du tableau \(\mathbf T\), qui peut être décomposé après reclassement des modalités en deux blocs distincts

\item {} 
\sphinxAtStartPar
Plus généralement \(p\) valeurs propres proches amènent à \(k+1\) blocs distincts

\item {} 
\sphinxAtStartPar
Si toutes les valeurs propres sont proches de 1, on aboutit à l’effet Guttman : il existe une correspondance entre chaque modalité ligne et une modalité colonne “associée”. Avec une réorganisation des modalités, les effectifs importants se trouvent alors le long de la diagonale.

\end{itemize}


\section{Qualité globale}
\label{\detokenize{afc:qualite-globale}}
\sphinxAtStartPar
A \(q\) fixé, la qualité globale de la représentation se mesure comme dans le cadre de l’ACP, comme le rapport entre les \(q\) premières valeurs propres \(\lambda_i\) et la somme sur tout le spectre.

\sphinxAtStartPar
On montre que la qualité de la représentation dans la \(i^e\) dimension s’écrit \(\frac{n\lambda_i}{\chi^2}\)


\section{Qualité de chaque modalité}
\label{\detokenize{afc:qualite-de-chaque-modalite}}
\sphinxAtStartPar
Comme dans l’ACP également, la qualité d’une modalité de \(X\) (resp. \(Y\)) se quantifie par le carré du cosinus de l’angle entre le vecteur représentant cette modalité dans \(\mathbb{R}^K\) (resp. \(\mathbb{R}^J\))  et sa projection orthogonale au sens de \({\bf D_K^{-1}}\) (resp. \({\bf D_J^{-1}}\)) dans le sous\sphinxhyphen{}espace principal de dimension \(q\). Ces cosinus se calculent très simplement en faisant le rapport des sommes appropriées des carrés des coordonnées extraites des lignes de \({\bf C_J}\) (resp. \({\bf C_K}\)).


\section{Inertie expliquée}
\label{\detokenize{afc:inertie-expliquee}}
\sphinxAtStartPar
L’inertie totale du nuage des profils lignes (resp. colonnes) est égale à la somme de toutes les valeurs propres \(\lambda_i\). La part due au \(j^e\) profil ligne (resp. \(k^e\) profil colonne) est \(f_{j.}\displaystyle\sum_i \left (\mathbf{C_J}(ji) \right )^2\) (resp. \(f_{.k}\displaystyle\sum_i \left (\mathbf{C_K}(ik) \right )^2\)).

\sphinxAtStartPar
Les contributions à l’inertie selon chaque axe se calculent de la même manière, sans sommation sur \(i\). Elles sont utilisées pour sélectionner les modalités les plus importantes (i.e. celles qui importent le plus dans la définition de la liaison entre \(X\) et \(Y\)).


\section{Choix de q}
\label{\detokenize{afc:choix-de-q}}
\sphinxAtStartPar
Comme dans le cas de l’ACP, le choix de l’espace de représentation est important. On peut estimer \(q\) comme en ACP (pourcentage de l’inertie expliquée, décroissance des valeurs propres), ou utiliser une approche probabiliste : si

\sphinxAtStartPar
\(\nu_{jk}^q = n f_{j.}f_{.k} + n\displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\)
est l’estimation d’ordre \(q\) de \(n_{jk}\) alors sous certaines conditions (\(n\) grand, modèle multinomial…), on montre que

\sphinxAtStartPar
\(\displaystyle\sum_{j=1}^J\displaystyle\sum_{k=1}^K\frac{\left (n_{jk}-\nu_{jk}^q \right )^2}{\nu_{jk}^q}\approx \displaystyle\sum_{i\geq q+1} \lambda_i\)

\sphinxAtStartPar
suit approximativement une loi \(\chi^2\) à \((J-q-1)(K-q-1)\) degrés de liberté. On peut donc retenir \(q\) comme étant la plus petite dimension telle que cette quantité est inférieure à la valeur limite de cette loi.


\section{Modèle statistique}
\label{\detokenize{afc:modele-statistique}}
\sphinxAtStartPar
On suppose que chaque fréquence \(f_{jk}\) correspond à l’observation d’une probabilité   théorique \(\pi_{jk}\) et on modélise donc \(\bf T\) par la distribution correspondante. Le modèle décrivant cette distribution permet d’expliciter la probabilité.


\section{Modèle log linéaire}
\label{\detokenize{afc:modele-log-lineaire}}
\sphinxAtStartPar
Souvent, le nombre \(n\) est fixé a priori. La distribution conjointe des effectifs \(n_{jk}\) est alors conditionnée par \(n\) et est une loi multinomiale de paramètre \(\pi_{jk}\) et d’espérance \(n\pi_{jk}\).

\sphinxAtStartPar
Par définition, les variables \(X\) et \(Y\) sont indépendantes si \(\pi_{jk}=\pi_{j.}\pi_{.k}\). Dans le cas contraire, on peut écrire

\sphinxAtStartPar
\(\pi_{jk} = \pi_{j.}\pi_{.k}\frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\)

\sphinxAtStartPar
En passant au log, on linéarise en

\sphinxAtStartPar
\(ln\left (\pi_{jk}\right ) =  ln (\pi_{j.}) + ln (\pi_{.k})  + ln \left( \frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\right )\)

\sphinxAtStartPar
Ce modèle est saturé car il comporte autant de paramètres que de données.  L’indépendance est vérifiée si le dernier terme de couplage est nul pour tout \((j,k)\). Les paramètres du modèle sont estimés en maximisant la log vraisemblance.


\section{Modèle de corrélation}
\label{\detokenize{afc:modele-de-correlation}}
\sphinxAtStartPar
Dans ce modèle, on écrit

\sphinxAtStartPar
\(\pi_{jk} = \pi_{j.}\pi_{.k} + \displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\)

\sphinxAtStartPar
où \({\bf u^i}\) (resp. \({\bf v^i}\)) sont les vecteurs propres de \({\bf BA}\) (resp. \({\bf AB}\)),  \(\lambda_i\) les valeurs propres associées (qui sont identiques pour les deux matrices), et \(q\leq min(J-1,K-1)\).

\sphinxAtStartPar
Les contraintes \(\displaystyle\sum_{j=1}^J u^i_j =  \displaystyle\sum_{k=1}^K v^i_k = 0\)  et \({\bf (u^i)^TD_J^{-1} u^l }= {\bf (v^i)^TD_K^{-1} v^l} = \delta_{il}\) (vecteurs propres orthonormés) permettent d’identifier les paramètres du modèle. \textbackslash{}
Les estimations des paramètres \(\pi_{j.}\pi_{.k} ,\lambda_i,u^i,v^i\) peuvent être réalisées par maximum de vraisemblance ou par moindres carrés.


\section{Exemple}
\label{\detokenize{afc:exemple}}
\sphinxAtStartPar
On utilise ici des données open source du \sphinxhref{https://www.data.gouv.fr/fr/posts/les-donnees-des-elections/}{gouvernement}, présentant le résultat du premier tour des élections présidentielles de 2017.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Region
&\sphinxstyletheadfamily 
\sphinxAtStartPar
HAMON
&\sphinxstyletheadfamily 
\sphinxAtStartPar
MACRON
&\sphinxstyletheadfamily 
\sphinxAtStartPar
ASSELINEAU
&\sphinxstyletheadfamily 
\sphinxAtStartPar
FILLON
&\sphinxstyletheadfamily 
\sphinxAtStartPar
CHEMINADE
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
Grand\sphinxhyphen{}Est
&
\sphinxAtStartPar
151296
&
\sphinxAtStartPar
615775
&
\sphinxAtStartPar
30223
&
\sphinxAtStartPar
586390
&
\sphinxAtStartPar
6078
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Nelle\sphinxhyphen{}Aquitaine
&
\sphinxAtStartPar
240175
&
\sphinxAtStartPar
851372
&
\sphinxAtStartPar
26667
&
\sphinxAtStartPar
602884
&
\sphinxAtStartPar
6264
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
AURA
&
\sphinxAtStartPar
256620
&
\sphinxAtStartPar
1026255
&
\sphinxAtStartPar
41352
&
\sphinxAtStartPar
846252
&
\sphinxAtStartPar
7602
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Bourgogne\sphinxhyphen{}FC
&
\sphinxAtStartPar
87382
&
\sphinxAtStartPar
338187
&
\sphinxAtStartPar
14330
&
\sphinxAtStartPar
304387
&
\sphinxAtStartPar
2842
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
Bretagne
&
\sphinxAtStartPar
180827
&
\sphinxAtStartPar
581076
&
\sphinxAtStartPar
13419
&
\sphinxAtStartPar
380815
&
\sphinxAtStartPar
3400
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
Centre\sphinxhyphen{}Val\sphinxhyphen{}de\sphinxhyphen{}Loire
&
\sphinxAtStartPar
83552
&
\sphinxAtStartPar
323724
&
\sphinxAtStartPar
12075
&
\sphinxAtStartPar
300324
&
\sphinxAtStartPar
2882
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
Corse
&
\sphinxAtStartPar
5780
&
\sphinxAtStartPar
28528
&
\sphinxAtStartPar
965
&
\sphinxAtStartPar
39453
&
\sphinxAtStartPar
253
\\
\hline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
Ile\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
430404
&
\sphinxAtStartPar
1612816
&
\sphinxAtStartPar
64406
&
\sphinxAtStartPar
1249770
&
\sphinxAtStartPar
9796
\\
\hline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
Occitanie
&
\sphinxAtStartPar
216362
&
\sphinxAtStartPar
740037
&
\sphinxAtStartPar
28603
&
\sphinxAtStartPar
566045
&
\sphinxAtStartPar
5524
\\
\hline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
Hauts\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
166640
&
\sphinxAtStartPar
630300
&
\sphinxAtStartPar
26043
&
\sphinxAtStartPar
521389
&
\sphinxAtStartPar
5688
\\
\hline
\sphinxAtStartPar
10
&
\sphinxAtStartPar
Normandie
&
\sphinxAtStartPar
113744
&
\sphinxAtStartPar
423075
&
\sphinxAtStartPar
14303
&
\sphinxAtStartPar
370188
&
\sphinxAtStartPar
3544
\\
\hline
\sphinxAtStartPar
11
&
\sphinxAtStartPar
Pays\sphinxhyphen{}de\sphinxhyphen{}la\sphinxhyphen{}Loire
&
\sphinxAtStartPar
143491
&
\sphinxAtStartPar
575832
&
\sphinxAtStartPar
15529
&
\sphinxAtStartPar
516428
&
\sphinxAtStartPar
3731
\\
\hline
\sphinxAtStartPar
12
&
\sphinxAtStartPar
PACA
&
\sphinxAtStartPar
113344
&
\sphinxAtStartPar
520909
&
\sphinxAtStartPar
25948
&
\sphinxAtStartPar
615455
&
\sphinxAtStartPar
4569
\\
\hline
\sphinxAtStartPar
13
&
\sphinxAtStartPar
Outremer
&
\sphinxAtStartPar
101948
&
\sphinxAtStartPar
389440
&
\sphinxAtStartPar
18725
&
\sphinxAtStartPar
314017
&
\sphinxAtStartPar
3425
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Region
&\sphinxstyletheadfamily 
\sphinxAtStartPar
MELENCHON
&\sphinxstyletheadfamily 
\sphinxAtStartPar
LASSALLE
&\sphinxstyletheadfamily 
\sphinxAtStartPar
FILLON
&\sphinxstyletheadfamily 
\sphinxAtStartPar
DUPONT\sphinxhyphen{}AIGNAN
&\sphinxstyletheadfamily 
\sphinxAtStartPar
POUTOU
&\sphinxstyletheadfamily 
\sphinxAtStartPar
LEPEN
&\sphinxstyletheadfamily 
\sphinxAtStartPar
ARTHAUD
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
Grand\sphinxhyphen{}Est
&
\sphinxAtStartPar
484810
&
\sphinxAtStartPar
30508
&
\sphinxAtStartPar
586390
&
\sphinxAtStartPar
182200
&
\sphinxAtStartPar
34468
&
\sphinxAtStartPar
825600
&
\sphinxAtStartPar
24272
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Nelle\sphinxhyphen{}Aquitaine
&
\sphinxAtStartPar
703505
&
\sphinxAtStartPar
91915
&
\sphinxAtStartPar
602884
&
\sphinxAtStartPar
155600
&
\sphinxAtStartPar
49649
&
\sphinxAtStartPar
640228
&
\sphinxAtStartPar
21442
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
AURA
&
\sphinxAtStartPar
805846
&
\sphinxAtStartPar
53282
&
\sphinxAtStartPar
846252
&
\sphinxAtStartPar
215951
&
\sphinxAtStartPar
43530
&
\sphinxAtStartPar
867874
&
\sphinxAtStartPar
24670
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Bourgogne\sphinxhyphen{}FC
&
\sphinxAtStartPar
276954
&
\sphinxAtStartPar
15843
&
\sphinxAtStartPar
304387
&
\sphinxAtStartPar
87263
&
\sphinxAtStartPar
18529
&
\sphinxAtStartPar
387658
&
\sphinxAtStartPar
11492
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
Bretagne
&
\sphinxAtStartPar
385736
&
\sphinxAtStartPar
19097
&
\sphinxAtStartPar
380815
&
\sphinxAtStartPar
87928
&
\sphinxAtStartPar
27092
&
\sphinxAtStartPar
306644
&
\sphinxAtStartPar
14296
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
Centre\sphinxhyphen{}Val\sphinxhyphen{}de\sphinxhyphen{}Loire
&
\sphinxAtStartPar
252307
&
\sphinxAtStartPar
13570
&
\sphinxAtStartPar
300324
&
\sphinxAtStartPar
82060
&
\sphinxAtStartPar
16282
&
\sphinxAtStartPar
329470
&
\sphinxAtStartPar
11365
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
Corse
&
\sphinxAtStartPar
21314
&
\sphinxAtStartPar
8711
&
\sphinxAtStartPar
39453
&
\sphinxAtStartPar
4462
&
\sphinxAtStartPar
1374
&
\sphinxAtStartPar
43041
&
\sphinxAtStartPar
495
\\
\hline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
Ile\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
1225311
&
\sphinxAtStartPar
36358
&
\sphinxAtStartPar
1249770
&
\sphinxAtStartPar
226266
&
\sphinxAtStartPar
45715
&
\sphinxAtStartPar
708340
&
\sphinxAtStartPar
23592
\\
\hline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
Occitanie
&
\sphinxAtStartPar
734223
&
\sphinxAtStartPar
75483
&
\sphinxAtStartPar
566045
&
\sphinxAtStartPar
135405
&
\sphinxAtStartPar
35219
&
\sphinxAtStartPar
762104
&
\sphinxAtStartPar
16777
\\
\hline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
Hauts\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
633322
&
\sphinxAtStartPar
22411
&
\sphinxAtStartPar
521389
&
\sphinxAtStartPar
160722
&
\sphinxAtStartPar
33653
&
\sphinxAtStartPar
1003221
&
\sphinxAtStartPar
29194
\\
\hline
\sphinxAtStartPar
10
&
\sphinxAtStartPar
Normandie
&
\sphinxAtStartPar
362535
&
\sphinxAtStartPar
13900
&
\sphinxAtStartPar
370188
&
\sphinxAtStartPar
98957
&
\sphinxAtStartPar
23816
&
\sphinxAtStartPar
452702
&
\sphinxAtStartPar
15196
\\
\hline
\sphinxAtStartPar
11
&
\sphinxAtStartPar
Pays\sphinxhyphen{}de\sphinxhyphen{}la\sphinxhyphen{}Loire
&
\sphinxAtStartPar
403454
&
\sphinxAtStartPar
16988
&
\sphinxAtStartPar
516428
&
\sphinxAtStartPar
109842
&
\sphinxAtStartPar
26340
&
\sphinxAtStartPar
364267
&
\sphinxAtStartPar
16018
\\
\hline
\sphinxAtStartPar
12
&
\sphinxAtStartPar
PACA
&
\sphinxAtStartPar
515419
&
\sphinxAtStartPar
29551
&
\sphinxAtStartPar
615455
&
\sphinxAtStartPar
119025
&
\sphinxAtStartPar
21316
&
\sphinxAtStartPar
774791
&
\sphinxAtStartPar
10439
\\
\hline
\sphinxAtStartPar
13
&
\sphinxAtStartPar
Outremer
&
\sphinxAtStartPar
256149
&
\sphinxAtStartPar
7748
&
\sphinxAtStartPar
314017
&
\sphinxAtStartPar
29505
&
\sphinxAtStartPar
17599
&
\sphinxAtStartPar
213553
&
\sphinxAtStartPar
13180
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
On décide dans l’analyse d’enlever le candidat LASSALLE, dont les votes sont concentrés dans les Pyrenées et en Corse (et qui introduit un biais dans l’étude).

\sphinxAtStartPar
A partir de ce tableau de données \(\bf T\), on calcule les tableaux de fréquences en lignes et en colonnes, ainsi que les profils ligne et colonne moyens.
Comme en ACP, on s’intéresse à l’inertie du nuage de points, mais pour ce faire on utilise la distance du \(\chi^2\). Avec cette métrique, la distance entre deux lignes (ou deux colonnes) ne dépend pas des poids respectifs des colonnes (ou lignes). Par exemple, les différents candidats obtiennent des scores très différents et l’usage de la métrique euclidienne aurait donné trop de poids aux candidats qui ont obtenu des scores élevés. De plus, la métrique du \(\chi^2\) possède la propriété d’équivalence distributionnelle : si on regroupe deux modalités lignes (colonnes), les distances entre les profils\sphinxhyphen{}colonne (lignes), ou entre les autres profils\sphinxhyphen{}lignes (colonnes) restent inchangées.

\sphinxAtStartPar
\sphinxincludegraphics{{presdist}.png}

\sphinxAtStartPar
On peut également calculer les taux de liaisons, définis pour deux individus \(j\) et \(k\) par \(\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}}\). Par exemple, le taux de liaison entre HAMON et Grand\sphinxhyphen{}Est est égal à  \sphinxhyphen{}0.2003, tandis que le taux de liaison entre CHEMINADE et Nouvelle\sphinxhyphen{}Aquitaine est égal à 0.2068. Le taux de liaison s’interpréte comme suit : le score du candidat dans la région est 20\% moins élevé (ou 20.6\% moins élevé) que le score théorique que l’on observerait si les votes étaient indépendants des régions.

\sphinxAtStartPar
Notons que \(f_{j.}f_{.k}\) représente le poids théorique de chaque case du tableau des fréquences. La somme de ces coefficients vaut 1. La moyenne de la série des taux de liaisons pondérée par les \(f_{j.}f_{.k}\) est nulle. De même, la variance de cette série avec la même pondération vaut \(\chi^2\), et ici est égale à 0.0301.

\sphinxAtStartPar
On réalise ensuite une AFC, par analyse spectrale des matrices \(\bf X^T\bf X\) et \(\bf X\bf X^T\), où \(\bf X\) est la matrice de terme général \(f_{jk}/\sqrt{f_{j.}f_{.k}}\).

\sphinxAtStartPar
Le nombre de valeurs propres produites par la recherche des facteurs principaux est égal au minimum du nombre de lignes et du nombre de colonnes du tableau de contingence. La première valeur propre est systématiquement égale à 1, et n’est pas utilisée dans les résultats. Les autres valeurs propres sont des nombres positifs inférieurs à 1 et leur somme est égale à \(\chi^2\).

\sphinxAtStartPar
\sphinxincludegraphics{{spectralelig}.png} \sphinxincludegraphics{{spectralecol}.png}

\sphinxAtStartPar
On utilise alors les vecteurs propres (axes factoriels) pour analyser les données lignes et colonnes. Pour chaque analyse, on reporte (illustré ici sur l’analyse en lignes) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
La masse, qui rappelle les fréquences marginales des lignes c’est\sphinxhyphen{}à\sphinxhyphen{}dire le profil colonne moyen. Contrairement à l’ACP normée, dans laquelle chaque individu était affecté du même poids, les régions ont ici un poids dépendant de l’effectif total d’électeurs inscrits dans la région.

\item {} 
\sphinxAtStartPar
La qualité qui indique les qualités de représentation des individus ligne sur les deux premiers axes factoriels : c’est la somme des carrés des composantes de chaque individu sur les 2 axes, normalisée par la somme des carrés des composantes sur tous les axes.

\item {} 
\sphinxAtStartPar
La contribution de chaque individu à la formation de chaque axe factoriel

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Region
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Masse
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Coord1
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Coord2
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Qualité
&\sphinxstyletheadfamily 
\sphinxAtStartPar
contrib1
&\sphinxstyletheadfamily 
\sphinxAtStartPar
contrib2
\\
\hline
\sphinxAtStartPar
0
&
\sphinxAtStartPar
Grand\sphinxhyphen{}Est
&
\sphinxAtStartPar
0.082561
&
\sphinxAtStartPar
\sphinxhyphen{}0.081478
&
\sphinxAtStartPar
0.030469
&
\sphinxAtStartPar
0.033810
&
\sphinxAtStartPar
0.1031
&
\sphinxAtStartPar
0.05
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
Nelle\sphinxhyphen{}Aquitaine
&
\sphinxAtStartPar
0.092573
&
\sphinxAtStartPar
0.022322
&
\sphinxAtStartPar
\sphinxhyphen{}0.009848
&
\sphinxAtStartPar
0.002607
&
\sphinxAtStartPar
0.0129
&
\sphinxAtStartPar
0.1404
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
AURA
&
\sphinxAtStartPar
0.116102
&
\sphinxAtStartPar
0.002463
&
\sphinxAtStartPar
0.020096
&
\sphinxAtStartPar
0.000032
&
\sphinxAtStartPar
0.0005
&
\sphinxAtStartPar
0.0074
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
Bourgogne\sphinxhyphen{}FC
&
\sphinxAtStartPar
0.042922
&
\sphinxAtStartPar
\sphinxhyphen{}0.049796
&
\sphinxAtStartPar
0.021913
&
\sphinxAtStartPar
0.012884
&
\sphinxAtStartPar
0.0203
&
\sphinxAtStartPar
0.0048
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
Bretagne
&
\sphinxAtStartPar
0.055616
&
\sphinxAtStartPar
0.074398
&
\sphinxAtStartPar
\sphinxhyphen{}0.000143
&
\sphinxAtStartPar
0.028325
&
\sphinxAtStartPar
0.0727
&
\sphinxAtStartPar
0.00599
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
Centre\sphinxhyphen{}Val\sphinxhyphen{}de\sphinxhyphen{}Loire
&
\sphinxAtStartPar
0.039694
&
\sphinxAtStartPar
\sphinxhyphen{}0.027564
&
\sphinxAtStartPar
0.034140
&
\sphinxAtStartPar
0.003982
&
\sphinxAtStartPar
0.0057
&
\sphinxAtStartPar
0.026
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
Corse
&
\sphinxAtStartPar
0.004089
&
\sphinxAtStartPar
\sphinxhyphen{}0.089963
&
\sphinxAtStartPar
0.097463
&
\sphinxAtStartPar
0.036278
&
\sphinxAtStartPar
0.0061
&
\sphinxAtStartPar
0.0528
\\
\hline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
Ile\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
0.157099
&
\sphinxAtStartPar
0.099907
&
\sphinxAtStartPar
0.022705
&
\sphinxAtStartPar
0.048773
&
\sphinxAtStartPar
0.3461
&
\sphinxAtStartPar
0.0099
\\
\hline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
Occitanie
&
\sphinxAtStartPar
0.090960
&
\sphinxAtStartPar
\sphinxhyphen{}0.024738
&
\sphinxAtStartPar
\sphinxhyphen{}0.020736
&
\sphinxAtStartPar
0.003126
&
\sphinxAtStartPar
0.0066
&
\sphinxAtStartPar
0.2115
\\
\hline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
Hauts\sphinxhyphen{}de\sphinxhyphen{}France
&
\sphinxAtStartPar
0.090114
&
\sphinxAtStartPar
\sphinxhyphen{}0.113101
&
\sphinxAtStartPar
\sphinxhyphen{}0.015944
&
\sphinxAtStartPar
0.061802
&
\sphinxAtStartPar
0.2215
&
\sphinxAtStartPar
0.1268
\\
\hline
\sphinxAtStartPar
10
&
\sphinxAtStartPar
Normandie
&
\sphinxAtStartPar
0.052720
&
\sphinxAtStartPar
\sphinxhyphen{}0.035129
&
\sphinxAtStartPar
0.014011
&
\sphinxAtStartPar
0.006437
&
\sphinxAtStartPar
0.0116
&
\sphinxAtStartPar
0.0004
\\
\hline
\sphinxAtStartPar
11
&
\sphinxAtStartPar
Pays\sphinxhyphen{}de\sphinxhyphen{}la\sphinxhyphen{}Loire
&
\sphinxAtStartPar
0.061053
&
\sphinxAtStartPar
0.049348
&
\sphinxAtStartPar
0.050417
&
\sphinxAtStartPar
0.012418
&
\sphinxAtStartPar
0.0311
&
\sphinxAtStartPar
0.1115
\\
\hline
\sphinxAtStartPar
12
&
\sphinxAtStartPar
PACA
&
\sphinxAtStartPar
0.076388
&
\sphinxAtStartPar
\sphinxhyphen{}0.085523
&
\sphinxAtStartPar
0.044826
&
\sphinxAtStartPar
0.035070
&
\sphinxAtStartPar
0.1061
&
\sphinxAtStartPar
0.1766
\\
\hline
\sphinxAtStartPar
13
&
\sphinxAtStartPar
Outremer
&
\sphinxAtStartPar
0.038108
&
\sphinxAtStartPar
0.073346
&
\sphinxAtStartPar
0.038647
&
\sphinxAtStartPar
0.026537
&
\sphinxAtStartPar
0.051
&
\sphinxAtStartPar
0.0222
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
On représente alors graphiquement les individus ligne et colonne sur le premier plan factoriel.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

\sphinxAtStartPar
\sphinxincludegraphics{{planlig}.png}
&
\sphinxAtStartPar
\sphinxincludegraphics{{plancol}.png}
\\
\hline
\sphinxAtStartPar
Individus ligne
&
\sphinxAtStartPar
Individus colonne
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
On en déduit alors l’analyse suivante (ici proposée sur les candidats) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
Premier axe : Le Pen représente 67\% de l’inertie de cet axe. Macron, à l’opposé en représente 19\%. Clairement, cet axe oppose Le Pen à Macron, mais les autres candidats “les plus importants” ont un score du même signe que celui de Macron.

\item {} 
\sphinxAtStartPar
Fillon représente 63\% de son inertie et s’oppose à Hamon, Poutou et Mélenchon. Macron et Le Pen sont insignifiants sur cet axe. Cet axe représente l’opposition classique droite / gauche.

\item {} 
\sphinxAtStartPar
Le Pen est placée assez loin de l’origine sur la représentation graphique:  l’inertie de la modalité Le Pen (54\%) est bien plus importante que sa masse (21\%). Dit autrement, les scores de Le Pen présentent une grande variabilité selon les régions, plus élevée que celle des scores de Macron (inertie 16\% pour une masse de 24,3\%) ou encore Mélenchon (inertie 5\%, inertie 19,8\%). Les électeurs de Le Pen, même s’ils sont plus nombreux que lors des scrutins précédents, restent inégalement répartis sur le territoire.

\item {} 
\sphinxAtStartPar
Enfin, la première source de variation dans les votes est une opposition Le Pen / Macron, indépendante des oppositions  droite / gauche traditionnels.

\end{itemize}

\sphinxstepscope


\chapter{Analyse des correspondances multiples}
\label{\detokenize{acm:analyse-des-correspondances-multiples}}\label{\detokenize{acm::doc}}
\index{Analyse@\spxentry{Analyse}!correspondances multiples@\spxentry{correspondances multiples}}\ignorespaces 
\sphinxAtStartPar
Tandis que l’analyse factorielle des correspondances permet d’expliquer la liaison entre deux variables qualitatives, l’analyse des correspondances multiples (ACM) s’intéresse au cas où l’on dispose de \(p\geq 2\) variables. C’est l’équivalent de l’ACP pour les variables qualitatives.


\section{Notations}
\label{\detokenize{acm:notations}}
\sphinxAtStartPar
On dispose d’un tableau de données \(\mathbf{H}=(h_{i,j})\) à \(n\) lignes et \(p\) colonnes, où \(n\) est le nombre d’individus, \(p\) le nombre de variables qualitatives mesurées et pour \(i\in[\![1,n]\!],j\in[\![1,p]\!],h_{ij}\in\mathcal{M}_j\), \(\mathcal{M}_j\) étant l’ensemble des modalités de la j\(^e\) variable. Si \(m_j\) est le cardinal de \(\mathcal{M}_j\), alors \(m=\sum_{k=1}^p m_k\) est le nombre total de modalités.

\index{Tableau@\spxentry{Tableau}!disjonctif@\spxentry{disjonctif}}\ignorespaces \label{acm:definition-0}
\begin{sphinxadmonition}{note}{Definition 43 (Tableau disjonctif complet)}



\sphinxAtStartPar
Le tableau disjonctif complet \(\mathbf T\) des données est un tableau \(n\times m\) tel que

\sphinxAtStartPar
\((\forall i\in[\![1,n]\!],j\in[\![1, m]\!])\; \mathbf T_{ij} = \left \{ \begin{array}{cl} 1&\textrm{si l'individu i possède la modalité j}\\0 & \textrm{sinon}\end{array}\right .\)
\end{sphinxadmonition}

\index{Tableau@\spxentry{Tableau}!Burt@\spxentry{Burt}}\ignorespaces 
\index{Burt@\spxentry{Burt}!tableau@\spxentry{tableau}}\ignorespaces 
\sphinxAtStartPar
On déduit de ce tableau disjonctif le tableau de Burt correspondant, \(\mathbf B=\mathbf T^T \mathbf T\), qui rassemble les croisements deux à  deux de toutes les variables, i.e tous les tableaux de contingence des variables deux à deux. Sur la diagonale de \(\mathbf B\) se trouvent les coefficients \(B_{ii}=n_i\), donnant le nombre d’individus possédant la modalité \(i\).  Les autres coefficients \(B_{ij} = \mathbf{T_{\bullet i}}^T \mathbf {T_{\bullet j}}\) quantifient le nombre d’individus ayant les modalités \(i\) et \(j\).


\section{Analyse}
\label{\detokenize{acm:analyse}}

\section{Tableau de contingence de l’ACM}
\label{\detokenize{acm:tableau-de-contingence-de-l-acm}}
\sphinxAtStartPar
En analyse des correspondances multiples, on traite \(\mathbf T\) comme un tableau de contingence. Les totaux en ligne sont alors égaux au nombre de variables \(p\), les totaux en colonne correspondent au nombre d’individus ayant la modalité correspondant à la colonne traitée. Pour une colonne \(j\), on note ce total \(n_j\) Le total de tous les coefficients de \(\mathbf T\) vaut donc \(np\).

\sphinxAtStartPar
Comme dans le cas de l’AFC, l’ACM considère les fréquences, les profils ligne et les profils colonne.

\sphinxAtStartPar
Pour les fréquences :
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(f_{ij}=T_{ij}/np\) est la fréquence conjointe et vaut donc \(1/np\) si l’individu \(i\) possède la modalité \(j\) et 0 sinon. On range ces coefficients dans une matrice \(\mathbf{F}\in\mathcal{M}_{nm}(\mathbb{R})\)

\item {} 
\sphinxAtStartPar
le poids des lignes est constant et vaut \(1/n\). On note alors \(\mathbf{a} = (\frac{1}{n}\cdots \frac{1}{n})^T\in\mathbb{R}^n\) le vecteur des poids des individus.

\item {} 
\sphinxAtStartPar
le poids des colonnes vaut \(n_j/np\), et est d’autant plus fort que la modalité \(j\) est fréquente. On note alors \(\mathbf{b} = (\frac{n_1}{np}\cdots \frac{n_m}{np})^T\in\mathbb{R}^m\) le vecteur des poids des modalités.

\end{itemize}

\sphinxAtStartPar
Comme en analyse factorielle des correspondances, on note \({\bf D_n}=diag\left ({\bf a}\right )\) et \({\bf D_m}=diag\left ({\bf b} \right )\).

\sphinxAtStartPar
Pour les profils ligne et colonne :
\begin{itemize}
\item {} 
\sphinxAtStartPar
on lit dans \(\mathbf T\) le i\(^e\) profil ligne, considéré comme un vecteur de \(\mathbb{R}^m\), de composantes \(T_{ij}/p,j\in[\![1,m]\!]\). Ces profils sont rangés dans une matrice \({\bf A}\in\mathcal{M}_{nm}(\mathbb{R})\) et on a \({\bf A}={\bf D_n^{-1}F}\).

\item {} 
\sphinxAtStartPar
on lit dans \(\mathbf T\) le j\(^e\) profil colonne, considéré comme un vecteur de \(\mathbb{R}^n\), de composantes \(T_{ij}/n_j,i\in[\![1,n]\!]\). Ces profils sont rangés dans \({\bf B}\in\mathcal{M}_{nm}(\mathbb{R})\) et on a \({\bf B}={\bf FD_m^{-1}}\).

\end{itemize}

\sphinxAtStartPar
L’ACM considère, comme l’AFC, deux nuages de points centrés :
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
le nuage des \(n\) individus dans \(\mathbb{R}^m\), i.e. les \(n\) lignes de la matrice \({\bf D_n^{-1}(F-ab^T})\). Chaque individu est pondéré par \(1/n\)

\item {} 
\sphinxAtStartPar
le nuage des \(m\) modalités dans \(\mathbb{R}^n\), i.e. les \(n\) lignes de la matrice \({\bf (F-ab^T)D_m^{-1}}\). Chaque modalité \(j\) est pondérée par \(n_j/np\).

\end{enumerate}


\section{Distances entre individus et entre modalités}
\label{\detokenize{acm:distances-entre-individus-et-entre-modalites}}
\sphinxAtStartPar
En analyse des correspondances multiples, on utilise la distance du \(\chi^2\) dans \(\mathbb{R}^m\) et \(\mathbb{R}^n\) :
\begin{itemize}
\item {} 
\sphinxAtStartPar
dans l’espace des individus, la métrique est \(\mathbf {D_m}^{-1}\) :

\end{itemize}

\sphinxAtStartPar
\(\chi^2(i,i') = (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet})^T\mathbf {D_m}^{-1} (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet}) = \displaystyle\sum_{j=1}^m\frac{1}{f_{\bullet j}}\left (\frac{T_{ij}-T_{i'j}}{p} \right )^2 = \frac {n}{p}\displaystyle\sum_{j=1}^m\frac{1}{{n_j}}\left (T_{ij}-T_{i'j}\right )^2\)
Deux individus sont proches s’ils possèdent les mêmes modalités, sachant que l’on donne plus de poids au fait que ces deux individus ont en commun une modalité rare (\(n_s\) petit).
\begin{itemize}
\item {} 
\sphinxAtStartPar
dans l’espace des modalités, la métrique est \(\mathbf {D_n}^{-1}\) :

\end{itemize}

\sphinxAtStartPar
\(\chi^2(j,j') = (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'})^T\mathbf {D_n}^{-1} (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'}) = n\displaystyle\sum_{i=1}^n\left (\frac{T_{ij}}{n_j} -\frac{T_{ij'}}{n_{j'}}\right )^2\)
et deux modalités sont proches si elles sont possédées par les mêmes individus.
\label{acm:remark-1}
\begin{sphinxadmonition}{note}{Remark 24}



\sphinxAtStartPar
On a de plus  \(\chi^2(j,\mathbf{a}) = \frac{n}{n_j}-1\) et  \(f_{\bullet j}\chi^2(j,\mathbf{a}) = \frac{1}{p}\left( 1-\frac{n_j}{n}\right )\). Donc la distance d’une modalité au centre du nuage est d’autant plus grande que la modalité est rare et la part de l’inertie totale, due à une modalité est d’autant plus grande que la modalité est rare.  On  évite donc en pratique de conserver dans l’analyse les modalités trop rares.

\sphinxAtStartPar
De même, puisque \(\displaystyle\sum_{k\in \mathcal{M}_j}f_{\bullet k}\chi^2(k,\mathbf{a}) = \frac{1}{p}\left( m_j-1\right )\), la part de l’inertie totale, due à une variable  \(j\) est d’autant plus grande que le nombre de modalités  de cette variable est grand. Là aussi, on  évite en pratique de conserver dans l’analyse des variables ayant des nombres de modalités trop différents.
\end{sphinxadmonition}


\section{Principe de l’ACM}
\label{\detokenize{acm:principe-de-l-acm}}
\sphinxAtStartPar
L’analyse en composantes multiples consiste alors à appliquer l’analyse factorielle des correspondances  du tableau des  contingences \(\mathbf T\), c’est\sphinxhyphen{}à\sphinxhyphen{}dire effectuer une ACP pondérée des nuages des point\sphinxhyphen{}individus et des point\sphinxhyphen{}modalités .

\sphinxAtStartPar
Une différence notable vient cependant de l’interprétation de l’inertie de ces nuages de points  individus (\(I(\mathbf{A})\)) et modalités (\(I(\mathbf{B})\)). En AFC, on pouvait interpréter statistiquement cette inertie en terme de \(\chi^2/n\) mesurant l’indépendance entre les deux variables qualitatives. Ici, ce n’est plus le cas puisque l’on peut montrer que \(I(\mathbf{A}) = I(\mathbf{B})= m/p-1\). L’inertie dépend donc du nombre moyen \(m/p\) de catégories par variable.
\label{acm:remark-2}
\begin{sphinxadmonition}{note}{Remark 25}



\sphinxAtStartPar
Les anglo\sphinxhyphen{}saxons considère que l’ACM consiste à effectuer l’analyse factorielle des correspondances  du tableau de Burt \(\mathbf T^T \mathbf T\), matrice symétrique de taille \(m\). Les profils ligne et colonne sont alors identiques et correspondent aux modalités que l’on veut analyser. On ne peut donc pas effectuer d’analyse des individus.
\end{sphinxadmonition}


\section{Interprétation des résultats}
\label{\detokenize{acm:interpretation-des-resultats}}

\section{Inertie expliquée}
\label{\detokenize{acm:inertie-expliquee}}
\sphinxAtStartPar
L’inertie totale, égale comme nous l’avons vu à \(m/p-1\) se calcule  également comme la somme des valeurs propres \(\lambda_1+\cdots +\lambda_r\), où \(r=min(n-1,m-p)\) est le nombre de valeurs propres non nulles issues de l’ACP. La part d’inertie expliquée par l’axe \(z\) est alors \(\lambda_z/(\lambda_1+\cdots +\lambda_r)\). En revanche, point important, le nombre d’axes retenus pour l’interprétation ou le recodage ne peut pas être choisi à partir de ces pourcentages d’inertie expliquées, contrairement à l’ACP.


\section{Contributions et représentation}
\label{\detokenize{acm:contributions-et-representation}}
\sphinxAtStartPar
En reprenant les résultats de l’AFC, on montre que :
\begin{itemize}
\item {} 
\sphinxAtStartPar
les individus les plus excentrés sur les plans factoriels sont ceux qui contribuent le plus

\item {} 
\sphinxAtStartPar
les modalités les plus excentrées ne sont pas nécessairement celles qui
contribuent le plus. En effet, leur contribution dépend de leur fréquence.

\item {} 
\sphinxAtStartPar
la contribution d’une variable qualitative \(j\) à un axe \(z\) donne une idée de la liaison entre cette variable et la composante principale correspondant à \(z\)

\item {} 
\sphinxAtStartPar
une représentation graphique consiste alors à représenter les variables qualitatives sur un plan factoriel \((z,z')\) : on propose en abscisses (respectivement ordonnées) les contributions des variables à l’axe \(z\) (resp. \(z'\))

\item {} 
\sphinxAtStartPar
on évalue la qualité de la représentation de la même manière qu’en ACP, à l’aide des cosinus carrés. Si deux individus sont bien projetés alors s’ils sont proches en projections, ils sont effectivement proches dans leur espace d’origine et on peut alors interpréter leur proximité :
deux individus se ressemblent (au sens de la distance du \(\chi^2\)) s’ils ont choisis les mêmes modalités et ; deux modalités se ressemblent (en terme de \(\chi^2\)) si elles sont possédées par les mêmes individus.

\end{itemize}


\section{Cas particulier \protect\(p\protect\)=2}
\label{\detokenize{acm:cas-particulier-p-2}}
\sphinxAtStartPar
Dans le cas \(p=2\), on observe \(2\) variables ayant respectivement \(m_1\) et \(m_2\) modalités. On se retrouve donc dans le cas où l’AFC s’applique et on peut :
\begin{itemize}
\item {} 
\sphinxAtStartPar
soit analyser le tableau \(\mathbf{T}\in\mathcal{M}_{n,m_1+m_2}(\mathbb{R})\) par analyse en composantes multiples,

\item {} 
\sphinxAtStartPar
soit analyser le tableau de contingence \(\mathbf{K}\in\mathcal{M}_{m_1,m_2}(\mathbb{R})\) par AFC.

\end{itemize}

\sphinxAtStartPar
Si on note \(Sp(\mathbf{K}) = (\mu_i)\) et \(Sp(\mathbf{T}) = (\lambda_i)\) alors on montre  que  \(\mu_r = (2\lambda_r - 1)^2\)
On en déduit qu’à chaque valeur  propre de l’AFC correspondent deux valeurs propres de l’ACM \(\lambda_r = \frac{1\pm \sqrt{\mu_r}}{2}\), et une relation entre les coordonnées factorielles des deux analyses

\sphinxAtStartPar
\(\begin{eqnarray*}
\mathbf{c}_1 = \begin{pmatrix} \mathbf{x_K}\\\mathbf{y_K}\end{pmatrix}\ &\textrm{pour}& \lambda_r = \frac{1+ \sqrt{\mu_r}}{2}\\
\mathbf{c}_2 = \begin{pmatrix} \mathbf{x_K}\\-\mathbf{y_K}\end{pmatrix}\ &\textrm{pour}& \lambda_r = \frac{1- \sqrt{\mu_r}}{2}
\end{eqnarray*}\)

\sphinxAtStartPar
où \(\mathbf{x_K},\mathbf{y_K}\) sont les composantes principales des profils ligne et colonne de \(K\).
De là viennent deux constats :
\begin{itemize}
\item {} 
\sphinxAtStartPar
dans l’analyse en composantes multiples de 2 variables, on ne retient que les valeurs propres supérieures strictement à 1/2, les composantes correspondant  aux valeurs propres inférieures se déduisant facilement

\item {} 
\sphinxAtStartPar
Les pourcentages d’inertie expliqués par les axes en ACM sont souvent très faibles et
ne peuvent donc pas être interprétés comme en AFC et en ACP.

\end{itemize}


\section{Exemple}
\label{\detokenize{acm:exemple}}
\sphinxAtStartPar
On souhaite évaluer l’effet de l’espèce de chêne sur des vins rouges vieillis en barrique. Un même vin a été vieilli dans six barriques différentes fabriquées avec deux types de chêne : les vins \(V_1\), \(V_5\) et \(V_6\) ont été élevés avec le premier type de chêne, tandis que les vins \(V_2\), \(V_3\) et \(V_4\) ont été élevés avec le second. Trois experts \(E_1,E_2,E_3\) ont ensuite choisi entre deux et cinq variables pour décrire les vins. Pour chaque vin et pour chaque variable, l’expert évalue l’intensité, codée soit comme une réponse binaire (i.e. fruité vs. non fruité), soit comme une réponse ternaire (i.e. pas fruité, un peu fruité, très fruité). On code le tout par un tableau disjonctif complet :

\sphinxAtStartPar
\sphinxincludegraphics{{tab0}.png}

\sphinxAtStartPar
L’objectif de l’étude est d’une part de proposer une typologie des vins et d’autre part de savoir s’il y a un accord entre les échelles utilisées par les experts.

\sphinxAtStartPar
La figure suivante présente le résultat de l’analyse spectrale en lignes. Les tableaux qui suivent donnent les coordonnées des individus (\(S\)), la qualité de leur représentation et leur contribution  (C\(\times 10^3\)) sur les trois premiers axes factoriels.

\sphinxAtStartPar
\sphinxincludegraphics{{spectralACM}.png}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline

\sphinxAtStartPar

&\sphinxstyletheadfamily 
\sphinxAtStartPar
Vin
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_4\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_5\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(V_6\)
\\
\hline
\sphinxAtStartPar
\(S\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}0.951
&
\sphinxAtStartPar
0.787
&
\sphinxAtStartPar
1.018
&
\sphinxAtStartPar
0.951
&
\sphinxAtStartPar
\sphinxhyphen{}1.018
&
\sphinxAtStartPar
\sphinxhyphen{}0.787
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}0.316
&
\sphinxAtStartPar
0.632
&
\sphinxAtStartPar
\sphinxhyphen{}0.316
&
\sphinxAtStartPar
\sphinxhyphen{}0.316
&
\sphinxAtStartPar
\sphinxhyphen{}0.316
&
\sphinxAtStartPar
0.632
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
3
&
\sphinxAtStartPar
0.43
&
\sphinxAtStartPar
0.387
&
\sphinxAtStartPar
0.103
&
\sphinxAtStartPar
\sphinxhyphen{}0.43
&
\sphinxAtStartPar
\sphinxhyphen{}0.103
&
\sphinxAtStartPar
\sphinxhyphen{}0.387
\\
\hline
\sphinxAtStartPar
\(cos^2\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0.754
&
\sphinxAtStartPar
0.516
&
\sphinxAtStartPar
0.863
&
\sphinxAtStartPar
0.754
&
\sphinxAtStartPar
0.863
&
\sphinxAtStartPar
0.516
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
2
&
\sphinxAtStartPar
0.083
&
\sphinxAtStartPar
0.333
&
\sphinxAtStartPar
0.083
&
\sphinxAtStartPar
0.083
&
\sphinxAtStartPar
0.083
&
\sphinxAtStartPar
0.333
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
3
&
\sphinxAtStartPar
0.154
&
\sphinxAtStartPar
0.125
&
\sphinxAtStartPar
0.009
&
\sphinxAtStartPar
0.154
&
\sphinxAtStartPar
0.009
&
\sphinxAtStartPar
0.125
\\
\hline
\sphinxAtStartPar
\(C\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
176.68
&
\sphinxAtStartPar
120.986
&
\sphinxAtStartPar
202.334
&
\sphinxAtStartPar
176.68
&
\sphinxAtStartPar
202.334
&
\sphinxAtStartPar
120.986
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
2
&
\sphinxAtStartPar
83.333
&
\sphinxAtStartPar
333.333
&
\sphinxAtStartPar
83.333
&
\sphinxAtStartPar
83.333
&
\sphinxAtStartPar
83.333
&
\sphinxAtStartPar
333.333
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
3
&
\sphinxAtStartPar
267.821
&
\sphinxAtStartPar
216.945
&
\sphinxAtStartPar
15.234
&
\sphinxAtStartPar
267.821
&
\sphinxAtStartPar
15.234
&
\sphinxAtStartPar
216.945
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\sphinxincludegraphics{{tab}.png}

\sphinxAtStartPar
On peut alors projeter les individus lignes ou colonnes sur le premier plan factoriel.

\sphinxAtStartPar
\sphinxincludegraphics{{plan12lig}.png}\sphinxincludegraphics{{plan12col}.png}

\sphinxAtStartPar
On peut alors par exemple utiliser le type de chêne (MONCHENE) comme une variable supplémentaire (ou illustrative) à projeter sur l’analyse après coup. On peut également projeter après analyse un nouveau vin (MONVIN, donc une observation supplémentaire), testé par les experts. Lorsque ces derniers n’étaient pas sûrs de la façon d’utiliser un descripteur, un modèle de réponse (1/2, 1/2) est utilisé pour représenter la réponse.

\sphinxAtStartPar
\sphinxincludegraphics{{plan12lig2}.png}\sphinxincludegraphics{{plan12col2}.png}

\sphinxstepscope


\chapter{Index}
\label{\detokenize{genindex:index}}\label{\detokenize{genindex::doc}}





\renewcommand{\indexname}{Proof Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{EM}
\item\relax\sphinxstyleindexentry{EM}\sphinxstyleindexextra{clustering}\sphinxstyleindexpageref{clustering:\detokenize{EM}}
\indexspace
\bigletter{FOCUS}
\item\relax\sphinxstyleindexentry{FOCUS}\sphinxstyleindexextra{selection}\sphinxstyleindexpageref{selection:\detokenize{FOCUS}}
\indexspace
\bigletter{SFS}
\item\relax\sphinxstyleindexentry{SFS}\sphinxstyleindexextra{selection}\sphinxstyleindexpageref{selection:\detokenize{SFS}}
\indexspace
\bigletter{algorithm\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{algorithm\sphinxhyphen{}3}\sphinxstyleindexextra{clustering}\sphinxstyleindexpageref{clustering:\detokenize{algorithm-3}}
\indexspace
\bigletter{algorithm\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{algorithm\sphinxhyphen{}4}\sphinxstyleindexextra{clustering}\sphinxstyleindexpageref{clustering:\detokenize{algorithm-4}}
\indexspace
\bigletter{axiomKolmo}
\item\relax\sphinxstyleindexentry{axiomKolmo}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{axiomKolmo}}
\indexspace
\bigletter{definition\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}0}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-0}}
\indexspace
\bigletter{definition\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}1}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-1}}
\indexspace
\bigletter{definition\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}10}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-10}}
\indexspace
\bigletter{definition\sphinxhyphen{}11}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}11}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-11}}
\indexspace
\bigletter{definition\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}12}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-12}}
\indexspace
\bigletter{definition\sphinxhyphen{}13}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}13}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-13}}
\indexspace
\bigletter{definition\sphinxhyphen{}14}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}14}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-14}}
\indexspace
\bigletter{definition\sphinxhyphen{}15}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}15}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-15}}
\indexspace
\bigletter{definition\sphinxhyphen{}16}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}16}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-16}}
\indexspace
\bigletter{definition\sphinxhyphen{}17}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}17}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-17}}
\indexspace
\bigletter{definition\sphinxhyphen{}18}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}18}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-18}}
\indexspace
\bigletter{definition\sphinxhyphen{}19}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}19}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-19}}
\indexspace
\bigletter{definition\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}2}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-2}}
\indexspace
\bigletter{definition\sphinxhyphen{}20}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}20}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-20}}
\indexspace
\bigletter{definition\sphinxhyphen{}21}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}21}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-21}}
\indexspace
\bigletter{definition\sphinxhyphen{}22}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}22}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-22}}
\indexspace
\bigletter{definition\sphinxhyphen{}23}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}23}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-23}}
\indexspace
\bigletter{definition\sphinxhyphen{}25}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}25}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-25}}
\indexspace
\bigletter{definition\sphinxhyphen{}28}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}28}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-28}}
\indexspace
\bigletter{definition\sphinxhyphen{}29}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}29}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-29}}
\indexspace
\bigletter{definition\sphinxhyphen{}31}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}31}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{definition-31}}
\indexspace
\bigletter{definition\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}4}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-4}}
\indexspace
\bigletter{definition\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}5}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-5}}
\indexspace
\bigletter{definition\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}6}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-6}}
\indexspace
\bigletter{definition\sphinxhyphen{}7}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}7}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-7}}
\indexspace
\bigletter{definition\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}8}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-8}}
\indexspace
\bigletter{definition\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{definition\sphinxhyphen{}9}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{definition-9}}
\indexspace
\bigletter{evenement}
\item\relax\sphinxstyleindexentry{evenement}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{evenement}}
\indexspace
\bigletter{example\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}0}\sphinxstyleindexextra{intro}\sphinxstyleindexpageref{intro:\detokenize{example-0}}
\indexspace
\bigletter{example\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}1}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-1}}
\indexspace
\bigletter{example\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}12}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-12}}
\indexspace
\bigletter{example\sphinxhyphen{}17}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}17}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-17}}
\indexspace
\bigletter{example\sphinxhyphen{}19}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}19}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-19}}
\indexspace
\bigletter{example\sphinxhyphen{}20}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}20}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-20}}
\indexspace
\bigletter{example\sphinxhyphen{}26}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}26}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-26}}
\indexspace
\bigletter{example\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}4}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-4}}
\indexspace
\bigletter{example\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}6}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-6}}
\indexspace
\bigletter{example\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{example\sphinxhyphen{}8}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{example-8}}
\indexspace
\bigletter{expalea}
\item\relax\sphinxstyleindexentry{expalea}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{expalea}}
\indexspace
\bigletter{issue}
\item\relax\sphinxstyleindexentry{issue}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{issue}}
\indexspace
\bigletter{km}
\item\relax\sphinxstyleindexentry{km}\sphinxstyleindexextra{clustering}\sphinxstyleindexpageref{clustering:\detokenize{km}}
\indexspace
\bigletter{kmbatch}
\item\relax\sphinxstyleindexentry{kmbatch}\sphinxstyleindexextra{clustering}\sphinxstyleindexpageref{clustering:\detokenize{kmbatch}}
\indexspace
\bigletter{property\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}1}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{property-1}}
\indexspace
\bigletter{property\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}10}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{property-10}}
\indexspace
\bigletter{property\sphinxhyphen{}11}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}11}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{property-11}}
\indexspace
\bigletter{property\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}2}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{property-2}}
\indexspace
\bigletter{property\sphinxhyphen{}27}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}27}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{property-27}}
\indexspace
\bigletter{property\sphinxhyphen{}33}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}33}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{property-33}}
\indexspace
\bigletter{property\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{property\sphinxhyphen{}5}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{property-5}}
\indexspace
\bigletter{proposition\sphinxhyphen{}32}
\item\relax\sphinxstyleindexentry{proposition\sphinxhyphen{}32}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{proposition-32}}
\indexspace
\bigletter{relief}
\item\relax\sphinxstyleindexentry{relief}\sphinxstyleindexextra{selection}\sphinxstyleindexpageref{selection:\detokenize{relief}}
\indexspace
\bigletter{remark\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}0}\sphinxstyleindexextra{selection}\sphinxstyleindexpageref{selection:\detokenize{remark-0}}
\indexspace
\bigletter{remark\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}1}\sphinxstyleindexextra{selection}\sphinxstyleindexpageref{selection:\detokenize{remark-1}}
\indexspace
\bigletter{remark\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}10}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-10}}
\indexspace
\bigletter{remark\sphinxhyphen{}13}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}13}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{remark-13}}
\indexspace
\bigletter{remark\sphinxhyphen{}14}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}14}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{remark-14}}
\indexspace
\bigletter{remark\sphinxhyphen{}15}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}15}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{remark-15}}
\indexspace
\bigletter{remark\sphinxhyphen{}2}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}2}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-2}}
\indexspace
\bigletter{remark\sphinxhyphen{}21}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}21}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{remark-21}}
\indexspace
\bigletter{remark\sphinxhyphen{}22}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}22}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{remark-22}}
\indexspace
\bigletter{remark\sphinxhyphen{}24}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}24}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{remark-24}}
\indexspace
\bigletter{remark\sphinxhyphen{}3}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}3}\sphinxstyleindexextra{statsdescriptives}\sphinxstyleindexpageref{statsdescriptives:\detokenize{remark-3}}
\indexspace
\bigletter{remark\sphinxhyphen{}30}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}30}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{remark-30}}
\indexspace
\bigletter{remark\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}4}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-4}}
\indexspace
\bigletter{remark\sphinxhyphen{}5}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}5}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{remark-5}}
\indexspace
\bigletter{remark\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}6}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-6}}
\indexspace
\bigletter{remark\sphinxhyphen{}7}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}7}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-7}}
\indexspace
\bigletter{remark\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{remark\sphinxhyphen{}9}\sphinxstyleindexextra{regression}\sphinxstyleindexpageref{regression:\detokenize{remark-9}}
\indexspace
\bigletter{theorem\sphinxhyphen{}0}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}0}\sphinxstyleindexextra{acp}\sphinxstyleindexpageref{acp:\detokenize{theorem-0}}
\indexspace
\bigletter{theorem\sphinxhyphen{}1}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}1}\sphinxstyleindexextra{acp}\sphinxstyleindexpageref{acp:\detokenize{theorem-1}}
\indexspace
\bigletter{theorem\sphinxhyphen{}10}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}10}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-10}}
\indexspace
\bigletter{theorem\sphinxhyphen{}12}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}12}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-12}}
\indexspace
\bigletter{theorem\sphinxhyphen{}14}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}14}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-14}}
\indexspace
\bigletter{theorem\sphinxhyphen{}15}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}15}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-15}}
\indexspace
\bigletter{theorem\sphinxhyphen{}16}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}16}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-16}}
\indexspace
\bigletter{theorem\sphinxhyphen{}17}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}17}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-17}}
\indexspace
\bigletter{theorem\sphinxhyphen{}4}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}4}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-4}}
\indexspace
\bigletter{theorem\sphinxhyphen{}6}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}6}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-6}}
\indexspace
\bigletter{theorem\sphinxhyphen{}8}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}8}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-8}}
\indexspace
\bigletter{theorem\sphinxhyphen{}9}
\item\relax\sphinxstyleindexentry{theorem\sphinxhyphen{}9}\sphinxstyleindexextra{elemstats}\sphinxstyleindexpageref{elemstats:\detokenize{theorem-9}}
\indexspace
\bigletter{tribu}
\item\relax\sphinxstyleindexentry{tribu}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{tribu}}
\indexspace
\bigletter{univers}
\item\relax\sphinxstyleindexentry{univers}\sphinxstyleindexextra{Rappels}\sphinxstyleindexpageref{Rappels:\detokenize{univers}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}