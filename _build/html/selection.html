
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Sélection de variables &#8212; Analyse de données</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/material.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystyle.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Analyse en composantes principales" href="acp.html" />
    <link rel="prev" title="Statistique descriptive" href="statsdescriptives.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#selection" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="intro.html" title="Analyse de données"
           class="md-header-nav__button md-logo">
          
              <img src="_static/isimainp.png" height="26"
                   alt="Analyse de données logo">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">Analyse de données</span>
          <span class="md-header-nav__topic"> Sélection de variables </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = ""versions.json"",
        target_loc = "../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="intro.html" class="md-tabs__link">Analyse de données</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="intro.html" title="Analyse de données" class="md-nav__button md-logo">
      
        <img src="_static/isimainp.png" alt=" logo" width="48" height="48">
      
    </a>
    <a href="intro.html"
       title="Analyse de données">Analyse de données</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">"Contents"</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#selection--page-root" class="md-nav__link">Sélection de variables</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#definitions" class="md-nav__link">Définitions</a>
        </li>
        <li class="md-nav__item"><a href="#caracteristiques-des-methodes-de-selection" class="md-nav__link">Caractéristiques des méthodes de sélection</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#initialisation" class="md-nav__link">Initialisation</a>
        </li>
        <li class="md-nav__item"><a href="#exploration-des-sous-ensembles" class="md-nav__link">Exploration des sous-ensembles</a>
        </li>
        <li class="md-nav__item"><a href="#evaluation-des-sous-ensembles" class="md-nav__link">Evaluation des sous-ensembles</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#filtres" class="md-nav__link">Filtres</a>
        </li>
        <li class="md-nav__item"><a href="#methodes-enveloppantes" class="md-nav__link">Méthodes enveloppantes</a>
        </li>
        <li class="md-nav__item"><a href="#methodes-integrees" class="md-nav__link">Méthodes intégrées</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#quelques-methodes-de-selection" class="md-nav__link">Quelques méthodes de sélection</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#suppression-des-descripteurs-a-variance-faible" class="md-nav__link">Suppression des descripteurs à variance faible</a>
        </li>
        <li class="md-nav__item"><a href="#algorithmes-de-selection-sequentielle" class="md-nav__link">Algorithmes de sélection séquentielle</a>
        </li>
        <li class="md-nav__item"><a href="#algorithme-focus" class="md-nav__link">Algorithme Focus</a>
        </li>
        <li class="md-nav__item"><a href="#algorithme-relief" class="md-nav__link">Algorithme relief</a>
        </li>
        <li class="md-nav__item"><a href="#methode-sac" class="md-nav__link">Méthode SAC</a>
        </li>
        <li class="md-nav__item"><a href="#algorithme-rfe" class="md-nav__link">Algorithme RFE</a>
        </li></ul>
            </nav>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="_sources/selection.md">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  
<h1 id="selection--page-root">Sélection de variables<a class="headerlink" href="#selection--page-root" title="Permalink to this headline">¶</a></h1>
<p>On s’intéresse ici à <span class="math notranslate nohighlight">\(n\)</span> individus  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> décrits par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives ou caractéristiques (features), <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^d\)</span>. Avec l’avènement des Big Data, et la généralisation des capteurs, <span class="math notranslate nohighlight">\(d\)</span> peut être très grand (plusieurs milliers), et analyser telles quelles les données brutes devient difficile d’un point de vue calculatoire et interprétation. De plus, il est rare que les caractéristiques soient totalement utiles et indépendantes.</p>
<p>Une étape souvent utilisée en analyse de données consiste donc à prétraiter cet espace, par exemple pour :</p>
<ul class="simple">
<li><p>le transformer en un format compatible avec des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité temporelle des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité spatiale du problème traité</p></li>
<li><p>découpler des variables et chercher les dépendances</p></li>
<li><p>introduire des a priori, ou des propriétés importantes pour les algorithmes (données centrées normées, descripteurs épars…)</p></li>
<li><p>permettre une interprétation plus intuitive et/ou graphique (<a class="reference internal" href="#tsne"><span class="std std-ref">figure 2</span></a>)</p></li>
</ul>
<div class="figure align-default" id="tsne">
<img alt="_images/tsne.png" src="_images/tsne.png"/>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Exemple de réduction de dimension (source: Maaten &amp; Hinton, 2008). Des images 28<span class="math notranslate nohighlight">\(\times\)</span> 28 de chiffres manuscrits sont représentées par un vecteur de 784 valeurs, puis transformés en vecteurs de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> pour les projeter dans le plan. La méthode utilisée permet d’optimiser la transformation de sorte à ce que les images représentant le même chiffre soient regroupées dans des nuages compacts.</span><a class="headerlink" href="#tsne" title="Permalink to this image">¶</a></p>
</div>
<p>Deux stratégies peuvent alors être utilisées :</p>
<ol class="simple">
<li><p>sélectionner un sous-ensemble des variables initiales comme descripteurs des individus</p></li>
<li><p>calculer de nouveaux descripteurs à partir des variables initiales.</p></li>
</ol>
<p>Nous nous intéressons ici à la première approche, la seconde (extraction de caractéristiques) étant abordée pour une approche linéaire dans le chapitre sur l’analyse en composantes principales.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 12 </span></p>

<p>Les méthodes d’extraction de caractéristiques peuvent être soit linéaires (on recherche des combinaisons linéaires des variables initiales  permettant d’optimiser un cerrtain critère), ou non linéaires (on parle également de manifold learning)</p>

</div>
<h2 id="definitions">Définitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<p>La sélection de caractéristiques consiste à choisir parmi les <span class="math notranslate nohighlight">\(d\)</span> descripteurs d’un ensemble d’individus <span class="math notranslate nohighlight">\(\mathbf x_i,i\in[\![1,n]\!]\)</span>, un sous-ensemble de  <span class="math notranslate nohighlight">\(t&lt;d\)</span>  caractéristiques jugées “les plus pertinentes”, les <span class="math notranslate nohighlight">\(d-t\)</span> restantes étant ignorées.</p>
<p>On note <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span> les <span class="math notranslate nohighlight">\(d\)</span> caractéristiques.  On note <span class="math notranslate nohighlight">\(Perf\)</span> une fonction qui permet d’évaluer un sous-ensemble de caractéristiques, et on suppose que <span class="math notranslate nohighlight">\(Perf\)</span> atteint son maximum pour le meilleur sous-ensemble de caractéristiques (“le plus pertinent”). Le problème de sélection se formule donc comme un problème d’optimisation</p>
<div class="math notranslate nohighlight">
\[\hat{F} = Arg\displaystyle\max_{U\subset F} Perf(U)\]</div>
<p>le cardinal <span class="math notranslate nohighlight">\(|\hat{F|}\)</span> de <span class="math notranslate nohighlight">\(\hat{F}\)</span> étant soit contrôlé par l’utilisateur, soit défini par l’algorithme de sélection.</p>
<p>On distingue alors trois stratégies :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(|\hat{F|}\)</span> est défini par l’utilisateur et l’optimisation s’effectue sur tous les sous-ensembles ayant ce cardinal</p></li>
<li><p>On connaît une mesure minimale de performance <span class="math notranslate nohighlight">\(\gamma\)</span>  et la sélection recherche le plus petit sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> dont la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> est supérieure ou égale à <span class="math notranslate nohighlight">\(\gamma\)</span></p></li>
<li><p>On cherche un compromis entre l’amélioration de la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> et la réduction de la taille du sous ensemble.</p></li>
</ul>
<p>La mesure de pertinence d’une caractéristique est donc au centre des algorithmes de sélection. Plusieurs définitions sont possibles, et nous dirons ici  qu’une caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> est :</p>
<ul class="simple">
<li><p>pertinente si son absence entraîne une détérioration significative de la performance de l’algorithme utilisé en aval (classification ou régression)</p></li>
<li><p>peu pertinente si elle n’est pas pertinente et s’il existe un sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> tel que la performance de <span class="math notranslate nohighlight">\(U\cup\{f_i\}\)</span> est significativement meilleure que la peformance de <span class="math notranslate nohighlight">\(U\)</span></p></li>
<li><p>non pertinente, si elle ne rentre pas dans les deux premières définitions. En général, ces caractéristiques sont supprimées.</p></li>
</ul>


<h2 id="caracteristiques-des-methodes-de-selection">Caractéristiques des méthodes de sélection<a class="headerlink" href="#caracteristiques-des-methodes-de-selection" title="Permalink to this headline">¶</a></h2>
<p>Une méthode de sélection basée sur l’optimisation de <span class="math notranslate nohighlight">\(Perf\)</span> utilise généralement trois étapes. Les  deux dernières sont itérées jusqu’à un test d’arrêt.</p>

<h3 id="initialisation">Initialisation<a class="headerlink" href="#initialisation" title="Permalink to this headline">¶</a></h3>
<p>L’initialisation consiste à choisir l’ensemble de départ des caractéristiques. Il peut s’agir de l’ensemble vide, de <span class="math notranslate nohighlight">\(F\)</span> tout entier, ou un sous-ensemble quelconque <span class="math notranslate nohighlight">\(U\subset F\)</span>.</p>


<h3 id="exploration-des-sous-ensembles">Exploration des sous-ensembles<a class="headerlink" href="#exploration-des-sous-ensembles" title="Permalink to this headline">¶</a></h3>
<p>A partir de cette initialisation, les stratégies d’exploration des sous-ensembles de caractéristiques se déclinent en trois catégories :</p>
<ol class="simple">
<li><p>génération exhaustive : tous les sous-ensembles de caractéristiques sont évalués. Si elle garantit de trouver la valeur optimale, cette méthode n’est que peu applicable dès que <span class="math notranslate nohighlight">\(|F|\)</span> devient important (<span class="math notranslate nohighlight">\(2^{|F|}\)</span> sous-ensembles possibles)</p></li>
<li><p>génération heuristique : une génération itérative est effectuée, chaque itération permettant de sélectionner ou de rejeter une ou plusieurs caractéristiques. La génération peut être ascendante (ajout de caractéristiques à partir de l’ensemble vide), descendante (suppression de caractéristiques à partir de <span class="math notranslate nohighlight">\(F\)</span>), ou mixte.</p></li>
<li><p>génération stochastique : pour un ensemble de données et une initialisation définie, une stratégie de recherche heuristique retourne toujours le même sous-ensemble, ce qui la rend très sensible au changement
de l’ensemble de données. La génération stochastique génère aléatoirement un nombre fini de sous-ensembles de caractéristiques afin de sélectionner le meilleur. La convergence est sous-optimale mais peut s’avérer préférable dans des algorithmes d’apprentissage, par exemple pour éviter le phénomène de surapprentissage.</p></li>
</ol>


<h3 id="evaluation-des-sous-ensembles">Evaluation des sous-ensembles<a class="headerlink" href="#evaluation-des-sous-ensembles" title="Permalink to this headline">¶</a></h3>

<h4 id="filtres">Filtres<a class="headerlink" href="#filtres" title="Permalink to this headline">¶</a></h4>
<p>Le critère d’évaluation utilisé évalue la pertinence d’une caractéristique selon des mesures
qui reposent sur les propriétés de données d’apprentissage.</p>
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> exemples  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> , on note <span class="math notranslate nohighlight">\(\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d\)</span>  une donnée d’apprentissage (la <span class="math notranslate nohighlight">\(j^e\)</span> caractéristique <span class="math notranslate nohighlight">\(f_j\)</span> ayant donc pour valeur <span class="math notranslate nohighlight">\(x_{ij}\)</span>) , d’étiquette <span class="math notranslate nohighlight">\(y_i\)</span> (en classification ou régression). Les méthodes de type filtres calculent un score pour évaluer le degré de pertinence de chacune des caractéristiques <span class="math notranslate nohighlight">\(f_i\)</span> , parmi lesquelles on peut citer</p>
<ul class="simple">
<li><p>Le critère de corrélation, utilisé en classification binaire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_i =\frac{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )\left (y_{k} -\mu_k\right )}{\sqrt{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )^2\displaystyle\sum_{k=1}^n\left (y_{k} -\mu_k\right )^2}}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mu_i\)</span> (resp. <span class="math notranslate nohighlight">\(\mu_k\)</span>) est la moyenne de la caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> observée sur <span class="math notranslate nohighlight">\(\mathbf x_1\cdots \mathbf x_n\)</span> (resp. moyenne des étiquettes)</p>
<ul class="simple">
<li><p>Le critère de Fisher,  qui permet de mesurer dans un problème de classification à <span class="math notranslate nohighlight">\(C\)</span> classes le degré de séparabilité des classes à l’aide
d’une caractéristique donnée</p></li>
</ul>
<div class="math notranslate nohighlight">
\[F_i = \frac{\displaystyle\sum_{c=1}^C n_c\left (\mu_c^i-\mu_i \right )^2}{\displaystyle\sum_{c=1}^C n_c(\Sigma_c^i)^2}\]</div>
<p>où <span class="math notranslate nohighlight">\(n_c, \mu_c^i\)</span> et <span class="math notranslate nohighlight">\(\Sigma_c^i\)</span> sont l’effectif, la moyenne et l’écart-type de la caractéristique  <span class="math notranslate nohighlight">\(f_i\)</span> dans la classe <span class="math notranslate nohighlight">\(c\)</span></p>
<ul class="simple">
<li><p>l’information mutuelle</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(i) = \displaystyle\sum_{\mathbf x_i} \displaystyle\sum_{y}P(X=\mathbf x_i,Y=y)log\left ( \frac{P(X=\mathbf x_i,Y=y)}{P(X=\mathbf x_i)P(Y=y)}\right )\]</div>
<p>qui mesure la dépendance entre les distributions de deux populations. Ici <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont deux variables aléatoires dont les réalisations sont les valeurs de <span class="math notranslate nohighlight">\(f_i\)</span> et des étiquettes de classes. Les probabilités sont estimées de manière fréquentiste.</p>
<p>Dans l’exemple suivant, on choisit de garder <span class="math notranslate nohighlight">\(|\hat{F|}=2\)</span> descripteurs, en contrôlant la pertinence par l’information mutuelle en classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Taille des données avant : "</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">mutual_info_classif</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Taille des données après : "</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Variables sélectionnées : "</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant :  (150, 4)
Taille des données après :  (150, 2)
Variables sélectionnées :  [False False  True  True]
</pre></div>
</div>
</div>
</div>


<h4 id="methodes-enveloppantes">Méthodes enveloppantes<a class="headerlink" href="#methodes-enveloppantes" title="Permalink to this headline">¶</a></h4>
<p>Le principal inconvénient des approches précédentes est le fait qu’elles ignorent l’influence des caractéristiques sélectionnées sur la performance de l’algorithme à utiliser par la suite. Les méthodes de type enveloppantes (wrappers)  évaluent un sous-ensemble de caractéristiques par sa performance
de classification en utilisant un algorithme d’apprentissage.  Les sous-ensembles de caractéristiques sélectionnés par cette méthode sont bien adaptés à l’algorithme de classification utilisé, mais ils ne sont pas nécessairement pour un autre. De plus, la complexité de l’algorithme d’apprentissage rend ces méthodes coûteuses.</p>
<p>Les principales différences entre les filtres et les méthodes enveloppantes pour la sélection des caractéristiques sont les suivantes :</p>
<ul class="simple">
<li><p>Les filtres mesurent la pertinence des caractéristiques par leur corrélation avec la variable dépendante, tandis que les méthodes enveloppantes mesurent l’utilité d’un sous-ensemble de caractéristiques en entraînant un modèle sur celles-ci.</p></li>
<li><p>Les filtres sont beaucoup plus rapides que les méthodes enveloppantes car elles n’impliquent pas l’apprentissage des modèles. D’un autre côté, les méthodes enveloppantes sont également très coûteuses en termes de calcul.</p></li>
<li><p>Les filtres utilisent des méthodes statistiques pour l’évaluation d’un sous-ensemble de caractéristiques, tandis que les méthodes enveloppantes utilisent la validation croisée.</p></li>
<li><p>Les filtres peuvent échouer à trouver le meilleur sous-ensemble de caractéristiques dans de nombreuses occasions, mais les méthodes enveloppantes peuvent toujours fournir le meilleur sous-ensemble de caractéristiques.</p></li>
<li><p>L’utilisation d’un sous-ensemble de caractéristiques à partir des méthodes enveloppantes amène plus facilement au phénomène de surapprentissage</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13 </span></p>

<p>Les wrappers sélectionnent les caractéristiques en se fondant sur une estimation du risque réel.</p>

</div>

<h4 id="methodes-integrees">Méthodes intégrées<a class="headerlink" href="#methodes-integrees" title="Permalink to this headline">¶</a></h4>
<p>Les méthodes intégrées incluent la sélection de variables lors du processus d’apprentissage. Un tel mécanisme intégré pour la sélection des caractéristiques peut être trouvé, par
exemple, dans les algorithmes de type SVM,  AdaBoost  ou dans les
arbres de décision.</p>




<h2 id="quelques-methodes-de-selection">Quelques méthodes de sélection<a class="headerlink" href="#quelques-methodes-de-selection" title="Permalink to this headline">¶</a></h2>

<h3 id="suppression-des-descripteurs-a-variance-faible">Suppression des descripteurs à variance faible<a class="headerlink" href="#suppression-des-descripteurs-a-variance-faible" title="Permalink to this headline">¶</a></h3>
<p>Une première idée simple consiste à supprimer les descripteurs ayant une faible variance, ces derniers n’étant pas discriminants dans la définition des individus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Avant sélection, "</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Après sélection, "</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Variables sélectionnées : "</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avant sélection,  (150, 4)
Après sélection,  (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>


<h3 id="algorithmes-de-selection-sequentielle">Algorithmes de sélection séquentielle<a class="headerlink" href="#algorithmes-de-selection-sequentielle" title="Permalink to this headline">¶</a></h3>
<p>Les algorithmes SFS (Sequential Forward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>) et SBS (Sequential Backward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>-rouge) ont été les premiers à être proposés. Ils utilisent des approches heuristiques de recherche en partant, pour la première, d’un ensemble de caractéristiques vide et pour la seconde de  <span class="math notranslate nohighlight">\(F\)</span> tout entier.</p>
<div class="proof algorithm admonition" id="SFS">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algorithmes SFS et SBS)</p>

<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span>, taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\leftarrow F\)</span></span>)</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(d-T\)</span></span>)</p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\( |{F}|\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(|\hat{F}|\)</span></span>)</p>
<ol class="simple">
<li><p>Evaluer <span class="math notranslate nohighlight">\(\{f_j\}\cup \hat{F}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus \{f_j\}\)</span></span>)</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{max}\)</span> = meilleure caractéristique <span class="math notranslate nohighlight">\(\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(f_{min}\)</span>=moins bonne caractéristique</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow\hat{F}\cup\{f_{max}\}, F=F\setminus f_{max}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus\hat{F}f_{min}\)</span></span>)</p></li>
</ol>
</li>
</ol>

</div><p>L’étape d’évaluation utilise des données d’apprentissage : une heuristique évalue, sur un critère de performance, l’intérêt d’ajouter (ou de supprimer) le descripteur <span class="math notranslate nohighlight">\(f_i\)</span>.</p>
<p>Des variantes autour de ces algorithmes simples ont été proposées depuis et par exemple :</p>
<ul class="simple">
<li><p>il est possible à chaque itération d’inclure (ou d’exclure) un sous-ensemble de caractéristiques, plutôt qu’une seule (méthodes GSFS et GSBS)</p></li>
<li><p>on peut appliquer <span class="math notranslate nohighlight">\(p\)</span> fois SFS puis <span class="math notranslate nohighlight">\(q\)</span> fois SBS, de manière itérative, avec <span class="math notranslate nohighlight">\(p,q\)</span> des paramètres qui peuvent évoluer au cours des itérations (algorithme SFFS et SFBS)</p></li>
</ul>
<p>Dans l’exemple suivant, l’heuristique choisie est l’algorithme des 3 plus proches voisins et la mesure de performance sous-jacente est la mesure de validation croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Taille des données avant sélection"</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Taille des données après sélection"</span><span class="p">,</span><span class="n">sfs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Variables sélectionnées : "</span><span class="p">,</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Taille des données après sélection (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>


<h3 id="algorithme-focus">Algorithme Focus<a class="headerlink" href="#algorithme-focus" title="Permalink to this headline">¶</a></h3>
<p>L’algorithme de filtrage Focus (<a class="reference internal" href="#FOCUS">Algorithm 2</a>}) repose sur une recherche exhaustive sur <span class="math notranslate nohighlight">\(F\)</span> pour trouver le sous-ensemble le plus performant de taille optimale.</p>
<div class="proof algorithm admonition" id="FOCUS">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algorithme FOCUS)</p>

<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span>, seuil <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>chaque sous-ensemble <span class="math notranslate nohighlight">\(S_i\)</span> de taille <span class="math notranslate nohighlight">\(i\)</span></p>
<ol class="simple">
<li><p>Si Inconsistance(A,<span class="math notranslate nohighlight">\(S_i\)</span>)&lt;<span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow S_i\)</span></p></li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\hat{F}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>

</div>

<h3 id="algorithme-relief">Algorithme relief<a class="headerlink" href="#algorithme-relief" title="Permalink to this headline">¶</a></h3>
<p>La méthode relief en classification binaire (<a class="reference internal" href="#relief">Algorithm 3</a>), propose de calculer une mesure globale de la pertinence des caractéristiques en accumulant la différence des distances entre des exemples d’apprentissage choisis aléatoirement et leurs plus proches voisins de la même classe et de l’autre classe.</p>
<div class="proof algorithm admonition" id="relief">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Algorithme Relief)</p>

<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , nombre d’itérations <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> un vecteur de poids des caractéristiques, <span class="math notranslate nohighlight">\(w_i\in[-1,1],i\in[\![1,d]\!]\)</span></p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_i\leftarrow 0\)</span></p></li>
</ol>
</li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>Choisir aléatoirement un exemple <span class="math notranslate nohighlight">\(\mathbf x_k\)</span></p></li>
<li><p>Chercher deux plus proches voisins de <span class="math notranslate nohighlight">\(\mathbf x_k\)</span>, l’un (<span class="math notranslate nohighlight">\(\mathbf x_p\)</span>) dans sa  classe, l’autre (<span class="math notranslate nohighlight">\(\mathbf x_q\)</span>) dans l’autre classe</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_j\leftarrow w_j+\frac{1}{nT}\left (|x_{kj} -x_{qj}|-|x_{kj} -x_{pj}| \right )\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>

</div>

<h3 id="methode-sac">Méthode SAC<a class="headerlink" href="#methode-sac" title="Permalink to this headline">¶</a></h3>
<p>L’algorithme SAC (Selection Adaptative de Caractéristiques)  construit un ensemble de classifieurs (ou de régresseurs) <span class="math notranslate nohighlight">\((M_1\cdots M_d)\)</span> appris sur chacun des descripteurs et sélectionne les meilleurs par discrimination linéaire de Fisher. Pour ce faire, l’algorithme construit un vecteur dont les éléments sont les performances <span class="math notranslate nohighlight">\(Perf(M_i)\)</span> des modèles <span class="math notranslate nohighlight">\(M_i\)</span>, triés par ordre décroissant. Deux moyennes <span class="math notranslate nohighlight">\(m_1(i)\)</span> et <span class="math notranslate nohighlight">\(m_2(i)\)</span> sont calculées, qui représentent les deux moyennes de performance d’apprentissage qui ont une valeur respectivement plus grande (plus petite) que la performance du modèle <span class="math notranslate nohighlight">\(M_i\)</span> :</p>
<div class="math notranslate nohighlight">
\[m_1(i) = \frac{1}{i}\displaystyle\sum_{j=1}^i Perf (M_j)\textrm{ et } m_2(i) = \frac{1}{d-i}\displaystyle\sum_{j=i+1}^d Perf (M_j)\]</div>
<p>Deux variances des performances  <span class="math notranslate nohighlight">\(v_1^2(i)\)</span> et <span class="math notranslate nohighlight">\( v_2^2(i)\)</span> sont alors calculées à partir de ces moyennes, et le sous-ensemble de caractéristiques sélectionné est celui qui maximise le discriminant de Fisher</p>
<div class="math notranslate nohighlight">
\[\frac{|m_1(i)-m_2(i)|}{v_1^2(i)+v_2^2(i)}\]</div>


<h3 id="algorithme-rfe">Algorithme RFE<a class="headerlink" href="#algorithme-rfe" title="Permalink to this headline">¶</a></h3>
<p>L’algorithme RLE (Recusrive Feature Elimination) trie les descripteurs en analysant, localement, la sensibilité de la performance.
Étant donné un prédicteur <span class="math notranslate nohighlight">\(f\)</span> qui attribue des poids aux caractéristiques (par exemple, les coefficients d’un modèle linéaire), l’objectif de l’algorithme est de sélectionner les caractéristiques en considérant de manière récursive des ensembles de caractéristiques de plus en plus petits. Tout d’abord, le prédicteur <span class="math notranslate nohighlight">\(f\)</span> est entraîné sur l’ensemble initial de caractéristiques et l’importance de chaque caractéristique est calculée par un algorithme dédié (critère de Gini, entropie…). Les caractéristiques les moins importantes sont éliminées de l’ensemble actuel de caractéristiques. Cette procédure est répétée de manière récursive sur l’ensemble élagué jusqu’à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span> 
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Taille des données avant sélection"</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Variables sélectionnées : "</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Classement des variables : "</span><span class="p">,</span><span class="n">s</span><span class="o">.</span><span class="n">ranking_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Variables sélectionnées :  [False False  True  True]
Classement des variables :  [2 3 1 1]
</pre></div>
</div>
</div>
</div>



<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>

          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="statsdescriptives.html" title="Statistique descriptive"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> "Previous" </span> Statistique descriptive </span>
              </div>
            </a>
          
          
            <a href="acp.html" title="Analyse en composantes principales"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> "Next" </span> Analyse en composantes principales </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2022.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>