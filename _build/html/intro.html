
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Analyse de données</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystyle.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="#" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint single-page" id="site-navigation">
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <p class="caption" role="heading">
 <span class="caption-text">
  Cours
 </span>
</p>
<ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Rappels">
   Rappels de probabilité
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-elemstats">
   Elements de statistiques
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-statsdescriptives">
   Statistique descriptive
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-selection">
   Sélection de variables
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-acp">
   Analyse en composantes principales
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-regression">
   Régression
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-clustering">
   Structures de classification
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#objectifs-de-la-classification">
   Objectifs de la classification
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#classification-ascendante-hierarchique">
   Classification ascendante hiérarchique
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#recherche-de-partitions">
   Recherche de partitions
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#modeles-de-melange">
   Modèles de mélange
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  TP
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP1_statsDescriptives">
   TP Statistiques descriptives
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP2_ACP">
   TP ACP
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#definition-de-quelques-outils">
   Définition de quelques outils
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#exercice-1-etude-de-proprietes-de-vins">
   Exercice 1 : étude de propriétés de vins
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#exercice-2-etude-de-donnees-image">
   Exercice 2 : étude de données image
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP_Regression_Lineaire">
   TP Régression linéaire
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Annexes
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-canonique">
   Analyse canonique
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-afc">
   Analyse Factorielle des correspondances
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-acm">
   Analyse des correspondances multiples
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-genindex">
   Index
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <p class="caption" role="heading">
 <span class="caption-text">
  Cours
 </span>
</p>
<ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Rappels">
   Rappels de probabilité
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-elemstats">
   Elements de statistiques
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-statsdescriptives">
   Statistique descriptive
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-selection">
   Sélection de variables
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-acp">
   Analyse en composantes principales
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-regression">
   Régression
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-clustering">
   Structures de classification
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#objectifs-de-la-classification">
   Objectifs de la classification
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#classification-ascendante-hierarchique">
   Classification ascendante hiérarchique
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#recherche-de-partitions">
   Recherche de partitions
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#modeles-de-melange">
   Modèles de mélange
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  TP
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP1_statsDescriptives">
   TP Statistiques descriptives
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP2_ACP">
   TP ACP
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#definition-de-quelques-outils">
   Définition de quelques outils
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#exercice-1-etude-de-proprietes-de-vins">
   Exercice 1 : étude de propriétés de vins
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#exercice-2-etude-de-donnees-image">
   Exercice 2 : étude de données image
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-TP_Regression_Lineaire">
   TP Régression linéaire
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Annexes
 </span>
</p>
<ul class="nav section-nav flex-column">
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-canonique">
   Analyse canonique
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-afc">
   Analyse Factorielle des correspondances
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-acm">
   Analyse des correspondances multiples
  </a>
 </li>
 <li class="toctree-l1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-genindex">
   Index
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h1>
<div class="section" id="elements-de-vocabulaire">
<h2>Eléments de vocabulaire<a class="headerlink" href="#elements-de-vocabulaire" title="Permalink to this headline">#</a></h2>
<p>On définit ici de manière informelle les termes utilisés dans la suite :</p>
<ul class="simple" id="index-0">
<li><p><strong>Population</strong> : ensemble de cardinalité finie, notée <span class="math notranslate nohighlight">\(N\)</span>, ou infinie ;</p></li>
</ul>
<ul class="simple" id="index-1">
<li><p><strong>Echantillon</strong> : sous-ensemble de la population, de cardinalité <span class="math notranslate nohighlight">\(n\)</span> ;</p></li>
</ul>
<ul class="simple" id="index-2">
<li><p><strong>Individu</strong> : sous-ensemble de la population ou de l’échantillon, de cardinalité 1 ;</p></li>
</ul>
<ul class="simple" id="index-3">
<li><p><strong>Caractère</strong> : nature de la caractéristique à laquelle on s’intéresse statistiquement. Il peut être <strong>qualitatif</strong> (nominal ou ordinal) ou <strong>quantitatif</strong> (discret ou continu).</p></li>
</ul>
</div>
<div class="section" id="probabilites-et-statistiques">
<h2>Probabilités et statistiques<a class="headerlink" href="#probabilites-et-statistiques" title="Permalink to this headline">#</a></h2>
<p>La question qui se pose est la suivante : comment définir ou estimer la valeur de probabilité associée à un  évènement ?
Plusieurs points de vue ont été proposés et adoptés que nous synthétisons très brièvement.</p>
<div class="section" id="approche-frequentiste">
<h3>Approche fréquentiste<a class="headerlink" href="#approche-frequentiste" title="Permalink to this headline">#</a></h3>
<p>Ce point de vue historique, souvent présenté comme le plus “naturel” ou “objectif”, consiste à définir une probabilité comme la limite de la fréquence d’observation de la caractéristique lorsque la taille de l’echantillon devient infinie. On suppose ici que les probabilités sont une loi de la nature qu’il faut mesurer par l’expérience. En pratique, la probabilité d’un  évènement est donc estimée/approximée en répétant un très grand nombre de fois l’expérience dans les mêmes conditions. C’est de ces expériences répétées que sont nés les outils de la statistique tels que la régression linéaire ou le test du <span class="math notranslate nohighlight">\(\chi^2\)</span>.
On rencontre néanmoins très rapidement des limitations avec ce type d’approche. D’une part, il est impossible d’un point de vue fréquentiste de traiter de petits échantillons de données. De plus, certains types de données ne sont
pas exploitables en raison de leur caractère non expérimental (par exemple, quelle probabilité associer à un évènement du type “nombre de votants aux prochaines élections” qui n’est pas répétable). Enfin, il est parfois difficile de définir un modèle  permettant de modéliser une erreur de mesure ou la variation observée d’un caractère dans une population.</p>
</div>
<div class="section" id="approche-bayesienne">
<h3>Approche bayésienne<a class="headerlink" href="#approche-bayesienne" title="Permalink to this headline">#</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">T. Bayes</p>
<p><img alt="" src="_images/bayes.jpeg" /></p>
</div>
<p>Un point de vue bien différent consiste à définir les probabilités comme une mesure subjective de l’incertitude. Dans ce cadre, tout événement peut être probabilisé à partir d’un a priori de l’observateur. Ce point de vue est appelé Bayésien (fait appel à la règle de Bayes) pour calculer la loi de probabilité des évènements à partir des échantillons de données a posteriori. L’intérêt majeur de ce type de démarche est que tout est probabilisable (jusqu’aux paramètres du modèle utilisé) et qu’il s’appuie sur des résultats de la théorie des probabilités, comme le théorème central limite.
Ce point de vue “subjectif” a longtemps été dénoncé par les adeptes de l’approche fréquentiste qui rejettent l’idée que l’on puisse définir un tel a priori sur les évènements. En effet, l’objection majeure que l’on oppose souvent à la méthodologie bayésienne est que deux observateurs différents, ayant des a priori différents, donneront des résultats ou des interprétations différentes.</p>
</div>
</div>
<div class="section" id="demarche-generale">
<h2>Démarche générale<a class="headerlink" href="#demarche-generale" title="Permalink to this headline">#</a></h2>
<p>De manière assez générale, une étude statistique consiste à obtenir des informations sur un caractère concernant une population de grande taille en s’appuyant sur celles d’un sous-ensemble de taille réduite (l’échantillon), afin le plus souvent d’orienter une décision. Le choix de l’échantillon se fera par tirage avec ou sans remise, par tirage uniforme (“au hasard”) ou non (tirage stratifié dans le cas d’un sondage par exemple, ou selon une loi de probabilité précise si une information a priori est disponible).
On estime alors des propriétés d’un caractère de l’échantillon. A partir de ces estimations, on cherche à donner “au mieux” des valeurs aux paramètres correspondant de la population (la moyenne, la variance,…). L’estimation pourra être ponctuelle ou par intervalle de confiance. On pourra également s’intéresser à des tests d’ajustement (ou d’adéquation) à une loi de type donné. La décision, étape finale de l’analyse statistique, se fera par des tests statistiques.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 1 </span></p>
<div class="example-content section" id="proof-content">
<p>Un cuisinier cherche à savoir si sa sauce est suffisamment salée. Après avoir remué sa casserole, il prélève une cuillérée de sauce (l’échantillon). Il la goûte (il estime le caractère salé de l’échantillon). En fonction du résultat, il décide que la casserole de sauce (la population) est suffisamment salée ou pas.</p>
</div>
</div><div class="admonition important">
<p class="admonition-title">Important</p>
<span class="target" id="index-4"></span><p id="index-5">La science des statistiques se décompose donc en :</p>
<ul class="simple">
<li><p>la <strong>statistique descriptive</strong>, dont l’objectif est de décrire le caractère d’un échantillon en résumant l’information qu’il contient ;</p></li>
<li><p>la <strong>statistique inférentielle</strong>, dont l’objectif est d’inférer, à partir de l’information recueillie sur l’échantillon, des propriétés valables sur la population, de manière la plus fiable possible.</p></li>
</ul>
</div>
<p id="index-6">A cette dichotomie s’ajoute la <strong>statistique exploratoire</strong>, branche de l’analyse de données, qui cherche à comprendre l’organisation des individus de l’échantillon (existe-t-il des groupes d’individus semblables ? les caractères mesurés sont-ils les plus pertinents ?…)</p>
<p>Nous n’abordons pas dans ce cours la statistique inférentielle.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-Rappels"></span><div class="tex2jax_ignore mathjax_ignore section" id="rappels-de-probabilite">
<h2>Rappels de probabilité<a class="headerlink" href="#rappels-de-probabilite" title="Permalink to this headline">#</a></h2>
<div class="section" id="experience-aleatoire">
<h3>Expérience aléatoire<a class="headerlink" href="#experience-aleatoire" title="Permalink to this headline">#</a></h3>
<div class="section" id="definitions">
<h4>Définitions<a class="headerlink" href="#definitions" title="Permalink to this headline">#</a></h4>
<div class="proof definition admonition" id="expalea">
<p class="admonition-title"><span class="caption-number">Definition 1 </span> (Expérience aléatoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une <strong>expérience aléatoire</strong> est une expérience dont on ne peut prévoir le résultat a priori. Répétée dans des conditions identiques, elle peut donner lieu à des résultats différents.</p>
</div>
</div><div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<div class="example-content section" id="proof-content">
<ul class="simple">
<li><p>Le lancé de dé</p></li>
<li><p>Les côtes exactes d’une pièce fabriquée dans un atelier</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="issue">
<p class="admonition-title"><span class="caption-number">Definition 2 </span> (Issue)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle <strong>issue</strong> d’une expérience aléatoire l’un des résultats possibles de cette expérience</p>
</div>
</div><div class="proof definition admonition" id="univers">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (Univers des possibles)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle <strong>univers des possibles</strong> d’une expérience aléatoire l’ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> des issues de cette expérience.</p>
</div>
</div><div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 3 </span></p>
<div class="example-content section" id="proof-content">
<p>Lorsque l’on joue à pile ou face avec une pièce de monnaie, l’expérience a deux issues possibles et <span class="math notranslate nohighlight">\(\Omega = \{P,F\}\)</span>.</p>
</div>
</div><p>L’univers des possibles n’est pas défini de manière unique, mais dépend de l’usage de l’experience. Par exemple, pour le lancer de deux dés, on peut être intéressé par :</p>
<ul class="simple">
<li><p>le résultat du lancer, dans ce cas <span class="math notranslate nohighlight">\(\Omega = \{(1,1), (1,2), \cdots (6,6)\}\)</span></p></li>
<li><p>la somme des deux faces et <span class="math notranslate nohighlight">\(\Omega = [\![2,12]\!]\)</span></p></li>
</ul>
<div class="proof definition admonition" id="evenement">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 4 </span> (Evènement)</p>
<div class="definition-content section" id="proof-content">
<p>Etant donnée une expérience aléatoire, un <strong>évènement</strong> est une assertion vraie ou fausse suivant l’issue de l’expérience. C’est donc un sous-ensemble <span class="math notranslate nohighlight">\(E\)</span> de <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
</div>
</div><div class="proof example admonition" id="example-6">
<p class="admonition-title"><span class="caption-number">Example 4 </span></p>
<div class="example-content section" id="proof-content">
<ul class="simple">
<li><p>Dans l’expérience du lancer de deux dés, on peut s’intéresser à l’évènement “la somme des deux faces est paire” ou encore  “la somme est supérieure à 7”.</p></li>
<li><p>Si l’expérience considérée concerne les jobs effectués sur une machine on peut considérer :</p></li>
</ul>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega=\mathbb{N}\)</span> et l’évènement “le nombre de jobs ne dépasse pas 10” : <span class="math notranslate nohighlight">\(E=[\![0,10]\!]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Omega=\mathbb{R}^*\)</span> et  l’évènement “le job dure plus de 15 s” et <span class="math notranslate nohighlight">\(E=]15,+\infty[\)</span></p></li>
</ol>
</div>
</div><p>Il existe certains évènements particuliers :</p>
<ul class="simple">
<li><p>l’évènement dit certain : c’est l’univers des possibles (par exemple “la somme des deux faces d’un lancer de deux dés est inférieure ou égale à 12”)</p></li>
<li><p>l’évènement impossible (“la somme des deux faces d’un lancer de deux dés est supérieure ou égale à 20”)</p></li>
<li><p>l’évènement simple : tout singleton de <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p>l’évènement composé : tout sous-ensemble de <span class="math notranslate nohighlight">\(\Omega\)</span> de cardinalité au moins égale à 2.</p></li>
</ul>
</div>
<div class="section" id="notation-et-operations-sur-les-evenements">
<h4>Notation et opérations sur les évènements<a class="headerlink" href="#notation-et-operations-sur-les-evenements" title="Permalink to this headline">#</a></h4>
<p>Les évènements peuvent être interprétés soit d’un point de vue ensembliste (Diagrammes de Venn), soit de manière équivalente d’un point de vue probabiliste.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Notation</strong></p></th>
<th class="head"><p><strong>Interprétation probabiliste</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\omega\)</span></p></td>
<td><p>issue possible, évènement élémentaire</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\)</span></p></td>
<td><p>évènement</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\omega\in A\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\omega\)</span> réalise <span class="math notranslate nohighlight">\(A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\subset B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> implique <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\cup B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> ou <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cap B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\bar A\)</span></p></td>
<td><p>contraire de <span class="math notranslate nohighlight">\(A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\emptyset\)</span></p></td>
<td><p>évènement impossible</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\Omega\)</span></p></td>
<td><p>évènement certain</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cap B=\emptyset\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> incompatibles</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\setminus B = A\cap \bar B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et pas <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="probabilites">
<h3>Probabilités<a class="headerlink" href="#probabilites" title="Permalink to this headline">#</a></h3>
<div class="section" id="objectif">
<h4>Objectif<a class="headerlink" href="#objectif" title="Permalink to this headline">#</a></h4>
<p>L’objectif des probabilités est de donner une <strong>mesure</strong> à la chance qu’a un évènement de se réaliser lors d’une expérience aléatoire. Pour ce faire, on définit une fonction <span class="math notranslate nohighlight">\(P:\Omega\rightarrow [0,1]\)</span> vérifiant certains axiomes et propriétés.</p>
<div class="proof definition admonition" id="tribu">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 5 </span> (Tribu)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(T\)</span> une famille d’évènements. Pour que <span class="math notranslate nohighlight">\(T\)</span> soit probabilisable, il faut que :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\emptyset\in T, \Omega\in T\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A_i\)</span> est une suite dans <span class="math notranslate nohighlight">\(T\)</span> alors <span class="math notranslate nohighlight">\(\cup_iA_i\in T\)</span> et <span class="math notranslate nohighlight">\(\cap_iA_i\in T\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A\in T\)</span> alors <span class="math notranslate nohighlight">\(\bar A\in T\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(T\)</span> est une <strong>tribu</strong> et <span class="math notranslate nohighlight">\((\Omega,T)\)</span> est un <strong>espace probabilisable</strong>.</p>
</div>
</div><p>En pratique, on choisit souvent la tribu engendrée par une famille de <span class="math notranslate nohighlight">\(n\)</span> évènements <span class="math notranslate nohighlight">\(A_i\)</span>, qui est l’ensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span> obtenues en effectuant l’union de <span class="math notranslate nohighlight">\(k\)</span> évènements <span class="math notranslate nohighlight">\(A_i,i\in [\![1,n]\!]\)</span>.</p>
<div class="proof example admonition" id="example-8">
<p class="admonition-title"><span class="caption-number">Example 5 </span></p>
<div class="example-content section" id="proof-content">
<p>Dans le cas du lancer d’un dé, <span class="math notranslate nohighlight">\(\Omega = \{1,2,3,4,5,6\}\)</span>, et :</p>
<ul class="simple">
<li><p>la tribu engendrée par la famille d’évènements <span class="math notranslate nohighlight">\(\{\{1,3,5\},\{2,4,6\}\}\)</span> est <span class="math notranslate nohighlight">\(\{\emptyset,\{1,3,5\},\{2,4,6\},\Omega\}\)</span>.</p></li>
<li><p>la tribu engendrée par la famille d’évènements <span class="math notranslate nohighlight">\(\{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}\)</span> est l’ensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span>. Plus généralement, si <span class="math notranslate nohighlight">\(\Omega\)</span> est dénombrable, cette tribu est appelée <strong>tribu discrète</strong>.</p></li>
</ul>
</div>
</div><p>On peut également s’intéresser, si <span class="math notranslate nohighlight">\(\Omega=\mathbb R\)</span>, à la tribu engendrée par les ouverts de <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, on parle alors de <strong>tribu borélienne</strong>.</p>
</div>
<div class="section" id="probabilite">
<h4>Probabilité<a class="headerlink" href="#probabilite" title="Permalink to this headline">#</a></h4>
<div class="margin sidebar">
<p class="sidebar-title">A. Kolmogorov</p>
<p><img alt="" src="_images/kolmogorov.jpeg" /></p>
</div>
<div class="proof axiom admonition" id="axiomKolmo">
<span id="index-2"></span><p class="admonition-title"><span class="caption-number">Axiom 1 </span> (Axiomatique de Kolmogorov)</p>
<div class="axiom-content section" id="proof-content">
<p>On appelle <strong>probabilité</strong> sur <span class="math notranslate nohighlight">\((\Omega,T)\)</span> une application <span class="math notranslate nohighlight">\(P\)</span> de <span class="math notranslate nohighlight">\(T\)</span> dans [0,1] vérifiant :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall A\in T)\; P(A)\in[0,1]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Omega)=1\)</span></p></li>
<li><p>Pour toute famille dénombrable <span class="math notranslate nohighlight">\((A_i)\)</span> d’évènements disjoints <span class="math notranslate nohighlight">\(P(\displaystyle\bigcup_i A_i) = \displaystyle\sum_iP(A_i)\)</span></p></li>
</ul>
</div>
</div><p><span class="math notranslate nohighlight">\((\Omega,T,P)\)</span> est un <strong>espace probabilisé</strong>.</p>
<div class="proof property admonition" id="property-10">
<p class="admonition-title"><span class="caption-number">Property 1 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\emptyset)=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A)\; P(\bar A)=1-P(A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\; P(A\setminus B) = P(A)-P(A\bigcap B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\; P(A\bigcup B) = P(A)+P(B)-P(A\bigcap B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\)</span> si <span class="math notranslate nohighlight">\(A\subset B\)</span> alors <span class="math notranslate nohighlight">\(P(A)\leq P(B)\)</span></p></li>
<li><p>Pour toute famille dénombrable <span class="math notranslate nohighlight">\((A_i)\)</span> d’évènements quelconques <span class="math notranslate nohighlight">\(P(\displaystyle\bigcup_i A_i) \leq \displaystyle\sum_iP(A_i)\)</span></p></li>
</ul>
</div>
</div></div>
<div class="section" id="conditionnement">
<h4>Conditionnement<a class="headerlink" href="#conditionnement" title="Permalink to this headline">#</a></h4>
<p>Les probabilités <strong>conditionnelles</strong> intègrent une information supplémentaire sous la forme de l’observation de la réalisation d’un évènement donné.</p>
<div class="proof definition admonition" id="definition-11">
<span id="index-3"></span><p class="admonition-title"><span class="caption-number">Definition 6 </span> (Probabilité conditionnelle)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(B\)</span> un évènement de probabilité non nulle. On appelle <strong>probabilité conditionnelle</strong> de <span class="math notranslate nohighlight">\(A\)</span> sachant <span class="math notranslate nohighlight">\(B\)</span> le rapport</p>
<p><span class="math notranslate nohighlight">\(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\)</span></p>
</div>
</div><p><span class="math notranslate nohighlight">\(P(A\mid B)\)</span> représente la probabilité que <span class="math notranslate nohighlight">\(A\)</span> se réalise sachant que <span class="math notranslate nohighlight">\(B\)</span> est réalisé.</p>
<p>Remarquons que l’on peut écrire <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\)</span>.</p>
<div class="proof example admonition" id="example-12">
<p class="admonition-title"><span class="caption-number">Example 6 </span></p>
<div class="example-content section" id="proof-content">
<p>7% des français sont atteints d’un cancer du poumon. 70% des malades sont des fumeurs et 50% des français fument. On recherche la probabilité d’être atteint d’un cancer du poumon lorsque l’on est fumeur.
L’évènement <span class="math notranslate nohighlight">\(A\)</span> est “avoir un cancer du poumon”, et <span class="math notranslate nohighlight">\(B\)</span> est “être fumeur”. D’après les données on a <span class="math notranslate nohighlight">\(P(A)\)</span>=0.07, <span class="math notranslate nohighlight">\(P(B)\)</span> = 0.5 et <span class="math notranslate nohighlight">\(P(B\mid A)\)</span> = 0.7.
On a alors <span class="math notranslate nohighlight">\(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\)</span> avec <span class="math notranslate nohighlight">\(P(A\bigcap B)=P(B\mid A)P(A)\)</span> d’où</p>
<p><span class="math notranslate nohighlight">\(P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B}\)</span> = 0.098</p>
</div>
</div></div>
<div class="section" id="independance">
<h4>Indépendance<a class="headerlink" href="#independance" title="Permalink to this headline">#</a></h4>
<div class="proof definition admonition" id="definition-13">
<span id="index-4"></span><p class="admonition-title"><span class="caption-number">Definition 7 </span> (Indépendance)</p>
<div class="definition-content section" id="proof-content">
<p>Deux évènements <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont dits <strong>indépendants</strong> si et seulement si <span class="math notranslate nohighlight">\(P(A\mid B) = P(A)\)</span>.</p>
</div>
</div><p>On a alors bien évidemment <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A)P(B)\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-14">
<p class="admonition-title"><span class="caption-number">Remark 1 </span></p>
<div class="remark-content section" id="proof-content">
<p>La notion d’indépendance est directement rattachée à <span class="math notranslate nohighlight">\(P\)</span> : <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> peuvent être indépendants pour une probabilité donnée, mais pas pour une autre.</p>
</div>
</div><p>On peut généraliser la notion d’indépendance à une famille d’évènements <span class="math notranslate nohighlight">\((A_i)_{i\in[\![1,n]\!]}\)</span> : on dira que les <span class="math notranslate nohighlight">\(A_i\)</span> sont <strong>mutuellement indépendants</strong> si pour tout <span class="math notranslate nohighlight">\(I\subset [\![1,n]\!]\)</span></p>
<p><span class="math notranslate nohighlight">\(P\left (\displaystyle\bigcap_{i\in I} A_i\right ) = \displaystyle\prod_{i\in I} P(A_i)\)</span></p>
<p>L’indépendance mutuelle est plus forte que l’indépendance deux à deux.</p>
<div class="proof remark dropdown admonition" id="remark-15">
<p class="admonition-title"><span class="caption-number">Remark 2 </span></p>
<div class="remark-content section" id="proof-content">
<p>La notion d’indépendance n’est pas une notion purement ensembliste. Deux évènements peuvent être indépendants pour une loi de probabilité et pas pour une autre.</p>
</div>
</div></div>
<div class="section" id="theoreme-des-probabilites-totales">
<h4>Théorème des probabilités totales<a class="headerlink" href="#theoreme-des-probabilites-totales" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-16">
<p class="admonition-title"><span class="caption-number">Theorem 1 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(\{B_i\}\)</span> un système complet d’évènements (qui forment donc une partition de <span class="math notranslate nohighlight">\(\Omega\)</span>). Pour tout évènement <span class="math notranslate nohighlight">\(A\)</span>, on peut écrire</p>
<p><span class="math notranslate nohighlight">\(P(A) = \displaystyle\sum_i P(A\bigcap B_i) = \displaystyle\sum_i P(A| B_i) P(B_i)\)</span></p>
</div>
</div></div>
<div class="section" id="regle-de-bayes">
<h4>Règle de Bayes<a class="headerlink" href="#regle-de-bayes" title="Permalink to this headline">#</a></h4>
<p id="index-5">A partir de l’égalité <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A|B)P(B)=P(B|A)P(A)\)</span>, on définit la règle de Bayes</p>
<p><span class="math notranslate nohighlight">\((\forall A,B)\quad P(B|A)=\frac{P(A|B)P(B)}{P(A)}\)</span></p>
<p>Si <span class="math notranslate nohighlight">\(B_i\)</span> est un système complet d’évènements, on a de plus</p>
<p><span class="math notranslate nohighlight">\(P(B_i|A)= \frac{P(A|B_i)P(B_i)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\displaystyle\sum_k P(A|B_k)P(B_k)}\)</span></p>
<div class="proof example admonition" id="example-17">
<p class="admonition-title"><span class="caption-number">Example 7 </span></p>
<div class="example-content section" id="proof-content">
<p>Un fabricant de boulons a trois usines de fabrication situées à Amiens, Besançon et Clermont-Ferrand. Amiens fournit 25% de la production, Besançon 20% et Clermont-Ferrand 55%. Les boulons de 5mm représentent 20% des boulons produits à Amiens, 30% à Besançon et 15% à Clermont-Ferrand. On répond à la question suivante : sachant que le boulon acheté a une taille de 5mm, quelle est la probabilité qu’il soit produit à Clermont-Ferrand ?</p>
<p>On note <span class="math notranslate nohighlight">\(B_1\)</span> (respectivement <span class="math notranslate nohighlight">\(B_2,B_3\)</span>) l’évènement “Le boulon est produit à Amiens (resp. Besançon, Clermont-Ferrand)”. On note également <span class="math notranslate nohighlight">\(A\)</span> l’évènement “Le boulon fait 5mm”. On cherche donc</p>
<p><span class="math notranslate nohighlight">\(P(B_3|A) = \frac{P(A|B_3)P(B_3)}{P(A)}= \frac{0.15*0.55}{0.1925}=0.428\)</span>.</p>
</div>
</div><p>On a calculé dans l’exemple une <strong>probabilité a posteriori</strong>, c’est à dire sachant une information supplémentaire (le boulon fait 5mm). La prise en compte de cette information modifie la valeur de la probabilité associée à <span class="math notranslate nohighlight">\(B_3\)</span>. La théorie des probabilités au travers de l’approche bayésienne est adaptée pour prendre en compte toute information nouvelle.</p>
</div>
</div>
<div class="section" id="variable-aleatoire">
<h3>Variable aléatoire<a class="headerlink" href="#variable-aleatoire" title="Permalink to this headline">#</a></h3>
<div class="section" id="concept-de-variable-aleatoire">
<h4>Concept de variable aléatoire<a class="headerlink" href="#concept-de-variable-aleatoire" title="Permalink to this headline">#</a></h4>
<p>Soit un espace probabilisé <span class="math notranslate nohighlight">\((\Omega, T,P)\)</span>.</p>
<div class="proof definition admonition" id="definition-18">
<span id="index-6"></span><p class="admonition-title"><span class="caption-number">Definition 8 </span> (Variable aléatoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable aléatoire est une application <span class="math notranslate nohighlight">\(X:\Omega\rightarrow E\)</span> (on prendra <span class="math notranslate nohighlight">\(E=\mathbb R\)</span>)</p>
</div>
</div><p>Pour obtenir la probabilité d’une valeur quelconque image par <span class="math notranslate nohighlight">\(X\)</span>, il suffit de dénombrer les <span class="math notranslate nohighlight">\(\omega\)</span> qui réalisent cette valeur.</p>
<div class="proof example admonition" id="example-19">
<p class="admonition-title"><span class="caption-number">Example 8 </span></p>
<div class="example-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(\Omega\)</span> = (Pile,Face), on considère la loi de probabilité <span class="math notranslate nohighlight">\(P\)</span> telle que : <span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; P(\omega)=\frac12\)</span>.
<span class="math notranslate nohighlight">\(P(X=1)= P(\{Pile\}) = \frac12\)</span>.</p>
</div>
</div><p>On dit que l’on transporte la loi de probabilité de <span class="math notranslate nohighlight">\(\Omega\)</span> sur <span class="math notranslate nohighlight">\(E\)</span> par l’application <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Les éléments de <span class="math notranslate nohighlight">\(E\)</span> sont les <strong>réalisations</strong> de la variable aléatoire.</p>
<div class="proof example admonition" id="example-20">
<p class="admonition-title"><span class="caption-number">Example 9 </span></p>
<div class="example-content section" id="proof-content">
<p>Si l’expérience consiste à observer le résultat du tirage de deux dés à 6 faces, <span class="math notranslate nohighlight">\(\Omega = \{(1,1), (1,2), \cdots (6,5), (6,6)\}\)</span>, on considère la loi de probabilité telle que <span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; P(\omega)=\frac{1}{36}\)</span>.</p>
<p>Si l’application <span class="math notranslate nohighlight">\(X\)</span> réalise la somme des deux éléments de <span class="math notranslate nohighlight">\(\omega\in\Omega\)</span>, alors on a par exemple <span class="math notranslate nohighlight">\(P(X=3)= P(\{(1,2),(2,1)\}) = \frac{2}{36}\)</span>, ou encore <span class="math notranslate nohighlight">\(P(X=5)= P(\{(1,4),(2,3),(3,2),(4,1)\}) = \frac{4}{36}\)</span>.</p>
</div>
</div></div>
<div class="section" id="variable-aleatoire-mesurable">
<h4>Variable aléatoire mesurable<a class="headerlink" href="#variable-aleatoire-mesurable" title="Permalink to this headline">#</a></h4>
<p>On définit sur <span class="math notranslate nohighlight">\(E\)</span> une tribu <span class="math notranslate nohighlight">\(T'\)</span>.  <span class="math notranslate nohighlight">\((E,T')\)</span> est alors un espace probabilisable, et tout élément <span class="math notranslate nohighlight">\(B\)</span> de <span class="math notranslate nohighlight">\(T'\)</span> est un évènement. On note alors <span class="math notranslate nohighlight">\(X^{-1}(B) = \{\omega\in\Omega,\; X(\omega)\in B\}\)</span></p>
<div class="proof definition admonition" id="definition-21">
<span id="index-7"></span><p class="admonition-title"><span class="caption-number">Definition 9 </span> (Variable aléatoire mesurable)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> est dite mesurable  si et seulement si : <span class="math notranslate nohighlight">\((\forall B\in T')\; X^{-1}(B)\in T\)</span></p>
</div>
</div><p>Dans les deux exemples précédents, on a par exemple <span class="math notranslate nohighlight">\(X^{-1}(1)= \{Pile\}\)</span> ou encore <span class="math notranslate nohighlight">\(X^{-1}(3) = \{(1,2),(2,1)\}\)</span> et <span class="math notranslate nohighlight">\(P(X=3)=P(X^{-1}(3)) = \frac{2}{36}\)</span>.</p>
<span class="target" id="index-8"></span><p id="index-9">On note souvent <span class="math notranslate nohighlight">\(P_X(B) = P(X^{-1}(B))=P(\{\omega / X(\omega)\in B\})\)</span> et on l’appelle <strong>probabilité image</strong> de <span class="math notranslate nohighlight">\(P\)</span> par <span class="math notranslate nohighlight">\(X\)</span>. En calculant la probabilité de chaque réalisation de la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span>, on peut en déduire la <strong>loi de probabilité</strong> (ou <strong>distribution</strong>) de <span class="math notranslate nohighlight">\(X\)</span>.</p>
<ul class="simple">
<li><p>Pour une variable aléatoire discrète <span class="math notranslate nohighlight">\(X\)</span>, la loi de probabilité est donc <span class="math notranslate nohighlight">\(P_X(x_i)= P(X=x_i) = P(\{\omega / X(\omega)=x_i\})\)</span>. <span class="math notranslate nohighlight">\(P_X\)</span> est appelée <strong>masse ponctuelle</strong>.</p></li>
</ul>
<span class="target" id="index-10"></span><ul class="simple" id="index-11">
<li><p>Pour une variable aléatoire continue <span class="math notranslate nohighlight">\(X\)</span>, la loi de probabilité est donc <span class="math notranslate nohighlight">\(f_X(x)dx = P(x\leq X\leq x+dx) = P(\{\omega /x\leq X(\omega)\leq x+dx\})\)</span>. <span class="math notranslate nohighlight">\(f_X\)</span> est appelée <strong>densité de probabilité</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">floor</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">tirage</span><span class="p">():</span>
    <span class="n">d1</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>   
    <span class="n">d2</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">d1</span><span class="o">+</span><span class="n">d2</span> <span class="o">-</span><span class="mi">1</span>          

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10000</span>                       
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>        
    <span class="n">f</span><span class="p">[</span><span class="n">tirage</span><span class="p">()</span> <span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="o">/</span><span class="n">n</span>                      

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loi de probabilité d&#39;une variable aléatoire discrète&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Rappels_1_0.png" src="_images/Rappels_1_0.png" />
</div>
</div>
<div class="proof definition admonition" id="definition-22">
<span id="index-12"></span><p class="admonition-title"><span class="caption-number">Definition 10 </span> (Fonction de répartition)</p>
<div class="definition-content section" id="proof-content">
<p>La fonction de répartition d’une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> est l’application <span class="math notranslate nohighlight">\(F_X\)</span> de <span class="math notranslate nohighlight">\(\mathbb R\)</span> dans [0,1] telle que <span class="math notranslate nohighlight">\(F_X(x) = P(X\leq x)\)</span>.</p>
</div>
</div><p><span class="math notranslate nohighlight">\(F_X\)</span> est donc monotone croissante, continue à droite et on a en particulier :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(a\leq X\leq b) = F_X(b)-F_X(a)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X&gt;x) = 1-P(X\leq x) = 1-F_X(x)\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">floor</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">tirage</span><span class="p">():</span>
    <span class="n">d1</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>   
    <span class="n">d2</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">d1</span><span class="o">+</span><span class="n">d2</span> <span class="o">-</span><span class="mi">1</span>          

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10000</span>                       
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>        
    <span class="n">f</span><span class="p">[</span><span class="n">tirage</span><span class="p">()</span> <span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="o">/</span><span class="n">n</span>     

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>          
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xmax</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
          <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="n">fn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymax</span><span class="o">=</span><span class="n">fn</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
          <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Rappels_3_0.png" src="_images/Rappels_3_0.png" />
</div>
</div>
<p>La notion de variable aléatoire est ainsi une formalisation de la notion de grandeur variant selon le résultat d’une expérience aléatoire. On peut alors préciser et formaliser la définition précédente.</p>
<div class="proof definition admonition" id="definition-23">
<p class="admonition-title"><span class="caption-number">Definition 11 </span> (Variable aléatoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable aléatoire est une application mesurable <span class="math notranslate nohighlight">\(X:(\Omega,T,P) \rightarrow (E,T')\)</span></p>
</div>
</div><div class="proof remark dropdown admonition" id="remark-24">
<p class="admonition-title"><span class="caption-number">Remark 3 </span></p>
<div class="remark-content section" id="proof-content">
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb N\)</span>, on parle de variable aléatoire (réelle) discrète</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb R\)</span>, on parle de variable aléatoire (réelle) continue. <span class="math notranslate nohighlight">\(T'\)</span> est alors la tribu <strong>borélienne</strong></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb N^n\)</span> ou <span class="math notranslate nohighlight">\(E=\mathbb R^n\)</span>, on parle de <strong>vecteur aléatoire</strong> de dimension <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ul>
</div>
</div></div>
<div class="section" id="caracteristiques-des-variables-aleatoires">
<h4>Caractéristiques des variables aléatoires<a class="headerlink" href="#caracteristiques-des-variables-aleatoires" title="Permalink to this headline">#</a></h4>
<p>Une loi de probabilité est caractérisée par un certain nombre de grandeurs :</p>
<ul class="simple">
<li><p>sa valeur centrale</p></li>
<li><p>sa dispersion</p></li>
<li><p>sa forme</p></li>
</ul>
<div class="section" id="esperance-mathematique-d-une-variable-aleatoire">
<h5>Espérance mathématique d’une variable aléatoire<a class="headerlink" href="#esperance-mathematique-d-une-variable-aleatoire" title="Permalink to this headline">#</a></h5>
<div class="proof definition admonition" id="definition-25">
<span id="index-13"></span><p class="admonition-title"><span class="caption-number">Definition 12 </span> (Espérance)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(X\)</span> une variable aléatoire. On définit l’espérance mathématique de <span class="math notranslate nohighlight">\(X\)</span>, et on note <span class="math notranslate nohighlight">\(\mathbb E(X)\)</span> par :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X = \displaystyle\sum_{x_i} x_iP(X=x_i)= \displaystyle\sum_{x_i} x_i P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discrète et si la somme converge.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X =\int_x xdP(x) = \int_x x f_X(x) dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue et si l’intégrale converge.</p></li>
</ul>
</div>
</div><p><span class="math notranslate nohighlight">\(\mathbb E(X)\)</span> est la moyenne arithmétique (également notée <span class="math notranslate nohighlight">\(\mu_X\)</span>) des différentes valeurs prises par <span class="math notranslate nohighlight">\(X\)</span> pondérées par leur probabilité.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_discrete</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">rv_discrete</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Espérance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">expect</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Espérance :  23.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_continuous</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">3.5</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mf">5.5</span> 
<span class="k">class</span> <span class="nc">distribution_gen</span><span class="p">(</span><span class="n">rv_continuous</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Espérance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">expect</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lb</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">ub</span><span class="o">=</span><span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Espérance :  7.904816400226159
</pre></div>
</div>
</div>
</div>
<p>On dira que <span class="math notranslate nohighlight">\(X\)</span> est <strong>centrée</strong> si <span class="math notranslate nohighlight">\(\mathbb{E}(X)=0\)</span>.</p>
<div class="proof example admonition" id="example-26">
<p class="admonition-title"><span class="caption-number">Example 10 </span></p>
<div class="example-content section" id="proof-content">
<p>Pour l’expérience d’un lancer de dé à 6 faces  : <span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X = \displaystyle\sum_{i=1}^6 i\frac16 = \frac72\)</span></p>
</div>
</div><div class="proof property admonition" id="property-27">
<p class="admonition-title"><span class="caption-number">Property 2 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(a)= a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(aX)= a\mathbb{E}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(X+a) = \mathbb{E}(X) +a\)</span></p></li>
</ul>
</div>
</div></div>
<div class="section" id="moment-d-une-fonction-d-une-variable-aleatoire">
<h5>Moment d’une fonction d’une variable aléatoire<a class="headerlink" href="#moment-d-une-fonction-d-une-variable-aleatoire" title="Permalink to this headline">#</a></h5>
<p id="index-14">Soit <span class="math notranslate nohighlight">\(\phi\)</span> l’application qui associe à toute variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> la variable aléatoire <span class="math notranslate nohighlight">\(Y=\phi(X)\)</span>.</p>
<div class="proof definition admonition" id="definition-28">
<p class="admonition-title"><span class="caption-number">Definition 13 </span> (Moment)</p>
<div class="definition-content section" id="proof-content">
<p>Le moment  <span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)]\)</span> de la fonction <span class="math notranslate nohighlight">\(\phi\)</span> de la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> est égal à</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)] = \displaystyle\sum_{x_i} \phi(x_i)P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discrète</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)] = \int_x \phi(x) f_X(x)dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="definition-29">
<span id="index-15"></span><p class="admonition-title"><span class="caption-number">Definition 14 </span> (Moment d’ordre <span class="math notranslate nohighlight">\(k\)</span>)</p>
<div class="definition-content section" id="proof-content">
<p>Le moment d’ordre <span class="math notranslate nohighlight">\(k\)</span> d’une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> est égal à :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(X^k) = \displaystyle\sum_{x_i} x_i^k P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discrète</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbb{E}(X^k) = \int_x x^k f_X(x)dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue</p></li>
</ul>
</div>
</div><p>Le moment d’ordre <span class="math notranslate nohighlight">\(k\)</span> est donc un cas particulier avec <span class="math notranslate nohighlight">\(Y=X^k\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-30">
<p class="admonition-title"><span class="caption-number">Remark 4 </span></p>
<div class="remark-content section" id="proof-content">
<p>L’espérance <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> est le moment d’ordre 1.</p>
</div>
</div><div class="proof definition admonition" id="definition-31">
<span id="index-16"></span><p class="admonition-title"><span class="caption-number">Definition 15 </span> (Moment centré d’ordre <span class="math notranslate nohighlight">\(k\)</span>)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle moment centré d’ordre <span class="math notranslate nohighlight">\(k\)</span> la quantité <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right]\)</span></p>
</div>
</div><p>Ainsi :</p>
<ul class="simple">
<li><p>pour une variable aléatoire discrète <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \displaystyle\sum_{x_i} (x_i-\mathbb{E}(X))^k P_X(x_i) = \displaystyle\sum_{x_i} (x_i-\mu_X)^k P_X(x_i)\)</span></p></li>
<li><p>pour une variable aléatoire continue <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \int_x (x-\mathbb{E}(X))^k f_X(x)dx = \int_x (x-\mu_X)^k f_X(x)dx\)</span></p></li>
</ul>
</div>
<div class="section" id="variance-d-une-variable-aleatoire">
<h5>Variance d’une variable aléatoire<a class="headerlink" href="#variance-d-une-variable-aleatoire" title="Permalink to this headline">#</a></h5>
<span class="target" id="index-17"></span><p id="index-18">Pour <span class="math notranslate nohighlight">\(k\)</span>=2, le moment centré d’ordre 2 est appelé la <strong>variance</strong> et est noté <span class="math notranslate nohighlight">\(\mathbb{V}(X)\)</span>. La racine carrée de la variance est <strong>l’écart type</strong> et est noté <span class="math notranslate nohighlight">\(\sigma_X\)</span>. On a donc <span class="math notranslate nohighlight">\(\sigma_X^2=\mathbb{V}(X)\)</span>.</p>
<div class="proof proposition admonition" id="proposition-32">
<p class="admonition-title"><span class="caption-number">Proposition 1 </span> (Formule de Koenig)</p>
<div class="proposition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\mathbb{V}(X) = \mathbb{E}(X^2)-\mu_X^2\)</span></p>
</div>
</div><p>En effet, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mu_X)^2 \right] = \mathbb{E}\left [(X^2-2\mu_XX+\mu_X^2 \right] = \mathbb{E}(X^2)-2\mu_X\mathbb{E}(X)+\mu_X^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_discrete</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">rv_discrete</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ecart-type : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variance :  61.0
Ecart-type :  7.810249675906654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_continuous</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">3.5</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mf">5.5</span> 
<span class="k">class</span> <span class="nc">distribution_gen</span><span class="p">(</span><span class="n">rv_continuous</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">()</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ecart-type : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variance:  0.029595913310970445
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ecart-type :  0.1720346282321395
</pre></div>
</div>
</div>
</div>
<div class="proof property admonition" id="property-33">
<p class="admonition-title"><span class="caption-number">Property 3 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall a,b\in\mathbb{R})\; \mathbb{V}(aX+b)= a^2\mathbb{V}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}\left [(X-a)^2\right ] = \mathbb V(X) +(\mathbb{E}(X)-a)^2\)</span> (théorème de Huygens)</p></li>
<li><p><span class="math notranslate nohighlight">\(\forall k&gt;0\; P(|X-\mathbb{E}(X)|\geq k\sigma_X)\leq \frac{1}{k^2}\)</span> (inégalité de Bienaymé-Tchebychev)</p></li>
</ul>
</div>
</div><p>On dira que la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> est <strong>réduite</strong> (ou <strong>normée</strong>) si <span class="math notranslate nohighlight">\(\mathbb{V}(X)=1\)</span>.</p>
</div>
<div class="section" id="moments-d-ordre-superieur">
<h5>Moments d’ordre supérieur<a class="headerlink" href="#moments-d-ordre-superieur" title="Permalink to this headline">#</a></h5>
<span class="target" id="index-19"></span><p id="index-20">On considère également souvent les moments d’ordre 3 (coefficient d’asymétrie ou skewness) et 4 (coefficient d’applatissement ou kurtosis).</p>
</div>
</div>
</div>
</div>
<span id="document-elemstats"></span><div class="tex2jax_ignore mathjax_ignore section" id="elements-de-statistiques">
<h2>Elements de statistiques<a class="headerlink" href="#elements-de-statistiques" title="Permalink to this headline">#</a></h2>
<p>Dans l’expression “étude statistique”, il faut distinguer :</p>
<ol class="simple">
<li><p><strong>les données statistiques</strong> : suivant l’étude, plusieurs problèmes peuvent être posés :</p>
<ul class="simple">
<li><p>Recueil des données (brutes) avec notamment le problème des sondages</p></li>
<li><p>Nature des données avec éventuellement la transformation des données brutes, notamment pour les séries chronologiques (série corrigée des variations saisonnières)</p></li>
<li><p>Organisation des données : il s’agit le plus souvent de résumer l’information par les techniques de la statistique descriptive</p></li>
</ul>
</li>
<li><p><strong>le modèle mathématique</strong> : une analyse du phénomène étudié doit permettre de traduire les problèmes posés par l’étude dans un langage formel, celui des probabilités. Après avoir fait des choix, des hypothèses sur la loi de probabilité et sur les paramètres de cette loi, on s’efforce de se placer dans un modèle statistique dans lequel des outils théoriques permettent de résoudre un certain nombre de problèmes théoriques. Dans ce modèle théorique, il s’agit de donner une interprétation aux données expérimentales et, souvent, des hypothèses implificatrices de “même loi” et d’indépendance sont faites.</p></li>
<li><p><strong>l’analyse statistique</strong> : l’utilisation d’outils statistiques adaptés au modèle retenu permet de faire l’interface entre les données statistiques et le modèle théorique choisi pour décrire le phénomène étudié.</p></li>
</ol>
<p>L’étude statistique peut alors se traduire sous diverses formes :</p>
<ul class="simple">
<li><p>préciser le modèle choisi, en estimant les paramètres intervenant dans celui-ci</p></li>
<li><p>juger la validité d’hypothèses faites sur ces paramètres qui se traduira non pas en ‘’confirmation d’hypothèses’’, mais en ‘’détecteur d’hypothèses fausses’’</p></li>
<li><p>juger l’adéquation du modèle retenu en termes de lois de probabilité avec la même réserve que ci-dessus</p></li>
</ul>
<p>Les résultats théoriques devront être interprétés dans le contexte de l’étude en considérant que ces résultats ont été obtenus dans le cadre d’un modèle théorique précis, d’où la nécessité d’une analyse correcte et d’une bonne formalisation. De plus, il faudra prendre en compte les techniques utilisées, qui ne permettent de répondre qu’à des questions précises. Enfin, dans le cas d’une application pratique, il faudra garder à l’esprit que les conclusions auront des conséquences économiques (ou autres).</p>
<div class="section" id="echantillon-d-une-variable-aleatoire">
<h3>Echantillon d’une variable aléatoire<a class="headerlink" href="#echantillon-d-une-variable-aleatoire" title="Permalink to this headline">#</a></h3>
<div class="section" id="definition">
<h4>Définition<a class="headerlink" href="#definition" title="Permalink to this headline">#</a></h4>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 16 </span> (Echantillon)</p>
<div class="definition-content section" id="proof-content">
<p>Soit une variable aléatoire <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \mathbb{R}\)</span>. On appelle <span class="math notranslate nohighlight">\(n\)</span>-échantillon de la variable aléatoire parente <span class="math notranslate nohighlight">\(X\)</span> la donnée de <span class="math notranslate nohighlight">\(n\)</span> variables aléatoires <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span>, définies sur le même espace, indépendantes, ayant même loi que <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
</div><p>On a donc pour tout <span class="math notranslate nohighlight">\((x_1\cdots x_n)^T\in\mathbb{R}^n\)</span></p>
<p><span class="math notranslate nohighlight">\(P(X_1&lt;x_1\cdots X_n&lt;x_n)=P(X_1&lt;x_1)\cdots P(X_n&lt;x_n)=P(X&lt;x_1)\cdots P(X&lt;x_n)\)</span></p>
<p>On considère alors une expérience aléatoire <span class="math notranslate nohighlight">\(\mathcal E\)</span> décrite par l’intermédiaire de la variable aléatoire <span class="math notranslate nohighlight">\(X\)</span>. Considérer un <span class="math notranslate nohighlight">\(n\)</span> échantillon de <span class="math notranslate nohighlight">\(X\)</span> consiste à supposer la possibilité de <span class="math notranslate nohighlight">\(n\)</span> répétitions de l’expérience <span class="math notranslate nohighlight">\(\mathcal E\)</span> dans des conditions identiques, sans interactions entre elles.</p>
<p>Chaque répétition conduit à l’observation d’une valeur prise par <span class="math notranslate nohighlight">\(X\)</span>, d’où l’observation de <span class="math notranslate nohighlight">\(n\)</span> valeurs <span class="math notranslate nohighlight">\(x_1\cdots x_n\)</span> à la suite des <span class="math notranslate nohighlight">\(n\)</span> répétitions, considérées comme une valeur effectivement prise par le <span class="math notranslate nohighlight">\(n\)</span>-échantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> de <span class="math notranslate nohighlight">\(X\)</span>. Les valeurs <span class="math notranslate nohighlight">\((x_1\cdots x_n)\)</span>  relèvent de l’observation : ce sont les données statistiques recueillies à la suite des <span class="math notranslate nohighlight">\(n\)</span> expériences : elles sont appelées réalisation du <span class="math notranslate nohighlight">\(n\)</span>-échantillon.</p>
<p>A noter que les hypothèses de même loi et d’indépendance sont simplificatrices.</p>
</div>
<div class="section" id="schema-de-bernoulli-et-modele-binomial">
<h4>Schéma de Bernoulli et modèle binomial<a class="headerlink" href="#schema-de-bernoulli-et-modele-binomial" title="Permalink to this headline">#</a></h4>
<p>Si <span class="math notranslate nohighlight">\(\mathcal E\)</span> n’a que deux éventualités possibles (réalisation ou non d’un évènement <span class="math notranslate nohighlight">\(A\)</span>), alors l’expérience peut être décrite par l’intermédiaire d’une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(\mathbb{1}_A\)</span>, fonction indicatrice de <span class="math notranslate nohighlight">\(A\)</span>), de Bernoulli <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \{0,1\}\)</span> avec <span class="math notranslate nohighlight">\(P(X=1)=P(A)=p\in]0,1[\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(\mathcal E\)</span> est répétée <span class="math notranslate nohighlight">\(n\)</span> fois dans des conditions identiques, sans interaction entre elles, on considère un <span class="math notranslate nohighlight">\(n\)</span>-échantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> de variable aléatoire parente <span class="math notranslate nohighlight">\(X\)</span>. Les valeurs prises par la variable aléatoire <span class="math notranslate nohighlight">\(S_n=X_1+\cdots X_n\)</span> représentent le nombre de réalisations de <span class="math notranslate nohighlight">\(A\)</span> à la suite des <span class="math notranslate nohighlight">\(n\)</span> répétitions. Une telle situation est dite relever du schéma de Bernoulli.</p>
<div class="proof property admonition" id="property-1">
<p class="admonition-title"><span class="caption-number">Property 4 </span></p>
<div class="property-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(S_n:(\Omega,\mathcal A,P)\mapsto [\![0,n]\!]\)</span> a une loi binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\forall k\in[\![0,n]\!]\; P(S_n=k)=\begin{pmatrix}n\\k\end{pmatrix} p^k (1-p)^{n-k}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(S_n)=np,\; \mathbb{V}(S_n)=np(1-p)\)</span></p></li>
</ul>
</div>
</div><p>En effet, d’après l’indépendance pour toute suite (<span class="math notranslate nohighlight">\(\delta_1\cdots \delta_n\)</span>) avec pour tout <span class="math notranslate nohighlight">\(k\in[\![1,n]\!]\)</span> <span class="math notranslate nohighlight">\(\delta_k\in\{0,1\}\)</span>, on a :</p>
<p><span class="math notranslate nohighlight">\(P(X_1=\delta_1\cdots X_n=\delta_n) = \displaystyle\prod_{k=1}^n P(X_k=\delta_k) = \displaystyle\prod_{k=1}^n P(X=\delta_k)=p^{s_n}(1-p)^{(n-s_n)}\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\delta_1+\cdots+ \delta_n=s_n\)</span> , les variables aléatoires ayant même loi de Bernoulli que <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Le nombre de solutions de <span class="math notranslate nohighlight">\(\delta_1+\cdots+ \delta_n=s_n\)</span> avec <span class="math notranslate nohighlight">\(s_n\in[\![0,n]\!]\)</span> et <span class="math notranslate nohighlight">\(\delta_k\in\{0,1\}\)</span> est <span class="math notranslate nohighlight">\(\begin{pmatrix}s_n\\n\end{pmatrix}\)</span>, d’où le résultat.</p>
<p>D’après la linéarité de l’espérance et l’égalité de Bienaymé, on a de plus
<span class="math notranslate nohighlight">\(\mathbb{E}(S_n) = \displaystyle\sum_{k=1}^n \mathbb{E}(X_k)=n\mathbb{E}(X)=np\quad \mathbb{V}(S_n)=\displaystyle\sum_{k=1}^n \mathbb{V}(X_k)=n\mathbb{V}(X)=np(1-p)\)</span></p>
</div>
<div class="section" id="moyenne-et-variances-empiriques-d-un-n-echantillon">
<h4>Moyenne et variances empiriques d’un <span class="math notranslate nohighlight">\(n\)</span>-échantillon<a class="headerlink" href="#moyenne-et-variances-empiriques-d-un-n-echantillon" title="Permalink to this headline">#</a></h4>
<p>Etant donné un <span class="math notranslate nohighlight">\(n\)</span>-échantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> d’une variable aléatoire parente <span class="math notranslate nohighlight">\(X\)</span>, on appelle :</p>
<span class="target" id="index-0"></span><ul id="index-1">
<li><p>moyenne empirique du <span class="math notranslate nohighlight">\(n\)</span>-échantillon la variable aléatoire</p></li>
<li><div class="math notranslate nohighlight">
\[\bar{X}_n=\frac1n \displaystyle\sum_{k=1}^n X_k\]</div>
</li>
<li><p>variance empirique biaisée du <span class="math notranslate nohighlight">\(n\)</span>-échantillon la variable aléatoire (Ne pas confondre avec la variable <span class="math notranslate nohighlight">\(S_n\)</span> du schéma de Bernoulli)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[S_n^2=\frac1n \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2=\frac1n \displaystyle\sum_{k=1}^n X_k^2 -\bar{X}_n^2\]</div>
<ul class="simple">
<li><p>variance empirique non biaisée du <span class="math notranslate nohighlight">\(n\)</span>-échantillon la variable aléatoire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[{S'}_n^2=\frac{1}{n-1} \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2\]</div>
<p>On a bien sûr <span class="math notranslate nohighlight">\((n-1){S'}_n^2=nS_n^2\)</span>.</p>
<p>Les valeurs prises par <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> coïncident avec la moyenne expérimentale <span class="math notranslate nohighlight">\(\bar{x}_n\)</span> des données expérimentales <span class="math notranslate nohighlight">\((x_1\cdots x_n)\)</span>, réalisation du <span class="math notranslate nohighlight">\(n\)</span>-échantillon. De même pour <span class="math notranslate nohighlight">\(S_n^2\)</span> pour la variance expérimentale.</p>
<div class="proof property admonition" id="property-2">
<p class="admonition-title"><span class="caption-number">Property 5 </span></p>
<div class="property-content section" id="proof-content">
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(\bar{X}_n)= \mathbb{E}(X)=m\; ;\; \mathbb{V}(\bar{X}_n) = \frac{\mathbb{V}(X)}{n}=\frac{\sigma^2}{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(S_n^2) = \frac{n-1}{n}\sigma^2\; ;\;  \mathbb{E}({S'}_n^2)=\sigma^2\)</span></p></li>
<li><p>Sous l’hypothèse de normalité, <span class="math notranslate nohighlight">\(\mathbb{V}({S'}_n^2)=\frac{2\sigma^4}{n-1}\)</span></p></li>
</ol>
</div>
</div><p>En effet :</p>
<ol class="simple">
<li><p>Immédiat d’après la linéarité de l’espérance, l’égalité de Bienaymé et la propriété <span class="math notranslate nohighlight">\(\mathbb{V}(\alpha X)=\alpha^2\mathbb{V}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((n-1){S'}_n^2=\displaystyle\sum_{k=1}^n X_k^2-n\bar{X_n^2}\)</span> d’où</p></li>
</ol>
<p><span class="math notranslate nohighlight">\((n-1)\mathbb{E}({S'}_n^2)=\displaystyle\sum_{k=1}^n\mathbb{E}(X_k^2)-n\mathbb{E}(\bar{X_n^2})=n(\sigma^2+m^2)-n\left (\frac{\sigma^2}{n}+m^2 \right )\)</span>
et le résultat.</p>
<p>Le dernier point est admis.</p>
</div>
<div class="section" id="echantillons-de-variables-aleatoires-normales">
<h4>Echantillons de variables aléatoires normales<a class="headerlink" href="#echantillons-de-variables-aleatoires-normales" title="Permalink to this headline">#</a></h4>
<p>Les lois de probabilité usuelles sont rappelées en fin de ce chapitre ({ref}`loisusuelles).</p>
<div class="section" id="etude-d-un-n-echantillon">
<h5>Etude d’un <span class="math notranslate nohighlight">\(n\)</span>-échantillon<a class="headerlink" href="#etude-d-un-n-echantillon" title="Permalink to this headline">#</a></h5>
<p>Soit un <span class="math notranslate nohighlight">\(n\)</span>-échantillon <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> de variable aléatoire parente <span class="math notranslate nohighlight">\(X\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>. On a les résultats suivants :</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\sqrt{n} \frac{\bar{X}_n-m}{\sigma}\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{nS_n^2}{\sigma^2} = \frac{(n-1)S'^2_n}{\sigma^2}\)</span> suit une loi <span class="math notranslate nohighlight">\(\chi^2_{n-1}\)</span></p></li>
<li><p>les variables aléatoires <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> et <span class="math notranslate nohighlight">\(S_n^2\)</span> sont indépendantes</p></li>
<li><p><span class="math notranslate nohighlight">\(T=\sqrt{n}\frac{\bar{X}_n-m}{S'_n}=\sqrt{n-1}\frac{\bar{X}_n-m}{S_n}\)</span> suit une loi de Student à <span class="math notranslate nohighlight">\(n-1\)</span> degrés de liberté.</p></li>
</ol>
</div>
<div class="section" id="etude-de-deux-echantillons-independants">
<h5>Etude de deux échantillons indépendants<a class="headerlink" href="#etude-de-deux-echantillons-independants" title="Permalink to this headline">#</a></h5>
<p>Soient un <span class="math notranslate nohighlight">\(n\)</span>-échantillon <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> de <span class="math notranslate nohighlight">\(X\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_1,\sigma_1)\)</span>, un <span class="math notranslate nohighlight">\(m\)</span>-échantillon <span class="math notranslate nohighlight">\(Y_1\cdots Y_m\)</span> de <span class="math notranslate nohighlight">\(Y\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_2,\sigma_2)\)</span>, les échantillons étant indépendants. Avec des notations évidentes, on a les résultats suivants :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F = \frac{\sigma_2^2 S'^2_n(X)}{\sigma_1^2 S'^2_m(Y)} = \frac{(m-1)n}{(n-1)m}\frac{\sigma_2^2S_n^2(X)}{\sigma_1^2S_m^2(Y)}\)</span> admet une loi de Fisher-Snédécor FS(<span class="math notranslate nohighlight">\(n-1\)</span>,<span class="math notranslate nohighlight">\(m-1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(T = \sqrt{\frac{(n+m-2)mn}{m+n}}\frac{(\bar{X}_n-\bar{Y}_m)-(m_1-m_2)}{\sqrt{nS_n^2(X)+mS_m^2(Y)}}\)</span> admet, sous l’hypothèse <span class="math notranslate nohighlight">\(\sigma_1=\sigma_2\)</span>, une loi de Student à <span class="math notranslate nohighlight">\((n+m-2)\)</span> degrés de liberté.</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 5 </span></p>
<div class="remark-content section" id="proof-content">
<p>Sous l’hypothèse <span class="math notranslate nohighlight">\(\sigma_1=\sigma_2=\sigma\)</span> :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{X}_n-\bar{Y}_m\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_1-m_2,\sigma\sqrt{\frac1n+\frac1m})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{nS_n^2(X)}{\sigma^2}+\frac{mS_m^2(Y)}{\sigma^2}\)</span> a une loi <span class="math notranslate nohighlight">\(\chi^2_{n-1+m-1}\)</span>.</p></li>
</ul>
</div>
</div></div>
</div>
</div>
<div class="section" id="loi-des-grands-nombres">
<h3>Loi des grands nombres<a class="headerlink" href="#loi-des-grands-nombres" title="Permalink to this headline">#</a></h3>
<div class="section" id="inegalite-de-tchebychev">
<h4>Inégalité de Tchebychev<a class="headerlink" href="#inegalite-de-tchebychev" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-4">
<p class="admonition-title"><span class="caption-number">Theorem 2 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> de moyenne <span class="math notranslate nohighlight">\(m\)</span> et d’écart-type <span class="math notranslate nohighlight">\(\sigma\)</span>. Alors :</p>
<p><span class="math notranslate nohighlight">\((\forall t&gt;0)\; P(|X-m|\geq t)\leq \frac{\sigma^2}{t^2}\quad\textrm{et}\quad (\forall u&gt;0)\; P(\frac{|X-m|}{\sigma}\geq u)\leq \frac{1}{u^2}\)</span></p>
</div>
</div><p>En effet :
Soit <span class="math notranslate nohighlight">\(A=\left \{|X-m|\geq t\right \}\)</span> et <span class="math notranslate nohighlight">\(\mathbb{1}_A(\omega)\)</span> = 1 si <span class="math notranslate nohighlight">\(\omega\in A\)</span>, 0 sinon. Alors :</p>
<p><span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; |X(\omega)-m|^2\geq |X(\omega)-m|^2\mathbb{1}_A(\omega) \geq t^2\mathbb{1}_A(\omega)\)</span></p>
<p>L’espérance étant croissante et vérifiant <span class="math notranslate nohighlight">\(\mathbb{E}(\mathbb{1}_A)=P(A)\)</span>, on a
<span class="math notranslate nohighlight">\(\sigma^2=\mathbb{E}(|X-m|^2)\geq t^2P(A) = t^2P(|X-m|\geq t)\)</span> et le résultat.</p>
<div class="proof remark dropdown admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 6 </span></p>
<div class="remark-content section" id="proof-content">
<p>Ces inégalités, souvent très grossières et d’intéret essentiellement théorique, n’ont d’utilité que pour <span class="math notranslate nohighlight">\(t&gt;\sigma\)</span> ou <span class="math notranslate nohighlight">\(u&gt;1\)</span> (une probabilité est toujours inférieure à 1). La seconde donne un majorant de la probabilité d’observer des valeurs prises par <span class="math notranslate nohighlight">\(X\)</span> à l’extérieur de l’intervalle <span class="math notranslate nohighlight">\([m-u\sigma,m+u\sigma]\)</span></p>
</div>
</div></div>
<div class="section" id="phenomene-de-regularite-statistique">
<h4>Phénomène de régularité statistique<a class="headerlink" href="#phenomene-de-regularite-statistique" title="Permalink to this headline">#</a></h4>
<p>Considérons plusieurs séquences de 100 lancers d’une pièce de monnaie et notons, pour chaque séquence, la suite <span class="math notranslate nohighlight">\((f_n)_{n\geq 1}\)</span> des fréquences des piles obtenus. Un exemple de simulation avec <span class="math notranslate nohighlight">\(p=0.4\)</span> est proposé dans la figure suivante avec le code ayant servi à la produire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span>  <span class="nn">random</span>  <span class="kn">import</span>  <span class="n">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">experience</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">0.4</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span><span class="n">p</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">+=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">f</span><span class="o">+=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">nb_sequences</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_sequences</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">experience</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$n$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f_n$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/elemstats_1_0.png" src="_images/elemstats_1_0.png" />
</div>
</div>
<p>La fluctuation de la fréquence est importante pour des petites valeurs de <span class="math notranslate nohighlight">\(n\)</span>, puis elle s’atténue, pour se stabiliser autour d’une valeur voisine de <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Cette constatation expérimentale conduit aux remarques suivantes, qui sont précisées dans la suite dans le cadre théorique :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_n\)</span> donne une idée de la valeur de <span class="math notranslate nohighlight">\(p\)</span> avec une plus ou moins grande précision</p></li>
<li><p>la probabilité apparaît comme une fréquence limite.</p></li>
</ul>
</div>
<div class="section" id="loi-faible-des-grands-nombres">
<h4>Loi faible des grands nombres<a class="headerlink" href="#loi-faible-des-grands-nombres" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 3 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> une suite de variables aléatoires indépendantes, identiquement distribuées (i.i.d) de même loi qu’une variable <span class="math notranslate nohighlight">\(X\)</span>, admettant une moyenne <span class="math notranslate nohighlight">\(m\)</span> et un écart-type <span class="math notranslate nohighlight">\(\sigma\)</span>. Si <span class="math notranslate nohighlight">\((\bar{X}_n)_{n\geq 1}\)</span> est la suite des moyennes empiriques associée à <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> alors</p>
<p><span class="math notranslate nohighlight">\((\forall t&gt;0)\; \displaystyle\lim_{n\rightarrow\infty} P(|\bar{X}_n-m|\geq t) = 0\)</span></p>
<p>On dit que la suite converge en probatilité vers <span class="math notranslate nohighlight">\(m\)</span> et on note <span class="math notranslate nohighlight">\(\bar{X}_n\xrightarrow[n\rightarrow\infty]{P} m\)</span></p>
</div>
</div><p>C’est une conséquence immédiate de l’inégalité de Tchebychev : <span class="math notranslate nohighlight">\(P(|\bar{X}_n-m|\geq t)\leq\frac{\sigma^2}{nt^2}\)</span> puisque <span class="math notranslate nohighlight">\(\mathbb{V}(\bar{X}_n)=\frac{\sigma^2}{n}\)</span></p>
<p>L’observation des valeurs prises par la moyenne empirique donne une bonne information sur la moyenne théorique <span class="math notranslate nohighlight">\(m\)</span> de <span class="math notranslate nohighlight">\(X\)</span>. La précision, au sens ci-dessus, est d’autant meilleure que <span class="math notranslate nohighlight">\(n\)</span> est grand.</p>
</div>
<div class="section" id="loi-forte-des-grands-nombres">
<h4>Loi forte des grands nombres<a class="headerlink" href="#loi-forte-des-grands-nombres" title="Permalink to this headline">#</a></h4>
<p>avec les hypothèses précédentes, on peut montrer que</p>
<p><span class="math notranslate nohighlight">\(P(\{\omega\in\Omega, \displaystyle\lim_{n\rightarrow\infty} \bar{X}_n(\omega)=m\})=1\)</span></p>
<p>Sauf cas très improbable (avec probabilité nulle), la suite des réalisations <span class="math notranslate nohighlight">\((\bar{x}_n)_{n\geq 1}\)</span> des moyennes expérimentales des mesures converge vers la moyenne théorique <span class="math notranslate nohighlight">\(m\)</span>. On dit que la suite <span class="math notranslate nohighlight">\((\bar{X}_n)_{n\geq 1}\)</span> converge presque sûrement vers <span class="math notranslate nohighlight">\(m\)</span> et on note <span class="math notranslate nohighlight">\(\bar{X}_n\xrightarrow[n\rightarrow\infty]{p.s.} m\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 7 </span></p>
<div class="remark-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X=\mathbb{1}_A\)</span> alors <span class="math notranslate nohighlight">\(m=p=P(A)\)</span> et la probabilité de l’évènement <span class="math notranslate nohighlight">\(A\)</span> apparaît comme une fréquence limite.</p>
</div>
</div></div>
</div>
<div class="section" id="approximation-de-mathcal-b-n-p-par-la-loi-de-poisson-mathcal-p-lambda">
<h3>Approximation de <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> par la loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(\lambda)\)</span><a class="headerlink" href="#approximation-de-mathcal-b-n-p-par-la-loi-de-poisson-mathcal-p-lambda" title="Permalink to this headline">#</a></h3>
<div class="section" id="theoreme-d-analyse">
<h4>Théorème d’analyse<a class="headerlink" href="#theoreme-d-analyse" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-8">
<p class="admonition-title"><span class="caption-number">Theorem 4 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(p\)</span> est une fonction de <span class="math notranslate nohighlight">\(n\)</span> telle que <span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty}np(n)=\lambda&gt;0\)</span>, alors pour tout <span class="math notranslate nohighlight">\(k\geq 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty}\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k} = e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p>
</div>
</div><p>En effet</p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{n(n-1)\cdots (n-k+1)}{k!}p^k(1-p)^{n-k}\)</span></p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{(np)^k}{k!}\displaystyle\prod_{j=0}^k\left (1-\frac{j}{n}\right )(1-p)^{n-k}\)</span></p>
<p>et le résultat est démontré en remarquant que <span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty} p(n)=0\)</span>.</p>
</div>
<div class="section" id="application">
<h4>Application<a class="headerlink" href="#application" title="Permalink to this headline">#</a></h4>
<p>Soit <span class="math notranslate nohighlight">\(S_n\)</span> une variable aléatoire de loi <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span>. Lorsque <span class="math notranslate nohighlight">\(n\)</span> est grand (&gt;50) et <span class="math notranslate nohighlight">\(p\)</span> petite (<span class="math notranslate nohighlight">\(np\)</span>&lt;10), on peut approcher la loi de <span class="math notranslate nohighlight">\(S_n\)</span> par une loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(np)\)</span>. On lit alors la valeur correspondante dans la table de la loi de Poisson, pour tout <span class="math notranslate nohighlight">\(k\in[\![0,n]\!]\)</span>
<span class="math notranslate nohighlight">\(P(S_n=k)\approx e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p>
<p>De plus, en remarquant que <span class="math notranslate nohighlight">\(\Sigma_n=n-S_n\)</span> suit <span class="math notranslate nohighlight">\(\mathcal{B}(n,1-p)\)</span>, on a</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(\Sigma_n=k)=P(S_n=n-k)=\begin{pmatrix}n\\p\end{pmatrix} p^{n-k}(1-p)^{k} \end{split}\]</div>
<p>et quand <span class="math notranslate nohighlight">\(n\)</span> est grand (&gt;50) et <span class="math notranslate nohighlight">\(p\)</span> voisin de 1 (<span class="math notranslate nohighlight">\(n(1-p)&lt;10\)</span>) on peut approcher la loi de <span class="math notranslate nohighlight">\(\Sigma_n\)</span> par une loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(n(1-p))\)</span>.</p>
</div>
</div>
<div class="section" id="theoreme-central-limite">
<h3>Théorème central limite<a class="headerlink" href="#theoreme-central-limite" title="Permalink to this headline">#</a></h3>
<div class="section" id="le-t-c-l">
<h4>Le T.C.L.<a class="headerlink" href="#le-t-c-l" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-9">
<p class="admonition-title"><span class="caption-number">Theorem 5 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit une suite <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> de variables aléatoires, i.i.d. de même loi qu’une variable parente <span class="math notranslate nohighlight">\(X\)</span>, définies sur le même espace <span class="math notranslate nohighlight">\((\Omega,\mathcal A,P)\)</span>. On considère la suite des moyennes empiriques <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> des <span class="math notranslate nohighlight">\(n\)</span>-échantillons <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(X\)</span> admet une moyenne <span class="math notranslate nohighlight">\(m\)</span> et un écart-type <span class="math notranslate nohighlight">\(\sigma\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\((\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\sqrt{n}\frac{\bar X_n-m}{\sigma} &lt;x\right) = \phi(x)\)</span>
où <span class="math notranslate nohighlight">\( \phi(x)\)</span> est la fonction de répartition de la loi normale centrée réduite <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
<p>On dit que <span class="math notranslate nohighlight">\(\left (\sqrt{n}\frac{\bar X_n-m}{\sigma}\right )_{n\geq 1}\)</span> converge en loi vers <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
</div>
</div><p>La figure suivante illustre ce modèle dans le cas où la variable aléatoire parente <span class="math notranslate nohighlight">\(X\)</span> suit un schéma de Bernoulli avec <span class="math notranslate nohighlight">\(P(X = 1)=0.1, P(X=0)=0.9\)</span>.</p>
<p><img alt="" src="_images/tcl.png" /></p>
</div>
<div class="section" id="commentaires">
<h4>Commentaires<a class="headerlink" href="#commentaires" title="Permalink to this headline">#</a></h4>
<p>Pour mesurer une grandeur de valeur inconnue <span class="math notranslate nohighlight">\(m\)</span>, il suffit d’une seule mesure lorsqu’il n’y a pas d’erreur expérimentale. Mais les mesures sont toujours entâchées d’erreur et une expérience ou mesure peut être modélisée par une variable aléatoire <span class="math notranslate nohighlight">\(X\)</span> dnot la moyenne théorique <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> est la valeur cherchée <span class="math notranslate nohighlight">\(m\)</span> si les mesures ne sont pas biaisées, c’est-à-dire affectées d’une erreur systématique.</p>
<p>Ayant effectué <span class="math notranslate nohighlight">\(n\)</span> mesures, on a une réalisation d’un <span class="math notranslate nohighlight">\(n\)</span>-échantillon de <span class="math notranslate nohighlight">\(X\)</span> et une valeur observée <span class="math notranslate nohighlight">\(\bar x_n\)</span> de la moyenne empirique <span class="math notranslate nohighlight">\(\bar X_n\)</span>. On peut prendre cette valeur comme estimation de <span class="math notranslate nohighlight">\(m\)</span>, l’écart <span class="math notranslate nohighlight">\(|\bar x_n-m|\)</span> étant une réalisation de <span class="math notranslate nohighlight">\(|\bar X_n-m|\)</span>.</p>
<ul class="simple">
<li><p>La loi forte des grands nombres justifie cette estimation en supposant  <span class="math notranslate nohighlight">\(\mathbb{E}(X)=m\)</span></p></li>
<li><p>L’inégalité de Tchebychev donne une idée grossière de l’écart en terme de probabilité</p></li>
<li><p>le théorème central limite donne une évaluation asymptotique de cet écart aléatoire</p></li>
</ul>
<p>Dans la pratique, pour <span class="math notranslate nohighlight">\(n\)</span> grand, dans le cadre de ce théorème, on a l’approximation suivante :</p>
<p><span class="math notranslate nohighlight">\((\forall a&lt;b)\;\;\;\; P\left (a\sqrt{n}\frac{\bar X_n-m}{\sigma} &lt;b\right)\approx \phi(b)-\phi(a)\)</span></p>
</div>
<div class="section" id="cas-particulier-theoreme-de-moivre-laplace">
<h4>Cas particulier : théorème de Moivre-Laplace<a class="headerlink" href="#cas-particulier-theoreme-de-moivre-laplace" title="Permalink to this headline">#</a></h4>
<div class="proof theorem admonition" id="theorem-10">
<p class="admonition-title"><span class="caption-number">Theorem 6 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(X=\mathbb{1}_A\)</span>  une variable aléatoire de Bernoulli avec <span class="math notranslate nohighlight">\(P(A)=p\)</span>. Dans les conditions du théorème central limite la variable <span class="math notranslate nohighlight">\(S_n=\displaystyle\sum_{k=1}^n X_k=n\bar X_n\)</span> suit une loi binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> et</p>
<p><span class="math notranslate nohighlight">\( (\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\frac{S_n-np}{\sqrt{np(1-p)}} &lt;x\right) = \phi(x)\)</span></p>
</div>
</div><p>On peut donc approcher une loi binomiale par une loi normale.</p>
</div>
</div>
<div class="section" id="modeles-probabilistes-usuels">
<span id="loisusuelles"></span><h3>Modèles probabilistes usuels<a class="headerlink" href="#modeles-probabilistes-usuels" title="Permalink to this headline">#</a></h3>
<p>On donne ici un catalogue non exhaustif des principaux modèles probabilistes, et leurs principales propriétés. Une illustration graphique des lois correspondantes est proposée dans les figures suivantes.</p>
<div class="section" id="lois-discretes">
<h4>Lois discrètes<a class="headerlink" href="#lois-discretes" title="Permalink to this headline">#</a></h4>
<p>On considère une variable aléatoire <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \mathcal D\)</span></p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Modèle</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathcal D}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{P(X=k)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{E}(X)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{V}(X)}\)</span></p></th>
<th class="head"><p>Utilisation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bernoulli</p></td>
<td><p><span class="math notranslate nohighlight">\(\{0,1\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X=1)=p,P(X=0)=1-p=q\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(pq\)</span></p></td>
<td><p>Expérience ayant 2 éventualités possibles</p></td>
</tr>
<tr class="row-odd"><td><p>Binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p) \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([\![0,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\k\end{pmatrix}p^k q^{n-k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(np\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(npq\)</span></p></td>
<td><p>Tirage avec remise</p></td>
</tr>
<tr class="row-even"><td><p>Hypergéométrique, <span class="math notranslate nohighlight">\(\mathcal{H}(m,N,n), m&lt;N\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([\![0,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\begin{pmatrix}m\\k\end{pmatrix}\begin{pmatrix}N-m\\n-k\end{pmatrix}}{\begin{pmatrix}N\\n\end{pmatrix}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\frac{m}{M}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{N-n}{N-1}n\frac{m}{N}\frac{N-m}{N}\)</span></p></td>
<td><p>Tirage sans remise</p></td>
</tr>
<tr class="row-odd"><td><p>Uniforme</p></td>
<td><p><span class="math notranslate nohighlight">\([\![1,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac1n\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{n+1}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{n2-1}{12}\)</span></p></td>
<td><p>Equiprobabilité des résultats</p></td>
</tr>
<tr class="row-even"><td><p>Poisson <span class="math notranslate nohighlight">\(\mathcal{P}(\lambda), \lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{N}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p>Files d’attente, Evènements rares</p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/discretes.png" /></p>
<div class="section" id="modele-de-bernoulli">
<h5>Modèle de Bernoulli<a class="headerlink" href="#modele-de-bernoulli" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">bernoulli</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1">#nombre de répétitions de l&#39;expérience</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># probabilité de succès</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.3
Variance:  0.21
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-binomiale">
<h5>Loi binomiale<a class="headerlink" href="#loi-binomiale" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">x</span> <span class="o">=</span> <span class="mi">7</span> 
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance:  1.6
Densité de probabilité :  0.000786432
Fonction de répartition :  0.9999220736
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-hypergeometrique">
<h5>Loi hypergéométrique<a class="headerlink" href="#loi-hypergeometrique" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hypergeom</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> 
<span class="n">M</span> <span class="o">=</span> <span class="mi">15</span> 
<span class="n">m</span> <span class="o">=</span> <span class="mi">9</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  3.0
Variance:  0.8571428571428571
Densité de probabilité :  0.23976023976023975
Fonction de répartition :  0.28671328671328666
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="distribution-de-poisson">
<h5>Distribution de Poisson<a class="headerlink" href="#distribution-de-poisson" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">poisson</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">Lambda</span> <span class="o">=</span> <span class="mi">2</span><span class="o">/</span><span class="mi">3</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.6666666666666666
Variance:  0.6666666666666666
Densité de probabilité :  0.3422780793550613
Fonction de répartition :  0.8556951983876534
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="lois-absolument-continues">
<h4>Lois absolument continues<a class="headerlink" href="#lois-absolument-continues" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Modèle</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathcal D}\)</span></p></th>
<th class="head"><p>Densité</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{E}(X)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{V}(X)}\)</span></p></th>
<th class="head"><p>Utilisation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Uniforme</p></td>
<td><p><span class="math notranslate nohighlight">\([a,b]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{b-a}\mathbb{1}_{]a,b[}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{b+a}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{(b-a)^2}{12}\)</span></p></td>
<td><p>Pas d’a priori sur la distribution</p></td>
</tr>
<tr class="row-odd"><td><p>Exponentiel <span class="math notranslate nohighlight">\(Exp(\lambda)\)</span><span class="math notranslate nohighlight">\(\lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}^+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x) =\lambda e^{-\lambda x} \mathbb{1}_{x&gt;0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\lambda^2}\)</span></p></td>
<td><p>Files d’attente, Durée de vie sans usure</p></td>
</tr>
<tr class="row-even"><td><p>Pareto  <span class="math notranslate nohighlight">\(\alpha&gt;1,x_0&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([x_0,+\infty[\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{\alpha-1}{x_0}\left (\frac{x_0}{x} \right )^\alpha \mathbb{1}_{x\geq x_0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\alpha-1}{\alpha-2}x_0\)</span> <span class="math notranslate nohighlight">\(\alpha&gt;2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{(\alpha-1)x_0^2}{(\alpha-3)(\alpha-2)^2}\)</span> <span class="math notranslate nohighlight">\(\alpha&gt;3\)</span></p></td>
<td><p>Revenu des ménages</p></td>
</tr>
<tr class="row-odd"><td><p>Normale <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-m)^2}{2\sigma^2}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(m\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2\)</span></p></td>
<td><p>voir T.C.L.</p></td>
</tr>
<tr class="row-even"><td><p>Gamma <span class="math notranslate nohighlight">\(\gamma(a,\lambda)\)</span><span class="math notranslate nohighlight">\(a&gt;0,\lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((\mathbb{R}^+)^*\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x) = \frac{\lambda^a}{\Gamma(a)}e^{-\lambda x}x^{a-1}\mathbb{1}_{x&gt;0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{a}{\lambda}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{a}{\lambda^2}\)</span></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Khi-deux <span class="math notranslate nohighlight">\(\chi_n^2\)</span> <span class="math notranslate nohighlight">\(n\)</span> degrés liberté</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x, k)=\frac{1}{2^\frac{k}{2}\Gamma(\frac{k}{2})} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2n\)</span></p></td>
<td><p>Test du khi-deux</p></td>
</tr>
<tr class="row-even"><td><p>Student <span class="math notranslate nohighlight">\(n\)</span> degrés liberté</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{\sqrt{\pi n}}\frac{\Gamma((n+1)/2)}{\Gamma(n/2)} \left (1+\frac{t^2}{n} \right )^{-\frac{n+1}{2}}\)</span> t&gt;0</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Test égalité moyenne</p></td>
</tr>
<tr class="row-odd"><td><p>Fisher-Snédécor <span class="math notranslate nohighlight">\(n\)</span> et <span class="math notranslate nohighlight">\(m\)</span> degrés liberté</p></td>
<td><p><span class="math notranslate nohighlight">\((\mathbb{R}^+)^*\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}n^{\frac{n}{2}}m^{\frac{m}{2}}\frac{x^{\frac{n-2}{2}}}{(nx+m)^{\frac{n+m}{2}}}\)</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/continues.png" /></p>
<div class="section" id="modele-uniforme">
<h5>Modèle uniforme<a class="headerlink" href="#modele-uniforme" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Espérance: &quot;</span><span class="p">,</span> <span class="n">mean</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">var</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">uniform</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Espérance:  3.0
Variance:  1.3333333333333333
Densité de probabilité :  0.25
Fonction de répartition :  0.375
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-normale">
<h5>Loi normale<a class="headerlink" href="#loi-normale" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">1.3</span> 
<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span> 
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span> 

<span class="n">mean</span><span class="p">,</span><span class="n">var</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.0
Variance :  1.0
Densité de probabilité :  0.17136859204780736
Fonction de répartition :  0.9031995154143897
</pre></div>
</div>
</div>
</div>
<p>Sous l’hypothèse de normalité, de nombreux outils statistiques sont disponibles. Souvent, l’hypothèse de normalité est justifiée par l’intermédiaire du théorème centrale limite. Des considérations, parfois abusives, permettent de se placer dans le cadre d’utilisation de ce théorème et de choisir un modèle normal alors qu’une étude des données statistiques met en défaut le choix de ce modèle (problème dit d’adéquation).</p>
<div class="proof property admonition" id="property-11">
<p class="admonition-title"><span class="caption-number">Property 6 </span></p>
<div class="property-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> est une variable aléatoire de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span> alors la variable <span class="math notranslate nohighlight">\(Z=\frac{X-m}{\sigma}\)</span> est la variable centrée réduite associée, et suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> dite aussi loi de Gauss-Laplace.</p>
</div>
</div><p>La fonction de répartition de <span class="math notranslate nohighlight">\(Z\)</span> est <span class="math notranslate nohighlight">\(\phi(Z) = P(Z&lt;z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{-\frac{t^2}{2}}dt\)</span>, dont les valeurs peuvent être lues dans une table.</p>
<div class="proof theorem admonition" id="theorem-12">
<p class="admonition-title"><span class="caption-number">Theorem 7 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(X_1\)</span> et <span class="math notranslate nohighlight">\(X_2\)</span> deux variables aléatoires indépendantes, de loi respective <span class="math notranslate nohighlight">\(\mathcal{N}(m_1,\sigma_1)\)</span> et <span class="math notranslate nohighlight">\(\mathcal{N}(m_2,\sigma_2)\)</span>. Alors la variable aléatoire <span class="math notranslate nohighlight">\(X=\alpha_1X_1+\alpha_2X_2\)</span> admet une loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span> avec</p>
<p><span class="math notranslate nohighlight">\(m = \alpha_1 m_1+\alpha_2 m_2\quad \textrm{et}\quad \sigma_2^2 = \alpha_1^2 \sigma_1+\alpha_2^2 \sigma_2^2\)</span></p>
<p>En particulier, étant données <span class="math notranslate nohighlight">\(n\)</span> variables aléatoires <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> i.i.d. de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>, alors la variable aléatoire <span class="math notranslate nohighlight">\(\bar X_n = \frac1n \displaystyle\sum_{k=1}^nX_k\)</span> suit une loi normale <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma/\sqrt{n})\)</span>.</p>
</div>
</div><div class="proof remark admonition" id="remark-13">
<p class="admonition-title"><span class="caption-number">Remark 8 </span></p>
<div class="remark-content section" id="proof-content">
<p>Dans ce cas, <span class="math notranslate nohighlight">\(\sqrt{n}\frac{\bar X_n-m}{\sigma}\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
</div>
</div></div>
<div class="section" id="loi-exponentielle">
<h5>Loi exponentielle<a class="headerlink" href="#loi-exponentielle" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">expon</span>

<span class="n">Lambda</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">mean</span><span class="p">,</span><span class="n">var</span> <span class="o">=</span> <span class="n">expon</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">Lambda</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Espérance : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">expon</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Espérance :  0.5
Variance :  0.25
Densité de probabilité :  0.2706705664732254
Fonction de répartition :  0.8646647167633873
</pre></div>
</div>
</div>
</div>
<p>On parle de loi de probabilité sans mémoire car elle vérifie :
<span class="math notranslate nohighlight">\( (\forall s,t\in(\mathbb{R}^+)^*\; P(X&gt;s+t |X&gt;t) = P(X&gt;s)\)</span></p>
</div>
<div class="section" id="distribution-gamma">
<h5>Distribution Gamma<a class="headerlink" href="#distribution-gamma" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> 
<span class="n">a</span> <span class="o">=</span> <span class="mi">3</span> 
<span class="n">Lambda</span> <span class="o">=</span> <span class="mf">1.8</span> 

<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>  <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span>  <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  1.6666666666666667
Variance :  0.925925925925926
Densité de probabilité :  0.11853315025792688
Fonction de répartition :  0.9052421318239862
</pre></div>
</div>
</div>
</div>
<p>Les propriétés de cette loi reposent sur celles de la fonction <span class="math notranslate nohighlight">\(\Gamma(a) = \int_0^{+\infty} x-{a-1}e^{-x}dx\)</span>, intégrale convergente pour tout <span class="math notranslate nohighlight">\(a&gt;0\)</span>.</p>
<div class="proof theorem admonition" id="theorem-14">
<p class="admonition-title"><span class="caption-number">Theorem 8 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont des variables aléatoires indépendantes de loi respective <span class="math notranslate nohighlight">\(\gamma(a,\lambda)\)</span> et <span class="math notranslate nohighlight">\(\gamma(b,\lambda)\)</span>, alors <span class="math notranslate nohighlight">\(X=X_1+X_2\)</span> est de loi <span class="math notranslate nohighlight">\(\gamma(a+b,\lambda)\)</span></p>
</div>
</div><div class="proof theorem admonition" id="theorem-15">
<p class="admonition-title"><span class="caption-number">Theorem 9 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> est de loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> alors la variable aléatoire <span class="math notranslate nohighlight">\(Y=X^2\)</span> admet une loi <span class="math notranslate nohighlight">\(\gamma(\frac12,\frac12)\)</span>.</p>
<p>Etant données plus généralement <span class="math notranslate nohighlight">\(n\)</span> variables aléatoires i.i.d. de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>, alors  la variable aléatoire <span class="math notranslate nohighlight">\(V=\displaystyle\sum_{k=1}^n \left (\frac{X_k-m}{\sigma}\right )^2\)</span> admet une loi <span class="math notranslate nohighlight">\(\gamma(\frac{n}{2},\frac12)\)</span>. C’est la loi du khi-deux à <span class="math notranslate nohighlight">\(n\)</span> degrés de liberté.</p>
</div>
</div></div>
<div class="section" id="loi-du-khi-deux">
<h5>Loi du Khi-deux<a class="headerlink" href="#loi-du-khi-deux" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">n</span><span class="o">=</span><span class="mi">2</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">chi2</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">chi2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">chi2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance :  4.0
Densité de probabilité :  0.11156508007421491
Fonction de répartition :  0.7768698398515702
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-de-student">
<h5>Loi de Student<a class="headerlink" href="#loi-de-student" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.0
Variance :  inf
Densité de probabilité :  0.027410122234342152
Fonction de répartition :  0.9522670168666454
</pre></div>
</div>
</div>
</div>
<p>L’utilisation pratique de cette loi est énoncée par le théorème suivant :</p>
<div class="proof theorem admonition" id="theorem-16">
<p class="admonition-title"><span class="caption-number">Theorem 10 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient deux variables aléatoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> indépendantes, de loi respective <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> et <span class="math notranslate nohighlight">\(\chi_n^2\)</span>. Alors la variable aléatoire <span class="math notranslate nohighlight">\(T=\frac{X}{\sqrt{Y/n}}\)</span> admet une loi de Student à <span class="math notranslate nohighlight">\(n\)</span> degrés de liberté.</p>
</div>
</div></div>
<div class="section" id="loi-de-fisher-snedecor">
<h5>Loi de Fisher-Snédécor<a class="headerlink" href="#loi-de-fisher-snedecor" title="Permalink to this headline">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">f</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">m</span><span class="o">=</span><span class="mi">4</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span><span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densité de probabilité : &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de répartition : &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance :  inf
Densité de probabilité :  0.06399999999999996
Fonction de répartition :  0.84
</pre></div>
</div>
</div>
</div>
<p>L’utilisation pratique de cette loi est énoncée par le théorème suivant :</p>
<div class="proof theorem admonition" id="theorem-17">
<p class="admonition-title"><span class="caption-number">Theorem 11 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient deux variables aléatoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> indépendantes, de loi respective <span class="math notranslate nohighlight">\(\chi_n^2\)</span> et <span class="math notranslate nohighlight">\(\chi_m^2\)</span>. Alors la variable aléatoire <span class="math notranslate nohighlight">\(T=\frac{X/n}{Y/m}\)</span> admet une loi de Fisher-Snédécor à <span class="math notranslate nohighlight">\(n\)</span> et <span class="math notranslate nohighlight">\(m\)</span> degrés de liberté.</p>
</div>
</div></div>
</div>
</div>
</div>
<span id="document-statsdescriptives"></span><div class="tex2jax_ignore mathjax_ignore section" id="statistique-descriptive">
<h2>Statistique descriptive<a class="headerlink" href="#statistique-descriptive" title="Permalink to this headline">#</a></h2>
<div class="section" id="definitions">
<h3>Définitions<a class="headerlink" href="#definitions" title="Permalink to this headline">#</a></h3>
<p>Dans la suite, nous nous intéressons à des unités statistiques ou individus statistiques ou unités d’observation (individus,  entreprises,  ménages, données abstraites…). Bien que le cas infini soit envisageable, nous nous restreignons ici à l’étude d’un nombre fini de ces unités. Un ou plusieurs caractères (ou variables) est mesuré sur chaque unité. Les variables sont désignées par simplicité par une lettre. Leurs valeurs possibles sont appelées modalités et l’ensemble des valeurs possibles ou des modalités est appelé le domaine. L’ensemble des individus statistiques forme la population.</p>
<div class="section" id="typologie-des-variables">
<h4>Typologie des variables<a class="headerlink" href="#typologie-des-variables" title="Permalink to this headline">#</a></h4>
<p>La typologie des variables définit le type de problème statistique que l’on doit aborder :</p>
<div class="proof definition admonition" id="definition-0">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 17 </span> (Variable qualitative)</p>
<div class="definition-content section" id="proof-content">
<p>La variable est dite qualitative lorsque les modalités sont des catégories. Suivant qu’il existe une relation d’ordre sur les catégories, on distingue :</p>
<ul class="simple">
<li><p>la variable qualitative nominale, si les modalités  ne peuvent pas être ordonnées</p></li>
<li><p>la variable qualitative ordinale, si les modalités peuvent être ordonnées</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 18 </span> (Variable quantitative)</p>
<div class="definition-content section" id="proof-content">
<p>La variable est dite quantitative lorsque les modalités sont des valeurs numériques (scalaires ou vectorielles) :</p>
<ul class="simple">
<li><p>la variable est quantitative discrète si les modalités forment un ensemble dénombrable</p></li>
<li><p>la variable quantitative est continue si les modalités vivent dans un espace continu.</p></li>
</ul>
</div>
</div><p>Dans certains cas (l’âge par exemple), une variable d’un type (quantitative continue ici) peut être exprimée d’une autre manière pour des raisons pratiques de collecte ou de mesure. De même, les variables qualitatives ordinales peuvent être codées, par exemple selon une échelle de satisfaction.</p>
<div class="proof definition admonition" id="definition-2">
<span id="index-2"></span><p class="admonition-title"><span class="caption-number">Definition 19 </span> (Série statistique)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle série statistique une suite de <span class="math notranslate nohighlight">\(n\)</span> valeurs prises par une variable <span class="math notranslate nohighlight">\(X\)</span> sur les unités d’observation, notées <span class="math notranslate nohighlight">\(x_1\cdots x_n\)</span>.</p>
</div>
</div></div>
<div class="section" id="variable-qualitative-nominale">
<h4>Variable qualitative nominale<a class="headerlink" href="#variable-qualitative-nominale" title="Permalink to this headline">#</a></h4>
<p id="index-3">Une variable qualitative nominale a des valeurs distinctes qui ne peuvent pas être ordonnées. On note <span class="math notranslate nohighlight">\(J\)</span> le nombre de valeurs distinctes ou de modalités, notées <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span>. On appelle effectif d’une modalité ou d’une valeur distincte le nombre de fois que cette modalité (ou valeur distincte) apparaît dans la série statistique. On note <span class="math notranslate nohighlight">\(n_j\)</span> l’effectif de la modalité <span class="math notranslate nohighlight">\(x_j\)</span>. La fréquence d’une modalité <span class="math notranslate nohighlight">\(j\)</span> est  alors égale à <span class="math notranslate nohighlight">\(f_j=\frac{n_j}{n}\)</span>.</p>
<p>Le tableau statistique d’une variable qualitative nominale peut être représenté par deux types de graphiques. Les effectifs sont représentés par un diagramme en tuyau d’orgue et les fréquences par un diagramme en secteurs. Pour ce dernier, si le nombre de modalités devient trop important, la représentation perd de son intérêt.</p>
<p><img alt="" src="_images/baton.png" /></p>
</div>
<div class="section" id="variable-qualitative-ordinale">
<h4>Variable qualitative ordinale<a class="headerlink" href="#variable-qualitative-ordinale" title="Permalink to this headline">#</a></h4>
<p id="index-4">Le domaine peut être muni d’une relation d’ordre.  Les valeurs distinctes d’une variable ordinale peuvent donc être ordonnées <span class="math notranslate nohighlight">\(x_1\leq x_2\cdots\leq  x_J\)</span>, à permutation près dans l’ordre croissant des indices. L’effectif cumulé <span class="math notranslate nohighlight">\(N_j\)</span> et la fréquence cumulée <span class="math notranslate nohighlight">\(F_j\)</span> des variables sont alors définis par
<span class="math notranslate nohighlight">\((\forall j\in[\![1,J]\!])\quad N_j=\displaystyle\sum_{i=1}^j n_i\quad \textrm {et}\quad F_j=\displaystyle\sum_{i=1}^j f_i\)</span></p>
<p>Les fréquences et les effectifs (cumulés ou non) peuvent être représentés sous la forme d’un diagramme en tuyaux d’orgue.</p>
</div>
<div class="section" id="variable-quantitative-discrete">
<h4>Variable quantitative discrète<a class="headerlink" href="#variable-quantitative-discrete" title="Permalink to this headline">#</a></h4>
<p id="index-5">Le domaine d’une telle variable est dénombrable. Comme pour les variables qualitatives ordinales, on peut calculer les effectifs (cumulés ou non) et les fréquences (cumulées ou non).</p>
<p>La répartition des valeurs de la variable peut être représentée par un diagramme en bâtonnets. Les fréquences cumulées  sont visualisées par la fonction de répartition de la variable , définie par</p>
<p><span class="math notranslate nohighlight">\(F(x) = \left \{
\begin{eqnarray}
0&amp;\textrm{ si} &amp;x&lt;x_1\\
F_j &amp;\textrm{ si}&amp;  x\in[x_j,x_{j+1}[\\
1&amp; \textrm{ si}&amp;  x_J\leq x
\end{eqnarray}\right .\)</span></p>
<p><img alt="" src="_images/baton2.png" /></p>
</div>
<div class="section" id="variable-quantitative-continue">
<h4>Variable quantitative continue<a class="headerlink" href="#variable-quantitative-continue" title="Permalink to this headline">#</a></h4>
<p id="index-6">Le domaine d’une  variable quantitative continue est infini et est assimilé à <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> ou à un intervalle de <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Cependant, la mesure étant limitée en précision, on peut traiter ces variables comme des variables discrètes.</p>
<p>La représentation graphique de ces variables (et la construction du tableau statistique) passe par le regroupement des modalités ou valeurs en classes. Le tableau ainsi construit est souvent appelé distribution groupée. La classe <span class="math notranslate nohighlight">\(j\)</span> est l’ensemble des valeurs incluses dans <span class="math notranslate nohighlight">\([c^-_j,c^+_j[\)</span>, où <span class="math notranslate nohighlight">\(c^-_j\)</span> et <span class="math notranslate nohighlight">\(c^+_j\)</span> sont les bornes inférieure et supérieure de la classe. Sur cet intervalle, on peut calculer la fréquence <span class="math notranslate nohighlight">\(f_j\)</span> de la classe, la fréquence cumulée, l’effectif <span class="math notranslate nohighlight">\(n_j\)</span>… La répartition en classes nécessite de définir a priori le nombre de classes <span class="math notranslate nohighlight">\(J\)</span> et l’amplitude <span class="math notranslate nohighlight">\(a_j\)</span> des intervalles. Si elles peuvent être définies de manière empirique, quelques règles permettent d’établir <span class="math notranslate nohighlight">\(J\)</span> et l’amplitude pour une série statistique de <span class="math notranslate nohighlight">\(n\)</span> observations. Par exemple :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J=1+3.3log_{10}(n)\)</span> (règle de Sturge)</p></li>
<li><p><span class="math notranslate nohighlight">\(J=2.5\sqrt[4\,]{n}\)</span> (règle de Yule)</p></li>
</ul>
<p>La représentation graphique se fait par exemple par histogramme.
Les histogrammes sont des représentations de la distribution des données, agrégées par intervalles. A partir de l’étendue des données, on subdivise l’intervalle en <span class="math notranslate nohighlight">\(k\)</span> bins, de tailles <span class="math notranslate nohighlight">\(t_k\)</span> non nécessairement identiques, et on compte le nombre d’individus <span class="math notranslate nohighlight">\(n_k\)</span> rentrant dans chaque bin. L’histogramme peut alors être :</p>
<ul class="simple">
<li><p>non normalisé : <span class="math notranslate nohighlight">\(h_k = n_k\)</span></p></li>
<li><p>normalisé: <span class="math notranslate nohighlight">\(h_k = n_k/t_k\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Comptage des individus</span>
<span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">findBin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">bin</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bins</span><span class="p">):</span>
            <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="nb">bin</span>
            <span class="k">if</span> <span class="n">left</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">i</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="n">count</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">findBin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">count</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">count</span>

        
<span class="c1"># Affichage de l&#39;histogramme</span>
<span class="k">def</span> <span class="nf">plot_hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>  <span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">,</span><span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">bins</span> <span class="o">=</span><span class="p">[</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="n">bin_width</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">)</span> <span class="p">]</span>
    <span class="n">bin_left</span> <span class="o">=</span> <span class="p">[</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">bins</span> <span class="p">]</span>
    <span class="n">bin_widths</span> <span class="o">=</span> <span class="p">[</span> <span class="n">r</span><span class="o">-</span><span class="n">l</span>  <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="n">bins</span> <span class="p">]</span>
    <span class="n">bin_height</span> <span class="o">=</span> <span class="p">[</span> 
        <span class="nb">float</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span> <span class="k">if</span> <span class="n">normed</span> <span class="k">else</span> <span class="n">c</span> 
        <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">count</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="p">),</span> <span class="n">bin_widths</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_left</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="n">bin_width</span><span class="p">,</span><span class="n">height</span><span class="o">=</span><span class="n">bin_height</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">bin_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">bin_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">subplot</span><span class="p">,</span> <span class="n">binsize</span> <span class="ow">in</span> <span class="p">((</span><span class="mi">141</span><span class="p">,</span> <span class="mi">5</span><span class="p">),(</span><span class="mi">142</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">143</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Taille des bins : &#39;</span><span class="p">,</span> <span class="n">binsize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plot_hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">binsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/statsdescriptives_1_0.png" src="_images/statsdescriptives_1_0.png" />
</div>
</div>
<p>Le choix de la largeur <span class="math notranslate nohighlight">\(t\)</span> des bins dépend des données, et par exemple on a :</p>
<ul class="simple">
<li><p>Loi de Scott : <span class="math notranslate nohighlight">\(t = \frac{3.5 \sigma}{Card(X)^{1/3}}\)</span>, où <span class="math notranslate nohighlight">\(\sigma\)</span> est l’écart type des données.</p></li>
<li><p>Loi de Freedman–Diaconis : <span class="math notranslate nohighlight">\( t = \frac{2 IQR}{Card(X)^{1/3}}\)</span>, où <span class="math notranslate nohighlight">\(IQR\)</span> est la distance interquartile.</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 9 </span></p>
<div class="remark-content section" id="proof-content">
<p>Toutes les classes n’ont pas nécessairement la même amplitude</p>
</div>
</div><p>Les effectifs (ou les fréquences) sont représenté(e)s par un histogramme. Si l’on s’intéresse à la représentation des effectifs (resp. des fréquences), la densité d’effectif <span class="math notranslate nohighlight">\(h_j\)</span> (resp. de fréquence <span class="math notranslate nohighlight">\(d_j\)</span>),  définie par <span class="math notranslate nohighlight">\(h_j=\frac{n_j}{a_j}\)</span> (resp. <span class="math notranslate nohighlight">\(d_j=\frac{f_j}{a_j}\)</span>), détermine la hauteur du rectangle représentant la classe <span class="math notranslate nohighlight">\(j\)</span>. L’aire de l’histogramme est égale à l’effectif total <span class="math notranslate nohighlight">\(n\)</span> pour l’histogramme des effectifs, et à 1 pour l’histogramme des fréquences.</p>
<p>Comme dans le cas discret, la fonction de répartition peut être calculée de la manière suivante :</p>
<p><span class="math notranslate nohighlight">\(F(x) = \left \{
\begin{eqnarray}
0&amp;\textrm{ si}&amp; x&lt;c^-_1\\
F_{j-1}+\frac{f_j}{c^+_j-c^-_j}(x-c^-_j) &amp;\textrm{ si}&amp; x\in[c^-_j,c^+_j[\\
1&amp; \textrm{ si}&amp;c^+_J\leq x
\end{eqnarray}\right .\)</span></p>
</div>
</div>
<div class="section" id="pre-traitement-des-donnees">
<h3>Pré-traitement des données<a class="headerlink" href="#pre-traitement-des-donnees" title="Permalink to this headline">#</a></h3>
<p>Faire une analyse de données, c’est traiter un tableau de taille <span class="math notranslate nohighlight">\(n\times d\)</span> où <span class="math notranslate nohighlight">\(n\)</span> est le nombre d’individus et <span class="math notranslate nohighlight">\(d\)</span> le nombre de variables (caractères) mesurées sur ces individus. En raison de la colecte des données, des erreurs de mesure ou d’autres facteurs, ce tableau est parfois incomplet et il convient de le prétraiter pour pouvoir effectuer l’analyse.</p>
<div class="section" id="points-aberrants">
<h4>Points aberrants<a class="headerlink" href="#points-aberrants" title="Permalink to this headline">#</a></h4>
<p>Une anomalie (ou point aberrant, ou outlier) est une observation (ou un sous-ensemble d’observations) qui semble incompatible avec le reste de l’ensemble de données.</p>
<p>S’il est parfois possible d’identifier graphiquement ces points aberrants à l’aide de boîtes à moustaches (voir <a class="reference internal" href="#boxplot"><span class="std std-ref">Pour résumer</span></a>), il existe une vaste littérature sur la détection d’anomalies qu’il n’est pas possible d’aborder ici. De plus, suivant le type de données manipulées (données séquentielles ou non), le type de méthode peut être différent. On mentionne donc ici quelques techniques simples :</p>
<ul class="simple">
<li><p>le détecteur de Hampel : on considère que <span class="math notranslate nohighlight">\(x_i\)</span> est un point aberrant si</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|x_i-x_{\frac12}|&gt;3.MADM\]</div>
<p>où <span class="math notranslate nohighlight">\(MADM = 1.4826.|x_i-x_{\frac12}|_\frac12\)</span>, et où <span class="math notranslate nohighlight">\(y_{\frac12}\)</span> est la médiane des données <span class="math notranslate nohighlight">\(y\)</span></p>
<ul class="simple">
<li><p>la règle empirique de l’écart-type : on considère que <span class="math notranslate nohighlight">\(x_i\)</span> est un point aberrant si</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|x_i-\bar x|&gt;3.\sigma\]</div>
<p>où  <span class="math notranslate nohighlight">\(\bar x\)</span> (respectivement <span class="math notranslate nohighlight">\(\sigma\)</span>) est la moyenne (resp. l’écart-type ) des données.</p>
<ul class="simple">
<li><p>la méthode LOF (Local Outlier Factor) qui repose sur le concept de densité locale, où la localité est donnée par les <span class="math notranslate nohighlight">\(k\)</span> voisins les plus proches, dont la distance est utilisée pour estimer la densité. En comparant la densité locale d’un objet aux densités locales de ses voisins, il est possible d’identifier des régions de densité similaire et des points dont la densité est nettement inférieure à celle de leurs voisins. Ces derniers sont considérés comme des valeurs aberrantes. La densité locale est estimée par la distance typique à laquelle un point peut être atteint à partir de ses voisins.</p></li>
<li><p>la méthode COF (Connectivity based Outlier Factor) basée sur le même principe que LOF, à ceci près que l’estimation de densité est effectuée en utilisant le minimum de la somme des distances reliant tous les voisins d’un point donné.</p></li>
</ul>
</div>
<div class="section" id="donnees-manquantes">
<h4>Données manquantes<a class="headerlink" href="#donnees-manquantes" title="Permalink to this headline">#</a></h4>
<p>Lors de la collecte des données, il arrive que certaines d’entre elles ne soient pas disponibles ou enregistrées. On distingue trois types de données manquantes :</p>
<ol class="simple">
<li><p>les données manquant de manière complètement aléatoire :  la probabilité qu’une donnée soit manquante ne dépend pas des valeurs connues ni de la valeur manquante elle-même.</p></li>
<li><p>les données manquant de manière aléatoire :  la probabilité qu’une donnée soit manquante peut dépendre de valeurs connues (d’autres variables parmi les <span class="math notranslate nohighlight">\(d\)</span>), mais pas de la variable dont les valeurs sont manquantes.</p></li>
<li><p>les données manquant de manière non aléatoire : la probabilité qu’une donnée soit manquante dépend d’autres variables qui ont également des valeurs manquantes, ou elle dépend de la variable elle-même.</p></li>
</ol>
<p>Pour résoudre ce problème de données manquantes, dans la mesure où ces dernières ne sont pas trop nombreuses, on a recours à des techniques d’<strong>imputation</strong>.</p>
<p>Dans le cas d’une imputation simple (une seule donnée manquante), on peut par exemple remplacer la valeur manquante dans une colonne <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span> par :</p>
<ul class="simple">
<li><p>une valeur fixe</p></li>
<li><p>une statistique sur la colonne <span class="math notranslate nohighlight">\(j\)</span> (la plus petite ou la plus grande valeur, la moyenne de la colonne, la valeur la plus fréquente…)</p></li>
<li><p>une valeur issue des <span class="math notranslate nohighlight">\(k\)</span> plus proches voisins de la ligne du tableau où la valeur en position <span class="math notranslate nohighlight">\(j\)</span> est manquante</p></li>
<li><p>une valeur calculée par régression (voir chapitre correspondant) sur l’ensemble du tableau</p></li>
<li><p>la valeur précédente (ou suivante) dans le cas où la colonne est une série ordonnée ou temporelle.</p></li>
</ul>
<p>Dans le cas d’une imputation multiple, où un sous-ensemble de valeurs doit être comblé, on peut adopter la stratégie suivante :</p>
<ol class="simple">
<li><p>Effectuer une imputation simple pour toutes les valeurs manquantes de l’ensemble de données.</p></li>
<li><p>Remettre les valeurs manquantes d’une variable <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span> à “manquante”.</p></li>
<li><p>Former un modèle pour prédire les valeurs manquantes de <span class="math notranslate nohighlight">\(j\)</span> en utilisant les valeurs disponibles de la variable <span class="math notranslate nohighlight">\(j\)</span> en tant que variable dépendante et les autres variables de l’ensemble de données comme indépendantes.</p></li>
<li><p>Prédire les valeurs manquantes dans la colonne <span class="math notranslate nohighlight">\(j\)</span> en utilisant le modèle entraîné à l’étape 3.</p></li>
<li><p>Répéter les étapes 2 à 4 pour toutes les autres colonnes présentant des valeurs manquantes.</p></li>
<li><p>Répéter l’étape 2-5 jusqu’à convergence (ou un nombre maximal d’itérations)</p></li>
<li><p>Répéter les étapes 1-6 plusieurs fois avec différentes initialisations de nombres aléatoires pour créer différentes versions de l’ensemble de données complet/imputé.</p></li>
</ol>
</div>
<div class="section" id="transformation-des-donnees-qualitatives">
<h4>Transformation des données qualitatives<a class="headerlink" href="#transformation-des-donnees-qualitatives" title="Permalink to this headline">#</a></h4>
<p>Pour pouvoir être traitées numériquement, les données qualitatives doivent être transformées. Plusieurs techniques existent parmi lesquelles :</p>
<ul class="simple">
<li><p>pour le cas des variables ordinales : on utilise le rang pour encoder les modalités de la variable. Par exemple, pour un niveau de diplomation Brevet<span class="math notranslate nohighlight">\(&lt;\)</span>Bac<span class="math notranslate nohighlight">\(&lt;\)</span>Licence<span class="math notranslate nohighlight">\(&lt;\)</span>Master<span class="math notranslate nohighlight">\(&lt;\)</span>Doctorat, on codera Licence par 3 et Doctorat par 5.</p></li>
<li><p>le one-hot encoding : pour une variable qualitative présentant <span class="math notranslate nohighlight">\(J\)</span> modalités, on construit un vecteur de taille <span class="math notranslate nohighlight">\(J\)</span> dont les composantes sont toutes nulles sauf la <span class="math notranslate nohighlight">\(J\)</span>-ème qui vaut 1. Par exemple, si <span class="math notranslate nohighlight">\(J\)</span>=3, on construit 1 vecteur de taille 3, et pour un individu ayant la modalité 2, on le code en (0 1 0). Lorsque <span class="math notranslate nohighlight">\(J\)</span> est élevé, on se retrouve avec un jeu de données volumineux.</p></li>
<li><p>les méthodes de plongement (embedding) : utilisées principalement en apprentissage profond (Deep learning) pour le traitement du langage naturel, ces classes de méthodes construisent une représentation de chaque modalité d’une variable qualitative en un vecteur numérique de taille fixe et choisie. Pour le mot “rouge” de la variable “couleur”, par exemple, l’encodage peut par exemple être représenté par le vecteur (0.31 0.57 0.12). En pratique, le calcul de ces représentations s’effectue classiquement par l’entraînement d’un réseau de neurones ayant pour entrée uniquement les variables qualitatives. Tout d’abord, un encodage one-hot est appliqué à la variable afin d’être mise en entrée du réseau, qui n’accepte que les entrées numériques. La sortie d’une des couches cachées du réseau constitue alors le vecteur recherché. On concatène ensuite ce vecteur aux données initiales, utilisées dans l’ajustement du modèle final.</p></li>
</ul>
</div>
<div class="section" id="normalisation">
<h4>Normalisation<a class="headerlink" href="#normalisation" title="Permalink to this headline">#</a></h4>
<p>Il arrive que les données collectées ne soient pas du même ordre de grandeur, notamment en raison des unités de mesure (un individu mesuré par sa taille en millimètres et son poids en tonnes par exemple). Cette différence de valeur absolue introduit un biais dans l’analyse des données (<a class="reference internal" href="#biais"><span class="std std-ref">figure 1</span></a>) qu’il convient de corriger. C’est le processus de normalisation des données.</p>
<p>Pour une colonne <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span>, on dispose de <span class="math notranslate nohighlight">\(n\)</span> valeurs <span class="math notranslate nohighlight">\(x_{ij},i\in[\![1,n]\!]\)</span>. On note : <span class="math notranslate nohighlight">\(x_{min} = \displaystyle\min_{i\in[\![1,n]\!]}x_{ij}\)</span>, <span class="math notranslate nohighlight">\(x_{max} = \displaystyle\max_{i\in[\![1,n]\!]}x_{ij}\)</span>,   <span class="math notranslate nohighlight">\(\bar x_j\)</span> la moyenne des <span class="math notranslate nohighlight">\(x_{ij}\)</span>, <span class="math notranslate nohighlight">\(\sigma_j\)</span> leur écart-type, <span class="math notranslate nohighlight">\(x_\frac14, x_\frac12\)</span> et <span class="math notranslate nohighlight">\(x_\frac34\)</span> les premier, deuxième et troisième quartiles. On distingue alors classiquement trois types de normalisation :</p>
<ol class="simple">
<li><p>la normalisation min-max : <span class="math notranslate nohighlight">\(x_{ij} = \frac{x_{ij}-x_{min}}{x_{max}-x_{min}}\)</span></p></li>
<li><p>la normalisation standard : <span class="math notranslate nohighlight">\(x_{ij}=\frac{x_{ij}-\bar x_j}{\sigma_j}\)</span></p></li>
<li><p>la normalisation robuste : <span class="math notranslate nohighlight">\(x_{ij}=\frac{x_{ij}-x_\frac12}{x_\frac34-x_\frac14}\)</span></p></li>
</ol>
<p>La normalisation standard dépend de la présence de points aberrants (qui affectent la moyenne).</p>
<div class="figure align-default" id="biais">
<img alt="_images/normK.png" src="_images/normK.png" />
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Effet de la normalisation sur un algorithme de classification (voir chapitre correspondant). En haut un jeu de données avec deux nuages de points allongés selon l’axe des <span class="math notranslate nohighlight">\(x\)</span>, certainement en raison d’une différence d’échelle entre les unités de mesure de <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>. Au milieu une classification par <span class="math notranslate nohighlight">\(k\)</span>-moyennes, <span class="math notranslate nohighlight">\(k\)</span>=2 sans normalisation, en utilisant la distance euclidienne. Les deux classes sont séparées suivant l’axe des <span class="math notranslate nohighlight">\(x\)</span>, ne reflétant pas la répartition naturelle des points. En bas, après normalisation, les deux nuages de points sont correctement séparés</span><a class="headerlink" href="#biais" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
<div class="section" id="statistique-descriptive-univariee">
<h3>Statistique descriptive univariée<a class="headerlink" href="#statistique-descriptive-univariee" title="Permalink to this headline">#</a></h3>
<p id="index-7">La statistique descriptive univariée consiste à étudier un ensemble d’unités d’observations, lorsque celles-ci sont décrites par une seule variable.</p>
<p>Soit donc <span class="math notranslate nohighlight">\(X\)</span> une variable et <span class="math notranslate nohighlight">\(x_j,j\in [\![1,n]\!]\)</span> l’ensemble des valeurs prises par cette variable, <span class="math notranslate nohighlight">\(n_i\)</span> étant le nombre de fois où la valeur <span class="math notranslate nohighlight">\(x_i\)</span> est prise. <span class="math notranslate nohighlight">\(X\)</span> peut être qualitative ou quantitative, les paramètres de description décrits dans la suite s’appliqueront à l’une de ces natures ou au deux.</p>
<div class="section" id="parametres-de-position">
<h4>Paramètres de position<a class="headerlink" href="#parametres-de-position" title="Permalink to this headline">#</a></h4>
<p>Plusieurs paramètres permettent de décrire la position “la plus représentative” d’une variable :</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 20 </span> (Mode)</p>
<div class="definition-content section" id="proof-content">
<p>Le mode est la valeur distincte correspondant à l’effectif le plus élevé. Il est noté <span class="math notranslate nohighlight">\(x_M\)</span>.</p>
</div>
</div><p>Le mode peut être calculé pour tout type de variable, n’est pas nécessairement unique. Lorsqu’une variable continue est découpée en classes, il est possible de définir une classe modale (classe correspondant à l’effectif le plus élevé)</p>
<div class="proof definition admonition" id="definition-5">
<p class="admonition-title"><span class="caption-number">Definition 21 </span> (Moyennes)</p>
<div class="definition-content section" id="proof-content">
<p>Les moyennes ne peuvent être définies que sur des variables quantitatives. Plusieurs moyennes peuvent être calculées, parmi lesquelles :</p>
<ul class="simple">
<li><p>la moyenne <strong>arithmétique</strong>  <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{n}{\displaystyle\sum_{i=1}^nx_i}=  \frac{1}{n}{\displaystyle\sum_{i=1}^J n_ix_i}\)</span>. C’est le moment à l’origine d’ordre 1.</p></li>
<li><p>la moyenne <strong>géométrique</strong> : si les <span class="math notranslate nohighlight">\(x_i\)</span> sont positifs, la moyenne géométrique est la quantité <span class="math notranslate nohighlight">\(G=\left (\displaystyle\prod_{i=1}^n x_i\right )^\frac{1}{n}\)</span>. C’est donc l’exponentielle de la moyenne arithmétique des logarithmes des valeurs observées.</p></li>
<li><p>la moyenne <strong>harmonique</strong> : si les <span class="math notranslate nohighlight">\(x_i\)</span> sont positifs, la moyenne harmonique est définie par <span class="math notranslate nohighlight">\(H=\frac{n}{\displaystyle\sum_{i=1}^J 1/x_i}\)</span></p></li>
<li><p>la moyenne <strong>pondérée</strong> : dans certains cas, on n’accorde pas la même importance à toutes les observations (fiabilité, confiance…). La moyenne pondérée est alors définie par
<span class="math notranslate nohighlight">\(\bar{x}_w= \frac{\displaystyle\sum_{i=1}^n w_ix_i}{\displaystyle\sum_{i=1}^n w_i}\)</span></p></li>
</ul>
</div>
</div><p>Dans le cas où <span class="math notranslate nohighlight">\(\forall i,w_i=1/n\)</span>, la moyenne pondérée est la moyenne arithmétique. De plus, dans tous les cas, on peut montrer que <span class="math notranslate nohighlight">\(H\leq G\leq \bar{x}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">ArithmeticMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># calculable directement avec np.mean(X)</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GeometricMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">p</span><span class="o">=</span><span class="mi">1</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">p</span><span class="o">*=</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">HarmonicMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">/</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">s</span>

<span class="k">def</span> <span class="nf">WeightedMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># Exemples de poids</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="p">((</span><span class="n">ArithmeticMean</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;Arithmétique&#39;</span><span class="p">),(</span><span class="n">GeometricMean</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;Géométrique&#39;</span><span class="p">),</span>
                             <span class="p">(</span><span class="n">HarmonicMean</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;Harmonique&#39;</span><span class="p">),(</span><span class="n">WeightedMean</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;Pondérée&#39;</span><span class="p">)):</span>
    <span class="n">m</span><span class="o">=</span><span class="n">method</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ArithmeticMean  :  1316.3086347078017
GeometricMean  :  1258.4787575642572
HarmonicMean  :  1198.219210728503
WeightedMean  :  1327.0068383275884
</pre></div>
</div>
<img alt="_images/statsdescriptives_3_1.png" src="_images/statsdescriptives_3_1.png" />
</div>
</div>
<div class="proof definition admonition" id="definition-6">
<p class="admonition-title"><span class="caption-number">Definition 22 </span> (Médiane)</p>
<div class="definition-content section" id="proof-content">
<p>La médiane, notée <span class="math notranslate nohighlight">\(x_\frac{1}{2}\)</span> est la valeur centrale de la série statistique triée par ordre croissant.</p>
</div>
</div><p>En d’autres termes, c’est la valeur de la série triée telle qu’au moins 50% des effectifs soient inférieurs à <span class="math notranslate nohighlight">\(x_\frac{1}{2}\)</span>. Elle peut être calculée sur des variables quantitatives ou qualitatives ordinales (dans le cas où des échelles de valeur ont été définies).</p>
<div class="proof definition admonition" id="definition-7">
<p class="admonition-title"><span class="caption-number">Definition 23 </span> (Quantiles)</p>
<div class="definition-content section" id="proof-content">
<p>Le quantile d’ordre <span class="math notranslate nohighlight">\(p\)</span> est défini par <span class="math notranslate nohighlight">\(x_p=F^{-1}(p)\)</span>, où <span class="math notranslate nohighlight">\(F\)</span> est la fonction de répartition.</p>
</div>
</div><p>La notion de quantile généralise la notion de médiane. Si la fonction de répartition était continue et strictement croissante, la définition de <span class="math notranslate nohighlight">\(x_p\)</span> serait unique. Or <span class="math notranslate nohighlight">\(F\)</span> est discontinue et définie par paliers et les valeurs de quantiles varient suivant par exemple l’utilisation ou non d’une méthode d’interpolation de <span class="math notranslate nohighlight">\(F\)</span>. Pour calculer <span class="math notranslate nohighlight">\(x_p\)</span>, on peut par exemple considérer que si <span class="math notranslate nohighlight">\(np\)</span> est pair,
<span class="math notranslate nohighlight">\(x_p=\frac{x_{np}+x_{np+1}}{2}\)</span>
on remarque alors que la médiane est le quantile d’ordre <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>
et sinon
<span class="math notranslate nohighlight">\(x_p=x_{\lceil{np}\rceil}\)</span>
En particulier, un quartile est chacune des 3 valeurs qui divisent les données triées en 4 parts égales, de sorte que chaque partie représente 1/4 de l’échantillon de population. On note <span class="math notranslate nohighlight">\(Q_i\)</span> le <span class="math notranslate nohighlight">\(i^e\)</span> quartile.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">style</span>  <span class="ow">in</span> <span class="p">((</span><span class="mi">25</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">),(</span><span class="mi">50</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">),(</span><span class="mi">75</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">)):</span>
    <span class="n">m</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">q</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;quartile &quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>quartile  25  :  905.9190521240237
quartile  50  :  1399.66320800781
quartile  75  :  1626.326538085935
</pre></div>
</div>
<img alt="_images/statsdescriptives_5_1.png" src="_images/statsdescriptives_5_1.png" />
</div>
</div>
</div>
<div class="section" id="parametres-de-dispersion">
<h4>Paramètres de dispersion<a class="headerlink" href="#parametres-de-dispersion" title="Permalink to this headline">#</a></h4>
<p>Il est très souvent utile d’apprécier la dispersion des mesures autour du paramètre de position. Pour cela, sur des variables quantitatives uniquement, plusieurs outils sont disponibles :</p>
<div class="proof definition admonition" id="definition-8">
<p class="admonition-title"><span class="caption-number">Definition 24 </span> (Etendue)</p>
<div class="definition-content section" id="proof-content">
<p>L’étendue est la simple différence entre la plus grande et la plus petite valeur observée.</p>
</div>
</div><div class="proof definition admonition" id="definition-9">
<p class="admonition-title"><span class="caption-number">Definition 25 </span> (Déviation maximale)</p>
<div class="definition-content section" id="proof-content">
<p>La déviation maximale est définie par
<span class="math notranslate nohighlight">\( maxdev(X) = max \{ |x_i - \bar{x}| \,|\, i\in[\![1,n]\!]\}\)</span></p>
</div>
</div><div class="proof definition admonition" id="definition-10">
<p class="admonition-title"><span class="caption-number">Definition 26 </span> (Déviation moyenne absolue)</p>
<div class="definition-content section" id="proof-content">
<p>La déviation moyenne absolue est définie par
<span class="math notranslate nohighlight">\( mad(X) = \frac{1}{n} \displaystyle\sum_{i=1}^n |x_i - \bar{x}|\)</span></p>
</div>
</div><div class="proof definition admonition" id="definition-11">
<p class="admonition-title"><span class="caption-number">Definition 27 </span> (Distance interquartile)</p>
<div class="definition-content section" id="proof-content">
<p>La distance interquartile <span class="math notranslate nohighlight">\(Q_3-Q_1\)</span> est la différence entre le troisième et le premier quartile. C’est une statistique robuste aux points aberrants.</p>
</div>
</div><div class="proof definition admonition" id="definition-12">
<p class="admonition-title"><span class="caption-number">Definition 28 </span> (Variance)</p>
<div class="definition-content section" id="proof-content">
<p>La variance est la somme des carrés des écarts à la moyenne, normalisée par le nombre d’observations
<span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )^2\)</span></p>
</div>
</div><p>Cette variance est dite biaisée. La variance non biaisée est obtenue en divisant non pas par <span class="math notranslate nohighlight">\(n\)</span>, mais par <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<div class="proof definition admonition" id="definition-13">
<p class="admonition-title"><span class="caption-number">Definition 29 </span> (Ecart type)</p>
<div class="definition-content section" id="proof-content">
<p>L’écart type est la racine carrée de la variance.</p>
</div>
</div><div class="proof definition admonition" id="definition-14">
<p class="admonition-title"><span class="caption-number">Definition 30 </span> (Ecart moyen absolu)</p>
<div class="definition-content section" id="proof-content">
<p>L’écart moyen absolu est la somme des valeurs absolues des écarts à la moyenne divisée par le nombre d’observations.</p>
</div>
</div><p>Notons qu’il s’agit de la distance <span class="math notranslate nohighlight">\(L_1\)</span> du vecteur des observations au vecteur composé de la valeur moyenne, divisé par le nombre d’observations. La variance est la distance <span class="math notranslate nohighlight">\(L_2\)</span> entre ces deux vecteurs. Lorsque la distance est calculée par rapport au vecteur composé de la valeur médiane, on parle d’écart médian absolu.</p>
<p><img alt="" src="_images/dispersion.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">max_dev</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mad</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigma</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">IQR</span><span class="p">(</span><span class="n">X</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">75</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span><span class="n">style</span><span class="p">,</span>  <span class="ow">in</span> <span class="p">((</span><span class="n">max_dev</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">),(</span><span class="n">mad</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">),(</span><span class="n">sigma</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">),(</span><span class="n">IQR</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">)):</span>
    <span class="n">s</span><span class="o">=</span><span class="n">method</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;+/-&quot;</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;black&#39;</span> <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">style</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">],[</span><span class="n">pos</span><span class="p">,</span><span class="n">pos</span><span class="p">],</span><span class="n">style</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max_dev  :  1316.3086347078017 +/- 738.0729570890783
mad  :  1316.3086347078017 +/- 327.4656915004233
sigma  :  1316.3086347078017 +/- 374.5723639541368
IQR  :  1316.3086347078017 +/- 720.4074859619113
</pre></div>
</div>
<img alt="_images/statsdescriptives_7_1.png" src="_images/statsdescriptives_7_1.png" />
</div>
</div>
</div>
<div class="section" id="parametres-de-forme">
<h4>Paramètres de forme<a class="headerlink" href="#parametres-de-forme" title="Permalink to this headline">#</a></h4>
<p>Les paramètres de forme sont souvent calculés en référence à la forme de la loi normale, pour évaluer la symétrie, l’aplatissement ou la dérive par rapport à cette loi.</p>
<div class="proof definition admonition" id="definition-15">
<p class="admonition-title"><span class="caption-number">Definition 31 </span> (Skewness)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[g_1 = \frac{m_3}{\sigma^3}\]</div>
</div>
</div><p>Le skewness est également appelé coefficient d’asymétrie de Fisher.</p>
<div class="proof definition admonition" id="definition-16">
<p class="admonition-title"><span class="caption-number">Definition 32 </span> (Kurtosis)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[K=\frac{m_4}{m_2^2}\]</div>
</div>
</div><p><span class="math notranslate nohighlight">\(K\)</span> permet de mesurer l’aplatissement.</p>
<div class="proof definition admonition" id="definition-17">
<p class="admonition-title"><span class="caption-number">Definition 33 </span> (Coefficient d’asymétrie de Yule)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[A_Y = \frac{x_{3/4}+x_{1/4}-2x_{1/2}}{x_{3/4}-x_{1/4}}\]</div>
</div>
</div><p>Ce coefficient est fondé sur les positions de trois quartiles (le premier, la médiane et le troisième) et est normalisé par la distance interquartile.</p>
<div class="proof definition admonition" id="definition-18">
<p class="admonition-title"><span class="caption-number">Definition 34 </span> (Coefficient d’asymétrie de Pearson)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[A_P = \frac{\bar{x}-x_M}{\sigma}\]</div>
</div>
</div><p>Ce coefficient est fondé sur la comparaison de la moyenne et du mode, et est normalisé par l’écart type.</p>
<p>Tous les coefficients d’asymétrie ont des propriétés similaires : ils sont nuls si la distribution est symétrique, négatifs si la distribution est allongée à gauche (left asymmetry), et positifs si la distribution est allongée à droite (right asymmetry).</p>
<p>On peut aussi chercher à mesurer l’aplatissement (ou kurtosis) d’une distribution de mesure. Dans ce cas, on utilise le coefficient d’aplatissement de Pearson ou de Fisher, respectivement donnés par
<span class="math notranslate nohighlight">\(\beta_2=\frac{m_4}{\sigma^4}\quad\textrm{et}\quad g_2=\beta_2-3\)</span></p>
<p>Une distribution est alors dite :</p>
<ul class="simple">
<li><p>mésokurtique si <span class="math notranslate nohighlight">\(g_2\)</span> est proche de 0</p></li>
<li><p>leptokurtique si <span class="math notranslate nohighlight">\(g_2&gt;0\)</span> (queues plus longues et distribution plus pointue)</p></li>
<li><p>platykyrtique si <span class="math notranslate nohighlight">\(g_2&lt;0\)</span> (queues plus courtes et distribution arrondie).</p></li>
</ul>
<div class="section" id="pour-resumer">
<span id="boxplot"></span><h5>Pour résumer<a class="headerlink" href="#pour-resumer" title="Permalink to this headline">#</a></h5>
<p>Les principales statistiques d’une série statistique peuvent être résumées dans des <strong>boîtes à moustache</strong>, qui permettent de voir sur un même graphique :</p>
<ul class="simple">
<li><p>la médiane</p></li>
<li><p>une boîte entre les premier et le troisième quartile</p></li>
<li><p>l’étendue</p></li>
<li><p>les points aberrants.</p></li>
</ul>
<p>Ce mode de représentation consiste à dessiner une boîte dont les extrémités dépendent du premier et du troisième quartiles <span class="math notranslate nohighlight">\(Q_1\)</span> et <span class="math notranslate nohighlight">\(Q_3\)</span> , en ajoutant une barre à l’intérieur
matérialisant le second quartile  <span class="math notranslate nohighlight">\(Q_2\)</span> (la valeur médiane de l’échantillon). A cette boîte, on ajoute des “moustaches” dont les extrémités dépendent :</p>
<ul class="simple">
<li><p>soit des valeurs extrémales prises par l’échantillon (minimum et maximum);</p></li>
<li><p>soit de la plus petite et de la plus grande valeur de l’échantillon appartenant à l’intervalle <span class="math notranslate nohighlight">\([Q_1 -\delta, Q_3+\delta ]\)</span>. La grandeur <span class="math notranslate nohighlight">\(\delta\)</span> est une mesure de la dispersion des données. Généralement, on utilise <span class="math notranslate nohighlight">\(\delta = 1.5(Q_3-Q_1)\)</span>.</p></li>
</ul>
<p>Les valeurs de l’ échantillon en dehors des moustaches sont parfois matérialisées par des points et sont alors considérées comme les points aberrants de l’échantillon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">annotate_boxplot</span><span class="p">(</span><span class="n">bpdict</span><span class="p">,</span> <span class="n">annotate_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">x_offset</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">x_loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">text_offset_x</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span>
                     <span class="n">text_offset_y</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">annotate_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">annotate_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">text_offset_x</span><span class="p">,</span> <span class="n">text_offset_y</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">})</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;Médiane&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;medians&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_1$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;boxes&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_3$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;boxes&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">2</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_1-1.5(Q_3-Q_1)$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;caps&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_3+1.5(Q_3-Q_1)$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;caps&#39;</span><span class="p">][(</span><span class="n">x_loc</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Données&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">150</span><span class="p">)})</span>

<span class="n">bpdict</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">whis</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;dict&#39;</span><span class="p">)</span>
<span class="n">annotate_boxplot</span><span class="p">(</span><span class="n">bpdict</span><span class="p">,</span> <span class="n">x_loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/statsdescriptives_9_0.png" src="_images/statsdescriptives_9_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="la-description-ne-fait-pas-tout">
<h4>La description ne fait pas tout…<a class="headerlink" href="#la-description-ne-fait-pas-tout" title="Permalink to this headline">#</a></h4>
<p>La description d’un ensemble de valeurx <span class="math notranslate nohighlight">\(x_j\)</span> par la moyenne, la variance, voire le comportement linéaire (coefficient de corrélation, voir plus loin) peut ne pas suffire à comprendre la distribution des données. Un exemple classique (analyse bivariée, section suivante) est le quartet d’Anscombe (figure ci-dessous), constitué de quatre ensembles de points  <span class="math notranslate nohighlight">\((x,y)\in\mathbb{R}^2\)</span> de même propriétés statistiques (moyenne, variance, coefficient de régression linéaire) mais qui sont distribués de manière totalement différente dans le plan.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">8.04</span><span class="p">,</span> <span class="mf">6.95</span><span class="p">,</span> <span class="mf">7.58</span><span class="p">,</span> <span class="mf">8.81</span><span class="p">,</span> <span class="mf">8.33</span><span class="p">,</span> <span class="mf">9.96</span><span class="p">,</span> <span class="mf">7.24</span><span class="p">,</span> <span class="mf">4.26</span><span class="p">,</span> <span class="mf">10.84</span><span class="p">,</span> <span class="mf">4.82</span><span class="p">,</span> <span class="mf">5.68</span><span class="p">]</span>
<span class="n">y2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">9.14</span><span class="p">,</span> <span class="mf">8.14</span><span class="p">,</span> <span class="mf">8.74</span><span class="p">,</span> <span class="mf">8.77</span><span class="p">,</span> <span class="mf">9.26</span><span class="p">,</span> <span class="mf">8.10</span><span class="p">,</span> <span class="mf">6.13</span><span class="p">,</span> <span class="mf">3.10</span><span class="p">,</span> <span class="mf">9.13</span><span class="p">,</span> <span class="mf">7.26</span><span class="p">,</span> <span class="mf">4.74</span><span class="p">]</span>
<span class="n">y3</span> <span class="o">=</span> <span class="p">[</span><span class="mf">7.46</span><span class="p">,</span> <span class="mf">6.77</span><span class="p">,</span> <span class="mf">12.74</span><span class="p">,</span> <span class="mf">7.11</span><span class="p">,</span> <span class="mf">7.81</span><span class="p">,</span> <span class="mf">8.84</span><span class="p">,</span> <span class="mf">6.08</span><span class="p">,</span> <span class="mf">5.39</span><span class="p">,</span> <span class="mf">8.15</span><span class="p">,</span> <span class="mf">6.42</span><span class="p">,</span> <span class="mf">5.73</span><span class="p">]</span>
<span class="n">x4</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">y4</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.58</span><span class="p">,</span> <span class="mf">5.76</span><span class="p">,</span> <span class="mf">7.71</span><span class="p">,</span> <span class="mf">8.84</span><span class="p">,</span> <span class="mf">8.47</span><span class="p">,</span> <span class="mf">7.04</span><span class="p">,</span> <span class="mf">5.25</span><span class="p">,</span> <span class="mf">12.50</span><span class="p">,</span> <span class="mf">5.56</span><span class="p">,</span> <span class="mf">7.91</span><span class="p">,</span> <span class="mf">6.89</span><span class="p">]</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;1.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span>
    <span class="s1">&#39;2.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span>
    <span class="s1">&#39;3.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">),</span>
    <span class="s1">&#39;4.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">y4</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                        <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;wspace&#39;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">,</span> <span class="s1">&#39;hspace&#39;</span><span class="p">:</span> <span class="mf">0.18</span><span class="p">})</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

    <span class="n">p1</span><span class="p">,</span> <span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># slope, intercept</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axline</span><span class="p">(</span><span class="n">xy1</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">p0</span><span class="p">),</span> <span class="n">slope</span><span class="o">=</span><span class="n">p1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">bar x$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">sigma$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;$r$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
            <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/statsdescriptives_11_0.png" src="_images/statsdescriptives_11_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="statistique-descriptive-bivariee">
<h3>Statistique descriptive bivariée<a class="headerlink" href="#statistique-descriptive-bivariee" title="Permalink to this headline">#</a></h3>
<p>On s’intéresse à deux variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>, mesurées sur les <span class="math notranslate nohighlight">\(n\)</span> unités d’observation. La série statistique est alors une suite de <span class="math notranslate nohighlight">\(n\)</span> couples <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> des valeurs prises par les deux variables sur chaque individu.</p>
<div class="section" id="cas-de-deux-variables-quantitatives">
<h4>Cas de deux variables quantitatives<a class="headerlink" href="#cas-de-deux-variables-quantitatives" title="Permalink to this headline">#</a></h4>
<p>Le couple est un couple de valeurs numériques. C’est donc un point dans le plan <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Les variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> peuvent être analysées séparément, en opérant une statistique univariée sur chacune de ces variables. Les paramètres calculés (de position, de dispersion…) sont dits marginaux. Cependant, il est intéressant d’étudier le lien entre ces deux variables, par l’intermédiaire des valeurs des couples. On définit pour cela un certain nombre d’outils :</p>
<div class="proof definition admonition" id="definition-19">
<p class="admonition-title"><span class="caption-number">Definition 35 </span> (Covariance)</p>
<div class="definition-content section" id="proof-content">
<p>La covariance de <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> est définie par :
<span class="math notranslate nohighlight">\(\sigma_{xy}=\frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )\left (y_i-\bar{y}\right )\)</span></p>
</div>
</div><span class="target" id="index-8"></span><div class="proof definition admonition" id="definition-20">
<span id="index-9"></span><p class="admonition-title"><span class="caption-number">Definition 36 </span> (Coefficient de corrélation)</p>
<div class="definition-content section" id="proof-content">
<p>Le coefficient de corrélation  de deux variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> est défini par
<span class="math notranslate nohighlight">\(r_{xy}=\frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}\)</span>.
Le coefficient de détermination est le carré du coefficient de corrélation.</p>
</div>
</div><p>Le coefficient de corrélation est donc la covariance normalisée par les écarts types marginaux des variables. Il mesure la dépendance linéaire entre <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>. Il est compris dans l’intervalle [-1,1] est est positif (resp. négatif) si les points sont alignés le long d’une droite croissante (resp. décroissante), d’autant plus grand en valeur absolue que la dépendance linéaire est vérifiée. Dans le cas où le coefficient est nul, il n’existe pas de dépendance linéaire.</p>
<p>Pour connaître plus précisément la relation linéaire qui lie <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>, on effectue une régression linéaire en calculant par exemple la droite de régression : si <span class="math notranslate nohighlight">\(y=a+bx\)</span>, il est facile de montrer que
<span class="math notranslate nohighlight">\(b=\frac{\sigma_{xy}}{\sigma_x^2}\quad\textrm{et}\quad a=\bar{y}-b\bar{x}\)</span></p>
<p>et la droite de régression s’écrit <span class="math notranslate nohighlight">\(y-\bar{y}=\frac{\sigma_{xy}}{\sigma_x^2}\left ( x-\bar{x}\right )\)</span>.</p>
<p>A partir de cette droite, on peut calculer les valeurs ajustées, obtenues à partir de la droite de régression : <span class="math notranslate nohighlight">\(y^*_i=a+bx_i\)</span>. Ce sont les valeurs théoriques des <span class="math notranslate nohighlight">\(y_i\)</span> et les résidus <span class="math notranslate nohighlight">\(e_i=y_i-y_i^*\)</span> représentent la partie inexpliquée des <span class="math notranslate nohighlight">\(y_i\)</span> par la droite de régression (ceux là même que l’on essaye de minimiser par la méthode des moindres carrés). Nous reviendrons dans le chapitre sur la régression sur l’analyse de ces résidus.</p>
</div>
<div class="section" id="cas-de-deux-variables-qualitatives">
<h4>Cas de deux variables qualitatives<a class="headerlink" href="#cas-de-deux-variables-qualitatives" title="Permalink to this headline">#</a></h4>
<p>Le couple est un couple de valeurs <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> où <span class="math notranslate nohighlight">\(x_i\)</span> et <span class="math notranslate nohighlight">\(y_i\)</span> prennent comme valeurs des modalités qualitatives. Notons <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span> et <span class="math notranslate nohighlight">\(y_1\cdots y_K\)</span> les valeurs distinctes prises.</p>
<p>Les données peuvent être regroupées sous la forme d’un <strong>tableau de contingence</strong> prenant la forme suivante :</p>
<span class="target" id="index-10"></span><p id="index-11"><span class="math notranslate nohighlight">\(\begin{array}{c|ccccc|c}
&amp;y_1&amp;\cdots&amp;y_k&amp;\cdots&amp;y_K&amp;total\\
\hline
x_1&amp;n_{11}&amp;\cdots&amp;n_{1k}&amp;\cdots&amp;n_{1K}&amp;n_{1.}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_j&amp;n_{j1}&amp;\cdots&amp;n_{jk}&amp;\cdots&amp;n_{jK}&amp;n_{j.}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_J&amp;n_{J1}&amp;\cdots&amp;n_{Jk}&amp;\cdots&amp;n_{JK}&amp;n_{J.}\\
\hline
total&amp;n_{.1}&amp;\cdots&amp;n_{.k}&amp;\cdots&amp;n_{.K}&amp;n\\
\end{array}
\)</span></p>
<p>où <span class="math notranslate nohighlight">\(n_{j.}\)</span> (resp <span class="math notranslate nohighlight">\(n_{.k}\)</span> )sont les effectifs marginaux représentant le nombre de fois où <span class="math notranslate nohighlight">\(x_j\)</span> (resp. <span class="math notranslate nohighlight">\(y_k\)</span>) apparaît, et <span class="math notranslate nohighlight">\(n_{jk}\)</span> le nombre d’apparition du couple <span class="math notranslate nohighlight">\((x_j,y_k)\)</span>.</p>
<p>Le tableau des fréquences <span class="math notranslate nohighlight">\(f_{jk}\)</span> s’obtient en divisant tous les effectifs par la taille <span class="math notranslate nohighlight">\(n\)</span> dans ce tableau.</p>
<p>Un tel tableau s’interprète toujours en comparant les fréquences en lignes ou les fréquences en colonnes (profils lignes ou colonnes), définies  respectivement par
<span class="math notranslate nohighlight">\(f_k^{(j)}= \frac{n_{jk}}{n_{j.}}=\frac{f_{jk}}{f_{j.}}\quad\textrm{ et }\quad f_j^{(k)}= \frac{n_{jk}}{n_{.k}}=\frac{f_{jk}}{f_{.k}}\)</span></p>
<p>Si l’on cherche un lien entre les variables, on construit un tableau d’effectifs théoriques qui représente la situation où les variables ne sont pas liées (indépendance). Ce tableau est constitué des effectifs
<span class="math notranslate nohighlight">\(n_{jk}^*=\frac{n_{j.}n_{.k}}{n}\)</span>
Les effectifs observés <span class="math notranslate nohighlight">\(n_{jk}\)</span> ont les mêmes marges que les <span class="math notranslate nohighlight">\(n_{jk}^*\)</span>, et les écarts à l’indépendance sont calculés par la différence <span class="math notranslate nohighlight">\(e_{jk}=n_{jk}-n_{jk}^*\)</span></p>
<p id="index-12">La dépendance du tableau se mesure au moyen du khi-deux défini par
<span class="math notranslate nohighlight">\(\chi^2_{obs}= \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{e_{jk}^2}{n_{jk}^*}\)</span>
qui peut être normalisé pour ne plus dépendre du nombre d’observations :
<span class="math notranslate nohighlight">\(\phi^2=\frac{\chi^2_{obs}}{n}\)</span></p>
<p>La construction du tableau des effectifs théoriques et sa comparaison au tableau des observations permet dans un premier temps de mettre en évidence les associations significatives entre modalités des deux variables. Pour cela, on calcule la contribution au <span class="math notranslate nohighlight">\(\chi^2\)</span> des modalités <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span> :</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\chi^2_{obs}}\frac{\left (n_{jk}-n_{j.}n_{.k}\right )^2}{n_{jk}^*}\]</div>
<p>Le signe de la différence <span class="math notranslate nohighlight">\(n_{jk}-n_{jk}^*\)</span> indique alors s’il y a une association positive ou négative entre les modalités <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Plus généralement, le <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est un indicateur de liaison entre les variables.  Dans le cas où <span class="math notranslate nohighlight">\(\chi^2_{obs}=0\)</span>, il y a indépendance. Pour rechercher la borne supérieure du khi-deux et voir dans quel cas elle est atteinte, on développe le carré et on obtient</p>
<div class="math notranslate nohighlight">
\[\chi^2_{obs} = n\left [\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}^2}{n_{j.}n_{.k}} -1\right ]\]</div>
<p>Comme <span class="math notranslate nohighlight">\(\frac{n_{jk}}{n_{.k}}\leq 1\)</span> on a <span class="math notranslate nohighlight">\( \frac{n_{jk}^2}{n_{j.}n_{.k}} \leq \frac{n_{jk}}{n_{.k}}\)</span> d’où</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{n_{jk}^2}{n_{j.}n_{.k}}\leq \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}}{n_{.k}} = \displaystyle\sum_{k=1}^K \frac{\displaystyle\sum_{j=1}^J n_{jk}}{n_{.k}}=\displaystyle\sum_{k=1}^K \frac{n_{.k}}{n_{.k}}=1\]</div>
<p>d’où <span class="math notranslate nohighlight">\(\chi^2_{obs}\leq n(K-1)\)</span>. On pourrait de même montrer que <span class="math notranslate nohighlight">\(\chi^2_{obs}\leq n(J-1)\)</span> et donc <span class="math notranslate nohighlight">\(\phi^2\leq min(J-1,K-1)\)</span>.</p>
<p>La borne est atteinte dans le cas de la dépendance fonctionnelle (si <span class="math notranslate nohighlight">\(\forall j \frac{n_{jk}}{n_{j.}}=1\)</span>, i.e. il n’existe qu’une case non nulle dans chaque ligne.)</p>
<p>A partir de ce khi-deux normalisé, on calcule finalement plusieurs coefficients permettant de mesurer l’indépendance, et parmi ceux-ci citons :</p>
<ul class="simple">
<li><p>le coefficient de Cramer:
<span class="math notranslate nohighlight">\(V=\sqrt{\frac{\phi^2}{min(J-1,K-1)}}\)</span></p></li>
<li><p>le coefficient de contingence de Pearson :
<span class="math notranslate nohighlight">\(C = \sqrt{\frac{\phi^2}{\phi^2 + 1}}\)</span></p></li>
<li><p>le coefficient de Tschuprow :
<span class="math notranslate nohighlight">\(T = \sqrt{\frac{\phi^2}{\sqrt{(K-1)(J-1)}}}\)</span></p></li>
</ul>
<p>Ces coefficients sont tous compris entre 0 (indépendance) et 1 (dépendance fonctionnelle). Pour estimer à partir de quelle valeur la dépendance fonctionnelle est significative, on procède de la manière suivante : si les <span class="math notranslate nohighlight">\(n\)</span> observations étaient prélevées dans une population où les variables sont indépendantes, on recherche les valeurs probables de <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>.</p>
<p>En s’appuyant sur la loi multinomiale et le test du <span class="math notranslate nohighlight">\(\chi^2\)</span>, on montre que <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est une réalisation d’une variable aléatoire <span class="math notranslate nohighlight">\(Z\)</span> suivant approximativement une loi <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-21">
<p class="admonition-title"><span class="caption-number">Remark 10 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(U_1\ldots U_p\)</span> <span class="math notranslate nohighlight">\(p\)</span> variables i.i.d de loi normale centrée réduite. On appelle loi du <span class="math notranslate nohighlight">\(\chi^2\)</span> à <span class="math notranslate nohighlight">\(p\)</span> degrés de liberté la loi de la variable <span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^pU_i^2\)</span>.</p>
</div>
</div><p>En effet, les <span class="math notranslate nohighlight">\(e_{jk}\)</span> sont liées par <span class="math notranslate nohighlight">\((K-1)(J-1)\)</span> relations linéaires puisqu’on estime les probabilités de réalisation de <span class="math notranslate nohighlight">\(x_j\)</span> et <span class="math notranslate nohighlight">\(y_k\)</span> respectivement par <span class="math notranslate nohighlight">\(n_{j,.}/n\)</span> et <span class="math notranslate nohighlight">\(n_{.k}/n\)</span>. Il suffit alors de fixer un risque d’erreur <span class="math notranslate nohighlight">\(\alpha\)</span> (une valeur qui, s’il y avait indépendance, n’aurait qu’une probabilité faible d’être dépassée), et on rejette l’hypothèse d’indépendance si <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>  est supérieur à la valeur critique qu’une variable <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span> a une probabilité <span class="math notranslate nohighlight">\(\alpha\)</span> de dépasser.
L’espérance d’un <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span> étant égale à son degré de liberté, <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est d’autant plus grand que le nombre de modalités <span class="math notranslate nohighlight">\(J\)</span> et/ou <span class="math notranslate nohighlight">\(K\)</span> est grand.</p>
<p>D’autres indices existent, qui ne dépendent pas de <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>, comme par exemple</p>
<p><span class="math notranslate nohighlight">\(\begin{equation} G^2 = 2\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J n_{jk} ln \left (\frac{ n_{jk}}{ n^*_{jk}} \right )\end{equation}\)</span></p>
<p>qui sous l’hypothèse d’indépendance suit une loi <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span>.</p>
</div>
<div class="section" id="cas-d-une-variable-quantitative-et-d-une-variable-qualitative">
<h4>Cas d’une variable quantitative et d’une variable qualitative<a class="headerlink" href="#cas-d-une-variable-quantitative-et-d-une-variable-qualitative" title="Permalink to this headline">#</a></h4>
<p>On s’intéresse ici au cas où les modalités <span class="math notranslate nohighlight">\(x_i\)</span> sont qualitatives, et où <span class="math notranslate nohighlight">\(y\)</span> est une variable quantitative, dont les modalités sont des réalisations d’une variable aléatoire <span class="math notranslate nohighlight">\(Y\)</span>.
Le rapport de corrélation théorique entre <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> est défini par</p>
<div class="math notranslate nohighlight">
\[\eta^2_{Y\mid x} = \frac{\sigma^2_{\mathbb{E}_{Y\mid x}}}{\sigma^2_Y}\]</div>
<p>Si <span class="math notranslate nohighlight">\(n_j\)</span> est le nombre d’observations de la modalité <span class="math notranslate nohighlight">\(x_j,j\in[\![1\,J]\!]\)</span>, <span class="math notranslate nohighlight">\(y_{ij}\)</span> la valeur de <span class="math notranslate nohighlight">\(Y\)</span> du <span class="math notranslate nohighlight">\(i^e\)</span> individu de la modalité <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(\bar{y}_1\ldots \bar{y}_J\)</span> sont les moyennes de <span class="math notranslate nohighlight">\(Y\)</span> pour ces modalités et <span class="math notranslate nohighlight">\(\bar{y}\)</span> la moyenne totale sur les <span class="math notranslate nohighlight">\(n\)</span> individus, le rapport de corrélation empirique est défini par</p>
<div class="math notranslate nohighlight">
\[e^2 = \frac{\frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2}{\sigma^2_y}\]</div>
<p>La quantité</p>
<p><span class="math notranslate nohighlight">\(\sigma^2_\cap = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\sigma_j^2\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\sigma_j^2 =  \frac{1}{n_j}\displaystyle\sum_{i=1}^{n_j}\left (y_{ij}-\bar{y}_j \right )^2\)</span>,  est appelée variance intra groupe (ou intra classe), et donne une idée de la variabilité à l’intérieur de chaque modalité.
La quantité
<span class="math notranslate nohighlight">\(\sigma_\cup = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2\)</span>
est la variance inter groupes (ou inter classes), et mesure la variabilité entre les différentes modalités.</p>
<p>Le théorème de décomposition de la variance (ou théorème de Huygens) affirme que la variance totale <span class="math notranslate nohighlight">\(\sigma^2_y\)</span>, calculée sans distinction de modalité s’écrit :
<span class="math notranslate nohighlight">\(\sigma^2_y = \sigma^2_\cap + \sigma^2_\cup\)</span></p>
<p>De ces définitions, on a alors :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e^2=0\)</span> si toutes les moyennes de <span class="math notranslate nohighlight">\(Y\)</span> sont égales, d’où l’absence de dépendance en moyenne</p></li>
<li><p><span class="math notranslate nohighlight">\(e^2=1\)</span> si tous les individus d’une modalité de <span class="math notranslate nohighlight">\(x\)</span> ont même valeur de <span class="math notranslate nohighlight">\(Y\)</span> et ceci pour chaque modalité</p></li>
<li><p><span class="math notranslate nohighlight">\(e^2\)</span> permet de comprendre, via le théorème de Huygens,  quelle variation est prédominante dans la variance totale. Ainsi par exemple, si la variable quantitative est la note d’un élève à un examen, et la variable qualitative son assiduité au cours correspondant, la variabilité entre les notes obtenues dans toute la promotion dépend de deux
facteurs : le fait que les étudiants assistent ou pas aux cours, et le fait qu’à assiduité
égale (i.e. à l’intérieur d’une même modalité d’assiduité) les étudiants n’ont pas le même niveau. <span class="math notranslate nohighlight">\(e^2\)</span>  permet alors de savoir lequel de ces deux facteurs est prédominant
pour expliquer la variabilité des notes dans toute la promotion.</p></li>
</ul>
<p>Pour déterminer à partir de quelle valeur <span class="math notranslate nohighlight">\(e^2\)</span> est significatif, on compare donc <span class="math notranslate nohighlight">\(\sigma^2_\cap\)</span> à <span class="math notranslate nohighlight">\(\sigma^2_\cup\)</span>. On peut montrer que si le rapport de corrélation théorique est nul, alors la variable <span class="math notranslate nohighlight">\(\frac{\left (\frac{e^2}{J-1}\right )}{\left (\frac{1-e^2}{n-J}\right )}\)</span> suit une loi de Fisher Snedecor, en supposant que les distributions conditionnelles de <span class="math notranslate nohighlight">\(Y\)</span> pour chaque modalité de <span class="math notranslate nohighlight">\(X\)</span> sont gaussiennes, de même espérance et de même variance.</p>
<div class="proof remark dropdown admonition" id="remark-22">
<p class="admonition-title"><span class="caption-number">Remark 11 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(U\)</span> et <span class="math notranslate nohighlight">\(V\)</span> deux variables aléatoires indépendantes suivant respectivement des lois <span class="math notranslate nohighlight">\(\chi^2_n\)</span> et <span class="math notranslate nohighlight">\(\chi^2_p\)</span>. On définit la loi de Fisher Snedecor par <span class="math notranslate nohighlight">\(F(n,p)=\frac{U/n}{V/P}\)</span>) <span class="math notranslate nohighlight">\(F(J-1,n-J)\)</span></p>
</div>
</div></div>
</div>
<div class="section" id="vers-une-analyse-multivariee">
<h3>Vers une analyse multivariée<a class="headerlink" href="#vers-une-analyse-multivariee" title="Permalink to this headline">#</a></h3>
<p>Bien évidemment, dans la majorité des cas, un individu sera décrit par <span class="math notranslate nohighlight">\(p\geq 2\)</span> variables. Si certains algorithmes de statistique descriptive multidimensionnelle sont abordés dans ce cours, il est néanmoins possible d’avoir une première approche exploratoire de ce cas.</p>
<div class="section" id="matrices-de-covariance-et-de-correlation">
<h4>Matrices de covariance et de corrélation<a class="headerlink" href="#matrices-de-covariance-et-de-correlation" title="Permalink to this headline">#</a></h4>
<p>La première idée, lorsque l’on a observé <span class="math notranslate nohighlight">\(d\)</span> variables sur <span class="math notranslate nohighlight">\(n\)</span> individus, est de calculer les <span class="math notranslate nohighlight">\(d\)</span> variances de ces variables, et les <span class="math notranslate nohighlight">\(\frac{p(p-1)}{2}\)</span> covariances. Ces mesures sont regroupées dans une matrice <span class="math notranslate nohighlight">\(p\times p\)</span>, symétrique, semi définie positive, appelée matrice de variance-covariance (ou matrice des covariances), et classiquement notée <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>.</p>
<p>De même, on peut former la matrice des corrélations entre les variables, à diagonale unité et symétrique. La matrice résultante, notée <span class="math notranslate nohighlight">\(\mathbf R\)</span>, est également semi définie positive et sa représentation graphique en fausses couleurs permet d’apprécier les dépendances linéaires entre variables.</p>
<p><img alt="" src="_images/batiments.png" /></p>
<p>Dans le cas de variables qualitatives, les coefficients de corrélation peuvent être remplacés par les coefficients de Cramer, de Tschuprow…</p>
</div>
<div class="section" id="tableaux-de-nuages">
<h4>Tableaux de nuages<a class="headerlink" href="#tableaux-de-nuages" title="Permalink to this headline">#</a></h4>
<p>On peut proposer à partir de là des représentations entre sous-ensembles de variables. La figure suivante propose un exemple de tels tableaux, parfois appelés splom (Scatter PLOt Matrix) :</p>
<ul class="simple">
<li><p>la partie triangulaire supérieure représente les nuages de points de couples de variables</p></li>
<li><p>la diagonale représente les histogrammes des variables</p></li>
<li><p>la partie trianglaire inférieure donne le coefficient de corrélation entre les deux variables, et une estimation de la densité de la distribution 2D des données</p></li>
</ul>
<p><img alt="" src="_images/batiments2.png" /></p>
</div>
<div class="section" id="tableaux-de-burt">
<h4>Tableaux de Burt<a class="headerlink" href="#tableaux-de-burt" title="Permalink to this headline">#</a></h4>
<p>Le tableau de Burt est une généralisation particulière de la table de contingence dans le cas où l’on étudie simultanément <span class="math notranslate nohighlight">\(p\)</span> variables qualitatives <span class="math notranslate nohighlight">\(X_1\ldots X_p\)</span>. Notons <span class="math notranslate nohighlight">\(c_j\)</span> le nombre de modalités de <span class="math notranslate nohighlight">\(X_j\)</span> et posons <span class="math notranslate nohighlight">\(c=\displaystyle\sum_{j=1}^p c_j\)</span>.</p>
<span class="target" id="index-13"></span><p id="index-14">Le tableau de Burt est une matrice carrée symétrique de taille <span class="math notranslate nohighlight">\(c\)</span>, constituée de <span class="math notranslate nohighlight">\(p^2\)</span> sous-matrices. Chacune des <span class="math notranslate nohighlight">\(p\)</span> sous-matrices diagonales est relative à l’une des <span class="math notranslate nohighlight">\(p\)</span> variables, la <span class="math notranslate nohighlight">\(j^e\)</span> étant carrée de taille <span class="math notranslate nohighlight">\(c_j\)</span>, diagonale, et de coefficients diagonaux les effectifs marginaux de <span class="math notranslate nohighlight">\(X_j\)</span>. La sous-matrice dans le bloc <span class="math notranslate nohighlight">\((k,l)\)</span> du tableau, <span class="math notranslate nohighlight">\(k\neq l\)</span>, est la table de contingence des variables <span class="math notranslate nohighlight">\(X_k\)</span> et <span class="math notranslate nohighlight">\(X_l\)</span>.</p>
</div>
</div>
</div>
<span id="document-selection"></span><div class="tex2jax_ignore mathjax_ignore section" id="selection-de-variables">
<h2>Sélection de variables<a class="headerlink" href="#selection-de-variables" title="Permalink to this headline">#</a></h2>
<p>On s’intéresse ici à <span class="math notranslate nohighlight">\(n\)</span> individus  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> décrits par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives ou caractéristiques (features), <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^d\)</span>. Avec l’avènement des Big Data, et la généralisation des capteurs, <span class="math notranslate nohighlight">\(d\)</span> peut être très grand (plusieurs milliers), et analyser telles quelles les données brutes devient difficile d’un point de vue calculatoire et interprétation. De plus, il est rare que les caractéristiques soient totalement utiles et indépendantes.</p>
<p>Une étape souvent utilisée en analyse de données consiste donc à prétraiter cet espace, par exemple pour :</p>
<ul class="simple">
<li><p>le transformer en un format compatible avec des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité temporelle des algorithmes qui seront utilisés</p></li>
<li><p>réduire la complexité spatiale du problème traité</p></li>
<li><p>découpler des variables et chercher les dépendances</p></li>
<li><p>introduire des a priori, ou des propriétés importantes pour les algorithmes (données centrées normées, descripteurs épars…)</p></li>
<li><p>permettre une interprétation plus intuitive et/ou graphique (<a class="reference internal" href="#tsne"><span class="std std-ref">figure 2</span></a>)</p></li>
</ul>
<div class="figure align-default" id="tsne">
<img alt="_images/tsne.png" src="_images/tsne.png" />
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Exemple de réduction de dimension (source: Maaten &amp; Hinton, 2008). Des images 28<span class="math notranslate nohighlight">\(\times\)</span> 28 de chiffres manuscrits sont représentées par un vecteur de 784 valeurs, puis transformés en vecteurs de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> pour les projeter dans le plan. La méthode utilisée permet d’optimiser la transformation de sorte à ce que les images représentant le même chiffre soient regroupées dans des nuages compacts.</span><a class="headerlink" href="#tsne" title="Permalink to this image">#</a></p>
</div>
<p>Deux stratégies peuvent alors être utilisées :</p>
<ol class="simple">
<li><p>sélectionner un sous-ensemble des variables initiales comme descripteurs des individus</p></li>
<li><p>calculer de nouveaux descripteurs à partir des variables initiales.</p></li>
</ol>
<p>Nous nous intéressons ici à la première approche, la seconde (extraction de caractéristiques) étant abordée pour une approche linéaire dans le chapitre sur l’analyse en composantes principales.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 12 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les méthodes d’extraction de caractéristiques peuvent être soit linéaires (on recherche des combinaisons linéaires des variables initiales  permettant d’optimiser un cerrtain critère), ou non linéaires (on parle également de manifold learning)</p>
</div>
</div><div class="section" id="definitions">
<h3>Définitions<a class="headerlink" href="#definitions" title="Permalink to this headline">#</a></h3>
<p>La sélection de caractéristiques consiste à choisir parmi les <span class="math notranslate nohighlight">\(d\)</span> descripteurs d’un ensemble d’individus <span class="math notranslate nohighlight">\(\mathbf x_i,i\in[\![1,n]\!]\)</span>, un sous-ensemble de  <span class="math notranslate nohighlight">\(t&lt;d\)</span>  caractéristiques jugées “les plus pertinentes”, les <span class="math notranslate nohighlight">\(d-t\)</span> restantes étant ignorées.</p>
<p>On note <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span> les <span class="math notranslate nohighlight">\(d\)</span> caractéristiques.  On note <span class="math notranslate nohighlight">\(Perf\)</span> une fonction qui permet d’évaluer un sous-ensemble de caractéristiques, et on suppose que <span class="math notranslate nohighlight">\(Perf\)</span> atteint son maximum pour le meilleur sous-ensemble de caractéristiques (“le plus pertinent”). Le problème de sélection se formule donc comme un problème d’optimisation</p>
<div class="math notranslate nohighlight">
\[\hat{F} = Arg\displaystyle\max_{U\subset F} Perf(U)\]</div>
<p>le cardinal <span class="math notranslate nohighlight">\(|\hat{F|}\)</span> de <span class="math notranslate nohighlight">\(\hat{F}\)</span> étant soit contrôlé par l’utilisateur, soit défini par l’algorithme de sélection.</p>
<p>On distingue alors trois stratégies :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(|\hat{F|}\)</span> est défini par l’utilisateur et l’optimisation s’effectue sur tous les sous-ensembles ayant ce cardinal</p></li>
<li><p>On connaît une mesure minimale de performance <span class="math notranslate nohighlight">\(\gamma\)</span>  et la sélection recherche le plus petit sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> dont la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> est supérieure ou égale à <span class="math notranslate nohighlight">\(\gamma\)</span></p></li>
<li><p>On cherche un compromis entre l’amélioration de la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> et la réduction de la taille du sous ensemble.</p></li>
</ul>
<p>La mesure de pertinence d’une caractéristique est donc au centre des algorithmes de sélection. Plusieurs définitions sont possibles, et nous dirons ici  qu’une caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> est :</p>
<ul class="simple">
<li><p>pertinente si son absence entraîne une détérioration significative de la performance de l’algorithme utilisé en aval (classification ou régression)</p></li>
<li><p>peu pertinente si elle n’est pas pertinente et s’il existe un sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> tel que la performance de <span class="math notranslate nohighlight">\(U\cup\{f_i\}\)</span> est significativement meilleure que la peformance de <span class="math notranslate nohighlight">\(U\)</span></p></li>
<li><p>non pertinente, si elle ne rentre pas dans les deux premières définitions. En général, ces caractéristiques sont supprimées.</p></li>
</ul>
</div>
<div class="section" id="caracteristiques-des-methodes-de-selection">
<h3>Caractéristiques des méthodes de sélection<a class="headerlink" href="#caracteristiques-des-methodes-de-selection" title="Permalink to this headline">#</a></h3>
<p>Une méthode de sélection basée sur l’optimisation de <span class="math notranslate nohighlight">\(Perf\)</span> utilise généralement trois étapes. Les  deux dernières sont itérées jusqu’à un test d’arrêt.</p>
<div class="section" id="initialisation">
<h4>Initialisation<a class="headerlink" href="#initialisation" title="Permalink to this headline">#</a></h4>
<p>L’initialisation consiste à choisir l’ensemble de départ des caractéristiques. Il peut s’agir de l’ensemble vide, de <span class="math notranslate nohighlight">\(F\)</span> tout entier, ou un sous-ensemble quelconque <span class="math notranslate nohighlight">\(U\subset F\)</span>.</p>
</div>
<div class="section" id="exploration-des-sous-ensembles">
<h4>Exploration des sous-ensembles<a class="headerlink" href="#exploration-des-sous-ensembles" title="Permalink to this headline">#</a></h4>
<p>A partir de cette initialisation, les stratégies d’exploration des sous-ensembles de caractéristiques se déclinent en trois catégories :</p>
<ol class="simple">
<li><p>génération exhaustive : tous les sous-ensembles de caractéristiques sont évalués. Si elle garantit de trouver la valeur optimale, cette méthode n’est que peu applicable dès que <span class="math notranslate nohighlight">\(|F|\)</span> devient important (<span class="math notranslate nohighlight">\(2^{|F|}\)</span> sous-ensembles possibles)</p></li>
<li><p>génération heuristique : une génération itérative est effectuée, chaque itération permettant de sélectionner ou de rejeter une ou plusieurs caractéristiques. La génération peut être ascendante (ajout de caractéristiques à partir de l’ensemble vide), descendante (suppression de caractéristiques à partir de <span class="math notranslate nohighlight">\(F\)</span>), ou mixte.</p></li>
<li><p>génération stochastique : pour un ensemble de données et une initialisation définie, une stratégie de recherche heuristique retourne toujours le même sous-ensemble, ce qui la rend très sensible au changement
de l’ensemble de données. La génération stochastique génère aléatoirement un nombre fini de sous-ensembles de caractéristiques afin de sélectionner le meilleur. La convergence est sous-optimale mais peut s’avérer préférable dans des algorithmes d’apprentissage, par exemple pour éviter le phénomène de surapprentissage.</p></li>
</ol>
</div>
<div class="section" id="evaluation-des-sous-ensembles">
<h4>Evaluation des sous-ensembles<a class="headerlink" href="#evaluation-des-sous-ensembles" title="Permalink to this headline">#</a></h4>
<div class="section" id="filtres">
<h5>Filtres<a class="headerlink" href="#filtres" title="Permalink to this headline">#</a></h5>
<p>Le critère d’évaluation utilisé évalue la pertinence d’une caractéristique selon des mesures
qui reposent sur les propriétés de données d’apprentissage.</p>
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> exemples  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> , on note <span class="math notranslate nohighlight">\(\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d\)</span>  une donnée d’apprentissage (la <span class="math notranslate nohighlight">\(j^e\)</span> caractéristique <span class="math notranslate nohighlight">\(f_j\)</span> ayant donc pour valeur <span class="math notranslate nohighlight">\(x_{ij}\)</span>) , d’étiquette <span class="math notranslate nohighlight">\(y_i\)</span> (en classification ou régression). Les méthodes de type filtres calculent un score pour évaluer le degré de pertinence de chacune des caractéristiques <span class="math notranslate nohighlight">\(f_i\)</span> , parmi lesquelles on peut citer</p>
<ul class="simple">
<li><p>Le critère de corrélation, utilisé en classification binaire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_i =\frac{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )\left (y_{k} -\mu_k\right )}{\sqrt{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )^2\displaystyle\sum_{k=1}^n\left (y_{k} -\mu_k\right )^2}}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mu_i\)</span> (resp. <span class="math notranslate nohighlight">\(\mu_k\)</span>) est la moyenne de la caractéristique <span class="math notranslate nohighlight">\(f_i\)</span> observée sur <span class="math notranslate nohighlight">\(\mathbf x_1\cdots \mathbf x_n\)</span> (resp. moyenne des étiquettes)</p>
<ul class="simple">
<li><p>Le critère de Fisher,  qui permet de mesurer dans un problème de classification à <span class="math notranslate nohighlight">\(C\)</span> classes le degré de séparabilité des classes à l’aide
d’une caractéristique donnée</p></li>
</ul>
<div class="math notranslate nohighlight">
\[F_i = \frac{\displaystyle\sum_{c=1}^C n_c\left (\mu_c^i-\mu_i \right )^2}{\displaystyle\sum_{c=1}^C n_c(\Sigma_c^i)^2}\]</div>
<p>où <span class="math notranslate nohighlight">\(n_c, \mu_c^i\)</span> et <span class="math notranslate nohighlight">\(\Sigma_c^i\)</span> sont l’effectif, la moyenne et l’écart-type de la caractéristique  <span class="math notranslate nohighlight">\(f_i\)</span> dans la classe <span class="math notranslate nohighlight">\(c\)</span></p>
<ul class="simple">
<li><p>l’information mutuelle</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(i) = \displaystyle\sum_{\mathbf x_i} \displaystyle\sum_{y}P(X=\mathbf x_i,Y=y)log\left ( \frac{P(X=\mathbf x_i,Y=y)}{P(X=\mathbf x_i)P(Y=y)}\right )\]</div>
<p>qui mesure la dépendance entre les distributions de deux populations. Ici <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont deux variables aléatoires dont les réalisations sont les valeurs de <span class="math notranslate nohighlight">\(f_i\)</span> et des étiquettes de classes. Les probabilités sont estimées de manière fréquentiste.</p>
<p>Dans l’exemple suivant, on choisit de garder <span class="math notranslate nohighlight">\(|\hat{F|}=2\)</span> descripteurs, en contrôlant la pertinence par l’information mutuelle en classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant : &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">mutual_info_classif</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données après : &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant :  (150, 4)
Taille des données après :  (150, 2)
Variables sélectionnées :  [False False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="methodes-enveloppantes">
<h5>Méthodes enveloppantes<a class="headerlink" href="#methodes-enveloppantes" title="Permalink to this headline">#</a></h5>
<p>Le principal inconvénient des approches précédentes est le fait qu’elles ignorent l’influence des caractéristiques sélectionnées sur la performance de l’algorithme à utiliser par la suite. Les méthodes de type enveloppantes (wrappers)  évaluent un sous-ensemble de caractéristiques par sa performance
de classification en utilisant un algorithme d’apprentissage.  Les sous-ensembles de caractéristiques sélectionnés par cette méthode sont bien adaptés à l’algorithme de classification utilisé, mais ils ne sont pas nécessairement pour un autre. De plus, la complexité de l’algorithme d’apprentissage rend ces méthodes coûteuses.</p>
<p>Les principales différences entre les filtres et les méthodes enveloppantes pour la sélection des caractéristiques sont les suivantes :</p>
<ul class="simple">
<li><p>Les filtres mesurent la pertinence des caractéristiques par leur corrélation avec la variable dépendante, tandis que les méthodes enveloppantes mesurent l’utilité d’un sous-ensemble de caractéristiques en entraînant un modèle sur celles-ci.</p></li>
<li><p>Les filtres sont beaucoup plus rapides que les méthodes enveloppantes car elles n’impliquent pas l’apprentissage des modèles. D’un autre côté, les méthodes enveloppantes sont également très coûteuses en termes de calcul.</p></li>
<li><p>Les filtres utilisent des méthodes statistiques pour l’évaluation d’un sous-ensemble de caractéristiques, tandis que les méthodes enveloppantes utilisent la validation croisée.</p></li>
<li><p>Les filtres peuvent échouer à trouver le meilleur sous-ensemble de caractéristiques dans de nombreuses occasions, mais les méthodes enveloppantes peuvent toujours fournir le meilleur sous-ensemble de caractéristiques.</p></li>
<li><p>L’utilisation d’un sous-ensemble de caractéristiques à partir des méthodes enveloppantes amène plus facilement au phénomène de surapprentissage</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les wrappers sélectionnent les caractéristiques en se fondant sur une estimation du risque réel.</p>
</div>
</div></div>
<div class="section" id="methodes-integrees">
<h5>Méthodes intégrées<a class="headerlink" href="#methodes-integrees" title="Permalink to this headline">#</a></h5>
<p>Les méthodes intégrées incluent la sélection de variables lors du processus d’apprentissage. Un tel mécanisme intégré pour la sélection des caractéristiques peut être trouvé, par
exemple, dans les algorithmes de type SVM,  AdaBoost  ou dans les
arbres de décision.</p>
</div>
</div>
</div>
<div class="section" id="quelques-methodes-de-selection">
<h3>Quelques méthodes de sélection<a class="headerlink" href="#quelques-methodes-de-selection" title="Permalink to this headline">#</a></h3>
<div class="section" id="suppression-des-descripteurs-a-variance-faible">
<h4>Suppression des descripteurs à variance faible<a class="headerlink" href="#suppression-des-descripteurs-a-variance-faible" title="Permalink to this headline">#</a></h4>
<p>Une première idée simple consiste à supprimer les descripteurs ayant une faible variance, ces derniers n’étant pas discriminants dans la définition des individus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Avant sélection, &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Après sélection, &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avant sélection,  (150, 4)
Après sélection,  (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithmes-de-selection-sequentielle">
<h4>Algorithmes de sélection séquentielle<a class="headerlink" href="#algorithmes-de-selection-sequentielle" title="Permalink to this headline">#</a></h4>
<p>Les algorithmes SFS (Sequential Forward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>) et SBS (Sequential Backward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>-rouge) ont été les premiers à être proposés. Ils utilisent des approches heuristiques de recherche en partant, pour la première, d’un ensemble de caractéristiques vide et pour la seconde de  <span class="math notranslate nohighlight">\(F\)</span> tout entier.</p>
<div class="proof algorithm admonition" id="SFS">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algorithmes SFS et SBS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span>, taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\leftarrow F\)</span></span>)</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\(d-T\)</span></span>)</p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\( |{F}|\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(|\hat{F}|\)</span></span>)</p>
<ol class="simple">
<li><p>Evaluer <span class="math notranslate nohighlight">\(\{f_j\}\cup \hat{F}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus \{f_j\}\)</span></span>)</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{max}\)</span> = meilleure caractéristique <span class="math notranslate nohighlight">\(\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(f_{min}\)</span>=moins bonne caractéristique</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow\hat{F}\cup\{f_{max}\}, F=F\setminus f_{max}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus\hat{F}f_{min}\)</span></span>)</p></li>
</ol>
</li>
</ol>
</div>
</div><p>L’étape d’évaluation utilise des données d’apprentissage : une heuristique évalue, sur un critère de performance, l’intérêt d’ajouter (ou de supprimer) le descripteur <span class="math notranslate nohighlight">\(f_i\)</span>.</p>
<p>Des variantes autour de ces algorithmes simples ont été proposées depuis et par exemple :</p>
<ul class="simple">
<li><p>il est possible à chaque itération d’inclure (ou d’exclure) un sous-ensemble de caractéristiques, plutôt qu’une seule (méthodes GSFS et GSBS)</p></li>
<li><p>on peut appliquer <span class="math notranslate nohighlight">\(p\)</span> fois SFS puis <span class="math notranslate nohighlight">\(q\)</span> fois SBS, de manière itérative, avec <span class="math notranslate nohighlight">\(p,q\)</span> des paramètres qui peuvent évoluer au cours des itérations (algorithme SFFS et SFBS)</p></li>
</ul>
<p>Dans l’exemple suivant, l’heuristique choisie est l’algorithme des 3 plus proches voisins et la mesure de performance sous-jacente est la mesure de validation croisée.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant sélection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données après sélection&quot;</span><span class="p">,</span><span class="n">sfs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Taille des données après sélection (150, 3)
Variables sélectionnées :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithme-focus">
<h4>Algorithme Focus<a class="headerlink" href="#algorithme-focus" title="Permalink to this headline">#</a></h4>
<p>L’algorithme de filtrage Focus (<a class="reference internal" href="#FOCUS">Algorithm 2</a>}) repose sur une recherche exhaustive sur <span class="math notranslate nohighlight">\(F\)</span> pour trouver le sous-ensemble le plus performant de taille optimale.</p>
<div class="proof algorithm admonition" id="FOCUS">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algorithme FOCUS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , taille de l’ensemble final  <span class="math notranslate nohighlight">\(T\)</span>, seuil <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>chaque sous-ensemble <span class="math notranslate nohighlight">\(S_i\)</span> de taille <span class="math notranslate nohighlight">\(i\)</span></p>
<ol class="simple">
<li><p>Si Inconsistance(A,<span class="math notranslate nohighlight">\(S_i\)</span>)&lt;<span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow S_i\)</span></p></li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\hat{F}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="algorithme-relief">
<h4>Algorithme relief<a class="headerlink" href="#algorithme-relief" title="Permalink to this headline">#</a></h4>
<p>La méthode relief en classification binaire (<a class="reference internal" href="#relief">Algorithm 3</a>), propose de calculer une mesure globale de la pertinence des caractéristiques en accumulant la différence des distances entre des exemples d’apprentissage choisis aléatoirement et leurs plus proches voisins de la même classe et de l’autre classe.</p>
<div class="proof algorithm admonition" id="relief">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Algorithme Relief)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , nombre d’itérations <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> un vecteur de poids des caractéristiques, <span class="math notranslate nohighlight">\(w_i\in[-1,1],i\in[\![1,d]\!]\)</span></p>
<ol class="simple">
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_i\leftarrow 0\)</span></p></li>
</ol>
</li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> à <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="simple">
<li><p>Choisir aléatoirement un exemple <span class="math notranslate nohighlight">\(\mathbf x_k\)</span></p></li>
<li><p>Chercher deux plus proches voisins de <span class="math notranslate nohighlight">\(\mathbf x_k\)</span>, l’un (<span class="math notranslate nohighlight">\(\mathbf x_p\)</span>) dans sa  classe, l’autre (<span class="math notranslate nohighlight">\(\mathbf x_q\)</span>) dans l’autre classe</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> à <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(w_j\leftarrow w_j+\frac{1}{nT}\left (|x_{kj} -x_{qj}|-|x_{kj} -x_{pj}| \right )\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="methode-sac">
<h4>Méthode SAC<a class="headerlink" href="#methode-sac" title="Permalink to this headline">#</a></h4>
<p>L’algorithme SAC (Selection Adaptative de Caractéristiques)  construit un ensemble de classifieurs (ou de régresseurs) <span class="math notranslate nohighlight">\((M_1\cdots M_d)\)</span> appris sur chacun des descripteurs et sélectionne les meilleurs par discrimination linéaire de Fisher. Pour ce faire, l’algorithme construit un vecteur dont les éléments sont les performances <span class="math notranslate nohighlight">\(Perf(M_i)\)</span> des modèles <span class="math notranslate nohighlight">\(M_i\)</span>, triés par ordre décroissant. Deux moyennes <span class="math notranslate nohighlight">\(m_1(i)\)</span> et <span class="math notranslate nohighlight">\(m_2(i)\)</span> sont calculées, qui représentent les deux moyennes de performance d’apprentissage qui ont une valeur respectivement plus grande (plus petite) que la performance du modèle <span class="math notranslate nohighlight">\(M_i\)</span> :</p>
<div class="math notranslate nohighlight">
\[m_1(i) = \frac{1}{i}\displaystyle\sum_{j=1}^i Perf (M_j)\textrm{ et } m_2(i) = \frac{1}{d-i}\displaystyle\sum_{j=i+1}^d Perf (M_j)\]</div>
<p>Deux variances des performances  <span class="math notranslate nohighlight">\(v_1^2(i)\)</span> et <span class="math notranslate nohighlight">\( v_2^2(i)\)</span> sont alors calculées à partir de ces moyennes, et le sous-ensemble de caractéristiques sélectionné est celui qui maximise le discriminant de Fisher</p>
<div class="math notranslate nohighlight">
\[\frac{|m_1(i)-m_2(i)|}{v_1^2(i)+v_2^2(i)}\]</div>
</div>
<div class="section" id="algorithme-rfe">
<h4>Algorithme RFE<a class="headerlink" href="#algorithme-rfe" title="Permalink to this headline">#</a></h4>
<p>L’algorithme RLE (Recusrive Feature Elimination) trie les descripteurs en analysant, localement, la sensibilité de la performance.
Étant donné un prédicteur <span class="math notranslate nohighlight">\(f\)</span> qui attribue des poids aux caractéristiques (par exemple, les coefficients d’un modèle linéaire), l’objectif de l’algorithme est de sélectionner les caractéristiques en considérant de manière récursive des ensembles de caractéristiques de plus en plus petits. Tout d’abord, le prédicteur <span class="math notranslate nohighlight">\(f\)</span> est entraîné sur l’ensemble initial de caractéristiques et l’importance de chaque caractéristique est calculée par un algorithme dédié (critère de Gini, entropie…). Les caractéristiques les moins importantes sont éliminées de l’ensemble actuel de caractéristiques. Cette procédure est répétée de manière récursive sur l’ensemble élagué jusqu’à ce que le nombre souhaité de caractéristiques à sélectionner soit finalement atteint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span> 
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des données avant sélection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables sélectionnées : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classement des variables : &quot;</span><span class="p">,</span><span class="n">s</span><span class="o">.</span><span class="n">ranking_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des données avant sélection (150, 4)
Variables sélectionnées :  [False False  True  True]
Classement des variables :  [2 3 1 1]
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-acp"></span><p>Les méthodes factorielles ont pour but de traiter et visualiser des données multidimensionnelles. La prise en compte simultanée de l’ensemble des variables est un problème difficile, rendu parfois plus simple car l’information apportée par les variables est redondante. Les méthodes factorielles visent alors à exploiter cette redondance pour tenter de remplacer les variables initiales par un nombre réduit de nouvelles variables, conservant au mieux l’information initiale.</p>
<p>Les principales méthodes de ce type incluent l’analyse factorielle des correspondances, l’analyse des correspondances multiples, l’analyse factorielle d’un tableau de distance (pour les tableaux de proximité) ou encore l’analyse factorielle discriminante. Ces méthodes sont proposées en annexe de ce cours.</p>
<p>Nous nous intéressons ici à une méthode de réduction de dimension linéaire sur données quantitatives, l’analyse en composantes principales.</p>
<div class="tex2jax_ignore mathjax_ignore section" id="analyse-en-composantes-principales">
<h2>Analyse en composantes principales<a class="headerlink" href="#analyse-en-composantes-principales" title="Permalink to this headline">#</a></h2>
<span class="target" id="index-0"></span><p id="index-1">Pour les données quantitatives, l’Analyse en Composantes Principales (ACP) est l’une des méthodes les plus utilisées. Elle considère que les nouvelles variables sont des combinaisons linéaires des variables initiales, non corrélées.</p>
<p><img alt="" src="_images/acpintro.png" /></p>
<p>Dans la suite, les données seront des tableaux <span class="math notranslate nohighlight">\(n\times d\)</span> de variables quantitatives, une ligne étant un individu, et les colonnes décrivant les paramètres mesurés. Les observations de <span class="math notranslate nohighlight">\(d\)</span> variables sur <span class="math notranslate nohighlight">\(n\)</span> individus sont donc rassemblées dans une matrice <span class="math notranslate nohighlight">\({\bf X}\in\mathcal{M}_{n,d}(\mathbb{R})\)</span> .  On notera <span class="math notranslate nohighlight">\(x^j\)</span> la j-ème variable, identifiée par la j-ème colonne <span class="math notranslate nohighlight">\({\bf X_{\bullet,j}}\)</span> de <span class="math notranslate nohighlight">\({\bf X}\)</span>, et <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> le i-ème individu (i.e. <span class="math notranslate nohighlight">\({\bf X_{i,\bullet}^T}\)</span>).</p>
<div class="section" id="principe-de-la-methode">
<h3>Principe de la méthode<a class="headerlink" href="#principe-de-la-methode" title="Permalink to this headline">#</a></h3>
</div>
<div class="section" id="pre-traitement-du-tableau">
<h3>Pré-traitement du tableau<a class="headerlink" href="#pre-traitement-du-tableau" title="Permalink to this headline">#</a></h3>
<p>En analyse en composantes principales, on raisonne souvent sur des variables centrées et/ou réduites.</p>
<div class="section" id="donnees-centrees">
<h4>Données centrées<a class="headerlink" href="#donnees-centrees" title="Permalink to this headline">#</a></h4>
<p>Notons <span class="math notranslate nohighlight">\(\mathbf{g} = \left ( \bar{x}^1\cdots \bar{x}^d\right )\)</span> le vecteur des moyennes arithmétiques de chaque variable (centre de gravité) :</p>
<p><span class="math notranslate nohighlight">\(\mathbf{g}={\bf X^TD\mathbf{1}}\)</span></p>
<p>où <span class="math notranslate nohighlight">\({\bf D}\)</span> est une matrice diagonale de poids,  chaque <span class="math notranslate nohighlight">\(d_{ii}\)</span> donnant l’importance de l’individu <span class="math notranslate nohighlight">\(i\)</span> dans les données (le plus souvent <span class="math notranslate nohighlight">\({\bf D}=\frac{1}{n}{ \mathbb{I}}\)</span>),  et <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> est le vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont toutes les composantes sont égales à 1. Le tableau <span class="math notranslate nohighlight">\({\bf Y}={\bf X}-\mathbf{1}\mathbf{g}^T=({ \mathbb{I}}-\mathbf{1}\mathbf{1}^T{\bf D}){\bf X}\)</span> est le tableau centré associé à <span class="math notranslate nohighlight">\({\bf X}\)</span>.</p>
</div>
<div class="section" id="donnees-reduites">
<h4>Données réduites<a class="headerlink" href="#donnees-reduites" title="Permalink to this headline">#</a></h4>
<p>La matrice de variance/covariance des données centrées est égale à
<span class="math notranslate nohighlight">\({\bf V} = {\bf X^TDX} - \mathbf{g}\mathbf{g^T} = {\bf Y^TDY}\)</span>.</p>
<p>Si on note <span class="math notranslate nohighlight">\({\bf D_{1/\sigma}}\)</span> la matrice diagonale des inverses des écarts-types des variables, alors  <span class="math notranslate nohighlight">\({\bf Z}={\bf YD_{1/\sigma}}\)</span>
est la matrice des données centrées réduites. La matrice <span class="math notranslate nohighlight">\({\bf R}={\bf D_{1/\sigma}VD_{1/\sigma}}={\bf Z^TDZ}\)</span>
est la matrice de corrélation des données et résume la structure des dépendances linéaires entre les <span class="math notranslate nohighlight">\(d\)</span> variables.</p>
</div>
</div>
<div class="section" id="projection-des-individus-sur-un-sous-espace">
<h3>Projection des individus sur un sous-espace<a class="headerlink" href="#projection-des-individus-sur-un-sous-espace" title="Permalink to this headline">#</a></h3>
<p>Le principe de la méthode est d’obtenir une représentation approchée du nuage des <span class="math notranslate nohighlight">\(n\)</span> individus dans un sous-espace <span class="math notranslate nohighlight">\(F_k\)</span> de dimension faible. Ceci s’effectue par un mécanisme de projection.</p>
<p>Le choix de l’espace de projection est dicté par le critère suivant, qui revient à déformer le moins possible les distances en projection : le sous-espace de dimension <span class="math notranslate nohighlight">\(k\)</span> recherché est tel que la moyenne des carrés des distances entre projections soit la plus grande possible. En définissant l’inertie d’un nuage de points comme la moyenne pondérée des carrés des distances au centre de gravité, le critère revient alors à maximiser l’inertie du nuage projeté sur <span class="math notranslate nohighlight">\(F_k\)</span>.</p>
<p>Soit <span class="math notranslate nohighlight">\({\bf P}\)</span> la projection orthogonale sur <span class="math notranslate nohighlight">\(F_k\)</span>. Le nuage de points projeté est associé au tableau <span class="math notranslate nohighlight">\({\bf XP^T}\)</span> puisque chaque individu <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> se projette sur <span class="math notranslate nohighlight">\(F_k\)</span> selon un vecteur colonne <span class="math notranslate nohighlight">\(\mathbf{Pe_i}\)</span> ou ligne <span class="math notranslate nohighlight">\(\mathbf{e_i P^T}\)</span>.</p>
<p>La matrice de variance du tableau <span class="math notranslate nohighlight">\({\bf XP^T}\)</span> est, dans le cas où les variables sont centrées :
<span class="math notranslate nohighlight">\({\bf (XP^T)^TD(XP^T) }= {\bf PVP^T}\)</span>.
L’inertie du nuage projeté est donc égale à <span class="math notranslate nohighlight">\(Tr({\bf PVP^TM})\)</span>, où <span class="math notranslate nohighlight">\({\bf M}\)</span> est une matrice symétrique définie positive de taille <span class="math notranslate nohighlight">\(d\)</span>, définissant la distance entre deux individus</p>
<p><span class="math notranslate nohighlight">\(d^2(\mathbf{e_i},\mathbf{e_j}) = (\mathbf{e_i}-\mathbf{e_j})^T{\bf M}(\mathbf{e_i}-\mathbf{e_j})\)</span></p>
<p>Mais</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
Tr({\bf PVP^TM})&amp;=&amp;Tr({\bf PVMP})\quad \textrm{car }{\bf P^TM}={\bf MP}\\
&amp;=&amp; Tr({\bf VMP^2})\quad \textrm{car }Tr({\bf AB})=Tr({\bf BA})\\
&amp;=&amp;Tr({\bf VMP})\quad \textrm{car } P\textrm{ est une projection}
\end{eqnarray*}\)</span></p>
<p>Le problème posé est donc de trouver la projection <span class="math notranslate nohighlight">\({\bf P}\)</span>, de rang <span class="math notranslate nohighlight">\(k\)</span> maximisant <span class="math notranslate nohighlight">\(Tr({\bf VMP})\)</span>. La projection <span class="math notranslate nohighlight">\({\bf P}\)</span> réalisant cette optimisation donnera alors <span class="math notranslate nohighlight">\(F_k\)</span>.</p>
<p>L’analyse en composantes principales consiste alors, de manière itérative, à chercher un sous-espace de dimension 1 d’inertie maximale, puis le sous-espace de dimension 1 orthogonal au précédent d’inertie maximale et ainsi de suite. Elle s’appuie sur le résultat suivant :</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 12 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(F_k\)</span> un sous-espace portant l’inertie maximale. Alors le sous-espace de dimension <span class="math notranslate nohighlight">\(k+1\)</span> portant l’inertie maximale est la somme directe de <span class="math notranslate nohighlight">\(F_k\)</span> et de la droite orthogonale à <span class="math notranslate nohighlight">\(F_k\)</span> portant l’inertie maximale.</p>
</div>
</div></div>
<div class="section" id="elements-principaux">
<h3>Elements principaux<a class="headerlink" href="#elements-principaux" title="Permalink to this headline">#</a></h3>
<span class="target" id="index-2"></span><div class="section" id="axes-principaux">
<span id="index-3"></span><h4>Axes principaux<a class="headerlink" href="#axes-principaux" title="Permalink to this headline">#</a></h4>
<p>Rechercher un sous-espace de dimension 1 d’inertie maximale revient à rechercher une droite de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> passant par le centre de gravité des données <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> maximisant l’inertie du nuage projeté sur cet axe. Soit <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> un vecteur directeur de cette droite. La projection orthogonale sur la droite est définie par la matrice de projection</p>
<p><span class="math notranslate nohighlight">\(\mathbf P=\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\)</span></p>
<p>L’inertie du nuage projeté sur <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span> vaut alors
<span class="math notranslate nohighlight">\(\begin{eqnarray*}
Tr({\bf VMP})&amp;=&amp;Tr\left ({\bf VM}\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\right )\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr({\bf VM}\mathbf{a}\mathbf{a^T}{\bf M})\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr(\mathbf{a^T}{\bf MVM}\mathbf{a})\quad \text{car } Tr(\mathbf{AB})=Tr(\mathbf{BA})\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}\mathbf{a^T}{\bf MVM}\mathbf{a}\quad \text{car } \mathbf{a^T}{\bf MVM}\mathbf{a}\in\mathbb{R}
\end{eqnarray*}\)</span></p>
<p>La matrice <span class="math notranslate nohighlight">\({\bf MVM}\)</span> est la matrice d’inertie du nuage (égale à la matrice de variance-covariance si <span class="math notranslate nohighlight">\({\bf M}=\mathbb I\)</span>).  Maximiser cette quantité revient à annuler sa dérivée par rapport à <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> d’où :</p>
<p><span class="math notranslate nohighlight">\(
\frac{d}{d\mathbf{a}}\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}}=\frac{(\mathbf{a^T}{\bf M}\mathbf{a})2{\bf MVM}\mathbf{a}-(\mathbf{a^T}{\bf MVM}\mathbf{a})2{\bf M}\mathbf{a}}{(\mathbf{a^T}{\bf M}\mathbf{a})^2}
\)</span>
et donc</p>
<p><span class="math notranslate nohighlight">\({\bf MVM}\mathbf{a}=\left (\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}} \right ){\bf M}\mathbf{a}\)</span></p>
<p>soit <span class="math notranslate nohighlight">\({\bf VM}\mathbf{a}=\lambda \mathbf{a}\)</span> car <span class="math notranslate nohighlight">\({\bf M}\)</span> est de rang plein. Donc <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf VM}\)</span>, et <span class="math notranslate nohighlight">\(\lambda\)</span> est la plus grande des valeurs propres de <span class="math notranslate nohighlight">\({\bf VM}\)</span>. Or <span class="math notranslate nohighlight">\({\bf M}\)</span> est symétrique, elle est diagonalisable sur une base de vecteurs propres orthonormés et on a le résultat suivant :</p>
<div class="proof theorem admonition" id="theorem-1">
<p class="admonition-title"><span class="caption-number">Theorem 13 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Le sous-espace <span class="math notranslate nohighlight">\(F_k\)</span> de dimension <span class="math notranslate nohighlight">\(k\)</span> portant l’inertie maximale est engendré par les <span class="math notranslate nohighlight">\(k\)</span> premiers vecteurs propres de <span class="math notranslate nohighlight">\({\bf VM}\)</span></p>
</div>
</div><p>Les droites portées par ces vecteurs propres sont les axes principaux. Dans la suite on supposera <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> <span class="math notranslate nohighlight">\(\mathbf M\)</span>-normé.</p>
</div>
<div class="section" id="facteurs-principaux">
<h4>Facteurs principaux<a class="headerlink" href="#facteurs-principaux" title="Permalink to this headline">#</a></h4>
<span class="target" id="index-4"></span><p id="index-5">On associe à  <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span> la forme linéaire <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>, coordonnée orthogonale sur l’axe <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span>. Le vecteur <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> définit une combinaison linéaire des variables descriptives <span class="math notranslate nohighlight">\(x^1\cdots x^d\)</span>. A l’axe principal <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est associé le facteur principal <span class="math notranslate nohighlight">\(\mathbf{u}=\mathbf{Ma}\)</span>. Puisque <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf VM}\)</span>, on peut alors écrire</p>
<p><span class="math notranslate nohighlight">\({\bf MVM}\mathbf{a}=\lambda {\bf M}\mathbf{a}\)</span></p>
<p>et donc <span class="math notranslate nohighlight">\({\bf MV}\mathbf{u}=\lambda \mathbf{u}\)</span>.  Les facteurs principaux sont donc les vecteurs propres de <span class="math notranslate nohighlight">\({\bf MV}\)</span></p>
</div>
<div class="section" id="composantes-principales">
<h4>Composantes principales<a class="headerlink" href="#composantes-principales" title="Permalink to this headline">#</a></h4>
<p>Les composantes principales sont les éléments de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> définis par <span class="math notranslate nohighlight">\(\mathbf{c_i}=\mathbf{Xu_i}\)</span>. Ce sont donc les vecteurs coordonnées des projections orthogonales des individus sur les axes propres <span class="math notranslate nohighlight">\(\mathbf{a_i}\)</span>.  Ce sont donc les combinaisons linéaires des <span class="math notranslate nohighlight">\(x^1\cdots x^p\)</span> de variance maximale sous la contrainte <span class="math notranslate nohighlight">\(\mathbf{u_i}^T{\bf M}\mathbf{u_i}=1\)</span>, et cette variance est égale à la valeur propre <span class="math notranslate nohighlight">\(\lambda_i\)</span> associée à <span class="math notranslate nohighlight">\(\mathbf{a_i}\)</span>.</p>
<p>En pratique, l’analyse en composantes principales consiste à calculer les <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> par diagonalisation de <span class="math notranslate nohighlight">\({\bf MV}\)</span>, puis à calculer les <span class="math notranslate nohighlight">\(\mathbf{c}=\mathbf{Xu}\)</span>. Le calcul explicite des vecteurs propres <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> n’a que peu d’intérêt.</p>
</div>
<div class="section" id="reconstitution">
<h4>Reconstitution<a class="headerlink" href="#reconstitution" title="Permalink to this headline">#</a></h4>
<p>Il est possible de reconstituer le tableau <span class="math notranslate nohighlight">\({\bf X}\)</span> centré des données (ou une approximation par une matrice de rang <span class="math notranslate nohighlight">\(k\)</span>) en utilisant les composantes. En effet, puisque <span class="math notranslate nohighlight">\(\mathbf{Xu_j}=\mathbf{c_j}\)</span> on a</p>
<p><span class="math notranslate nohighlight">\({\bf X}\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}} = \displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)</span></p>
<p>Mais <span class="math notranslate nohighlight">\(\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}}=\mathbb{I}\)</span>  car les <span class="math notranslate nohighlight">\(\mathbf{u_j}\)</span> sont orthonormés pour la métrique <span class="math notranslate nohighlight">\({\bf M^{-1}}\)</span> donc</p>
<p><span class="math notranslate nohighlight">\({\bf X}=\displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)</span>
et si l’on s’intéresse à l’approximation de <span class="math notranslate nohighlight">\({\bf X}\)</span> on ne somme que les <span class="math notranslate nohighlight">\(k\)</span> premiers termes.</p>
<p>A noter que lorsque <span class="math notranslate nohighlight">\({\bf M}=\mathbb{I}, {\bf X}= \displaystyle\sum_j\sqrt{\lambda_j}\mathbf{z_j}\mathbf{v_j^T}\)</span> où les <span class="math notranslate nohighlight">\(\mathbf{z_j}\)</span> sont les vecteurs propres unitaires de <span class="math notranslate nohighlight">\({\bf XX^T}\)</span> et les <span class="math notranslate nohighlight">\(\mathbf{v_j}\)</span> les vecteurs propres unitaires de <span class="math notranslate nohighlight">\({\bf X^TX}\)</span> (décomposition dite en valeurs singulières).</p>
</div>
</div>
<div class="section" id="interpretation-des-resultats">
<h3>Interprétation des résultats<a class="headerlink" href="#interpretation-des-resultats" title="Permalink to this headline">#</a></h3>
</div>
<div class="section" id="quelle-dimension-pour-f-k">
<h3>Quelle dimension pour <span class="math notranslate nohighlight">\(F_k\)</span> ?<a class="headerlink" href="#quelle-dimension-pour-f-k" title="Permalink to this headline">#</a></h3>
<p>Le but premier de l’ACP est de réduire la dimension pour permettre une visualisation efficace des données, tout en préservant l’information (ici représentée par la variance du nuage de points).  Il faut donc se doter d’outils permettant de répondre à la question : quelle dimension pour <span class="math notranslate nohighlight">\(F_k\)</span> ? Il n’y a pas de réponse théorique satisfaisante, l’essentiel étant d’avoir une représentation suffisamment expressive pour permettre une interprétation correcte du nuage de points.
En préambule, il convient de remarquer que la réduction de dimension ne sera possible que si les variables <span class="math notranslate nohighlight">\(x^1\cdots x^d\)</span> ne sont pas indépendantes.</p>
<div class="section" id="critere-theorique">
<h4>Critère théorique<a class="headerlink" href="#critere-theorique" title="Permalink to this headline">#</a></h4>
<p>On détermine ici si les valeurs propres sont significativement différentes entre elles à partir d’un certain rang : si la réponse est négative on conserve les
premières valeurs propres.</p>
<p>On fait l’hypothèse que les <span class="math notranslate nohighlight">\(n\)</span> individus proviennent d’un tirage aléatoire dans une population gaussienne  où <span class="math notranslate nohighlight">\(\lambda_{k+1}=\cdots =\lambda_{d}\)</span>. Si l’hypothèse est vérifiée, la moyenne arithmétique <span class="math notranslate nohighlight">\(\alpha\)</span> des <span class="math notranslate nohighlight">\(d-k\)</span> dernières valeurs propres et leur moyenne géométrique <span class="math notranslate nohighlight">\(\gamma\)</span> sont peu différentes. On admet que :</p>
<p><span class="math notranslate nohighlight">\(c=\left ( n-\frac{2p+11}{6}\right )(d-k) ln\frac{\alpha}{\gamma}\)</span>
suit une loi du <span class="math notranslate nohighlight">\(\chi^2\)</span> de degré de liberté <span class="math notranslate nohighlight">\(\frac{(d-k+2)(d-k-1)}{2}\)</span> et on rejette l’hypothèse d’égalité des <span class="math notranslate nohighlight">\(d-k\)</span> valeurs propres si <span class="math notranslate nohighlight">\(c\)</span> est trop grand.</p>
</div>
<div class="section" id="pourcentage-d-inertie">
<h4>Pourcentage d’inertie<a class="headerlink" href="#pourcentage-d-inertie" title="Permalink to this headline">#</a></h4>
<p>Le critère couramment utilisé est le pourcentage d’inertie totale expliquée, qui s’exprime sur les <span class="math notranslate nohighlight">\(k\)</span> premiers axes par :</p>
<div class="math notranslate nohighlight">
\[\frac{\displaystyle\sum_{j=1}^k \lambda_j}{\displaystyle\sum_{j=1}^d \lambda_j}\]</div>
<p>Un seuil par exemple de 90% d’inertie totale expliquée donne une valeur de <span class="math notranslate nohighlight">\(k\)</span> correspondante. Attention cependant, le pourcentage d’inertie doit faire intervenir le nombre de variables initiales.</p>
<p><img alt="" src="_images/scree.png" /></p>
</div>
<div class="section" id="mesures-locales">
<h4>Mesures locales<a class="headerlink" href="#mesures-locales" title="Permalink to this headline">#</a></h4>
<p>Le pourcentage d’inertie expliquée est un critère global qui doit être complété par d’autres considérations. Supposons que le plan <span class="math notranslate nohighlight">\(F_2\)</span> explique une part importante d’inertie, et que, en projection sur ce plan, deux individus soient très proches. Cette proximité peut être illusoire si les deux individus se trouvent éloignés dans l’orthogonal de <span class="math notranslate nohighlight">\(F_2\)</span>. Pour prendre en compte ce phénomène, il faut envisager pour chaque individu <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> la qualité de sa représentation, souvent exprimée par le <strong>cosinus de l’angle entre le plan principal et le vecteur <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span></strong>. Si ce cosinus est grand, <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> est voisin du plan, on peut  alors examiner la position de sa projection sur le plan par rapport à d’autres points.</p>
<p>Dans la figure suivante, <span class="math notranslate nohighlight">\({\bf e_i} \)</span> et <span class="math notranslate nohighlight">\({\bf e_j}\)</span> se projettent sur <span class="math notranslate nohighlight">\(F_2\)</span> en <span class="math notranslate nohighlight">\({\bf p}\)</span> mais sont éloignés dans <span class="math notranslate nohighlight">\(F_2^\perp\)</span>.</p>
<p><img alt="" src="_images/proj.png" /></p>
</div>
<div class="section" id="criteres-empiriques">
<h4>Critères empiriques<a class="headerlink" href="#criteres-empiriques" title="Permalink to this headline">#</a></h4>
<p id="index-6">Lorsqu’on travaille sur données centrées réduites, on retient les composantes principales correspondant à des valeurs propres supérieures à 1 (critère de Kaiser) : en effet les composantes principales <span class="math notranslate nohighlight">\(c_j\)</span> étant des combinaisons linéaires des <span class="math notranslate nohighlight">\(z-j\)</span> de variance maximale <span class="math notranslate nohighlight">\(V(c_j)=\lambda\)</span>, seules les composantes de variance supérieure à celle des variables initiales présentent un intérêt.</p>
</div>
</div>
<div class="section" id="interpretation-des-resultats-exemple">
<h3>Interprétation des résultats : exemple<a class="headerlink" href="#interpretation-des-resultats-exemple" title="Permalink to this headline">#</a></h3>
<p>Une analyse en composantes principales est réalisée sur un jeu de données composé de <span class="math notranslate nohighlight">\(d\)</span>=9 indicateurs de qualité pour <span class="math notranslate nohighlight">\(n\)</span>=329 villes américaines. Les paragraphes suivants sont illustrés par ces données.</p>
<div class="section" id="correlation-variables-facteurs">
<h4>Corrélation variables-facteurs<a class="headerlink" href="#correlation-variables-facteurs" title="Permalink to this headline">#</a></h4>
<p>Pour donner du sens aux composantes principales <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, il faut les relier aux variables initiales <span class="math notranslate nohighlight">\(x^j\)</span> en calculant les coefficients de corrélation linéaire  <span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)\)</span> et en seuillant ces coefficients en valeur absolue.</p>
<p>Lorsque l’on travaille sur des données centrées réduites (métrique <span class="math notranslate nohighlight">\(\mathbf D_{1/\sigma}\)</span>), le calcul de <span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)\)</span> se réduit à</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\frac{\mathbf{c}^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}\)</span></p>
<p>Or <span class="math notranslate nohighlight">\(\mathbf{c}=Z\mathbf{u}\)</span> où <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>, facteur principal associé à <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, est vecteur propre de la matrice de corrélation <span class="math notranslate nohighlight">\(\mathbf R\)</span> associé à <span class="math notranslate nohighlight">\(\lambda\)</span>. Donc</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\frac{\mathbf{u}^T\mathbf Z^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}=\frac{(\mathbf{z^j})^T\mathbf D\mathbf Z\mathbf{u}}{\sqrt{\lambda}}\)</span></p>
<p><span class="math notranslate nohighlight">\((\mathbf{z^j})^T\mathbf D\mathbf Z\)</span> est la <span class="math notranslate nohighlight">\(j^e\)</span> ligne de <span class="math notranslate nohighlight">\(\mathbf Z^T\mathbf D\mathbf Z=\mathbf R\)</span> donc <span class="math notranslate nohighlight">\((\mathbf{z^j})^T\mathbf D \mathbf Z \mathbf{u}\)</span> est la <span class="math notranslate nohighlight">\(j^e\)</span> composante de <span class="math notranslate nohighlight">\(\mathbf R\mathbf{u}=\lambda \mathbf{u}\)</span> d’où</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\sqrt{\lambda}u_j\)</span></p>
<p>Ces calculs s’effectuent pour chaque composante principale. Pour un couple de composantes principales <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> et <span class="math notranslate nohighlight">\(\mathbf{c_2}\)</span> par exemple on représente fréquemment les corrélations sur une figure appelée « cercle des corrélations» où chaque variable <span class="math notranslate nohighlight">\(x^j\)</span> est repérée par un point d’abscisse <span class="math notranslate nohighlight">\(r(\mathbf{c_1},x^j)\)</span> et d’ordonnée <span class="math notranslate nohighlight">\(r(\mathbf{c_2},x^j)\)</span>.</p>
<p><img alt="" src="_images/cercle.png" /></p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 14 </span></p>
<div class="remark-content section" id="proof-content">
<p>Attention de ne pas interpréter des proximités entre points variables, si ceux-ci ne sont pas proches de la circonférence.</p>
</div>
</div><p>Notons que dans le cas de la métrique <span class="math notranslate nohighlight">\(D_{1/\sigma}\)</span>, le cercle des corrélations est la projection de l’ensemble des variables centrées-réduites sur le sous-espace engendré par <span class="math notranslate nohighlight">\(\mathbf{c_1},\mathbf{c_2}\)</span>. En ce sens, le cercle de corrélation est le pendant, dans l’espace des variables, de la projection des individus sur le premier plan principal.</p>
</div>
</div>
<div class="section" id="positionnement-des-individus">
<h3>Positionnement des individus<a class="headerlink" href="#positionnement-des-individus" title="Permalink to this headline">#</a></h3>
<p>Dire que <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> est très corrélée à <span class="math notranslate nohighlight">\(x^j\)</span> signifie que les individus ayant une forte coordonnée positive sur l’axe 1 sont caractérisés par une valeur de <span class="math notranslate nohighlight">\(x^j\)</span> nettement supérieure à la moyenne.</p>
<p>Il est très utile aussi de calculer pour chaque axe la contribution apportée par les divers individus à cet axe. Si <span class="math notranslate nohighlight">\(c_{ki}\)</span> est la valeur de la composante <span class="math notranslate nohighlight">\(k\)</span> pour le <span class="math notranslate nohighlight">\(i^e\)</span> individu, alors par construction</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=1}^np_ic_{ki}^2=\lambda_k\]</div>
<p>où <span class="math notranslate nohighlight">\(p_i\)</span> est le poids de l’individu <span class="math notranslate nohighlight">\(i\)</span>. On appelle alors contribution de l’individu <span class="math notranslate nohighlight">\(i\)</span> à la composante <span class="math notranslate nohighlight">\(\mathbf{c_k}\)</span> la quantité <span class="math notranslate nohighlight">\(\frac{p_ic_{ki}^2}{\lambda_k}\)</span>. Dans le cas où le poids est différent de <span class="math notranslate nohighlight">\(1/n\)</span> (certains individus sont “plus importants” que d’autres), la contribution est riche d’interprétation. Dans le cas contraire, elle n’apporte rien de plus que les coordonnées de l’individu.</p>
<p>On peut alors positionner les individus sur les sous-espaces des premières composantes principales (plans factoriels). La figure suivante présente le positionnement de 329 villes américaines, où les 9 variables de qualité de vie précédentes ont été mesurées. Par soucis de lisibilité, seul les villes qui contribuent le plus à la création de la première composante principale ont leurs noms inscrits.</p>
<p><img alt="" src="_images/individus.png" /></p>
<p>On peut également superposer les deux informations précédentes pour corréler le positionnement des villes selon les variables originales. La figure suivante présente les 329 villes précédentes, plongées dans <span class="math notranslate nohighlight">\(F_3\)</span>, les  anciennes variables étant matérialisées par des vecteurs dont la direction et la norme indiquent à quel point chaque variable contribue aux 3 premières composantes principales.</p>
<p><img alt="" src="_images/biplot.png" /></p>
<p>Il n’est pas souhaitable, et ceci surtout pour les premières composantes,  qu’un individu ait une contribution excessive car cela serait un facteur d’instabilité, le fait de retirer cet individu modifiant profondément le résultat de l’analyse. Si ce cas se produisait il y aurait intérêt à effectuer l’analyse en éliminant cet individu puis en le mettant en élément supplémentaire, s’il ne s’agit pas d’une donnée erronée qui a été ainsi mise en évidence.</p>
</div>
<div class="section" id="facteur-de-taille-facteur-de-forme">
<h3>Facteur de taille, facteur de forme<a class="headerlink" href="#facteur-de-taille-facteur-de-forme" title="Permalink to this headline">#</a></h3>
<p>Le théorème de Frobenius stipule qu’une matrice symétrique n’ayant que des termes positifs admet un premier vecteur propre dont toutes les composantes sont de même signe. Si ce signe est positif, la première composante est alors corrélée positivement avec toutes les variables et les individus sont rangés sur l’axe 1 par valeurs croissantes de l’ensemble des variables. Si de plus les corrélations entre variables sont du même ordre de grandeur, la première composante principale est proportionnelle à la moyenne des variables initiales. Cette première composante définit alors un facteur de taille.</p>
<p>La deuxième composante principale différencie alors des individus de “taille” semblable, on l’appelle souvent facteur de forme.</p>
</div>
<div class="section" id="ajout-de-variable-et-ou-d-individu">
<h3>Ajout de variable et ou d’individu<a class="headerlink" href="#ajout-de-variable-et-ou-d-individu" title="Permalink to this headline">#</a></h3>
<p>Toutes les interprétations précédentes expliquent les résultats à l’aide des données initiales, qui ont permis de les calculer. On risque alors de prendre pour une propriété intrinsèque des données un simple artefact de la méthode (par exemple il existe de fortes corrélations entre la première composante principale et certaines variables, puisque <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> maximise <span class="math notranslate nohighlight">\(\sum_j r^2(\mathbf{c},x^j)\)</span>).</p>
<p>En revanche une forte corrélation entre une composante principale et une variable qui n’a pas servi à l’analyse sera significative. D’où la pratique courante de partager en deux groupes l’ensemble des variables : d’une part les variables actives qui servent à déterminer les axes principaux, d’autre part les variables passives ou supplémentaires que l’on relie a posteriori aux composantes principales. On distingue alors les variables supplémentaires suivant leur type, numérique (à placer dans les cercles de corrélation) ou qualitative (donnée d’une partition des <span class="math notranslate nohighlight">\(n\)</span> individus en <span class="math notranslate nohighlight">\(k\)</span> classes).</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<p>On étudie les consommations annuelles en 1972, exprimées en devises, de 8 denrées alimentaires (les variables), les individus étant 8 catégories socio-professionnelles (CSP) . Les données sont des moyennes par CSP :</p>
<p><span class="math notranslate nohighlight">\(
\begin{array}{|c|cccccccc|}
\hline
  &amp;PAO  &amp;PAA  &amp;VIO&amp; VIA&amp;  POT&amp;  LEC &amp;RAI&amp; PLP\\
\hline
AGRI  &amp;167  &amp;1  &amp;163&amp; 23&amp; 41  &amp;8&amp; 6 &amp;6\\
SAAG  &amp;162&amp; 2 &amp;141&amp; 12  &amp;40 &amp;12&amp;  4&amp;  15\\
PRIN  &amp;119&amp; 6 &amp;69 &amp;56 &amp;39&amp;  5 &amp;13 &amp;41\\
CSUP  &amp;87 &amp;11 &amp;63 &amp;111&amp; 27&amp; 3 &amp;18 &amp;39\\
CMOY  &amp;103  &amp;5  &amp;68 &amp;77 &amp;32&amp;  4 &amp;11 &amp;30\\
EMPL  &amp;111  &amp;4  &amp;72 &amp;66 &amp;34&amp;  6 &amp;10 &amp;28\\
OUVR  &amp;130  &amp;3  &amp;76 &amp;52 &amp;43&amp;  7 &amp;7  &amp;16\\
INAC  &amp;138  &amp;7  &amp;117  &amp;74&amp;  53&amp; 8 &amp;12 &amp;20\\
\hline
\end{array}
\)</span></p>
<p>avec les notations suivantes :</p>
<p>AGRI = Exploitants agricoles, SAAG= Salariés agricoles,   PRIN = Professions indépendantes, CSUP = Cadres supérieurs, CMOY= Cadres moyens, EMPL= Employés, OUVR = Ouvriers, INAC = Inactifs.</p>
<p>et</p>
<p>PAO = Pain ordinaire, PAA = Autre pain, VIO = Vin ordinaire, VIA=Autre vin, POT= Pommes de terre, LEC=Légumes secs, RAI=Raisin de table, PLP= Plats préparés.</p>
<p>La matrice de corrélation des variables est alors</p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}
   1.0000   &amp;  -.7737    &amp; 0.9262    &amp; -.9058    &amp; 0.6564  &amp;   0.8886   &amp;  -.8334  &amp;   -.8558\\
    -.7737    &amp; 1.0000    &amp; -.6040    &amp; 0.9044    &amp; -.3329    &amp; -.6734    &amp; 0.9588    &amp; 0.7712\\
   0.9262    &amp; -.6040    &amp; 1.0000    &amp; -.7502    &amp; 0.5171    &amp; 0.7917   &amp;  -.6690     &amp;-.8280\\
  -.9058    &amp; 0.9044    &amp; -.7502    &amp; 1.0000    &amp; -.4186    &amp; -.8386    &amp; 0.9239     &amp;0.7198\\
    0.6564   &amp;  -.3329    &amp; 0.5171    &amp; -.4186    &amp; 1.0000   &amp;  0.6029   &amp;  -.4099    &amp; -.5540\\
  0.8886   &amp;  -.6734    &amp; 0.7917   &amp;  -.8386    &amp; 0.6029   &amp;  1.0000   &amp;  -.8245    &amp; -.7509\\
  -.8334    &amp; 0.9588    &amp; -.6690    &amp; 0.9239    &amp; -.4099    &amp; -.8245    &amp; 1.0000     &amp;0.8344\\
   -.8558    &amp; 0.7712   &amp;  -.8280   &amp;  0.7198   &amp;  -.5540    &amp; -.7509  &amp;   0.8344    &amp; 1.0000\\
\end{pmatrix}\)</span></p>
<p>et son analyse spectrale donne</p>
<p><span class="math notranslate nohighlight">\(\begin{array}{|c||c|c|c|}
\hline
                      &amp;    \textrm{Valeur propre}  &amp;      \textrm{Variance expliquée}  &amp;  \textrm{Variance cumulative expliquée}\\
\hline
                     1  &amp;  6.20794684      &amp;      0.7760  &amp;      0.7760\\
                     2   &amp; 0.87968139      &amp;      0.1100    &amp;    0.8860\\
                     3    &amp;0.41596112    &amp;        0.0520      &amp;  0.9379\\
                     4    &amp;0.30645467    &amp;        0.0383      &amp;  0.9763\\
                     5    &amp;0.16844150    &amp;        0.0211      &amp;  0.9973\\
                     6    &amp;0.01806771    &amp;       0.0023       &amp; 0.9996\\
                     7    &amp;0.00344677    &amp;       0.0004       &amp; 1.0000\\
                     8    &amp;0.00000000        &amp;              0.0000      &amp;  1.0000\\
\hline
\end{array}\)</span></p>
<p>Le critère de Kaiser  conduit à sélectionner un seul axe, qui retient 77% de l’inertie totale. L’axe 2 retenant 11% de l’inertie, il peut être  intéressant de le rajouter à l’étude pour expliquer près de 90% de la variance des données. Les suivantes représentent les variables et les individus dans le plan des deux premiers vecteurs propres.</p>
<p><img alt="" src="_images/ex1.png" /></p>
<p>L’interprétation de ce plan se fait séquentiellement, pour chaque axe et chaque nuage de points, en regardant les contributions à la formation des axes:</p>
<ul class="simple">
<li><p>Axe 1 :</p></li>
</ul>
<p>1- <strong>Variables</strong> :  les variables contribuant le plus à la formation de l’axe 1 sont celles dont les coordonnées sur cet axe sont proches de 1 en valeur absolue.
PAA et VIO sont très proches de la contribution moyenne, on les intègre donc dans l’interprétation de l’axe si elles vont dans le sens de l’interprétation que l’on peut en faire, sans elles. L’axe 1 oppose les individus consommant du pain ordinaire, des légumes secs (et éventuellement du vin ordinaire) à ceux qui consomment du raisin, du vin (éventuellement du pain) plus sophistiqué et des plats préparés. L’axe 1, et donc la première composante principale, mesure la répartition entre aliments ordinaires bon marché et aliments plus recherchés.</p>
<p>Toutes les variables sont bien représentées sur l’axe (la qualité de représentation est égale à la coordonnée au carré). D’un point de vue graphique, une variable bien représentée est proche du bord du cercle des corrélation et à proximité de l’axe. La première composante principale explique donc correctement tous les types de consommations alimentaires.</p>
<p>2- <strong>Individus</strong> : de même, les individus contribuant le plus à la formation de l’axe 1 sont ceux dont les coordonnées sur cet axe sont les plus élevées en valeur absolue. Le premier axe met donc en opposition les agriculteurs et les cadres supérieurs quant à leurs habitudes alimentaires. Les autres catégories socio-professionnelles, assez bien représentées sur l’axe à l’exception des inactifs (cf. contributions des individus sur l’axe 1), s’échelonnent suivant la hiérarchie habituelle. Elles sont bien expliquées par l’axe.</p>
<ul class="simple">
<li><p>Axe 2 :</p></li>
</ul>
<p>1- <strong>Variables</strong> : L’axe 2 est défini par les variables POT et PAA. Compte tenu de la différence de contribution existant entre ces deux variables, de la contribution élevée de POT (55%), et de la qualité de représentation moyenne de PAA, la deuxième composante principale peut être considérée comme essentiellement liée à la consommation de pommes de terre. Les variables, à l’exception de POT et de PAA (dans une moindre mesure) sont assez mal représentées sur l’axe. La deuxième composante principale n’explique donc qu’un aspect très particulier de la consommation alimentaire.</p>
<p>2- <strong>Individus</strong> : Pour repérer les individus ayant une contribution significative, on compare les coordonnées des individus sur l’axe 2, à la racine de la deuxième valeur propre  =0,94, le signe donnant le sens de contribution.</p>
<p>L’axe 1 reflète donc l’opposition qui existe entre les catégories socio-professionnelles dans leur alimentation, opposant les CSP modestes qui consomment des produits basiques aux catégories favorisées qui consomment des produits plus recherchés. L’axe 2 reflète quant à lui la particularité des inactifs quant à leur alimentation, fortement composée de pommes de terre (un retour aux données d’origine vient confirmer cette conclusion).</p>
</div>
<div class="section" id="implementation">
<h3>Implémentation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h3>
<p>De nombreuses librairies Python permettent d’utiliser facilement l’ACP, notamment <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> qui propose une méthode <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA">PCA</a>.</p>
<p>Nous proposons ici d’implémenter entièrement l’ACP, pour bien comprendre les mécanismes de cette approche.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 

<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Données</span>
<span class="n">vins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/vins.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">cat_vins</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">vins</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">% a</span><span class="s1">lcool&#39;</span><span class="p">,</span> <span class="s1">&#39;acide malique&#39;</span><span class="p">,</span> <span class="s1">&#39;cendres&#39;</span><span class="p">,</span> <span class="s1">&#39;alcalinité&#39;</span><span class="p">,</span> <span class="s1">&#39;magnésium&#39;</span><span class="p">,</span> <span class="s1">&#39;phénols&#39;</span> <span class="p">,</span> 
                <span class="s1">&#39;flavonoïdes&#39;</span><span class="p">,</span> <span class="s1">&#39;non flavanoïdes&#39;</span><span class="p">,</span> <span class="s1">&#39;proanthocyanidines&#39;</span><span class="p">,</span> <span class="s1">&#39;couleur&#39;</span><span class="p">,</span> <span class="s1">&#39;teinte&#39;</span><span class="p">,</span> 
                <span class="s1">&#39;OD280/OD315&#39;</span><span class="p">,</span><span class="s1">&#39;proline&#39;</span><span class="p">]</span>

<span class="c1"># Affichage d&#39;un tableau</span>
<span class="k">def</span> <span class="nf">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">tab</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CP&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span>
    <span class="n">r</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; </span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;  </span><span class="si">%8.8s</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">ind</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">%.2f</span><span class="s2"> </span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">%.2f</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">r</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="preparation-des-donnees">
<h3>Préparation des données<a class="headerlink" href="#preparation-des-donnees" title="Permalink to this headline">#</a></h3>
<div class="section" id="id1">
<h4>Données centrées<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(g=X^TD{\bf 1}\)</span> = Vecteur des moyennes arithmétiques de chaque variable</p>
<p><span class="math notranslate nohighlight">\(D=\frac{1}{n}I\)</span> = Matrice diagonale de poids, chaque <span class="math notranslate nohighlight">\(d_{ii}\)</span> donnant l’importance de l’individu <span class="math notranslate nohighlight">\(i\)</span> dans les données</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">un</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">un</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;g = </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>g = 
 [[1.30006180e+01]
 [2.33634831e+00]
 [2.36651685e+00]
 [1.94949438e+01]
 [9.97415730e+01]
 [2.29511236e+00]
 [2.02926966e+00]
 [3.61853933e-01]
 [1.59089888e+00]
 [5.05808988e+00]
 [9.57449438e-01]
 [2.61168539e+00]
 [7.46893258e+02]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(Y = X - {\bf 1} g^T = (I - {\bf 11}^TD)X\)</span> = Tableau centré associé à <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">un</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id2">
<h4>Données réduites<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(V=X^TDX-gg^T=Y^TDY\)</span> = Matrice de variance/covariance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Yt</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(Z = Y D_{1/\sigma}\)</span> = Matrice des données centrées réduites</p>
<p><span class="math notranslate nohighlight">\(R = D_{1/\sigma}VD_{1/\sigma} = Z^T  D  Z\)</span> = Matrice (symétrique) de variance/covariance des données centrées réduites.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Xt</span><span class="p">]</span>
<span class="n">i_sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="o">/</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">]</span>
<span class="n">D_sigma</span> <span class="o">=</span> <span class="n">i_sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">D_sigma</span><span class="p">,</span> <span class="n">V</span><span class="p">),</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Matrice de corrélation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/acp_9_0.png" src="_images/acp_9_0.png" />
</div>
</div>
</div>
<div class="section" id="inertie-du-nuage-de-points">
<h4>Inertie du nuage de points<a class="headerlink" href="#inertie-du-nuage-de-points" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(M\)</span> est une matrice symétrique définie positive correspondant à la métrique</p>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(M=D^2_{1/\sigma}\)</span> on calcule <span class="math notranslate nohighlight">\( \frac{1}{n}\displaystyle\sum_{i=1}^n (e_i - g)^T M (e_i-g) = \frac{1}{n}\displaystyle\sum_{i=1}^n (y_i)^T M y_i = Tr(VM)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(M=I\)</span> on calcule <span class="math notranslate nohighlight">\( \frac{1}{n}\displaystyle\sum_{i=1}^n (z_i)^T M z_i = Tr(RM)\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calcul_inertie_somme</span> <span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">inertie</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">inertie</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> 
    <span class="k">return</span> <span class="n">inertie</span> <span class="o">/</span> <span class="n">n</span>
    
<span class="k">def</span> <span class="nf">calcul_inertie_trace</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

<span class="c1"># Si les données sont centrées mais pas encore réduites  on travaille avec Y et V</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">D_sigma</span><span class="p">,</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_somme</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_trace</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

<span class="c1"># Si les données sont centrées réduites, on travaille avec Z et R</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_somme</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_trace</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12.999999999999995
13.000000000000004
12.999999999999998
13.000000000000004
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="analyse-spectrale">
<h3>Analyse spectrale<a class="headerlink" href="#analyse-spectrale" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigenvalues</span><span class="p">,</span><span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="p">[</span><span class="n">eigenvectors</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="calcul-des-composantes-principales">
<h4>Calcul des composantes principales<a class="headerlink" href="#calcul-des-composantes-principales" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pourcentage-d-inertie-expliquee-par-un-axe">
<h4>Pourcentage d’inertie expliquée par un axe<a class="headerlink" href="#pourcentage-d-inertie-expliquee-par-un-axe" title="Permalink to this headline">#</a></h4>
<p>Pourcentage d’inertie cumulée expliquée par les <span class="math notranslate nohighlight">\(k\)</span> premiers axes : <span class="math notranslate nohighlight">\(\frac{\displaystyle\sum_{j=1}^k\lambda_j}{\displaystyle\sum_{j=1}^d\lambda_j}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i_lambda</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">/</span><span class="n">d</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">eigenvalues</span><span class="p">]</span>
<span class="n">i_cum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="o">/</span><span class="n">d</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">i_lambda</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Valeurs propres&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">i_cum</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">% d</span><span class="s1">e variance expliquée&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/acp_17_0.png" src="_images/acp_17_0.png" />
</div>
</div>
</div>
<div class="section" id="critere-de-kaiser">
<h4>Critère de Kaiser<a class="headerlink" href="#critere-de-kaiser" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On retient &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nb_l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; axes&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On retient 3 axes
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="analyse-des-resultats">
<h3>Analyse des résultats<a class="headerlink" href="#analyse-des-resultats" title="Permalink to this headline">#</a></h3>
<div class="section" id="id3">
<h4>Corrélation variables/facteurs<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="cercle-des-correlations-pour-un-couple-de-composantes-principales">
<h4>Cercle des corrélations pour un couple de composantes principales<a class="headerlink" href="#cercle-des-correlations-pour-un-couple-de-composantes-principales" title="Permalink to this headline">#</a></h4>
<p>Pour <span class="math notranslate nohighlight">\(c_1\)</span> et <span class="math notranslate nohighlight">\(c_2\)</span>, chaque variable <span class="math notranslate nohighlight">\(x_j\)</span> est repérée par un point d’abscisse <span class="math notranslate nohighlight">\(r(c_1,x^j)\)</span> et d’ordonnée <span class="math notranslate nohighlight">\(r(c_2, x_j)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i1</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i2</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i3</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i12</span> <span class="o">=</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span>
<span class="n">i13</span> <span class="o">=</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i3</span>
<span class="n">i23</span> <span class="o">=</span> <span class="n">i2</span> <span class="o">+</span> <span class="n">i3</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i12</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i13</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP2/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i23</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/acp_23_0.png" src="_images/acp_23_0.png" />
</div>
</div>
</div>
<div class="section" id="contribution-des-variables">
<h4>Contribution des variables<a class="headerlink" href="#contribution-des-variables" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contributions_variables</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
    <span class="n">contributions_variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">variables</span><span class="p">,</span> <span class="n">contributions_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
  % alcool	0.02 	0.23 	0.04 	0.00 	0.07 	0.05 	0.00 	0.00 	0.16 	0.07 	0.26 	0.05 	0.04
  acide ma	0.06 	0.05 	0.01 	0.29 	0.00 	0.29 	0.18 	0.00 	0.00 	0.01 	0.01 	0.01 	0.10
   cendres	0.00 	0.10 	0.39 	0.05 	0.02 	0.02 	0.02 	0.02 	0.03 	0.00 	0.09 	0.25 	0.00
  alcalini	0.06 	0.00 	0.37 	0.00 	0.00 	0.01 	0.08 	0.01 	0.18 	0.00 	0.04 	0.23 	0.00
  magnésiu	0.02 	0.09 	0.02 	0.12 	0.53 	0.00 	0.10 	0.00 	0.02 	0.00 	0.07 	0.01 	0.00
   phénols	0.16 	0.00 	0.02 	0.04 	0.02 	0.01 	0.00 	0.22 	0.16 	0.09 	0.08 	0.09 	0.10
  flavonoï	0.18 	0.00 	0.02 	0.02 	0.01 	0.00 	0.00 	0.69 	0.04 	0.00 	0.00 	0.00 	0.03
  non flav	0.09 	0.00 	0.03 	0.04 	0.25 	0.07 	0.35 	0.01 	0.05 	0.00 	0.04 	0.01 	0.05
  proantho	0.10 	0.00 	0.02 	0.16 	0.02 	0.28 	0.14 	0.01 	0.14 	0.01 	0.04 	0.06 	0.02
   couleur	0.01 	0.28 	0.02 	0.00 	0.01 	0.18 	0.05 	0.00 	0.00 	0.37 	0.00 	0.00 	0.08
    teinte	0.09 	0.08 	0.01 	0.18 	0.03 	0.01 	0.05 	0.01 	0.19 	0.07 	0.01 	0.00 	0.27
  OD280/OD	0.14 	0.03 	0.03 	0.03 	0.01 	0.07 	0.00 	0.02 	0.01 	0.36 	0.02 	0.00 	0.27
   proline	0.08 	0.13 	0.02 	0.05 	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.33 	0.29 	0.03
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="representation-des-individus">
<h4>Représentation des individus<a class="headerlink" href="#representation-des-individus" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i12</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i13</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP2/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i23</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/acp_27_0.png" src="_images/acp_27_0.png" />
</div>
</div>
</div>
<div class="section" id="contribution-des-individus">
<h4>Contribution des individus<a class="headerlink" href="#contribution-des-individus" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(\frac{p_ic_{ki}^2}{\lambda_k}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contributions_individus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> 
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="n">contributions_individus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    
<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">contributions_individus</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.06
         1	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
         2	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
         3	0.02 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
         4	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         5	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         6	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.06
         7	0.01 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
         8	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
         9	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        10	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.01 	0.02 	0.00
        11	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        12	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        13	0.01 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.05 	0.00
        14	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        15	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.03
        16	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        17	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01
        18	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.03
        19	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
        20	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04 	0.01 	0.01
        21	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
        22	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.04
        23	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01
        24	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        25	0.00 	0.00 	0.06 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.01
        27	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00
        28	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        29	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
        30	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.01
        31	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.04 	0.00
        32	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        33	0.00 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        34	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        35	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        36	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.04 	0.00
        37	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        38	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        39	0.01 	0.01 	0.00 	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.01 	0.05 	0.00
        40	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        41	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
        42	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00
        43	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        44	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01
        45	0.00 	0.01 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        46	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        47	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        48	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        49	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
        50	0.01 	0.00 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.02
        51	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00
        52	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.01
        53	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        54	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        55	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        56	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        57	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        58	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        59	0.00 	0.02 	0.08 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.00 	0.00 	0.00 	0.05 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
        61	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.00 	0.02
        62	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04 	0.00 	0.01
        63	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11
        64	0.00 	0.01 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.06
        65	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.03
        66	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        67	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
        68	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.00 	0.03 	0.02 	0.02 	0.00 	0.00
        69	0.00 	0.00 	0.01 	0.01 	0.12 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00 	0.00 	0.01
        70	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02
        71	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.05 	0.00 	0.03 	0.02 	0.03
        72	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.02 	0.00 	0.01
        73	0.01 	0.00 	0.04 	0.01 	0.03 	0.00 	0.00 	0.00 	0.02 	0.00 	0.03 	0.11 	0.00
        74	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.02 	0.01 	0.00 	0.01 	0.04 	0.00
        75	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.05
        76	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        77	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        78	0.00 	0.00 	0.01 	0.01 	0.04 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        79	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02
        80	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02
        81	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        82	0.00 	0.01 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        83	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
        84	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.04 	0.05 	0.04
        85	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        86	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
        87	0.00 	0.01 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        88	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        89	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        90	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        91	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        92	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.03
        93	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01
        94	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00 	0.09
        95	0.01 	0.00 	0.00 	0.01 	0.09 	0.02 	0.07 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
        96	0.00 	0.00 	0.01 	0.01 	0.07 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.00
        97	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        98	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.05
        99	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00 	0.01 	0.08
       100	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.02
       101	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
       102	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
       103	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00
       104	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.03
       105	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.11
       106	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       107	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.05
       108	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04
       109	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.04 	0.01 	0.01
       110	0.00 	0.00 	0.00 	0.07 	0.03 	0.02 	0.03 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00
       112	0.00 	0.00 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01
       113	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01
       114	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
       115	0.00 	0.03 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.01 	0.01 	0.00 	0.00 	0.10
       116	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04
       117	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
       118	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       119	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       120	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00
       121	0.00 	0.00 	0.11 	0.00 	0.00 	0.01 	0.00 	0.04 	0.06 	0.02 	0.00 	0.00 	0.00
       122	0.00 	0.00 	0.04 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       123	0.00 	0.00 	0.00 	0.08 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       124	0.00 	0.00 	0.01 	0.09 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00
       125	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       126	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01 	0.01 	0.00 	0.01
       127	0.00 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       128	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       129	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
       130	0.00 	0.00 	0.01 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       131	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
       132	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00
       133	0.01 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.06
       134	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
       135	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       136	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.04 	0.00 	0.01
       137	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       139	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00
       140	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
       141	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.05
       142	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       143	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       144	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.01 	0.00
       145	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.03
       146	0.02 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
       147	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       148	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02
       150	0.01 	0.01 	0.00 	0.00 	0.03 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01
       151	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
       152	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       153	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       154	0.01 	0.00 	0.01 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04
       155	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       156	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       157	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.06 	0.01
       158	0.00 	0.03 	0.01 	0.01 	0.01 	0.09 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.01
       159	0.00 	0.01 	0.00 	0.00 	0.01 	0.07 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
       160	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.02
       161	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       162	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       163	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       164	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       165	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
       166	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.04
       167	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.02
       168	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
       169	0.01 	0.02 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       171	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       172	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       173	0.01 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       174	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       175	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00
       176	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       177	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tableau-des-cosinus-carres">
<h4>Tableau des cosinus carrés<a class="headerlink" href="#tableau-des-cosinus-carres" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(\frac{c_{ki}^2}{\displaystyle\sum_{j=1}^d c_{ji}^2}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cosinus_carres</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># on prend la représentation de l&#39;individu i sur chacune des composantes</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">x</span><span class="o">*</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">]])</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">/</span><span class="n">tot</span><span class="p">)</span>
    <span class="n">cosinus_carres</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">cosinus_carres</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.69 	0.13 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.02 	0.03 	0.01 	0.07
         1	0.43 	0.01 	0.36 	0.01 	0.01 	0.08 	0.00 	0.00 	0.09 	0.01 	0.01 	0.00 	0.00
         2	0.57 	0.10 	0.09 	0.05 	0.01 	0.03 	0.02 	0.00 	0.01 	0.00 	0.13 	0.01 	0.00
         3	0.60 	0.32 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00
         4	0.14 	0.11 	0.58 	0.02 	0.01 	0.02 	0.03 	0.00 	0.02 	0.01 	0.02 	0.04 	0.00
         5	0.60 	0.29 	0.03 	0.02 	0.03 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
         6	0.52 	0.12 	0.08 	0.00 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.02 	0.02 	0.09
         7	0.38 	0.23 	0.00 	0.13 	0.00 	0.18 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.05
         8	0.51 	0.07 	0.25 	0.00 	0.06 	0.00 	0.00 	0.00 	0.02 	0.03 	0.03 	0.02 	0.00
         9	0.71 	0.06 	0.09 	0.01 	0.02 	0.00 	0.07 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00
        10	0.73 	0.10 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.09 	0.00 	0.02 	0.03 	0.00
        11	0.39 	0.05 	0.18 	0.10 	0.07 	0.04 	0.02 	0.00 	0.12 	0.00 	0.03 	0.00 	0.01
        12	0.53 	0.05 	0.09 	0.02 	0.17 	0.01 	0.01 	0.00 	0.03 	0.00 	0.09 	0.00 	0.00
        13	0.50 	0.05 	0.06 	0.00 	0.17 	0.02 	0.09 	0.00 	0.01 	0.02 	0.00 	0.06 	0.00
        14	0.65 	0.15 	0.06 	0.00 	0.04 	0.02 	0.04 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00
        15	0.45 	0.24 	0.00 	0.18 	0.02 	0.02 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.05
        16	0.39 	0.44 	0.06 	0.07 	0.00 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00
        17	0.39 	0.29 	0.07 	0.13 	0.02 	0.01 	0.00 	0.03 	0.00 	0.01 	0.02 	0.01 	0.02
        18	0.54 	0.27 	0.01 	0.04 	0.06 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.04 	0.03
        19	0.45 	0.12 	0.00 	0.02 	0.08 	0.20 	0.00 	0.00 	0.03 	0.01 	0.00 	0.07 	0.00
        20	0.70 	0.04 	0.01 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00 	0.02 	0.13 	0.02 	0.01
        21	0.19 	0.01 	0.14 	0.17 	0.02 	0.24 	0.01 	0.02 	0.00 	0.08 	0.05 	0.07 	0.00
        22	0.74 	0.00 	0.01 	0.00 	0.02 	0.12 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.08
        23	0.50 	0.05 	0.00 	0.03 	0.03 	0.11 	0.08 	0.02 	0.01 	0.04 	0.09 	0.00 	0.04
        24	0.49 	0.02 	0.12 	0.00 	0.05 	0.13 	0.06 	0.01 	0.02 	0.02 	0.00 	0.02 	0.07
        25	0.05 	0.05 	0.77 	0.09 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.44 	0.07 	0.00 	0.01 	0.18 	0.05 	0.03 	0.00 	0.12 	0.01 	0.08 	0.00 	0.03
        27	0.26 	0.00 	0.32 	0.04 	0.02 	0.06 	0.02 	0.00 	0.01 	0.02 	0.09 	0.13 	0.02
        28	0.51 	0.05 	0.21 	0.06 	0.07 	0.04 	0.00 	0.00 	0.00 	0.00 	0.02 	0.03 	0.00
        29	0.65 	0.00 	0.15 	0.01 	0.03 	0.01 	0.00 	0.03 	0.03 	0.00 	0.00 	0.00 	0.08
        30	0.49 	0.12 	0.15 	0.01 	0.03 	0.03 	0.01 	0.00 	0.09 	0.03 	0.02 	0.01 	0.01
        31	0.59 	0.18 	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.02 	0.00 	0.05 	0.10 	0.00
        32	0.52 	0.00 	0.01 	0.11 	0.08 	0.00 	0.19 	0.01 	0.05 	0.01 	0.00 	0.01 	0.01
        33	0.23 	0.17 	0.09 	0.35 	0.00 	0.01 	0.06 	0.00 	0.02 	0.00 	0.03 	0.03 	0.00
        34	0.42 	0.10 	0.05 	0.24 	0.00 	0.11 	0.01 	0.00 	0.01 	0.01 	0.03 	0.01 	0.01
        35	0.75 	0.01 	0.04 	0.02 	0.01 	0.01 	0.09 	0.00 	0.03 	0.01 	0.01 	0.01 	0.02
        36	0.28 	0.06 	0.03 	0.24 	0.01 	0.06 	0.00 	0.00 	0.13 	0.00 	0.01 	0.17 	0.01
        37	0.35 	0.00 	0.00 	0.25 	0.03 	0.05 	0.05 	0.00 	0.00 	0.04 	0.20 	0.00 	0.03
        38	0.37 	0.10 	0.34 	0.10 	0.00 	0.03 	0.00 	0.02 	0.00 	0.01 	0.01 	0.01 	0.01
        39	0.34 	0.17 	0.01 	0.08 	0.09 	0.13 	0.05 	0.00 	0.03 	0.00 	0.03 	0.08 	0.00
        40	0.67 	0.06 	0.00 	0.02 	0.02 	0.05 	0.03 	0.00 	0.05 	0.00 	0.07 	0.01 	0.01
        41	0.08 	0.01 	0.11 	0.30 	0.02 	0.30 	0.00 	0.03 	0.02 	0.00 	0.01 	0.12 	0.00
        42	0.67 	0.09 	0.01 	0.02 	0.01 	0.05 	0.07 	0.00 	0.06 	0.00 	0.00 	0.01 	0.00
        43	0.05 	0.03 	0.01 	0.47 	0.02 	0.24 	0.08 	0.01 	0.07 	0.00 	0.02 	0.00 	0.00
        44	0.66 	0.00 	0.06 	0.09 	0.03 	0.02 	0.00 	0.00 	0.06 	0.00 	0.01 	0.03 	0.03
        45	0.14 	0.34 	0.00 	0.05 	0.00 	0.39 	0.00 	0.00 	0.01 	0.00 	0.03 	0.04 	0.00
        46	0.56 	0.11 	0.02 	0.15 	0.02 	0.08 	0.04 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        47	0.70 	0.04 	0.12 	0.09 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
        48	0.58 	0.22 	0.00 	0.01 	0.02 	0.04 	0.01 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        49	0.57 	0.24 	0.03 	0.00 	0.02 	0.04 	0.00 	0.02 	0.00 	0.04 	0.00 	0.02 	0.01
        50	0.53 	0.00 	0.20 	0.06 	0.00 	0.08 	0.00 	0.00 	0.01 	0.01 	0.08 	0.01 	0.02
        51	0.68 	0.05 	0.00 	0.00 	0.05 	0.00 	0.01 	0.00 	0.10 	0.01 	0.08 	0.03 	0.00
        52	0.68 	0.14 	0.02 	0.00 	0.02 	0.00 	0.00 	0.00 	0.11 	0.00 	0.01 	0.00 	0.01
        53	0.45 	0.32 	0.01 	0.13 	0.03 	0.01 	0.02 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        54	0.56 	0.13 	0.11 	0.01 	0.09 	0.02 	0.02 	0.01 	0.01 	0.00 	0.01 	0.00 	0.03
        55	0.61 	0.18 	0.03 	0.01 	0.08 	0.03 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00
        56	0.66 	0.18 	0.03 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.08 	0.00 	0.01
        57	0.56 	0.17 	0.01 	0.03 	0.06 	0.01 	0.01 	0.00 	0.04 	0.00 	0.08 	0.00 	0.02
        58	0.66 	0.20 	0.01 	0.00 	0.00 	0.01 	0.05 	0.00 	0.03 	0.02 	0.01 	0.00 	0.00
        59	0.03 	0.28 	0.61 	0.03 	0.01 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.14 	0.11 	0.04 	0.49 	0.06 	0.00 	0.07 	0.00 	0.07 	0.01 	0.00 	0.00 	0.02
        61	0.29 	0.06 	0.22 	0.18 	0.01 	0.04 	0.01 	0.01 	0.08 	0.00 	0.07 	0.00 	0.03
        62	0.00 	0.17 	0.35 	0.16 	0.02 	0.04 	0.04 	0.00 	0.04 	0.00 	0.16 	0.01 	0.01
        63	0.33 	0.29 	0.00 	0.04 	0.00 	0.05 	0.09 	0.01 	0.01 	0.00 	0.03 	0.00 	0.15
        64	0.03 	0.32 	0.04 	0.40 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.07 	0.10
        65	0.17 	0.12 	0.07 	0.10 	0.00 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.14 	0.10
        66	0.28 	0.20 	0.23 	0.11 	0.03 	0.09 	0.01 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01
        67	0.00 	0.54 	0.11 	0.00 	0.03 	0.00 	0.17 	0.00 	0.00 	0.11 	0.01 	0.02 	0.00
        68	0.06 	0.00 	0.04 	0.53 	0.03 	0.01 	0.01 	0.01 	0.13 	0.09 	0.09 	0.00 	0.00
        69	0.13 	0.06 	0.05 	0.04 	0.57 	0.01 	0.07 	0.01 	0.04 	0.02 	0.00 	0.00 	0.01
        70	0.29 	0.09 	0.05 	0.16 	0.11 	0.01 	0.00 	0.02 	0.06 	0.02 	0.14 	0.01 	0.05
        71	0.18 	0.06 	0.25 	0.00 	0.04 	0.01 	0.14 	0.01 	0.15 	0.01 	0.09 	0.04 	0.03
        72	0.08 	0.17 	0.00 	0.00 	0.00 	0.04 	0.38 	0.01 	0.16 	0.00 	0.12 	0.00 	0.02
        73	0.23 	0.00 	0.39 	0.03 	0.14 	0.00 	0.01 	0.00 	0.04 	0.00 	0.04 	0.11 	0.00
        74	0.30 	0.15 	0.02 	0.00 	0.09 	0.00 	0.17 	0.09 	0.03 	0.02 	0.02 	0.10 	0.01
        75	0.06 	0.50 	0.20 	0.06 	0.04 	0.00 	0.01 	0.01 	0.00 	0.03 	0.00 	0.00 	0.07
        76	0.01 	0.35 	0.46 	0.00 	0.00 	0.02 	0.03 	0.00 	0.04 	0.00 	0.05 	0.01 	0.02
        77	0.26 	0.24 	0.01 	0.06 	0.14 	0.08 	0.11 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        78	0.09 	0.03 	0.08 	0.05 	0.37 	0.14 	0.22 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        79	0.02 	0.13 	0.38 	0.11 	0.00 	0.05 	0.17 	0.00 	0.01 	0.00 	0.08 	0.01 	0.04
        80	0.04 	0.83 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.03 	0.00 	0.03
        81	0.25 	0.48 	0.03 	0.02 	0.02 	0.00 	0.05 	0.01 	0.08 	0.03 	0.02 	0.00 	0.00
        82	0.02 	0.49 	0.15 	0.09 	0.09 	0.00 	0.04 	0.01 	0.08 	0.00 	0.02 	0.00 	0.00
        83	0.61 	0.00 	0.02 	0.05 	0.12 	0.00 	0.13 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00
        84	0.07 	0.22 	0.04 	0.03 	0.08 	0.08 	0.07 	0.01 	0.03 	0.00 	0.15 	0.16 	0.07
        85	0.10 	0.64 	0.01 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.04 	0.06 	0.00
        86	0.07 	0.56 	0.07 	0.12 	0.01 	0.01 	0.01 	0.00 	0.13 	0.00 	0.00 	0.00 	0.03
        87	0.02 	0.38 	0.36 	0.07 	0.02 	0.00 	0.02 	0.01 	0.05 	0.04 	0.01 	0.01 	0.00
        88	0.17 	0.45 	0.13 	0.02 	0.07 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11 	0.01 	0.01
        89	0.03 	0.60 	0.06 	0.00 	0.15 	0.00 	0.05 	0.01 	0.02 	0.00 	0.01 	0.03 	0.04
        90	0.20 	0.49 	0.00 	0.02 	0.09 	0.03 	0.06 	0.00 	0.00 	0.00 	0.04 	0.05 	0.00
        91	0.28 	0.39 	0.07 	0.04 	0.03 	0.06 	0.00 	0.00 	0.02 	0.02 	0.02 	0.02 	0.04
        92	0.36 	0.23 	0.00 	0.03 	0.13 	0.08 	0.03 	0.02 	0.03 	0.02 	0.00 	0.02 	0.05
        93	0.06 	0.60 	0.00 	0.18 	0.01 	0.03 	0.00 	0.01 	0.00 	0.01 	0.00 	0.07 	0.02
        94	0.09 	0.47 	0.00 	0.01 	0.08 	0.01 	0.05 	0.04 	0.08 	0.00 	0.00 	0.00 	0.15
        95	0.20 	0.00 	0.02 	0.03 	0.44 	0.06 	0.22 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        96	0.02 	0.01 	0.10 	0.09 	0.65 	0.03 	0.01 	0.01 	0.00 	0.00 	0.06 	0.03 	0.00
        97	0.11 	0.67 	0.12 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.05
        98	0.42 	0.17 	0.00 	0.07 	0.00 	0.09 	0.06 	0.01 	0.05 	0.03 	0.00 	0.02 	0.07
        99	0.12 	0.29 	0.03 	0.09 	0.03 	0.02 	0.27 	0.00 	0.02 	0.00 	0.00 	0.02 	0.09
       100	0.06 	0.50 	0.22 	0.00 	0.03 	0.01 	0.01 	0.00 	0.01 	0.04 	0.00 	0.08 	0.03
       101	0.03 	0.56 	0.24 	0.01 	0.01 	0.00 	0.02 	0.00 	0.07 	0.00 	0.00 	0.00 	0.05
       102	0.01 	0.27 	0.20 	0.09 	0.02 	0.09 	0.05 	0.01 	0.18 	0.00 	0.02 	0.00 	0.07
       103	0.04 	0.74 	0.06 	0.02 	0.00 	0.02 	0.00 	0.01 	0.02 	0.03 	0.01 	0.05 	0.00
       104	0.02 	0.64 	0.03 	0.03 	0.01 	0.01 	0.03 	0.00 	0.02 	0.04 	0.00 	0.08 	0.08
       105	0.25 	0.23 	0.07 	0.01 	0.08 	0.00 	0.09 	0.02 	0.01 	0.02 	0.05 	0.00 	0.16
       106	0.02 	0.70 	0.03 	0.02 	0.03 	0.01 	0.01 	0.02 	0.01 	0.05 	0.02 	0.01 	0.06
       107	0.37 	0.26 	0.01 	0.00 	0.05 	0.05 	0.00 	0.06 	0.07 	0.01 	0.00 	0.01 	0.12
       108	0.00 	0.60 	0.02 	0.09 	0.01 	0.14 	0.01 	0.00 	0.01 	0.00 	0.04 	0.00 	0.07
       109	0.22 	0.19 	0.28 	0.03 	0.01 	0.06 	0.01 	0.00 	0.03 	0.00 	0.14 	0.03 	0.01
       110	0.07 	0.07 	0.00 	0.44 	0.18 	0.08 	0.10 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.01 	0.55 	0.00 	0.13 	0.01 	0.07 	0.11 	0.00 	0.01 	0.05 	0.05 	0.00 	0.00
       112	0.12 	0.04 	0.29 	0.21 	0.04 	0.02 	0.09 	0.01 	0.05 	0.04 	0.04 	0.03 	0.02
       113	0.02 	0.53 	0.12 	0.07 	0.01 	0.10 	0.03 	0.00 	0.06 	0.01 	0.02 	0.00 	0.03
       114	0.03 	0.43 	0.20 	0.00 	0.08 	0.00 	0.11 	0.00 	0.11 	0.00 	0.02 	0.00 	0.02
       115	0.01 	0.61 	0.07 	0.03 	0.04 	0.04 	0.08 	0.00 	0.02 	0.02 	0.00 	0.00 	0.07
       116	0.01 	0.80 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00 	0.01 	0.00 	0.02 	0.07
       117	0.00 	0.57 	0.07 	0.01 	0.18 	0.00 	0.00 	0.01 	0.02 	0.00 	0.10 	0.00 	0.03
       118	0.44 	0.12 	0.27 	0.06 	0.02 	0.05 	0.00 	0.01 	0.03 	0.01 	0.00 	0.01 	0.00
       119	0.03 	0.54 	0.01 	0.20 	0.01 	0.04 	0.10 	0.00 	0.00 	0.00 	0.03 	0.01 	0.02
       120	0.07 	0.25 	0.16 	0.18 	0.02 	0.00 	0.01 	0.00 	0.24 	0.00 	0.05 	0.02 	0.01
       121	0.05 	0.00 	0.75 	0.00 	0.00 	0.02 	0.01 	0.06 	0.08 	0.02 	0.00 	0.00 	0.00
       122	0.10 	0.03 	0.63 	0.09 	0.01 	0.10 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       123	0.01 	0.02 	0.01 	0.75 	0.00 	0.13 	0.03 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00
       124	0.05 	0.10 	0.11 	0.69 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00
       125	0.00 	0.60 	0.03 	0.15 	0.01 	0.00 	0.04 	0.01 	0.10 	0.00 	0.03 	0.02 	0.01
       126	0.00 	0.21 	0.06 	0.21 	0.02 	0.12 	0.09 	0.06 	0.13 	0.04 	0.04 	0.00 	0.03
       127	0.15 	0.09 	0.66 	0.00 	0.02 	0.04 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00
       128	0.03 	0.47 	0.21 	0.06 	0.00 	0.06 	0.02 	0.03 	0.01 	0.05 	0.02 	0.00 	0.05
       129	0.34 	0.14 	0.06 	0.23 	0.03 	0.10 	0.01 	0.00 	0.01 	0.01 	0.06 	0.02 	0.00
       130	0.14 	0.00 	0.11 	0.17 	0.41 	0.00 	0.05 	0.01 	0.01 	0.08 	0.00 	0.02 	0.00
       131	0.53 	0.01 	0.05 	0.01 	0.17 	0.05 	0.08 	0.02 	0.00 	0.01 	0.01 	0.03 	0.02
       132	0.63 	0.01 	0.00 	0.01 	0.09 	0.00 	0.19 	0.03 	0.02 	0.02 	0.01 	0.00 	0.00
       133	0.39 	0.01 	0.02 	0.00 	0.25 	0.09 	0.09 	0.00 	0.01 	0.04 	0.01 	0.00 	0.09
       134	0.44 	0.02 	0.10 	0.06 	0.09 	0.16 	0.01 	0.02 	0.06 	0.04 	0.01 	0.00 	0.01
       135	0.68 	0.01 	0.09 	0.04 	0.04 	0.05 	0.04 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00
       136	0.78 	0.00 	0.00 	0.00 	0.01 	0.06 	0.04 	0.00 	0.00 	0.02 	0.07 	0.00 	0.01
       137	0.67 	0.02 	0.13 	0.01 	0.01 	0.03 	0.09 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.74 	0.01 	0.08 	0.00 	0.09 	0.01 	0.05 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
       139	0.59 	0.01 	0.16 	0.06 	0.02 	0.02 	0.00 	0.06 	0.01 	0.01 	0.03 	0.03 	0.00
       140	0.71 	0.01 	0.03 	0.07 	0.03 	0.05 	0.00 	0.01 	0.02 	0.00 	0.00 	0.02 	0.05
       141	0.56 	0.01 	0.10 	0.01 	0.01 	0.09 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.09
       142	0.65 	0.02 	0.07 	0.08 	0.02 	0.11 	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.01
       143	0.51 	0.02 	0.01 	0.04 	0.04 	0.22 	0.09 	0.01 	0.01 	0.01 	0.02 	0.00 	0.02
       144	0.37 	0.10 	0.12 	0.01 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.08 	0.02 	0.00
       145	0.63 	0.03 	0.07 	0.01 	0.04 	0.02 	0.03 	0.00 	0.01 	0.07 	0.01 	0.04 	0.05
       146	0.70 	0.02 	0.08 	0.05 	0.00 	0.10 	0.00 	0.00 	0.02 	0.02 	0.00 	0.01 	0.00
       147	0.77 	0.10 	0.00 	0.07 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       148	0.68 	0.21 	0.02 	0.03 	0.01 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.51 	0.25 	0.02 	0.01 	0.13 	0.00 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02
       150	0.28 	0.29 	0.01 	0.00 	0.26 	0.00 	0.08 	0.03 	0.01 	0.00 	0.00 	0.02 	0.01
       151	0.34 	0.22 	0.01 	0.01 	0.18 	0.05 	0.14 	0.01 	0.00 	0.02 	0.00 	0.01 	0.01
       152	0.21 	0.15 	0.12 	0.00 	0.20 	0.06 	0.15 	0.00 	0.00 	0.07 	0.01 	0.01 	0.01
       153	0.41 	0.25 	0.05 	0.02 	0.01 	0.18 	0.06 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       154	0.53 	0.01 	0.10 	0.03 	0.01 	0.25 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.05
       155	0.63 	0.16 	0.00 	0.08 	0.02 	0.00 	0.10 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
       156	0.53 	0.24 	0.04 	0.11 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03 	0.00
       157	0.60 	0.09 	0.13 	0.01 	0.02 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.09 	0.01
       158	0.04 	0.42 	0.05 	0.03 	0.03 	0.36 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00
       159	0.13 	0.29 	0.01 	0.03 	0.05 	0.42 	0.03 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00
       160	0.72 	0.04 	0.00 	0.07 	0.01 	0.01 	0.00 	0.01 	0.09 	0.00 	0.00 	0.02 	0.03
       161	0.53 	0.15 	0.00 	0.14 	0.01 	0.07 	0.03 	0.02 	0.00 	0.00 	0.03 	0.01 	0.01
       162	0.73 	0.03 	0.06 	0.07 	0.01 	0.01 	0.07 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01
       163	0.70 	0.05 	0.08 	0.01 	0.06 	0.05 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01
       164	0.60 	0.17 	0.07 	0.00 	0.01 	0.02 	0.07 	0.00 	0.04 	0.02 	0.01 	0.00 	0.00
       165	0.77 	0.05 	0.01 	0.02 	0.03 	0.01 	0.02 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00
       166	0.38 	0.44 	0.01 	0.00 	0.01 	0.02 	0.00 	0.00 	0.03 	0.04 	0.00 	0.00 	0.05
       167	0.60 	0.11 	0.10 	0.01 	0.01 	0.01 	0.03 	0.00 	0.00 	0.08 	0.02 	0.00 	0.03
       168	0.40 	0.36 	0.05 	0.01 	0.01 	0.03 	0.05 	0.00 	0.07 	0.00 	0.00 	0.00 	0.00
       169	0.30 	0.36 	0.11 	0.02 	0.07 	0.05 	0.05 	0.01 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.82 	0.01 	0.06 	0.00 	0.03 	0.01 	0.01 	0.00 	0.03 	0.00 	0.03 	0.00 	0.00
       171	0.70 	0.04 	0.09 	0.00 	0.01 	0.04 	0.05 	0.00 	0.02 	0.03 	0.00 	0.00 	0.00
       172	0.43 	0.34 	0.06 	0.00 	0.03 	0.03 	0.05 	0.00 	0.01 	0.00 	0.01 	0.03 	0.01
       173	0.56 	0.24 	0.01 	0.06 	0.02 	0.06 	0.05 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       174	0.64 	0.29 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.01 	0.00
       175	0.38 	0.41 	0.05 	0.01 	0.09 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
       176	0.38 	0.36 	0.02 	0.03 	0.04 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.01 	0.02
       177	0.49 	0.36 	0.05 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-regression"></span><div class="tex2jax_ignore mathjax_ignore section" id="regression">
<h2>Régression<a class="headerlink" href="#regression" title="Permalink to this headline">#</a></h2>
<p id="index-0">On s’intéresse ici à l’explication d’une variable (aléatoire) <span class="math notranslate nohighlight">\(Y\)</span> (la variable expliquée) par une (ou plusieurs) variable(s) aléatoire(s) <span class="math notranslate nohighlight">\(X_j\)</span> (prédicteurs, ou variables explicatives).</p>
<div class="section" id="regression-simple">
<h3>Régression simple<a class="headerlink" href="#regression-simple" title="Permalink to this headline">#</a></h3>
<p>On dispose de <span class="math notranslate nohighlight">\(n\)</span> couples de variables quantitatives <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span> constituant un échantillon d’observations indépendantes de <span class="math notranslate nohighlight">\((X,Y)\)</span> et on cherche une relation statistique pouvant exister entre <span class="math notranslate nohighlight">\(Y\)</span> et <span class="math notranslate nohighlight">\(X\)</span>.
On rappelle ici quelques résultats élémentaires sur la régression linéaire simple.</p>
<div class="section" id="modele-theorique">
<h4>Modèle théorique<a class="headerlink" href="#modele-theorique" title="Permalink to this headline">#</a></h4>
<p>Théoriquement, on cherche une fonction <span class="math notranslate nohighlight">\(f\)</span> telle que <span class="math notranslate nohighlight">\(f(X)\)</span> soit aussi proche que possible de <span class="math notranslate nohighlight">\(Y\)</span>. Par proximité, on entend ici au sens des moindres carrés, et donc on cherche <span class="math notranslate nohighlight">\(f\)</span> telle que <span class="math notranslate nohighlight">\(\mathbb{E}\left ( (Y-f(X))^2\right )\)</span> soit minimale. On sait alors que la fonction <span class="math notranslate nohighlight">\(f\)</span> qui satisfait cette propriété est :</p>
<p><span class="math notranslate nohighlight">\(f(X) = \mathbb{E}(Y\mid X)\)</span></p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 37 </span> (Fonction de régression)</p>
<div class="definition-content section" id="proof-content">
<p>La fonction <span class="math notranslate nohighlight">\(x\mapsto \mathbb{E}(Y\mid X=x)\)</span> est la fonction de régression de <span class="math notranslate nohighlight">\(Y\)</span> en <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
</div><p>La qualité de l’approximation est mesurée par le rapport de corrélation.</p>
<div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 38 </span> (Rapport de corrélation)</p>
<div class="definition-content section" id="proof-content">
<p>Le rapport de corrélation entre deux variables aléatoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> est défini par le rapport entre la variation expliquée et la variation totale :</p>
<p><span class="math notranslate nohighlight">\(\eta_{Y\mid X}^2 = \frac{\sigma_{\mathbb{E}(Y\mid X)}^2}{\sigma_Y^2}\)</span></p>
</div>
</div><p>En pratique, <span class="math notranslate nohighlight">\(Y\)</span> est approchée par <span class="math notranslate nohighlight">\(Y=\mathbb{E}(Y\mid X)+\varepsilon\)</span>, où <span class="math notranslate nohighlight">\(\varepsilon\)</span> est un résidu aléatoire de moyenne nulle, non corrélé à <span class="math notranslate nohighlight">\(X\)</span> et à <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)\)</span> et tel que <span class="math notranslate nohighlight">\(\sigma_\varepsilon^2= (1-\eta_{Y\mid X}^2)\sigma_Y^2\)</span>.</p>
<p>Le cadre le plus utilisé est celui de la régression linéaire, c’est-à-dire lorsque <span class="math notranslate nohighlight">\(Y=a+bX+\varepsilon\)</span> et donc <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)=a+bX\)</span>, ce qui est le cas lorsque <span class="math notranslate nohighlight">\((X,Y)\)</span> est un couple de variables aléatoires gaussiennes.</p>
<p>Puisque <span class="math notranslate nohighlight">\(\mathbb{E}(\varepsilon)=0\)</span>, la droite de régression passe par le point <span class="math notranslate nohighlight">\((\mathbb{E}(X),\mathbb{E}(Y))\)</span>. Ainsi</p>
<p><span class="math notranslate nohighlight">\(Y-\mathbb{E}(Y)=b(X-\mathbb{E}(X))+\varepsilon\)</span></p>
<p>En multipliant par <span class="math notranslate nohighlight">\(X-\mathbb{E}(X)\)</span> et en prenant l’espérance, on trouve à gauche la covariance de <span class="math notranslate nohighlight">\((X,Y)\)</span> et à droite la variance de <span class="math notranslate nohighlight">\(X\)</span>, soit</p>
<p><span class="math notranslate nohighlight">\(\begin{array}{ccll}
\sigma_{XY}&amp;=&amp; b\sigma_X^2+\mathbb{E}(\varepsilon(X-\mathbb{E}(X)))&amp;\\
&amp;=&amp; b\sigma_X^2 + \sigma_{\varepsilon X}&amp;[\mathbb{E}(\varepsilon)=0]\\ 
&amp;=&amp; b\sigma_X^2 &amp;[X\text{ et } \varepsilon\text{ non corrélés}]\\ 
\end{array}
\)</span></p>
<p>d’où
<span class="math notranslate nohighlight">\(b = \frac{\sigma_{XY}}{\sigma_X^2} = r_{XY}\frac{\sigma_Y}{\sigma_X}\)</span></p>
<p>L’équation de la droite de régression est donc finalement</p>
<p><span class="math notranslate nohighlight">\(Y-\mathbb{E}(Y)=r_{XY}\frac{\sigma_Y}{\sigma_X}(X-\mathbb{E}(X))+\varepsilon\)</span></p>
<p>En calculant la variance des deux termes, et puisque <span class="math notranslate nohighlight">\(\varepsilon\)</span> et <span class="math notranslate nohighlight">\(X\)</span> ne sont pas corrélés, on trouve</p>
<p><span class="math notranslate nohighlight">\(r_{XY}^2 = \eta_{Y\mid X}^2\)</span></p>
</div>
<div class="section" id="ajustement-aux-donnees">
<h4>Ajustement aux données<a class="headerlink" href="#ajustement-aux-donnees" title="Permalink to this headline">#</a></h4>
<p>On cherche ici à ajuster le modèle linéaire théorique aux <span class="math notranslate nohighlight">\(n\)</span> couples d’observations indépendantes <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span>. Il s’agit donc de trouver <span class="math notranslate nohighlight">\(a,b\)</span> ainsi que la variance du résidu <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p>
<p>La méthode la plus classique est la méthode des moindres carrés : on cherche à ajuster au nuage de points  <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span> une droite d’équation <span class="math notranslate nohighlight">\(y^*=\alpha +\beta x\)</span> de sorte à minimiser</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n (y_i^*-y_i)^2 = \displaystyle\sum_{i=1}^n (\alpha + \beta x_i-y_i)^2\)</span></p>
<p>En annulant le gradient de cette fonction à deux variables <span class="math notranslate nohighlight">\((\alpha,\beta)\)</span>, on trouve facilement</p>
<p><span class="math notranslate nohighlight">\(\beta = \frac{\sigma_{xy}}{\sigma_x^2} = r_{xy}\frac{\sigma_y}{\sigma_x}\)</span></p>
<p>de sorte que <span class="math notranslate nohighlight">\(y^* = \bar y + r_{xy}\frac{\sigma_y}{\sigma_x}(x-\bar x)\)</span>.</p>
<p>La droite de régression linéaire passe donc par le centre de masse du nuage de points.</p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 15 </span></p>
<div class="remark-content section" id="proof-content">
<p>les <span class="math notranslate nohighlight">\(x_i\)</span> et <span class="math notranslate nohighlight">\(y_i\)</span> étant des réalisations de variables aléatoires, tous les termes de l’équation de la droite de régression linéaire le sont également.</p>
</div>
</div><div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 16 </span></p>
<div class="remark-content section" id="proof-content">
<p>On peut montrer que <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> et <span class="math notranslate nohighlight">\(y^*\)</span> sont des estimateurs sans biais de <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> et <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)\)</span>.</p>
</div>
</div><p>La figure suivante illustre la régression linéaire d’un ensemble de points, décomposé en un ensemble d’apprentissage (bleu) sur lequel la droite de régression a été apprise et un ensemble de test (vert) sur lequel les valeurs ont été prédites (magenta).</p>
<p><img alt="" src="_images/regressionlin.png" /></p>
</div>
</div>
<div class="section" id="regression-multiple">
<h3>Régression multiple<a class="headerlink" href="#regression-multiple" title="Permalink to this headline">#</a></h3>
<div class="section" id="ajustement-lineaire-d-un-ensemble-d-observations">
<span id="index-2"></span><h4>Ajustement linéaire d’un ensemble d’observations<a class="headerlink" href="#ajustement-lineaire-d-un-ensemble-d-observations" title="Permalink to this headline">#</a></h4>
<p>La régression multiple généralise la régression simple au cas de <span class="math notranslate nohighlight">\(p\geq 2\)</span> prédicteurs quantitatifs (ou variables explicatives). Ici on considère un échantillon de <span class="math notranslate nohighlight">\(n\)</span> individus, sur lesquels <span class="math notranslate nohighlight">\(p+1\)</span> variables sont mesurées : une variable à expliquer <span class="math notranslate nohighlight">\(\mathbf Y = (y_1\cdots y_n)^T\in\mathbb{R}^n\)</span> et <span class="math notranslate nohighlight">\(p\)</span> variables explicatives <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> linéairement indépendantes, mais possiblement en relation.</p>
<p>On cherche</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\)</span></p>
<p>proche de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> au sens des moindres carrés. <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> est le vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont toutes les composantes valent 1.</p>
<p>En notant
<span class="math notranslate nohighlight">\(X = \begin{pmatrix}\mathbf{1} &amp; \mathbf{X_1}\cdots \mathbf{X_p}\end{pmatrix}\in\mathcal{M}_{n,p+1}(\mathbb{R})\quad\text{et}\quad \boldsymbol{\beta}=(\beta_0\cdots \beta_p)^T
\in\mathbb{R}^{p+1}\)</span></p>
<p>on a <span class="math notranslate nohighlight">\(\mathbf Y^*=\mathbf X\boldsymbol \beta\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^*\)</span> est par définition des moindres carrés la projection de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sur <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, soit (Voir cours analyse numérique) :</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^* = \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)</span></p>
<p>et donc</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)</span></p>
<p>et on a donc les paramètres de la régression multiple.</p>
<div class="proof remark dropdown admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 17 </span></p>
<div class="remark-content section" id="proof-content">
<p>Dans le cas où la métrique utilisée est définie par une matrice symétrique définie positive <span class="math notranslate nohighlight">\(D\)</span> de taille <span class="math notranslate nohighlight">\(p\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf D \mathbf X)^{-1}\mathbf X^T \mathbf D \mathbf Y\)</span></p>
</div>
</div></div>
<div class="section" id="modele">
<h4>Modèle<a class="headerlink" href="#modele" title="Permalink to this headline">#</a></h4>
<p>On suppose que les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> et <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sont <span class="math notranslate nohighlight">\(n\)</span> réalisations indépendantes de <span class="math notranslate nohighlight">\(p+1\)</span> variables aléatoires <span class="math notranslate nohighlight">\(\chi_i\)</span> et <span class="math notranslate nohighlight">\(\omega\)</span>. De même qu’en régression simple, la recherche de la meilleure approximation de <span class="math notranslate nohighlight">\(\omega\)</span> par une fonction des <span class="math notranslate nohighlight">\(\chi_i\)</span> amène à <span class="math notranslate nohighlight">\(\mathbb{E}(\omega\mid \chi_1\cdots \chi_p)\)</span> et l’hypothèse de régression multiple est</p>
<p><span class="math notranslate nohighlight">\(\mathbb{E}(\omega\mid \chi_0\cdots \chi_p) = b_0+\displaystyle\sum_{i=1}^p b_i\chi_i+\varepsilon\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\mathbb{E}(\varepsilon)=0, \sigma_\varepsilon=\sigma^2\)</span> et <span class="math notranslate nohighlight">\(\varepsilon\)</span> non corrélée aux <span class="math notranslate nohighlight">\(\chi_i\)</span>.</p>
<p>On peut montrer que <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> est un estimateur sans biais du vecteur aléatoire  <span class="math notranslate nohighlight">\((b_0\cdots b_p)\)</span>, et en est la meilleure approximation. De plus, la meilleure estimation sans biais de la variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> est</p>
<p><span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1}\)</span></p>
</div>
</div>
<div class="section" id="modele-lineaire-generalise">
<h3>Modèle linéaire généralisé<a class="headerlink" href="#modele-lineaire-generalise" title="Permalink to this headline">#</a></h3>
<div class="section" id="position-du-probleme">
<h4>Position du problème<a class="headerlink" href="#position-du-probleme" title="Permalink to this headline">#</a></h4>
<p>Dans le cas le plus général, on ne cherche pas à expliquer une seule variable mais <span class="math notranslate nohighlight">\(k\in\mathbb{N}\)</span>, obtenues par répétitions de l’expérience, les <span class="math notranslate nohighlight">\(\mathbf X_j\)</span> restant identiques : pour <span class="math notranslate nohighlight">\(i\in[\![1,k]\!]\)</span> <span class="math notranslate nohighlight">\(\mathbf{Y}_i\in\mathbb{R}^n\)</span> est la <span class="math notranslate nohighlight">\(i^e\)</span> observation.</p>
</div>
<div class="section" id="solution-a-partir-des-donnees">
<h4>Solution à partir des données<a class="headerlink" href="#solution-a-partir-des-donnees" title="Permalink to this headline">#</a></h4>
<p>Le modèle fait l’hypothèse que le centre de gravité <span class="math notranslate nohighlight">\(\mathbf g\)</span> des <span class="math notranslate nohighlight">\(k\)</span> observations se situe dans <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, soit <span class="math notranslate nohighlight">\(\mathbf g = \mathbf X \boldsymbol \beta\)</span>
La plupart du temps, on ne connaît cependant qu’une seule des <span class="math notranslate nohighlight">\(k\)</span> observations <span class="math notranslate nohighlight">\(\mathbf Y\)</span>, et le problème revient à approximer le mieux possible <span class="math notranslate nohighlight">\(\mathbf g\)</span> en ne connaissant que <span class="math notranslate nohighlight">\(\mathbf Y\)</span>.</p>
<p>Cette approximation <span class="math notranslate nohighlight">\(\mathbf g^*\)</span> s’exprime comme la projection orthogonale de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sur <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, selon une métrique <span class="math notranslate nohighlight">\(\mathbf M\)</span>, à choisir de sorte que <span class="math notranslate nohighlight">\(\mathbf g^*\)</span> soit la plus proche possible de <span class="math notranslate nohighlight">\(\mathbf g\)</span>. Dit autrement, en répétant la projection avec <span class="math notranslate nohighlight">\(\mathbf Y_1\cdots \mathbf Y_k\)</span>, les <span class="math notranslate nohighlight">\(k\)</span> approximations <span class="math notranslate nohighlight">\(g^*_i=\mathbf X (\mathbf X^T\mathbf M\mathbf X)^{-1} \mathbf X^T \mathbf M \mathbf Y_i, i\in[\![1,k]\!]\)</span> doivent être le plus concentrées possible autour de <span class="math notranslate nohighlight">\(\mathbf g\)</span>.</p>
<p>Ceci revient donc à trouver <span class="math notranslate nohighlight">\(\mathbf M\)</span> de sorte à ce que l’inertie du nuage des <span class="math notranslate nohighlight">\(\mathbf g_i^*\)</span> soit minimale. On montre (théorème de Gauss-Markov généralisé) que <span class="math notranslate nohighlight">\(\mathbf M=\mathbf V^{-1}\)</span>, où <span class="math notranslate nohighlight">\(\mathbf V\)</span> est la matrice de variance-covariance du nuage des <span class="math notranslate nohighlight">\(\mathbf Y_i\)</span>. Ainsi, pour une seule observation, on en déduit</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbf g^*&amp;=&amp;\mathbf X(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y\\
\boldsymbol \beta&amp;=&amp;(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y
\end{eqnarray*}\)</span></p>
</div>
<div class="section" id="id1">
<h4>Modèle<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<p>En ayant une infinité d’observations, on approche le modèle probabiliste. On suppose que <span class="math notranslate nohighlight">\(\mathbf Y\)</span> est une réalisation d’un vecteur aléatoire d’espérance <span class="math notranslate nohighlight">\(\mathbf X\mathbf b\)</span> et de matrice de variance-covariance <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>. Le modèle s’écrit alors  <span class="math notranslate nohighlight">\(\mathbf Y=\mathbf X\mathbf b+\varepsilon\)</span>, avec <span class="math notranslate nohighlight">\(\varepsilon\)</span> centré de variance <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>, et le problème est donc d’estimer <span class="math notranslate nohighlight">\(\mathbf b\)</span>. On montre que  <span class="math notranslate nohighlight">\(\mathbf b = (\mathbf X^T \boldsymbol\Sigma^{-1}\mathbf X)^{-1}\mathbf X^T\boldsymbol\Sigma^{-1}\mathbf Y\)</span>, appelé estimation des moindres carrés généralisés est, sous des hypothèses larges, l’estimation de variance minimale de <span class="math notranslate nohighlight">\(\mathbf b\)</span>.</p>
</div>
</div>
<div class="section" id="modeles-regularises">
<h3>Modèles régularisés<a class="headerlink" href="#modeles-regularises" title="Permalink to this headline">#</a></h3>
<p>On peut montrer que l’estimateur des moindres carrés est de variance minimale parmi les estimateurs linéaires sans biais. Cependant, la variance aboutit dans certains cas à des erreurs de prédiction importantes. Dans ce cas, on cherche des estimateurs de variance plus petite quitte à avoir un (léger) biais. Pour ce faire, on peut supprimer l’effet de certaines variables explicatives ce qui revient à leur attribuer un poids nul.
Par ailleurs, dans le cas où <span class="math notranslate nohighlight">\(p\)</span> est grand, l’interprétation des résultats obtenus est parfois complexe. Ainsi, on pourra préférer un modèle estimé avec moins de variables explicatives afin de privilégier l’interprétation plutôt que la précision.</p>
<p>Dans cette section, on s’intéresse à des méthodes permettant de produire des estimateurs dont les valeurs sont d’amplitudes réduites. On parle de modèles parcimonieux lorsque des variables ont des coefficients nuls.</p>
<div class="section" id="regression-ridge">
<h4>Régression Ridge<a class="headerlink" href="#regression-ridge" title="Permalink to this headline">#</a></h4>
<span class="target" id="index-3"></span><p id="index-4">Dans l’approche moindres carrés linéaires classique, on cherche <span class="math notranslate nohighlight">\(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\)</span> proche de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> au sens de la minimisation de <span class="math notranslate nohighlight">\(\|\mathbf Y^*-\mathbf Y\|^2 \)</span>. On cherche donc <span class="math notranslate nohighlight">\(\boldsymbol\beta_{mc}\in\mathbb{R}^{p+1}\)</span> tel que :</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_{mc} = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2\right ]\)</span></p>
<p>Dans l’approche Ridge regression (ou régression de Tikhonov), on pénalise l’amplitude des coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>. Pour ce faire, on pose <span class="math notranslate nohighlight">\(\boldsymbol\beta_{\setminus 0}\)</span> le vecteur des <span class="math notranslate nohighlight">\(p\)</span> dernières composantes de <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> et on cherche le vecteur <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_r = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_2\right ]\)</span></p>
<p>Le réel positif <span class="math notranslate nohighlight">\(\lambda\)</span>, pondérant  <span class="math notranslate nohighlight">\(\| \boldsymbol\beta_{\setminus 0}\|^2_2\)</span> appelée fonction de pénalité, permet de réguler l’importance du second terme sur la minimisation. Un <span class="math notranslate nohighlight">\(\lambda\)</span> grand impose à la minimisation d’avoir une amplitude faible des coefficients <span class="math notranslate nohighlight">\(\beta_j,j\in[\![1,p]\!]\)</span>, et une variance faible de l’estimateur de <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span>.</p>
<p>Contrairement à la régression linéaire multiple classique où les variables ne sont pas nécessairement normalisées, ici il est nécessaire de réduire les variables explicatives. En pratique on les centre également, et dans ce cas :</p>
<ol class="simple">
<li><p>la première composante de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> est prise égale à la moyenne empirique des <span class="math notranslate nohighlight">\(y_i\)</span> avant centrage</p></li>
<li><p>les <span class="math notranslate nohighlight">\(p\)</span> autres composantes de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span>  sont obtenues par minimisation :</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}_r = arg\displaystyle\min_{\mathbf v\in\mathbb{R}^{p}} \left ((\mathbf Y-\mathbf X\mathbf v)^T(\mathbf Y-\mathbf X\mathbf v) + \lambda \mathbf v^T\mathbf v\right )\)</span></p>
<p>dont la solution analytique est <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X + \lambda \mathbb{I})^{-1}\mathbf X^T\mathbf Y\)</span>.</p>
<p>Le choix de <span class="math notranslate nohighlight">\(\lambda\)</span> n’est pas évident. La solution la plus simple consiste à prendre plusieurs valeurs, à tester les solutions proposées par ces valeurs et à retenir le <span class="math notranslate nohighlight">\(\lambda\)</span> ayant obtenu le meilleur score (par exemple la précision sur un ensemble de test). De manière moins expérimentale, il existe des algorithmes (basés sur la décomposition en valeurs singulières) permettant de choisir une ‘’bonne’’ valeur de paramètre.</p>
</div>
<div class="section" id="regression-lasso">
<h4>Régression Lasso<a class="headerlink" href="#regression-lasso" title="Permalink to this headline">#</a></h4>
<span class="target" id="index-5"></span><p id="index-6">La régression Lasso (Least Absolute Shrinkage and Selection Operator) est, dans son principe, très proche de la régression Ridge, la seule différence résidant dans la norme utilisée dans la fonction de pénalité : on cherche <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> minimisant</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_l = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_1\right ]\)</span></p>
<p>Contrairement à la régression Ridge, il n’y a pas de solution analytique (la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> rend la fonction non différentiable) et on doit donc recourir à des méthodes de résolution numérique. Lorsque <span class="math notranslate nohighlight">\(\lambda\)</span> est grand, la minimisation force la fonction de pénalité à être petite : étant donné que cette dernière est une somme de valeurs absolues, la minimisation impose à certains coefficients <span class="math notranslate nohighlight">\(\beta_j,j\in[\![1,p]\!]\)</span> d’être nuls. On parle alors de régression parcimonieuse (et la régression peut donc être vue comme une méthode de sélection de variables).</p>
<p>Quand <span class="math notranslate nohighlight">\(p&gt;n\)</span>, la méthode ne sélectionne que <span class="math notranslate nohighlight">\(n\)</span> variables. De plus, si plusieurs variables sont corrélées entre elles, Lasso ignore toutes sauf une. Et, pire, même si <span class="math notranslate nohighlight">\(n&gt;p\)</span>, et s’il y a de fortes corrélations entre les variables explicatives, on trouve empiriquement que Ridge donne de meilleurs résultats que Lasso.</p>
</div>
<div class="section" id="regression-elasticnet">
<h4>Régression Elasticnet<a class="headerlink" href="#regression-elasticnet" title="Permalink to this headline">#</a></h4>
<span class="target" id="index-7"></span><p id="index-8">On suppose ici que <span class="math notranslate nohighlight">\(\mathbf X\)</span> est centré réduit, et <span class="math notranslate nohighlight">\(\mathbf Y\)</span> est centré (donc <span class="math notranslate nohighlight">\(\beta_0=0\)</span>). La régression Elasticnet est un mélange de Ridge et Lasso : on cherche <span class="math notranslate nohighlight">\(\boldsymbol\beta_e\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_e = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p}}\left [\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda_1 \| \boldsymbol\beta\|^2_1 + \lambda_2 \| \boldsymbol\beta\|^2_2\right ]\)</span></p>
<p>En notant <span class="math notranslate nohighlight">\(\lambda =\lambda_1+\lambda_2\)</span> et <span class="math notranslate nohighlight">\( \alpha = \lambda_1/\lambda\)</span> on minimise alors</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda(\alpha \| \boldsymbol\beta\|^2_1 + (1-\alpha) \| \boldsymbol\beta\|^2_2)\)</span></p>
<p>On montre alors que la solution de la régression Elasticnet peut être obtenue à l’aide de la solution de la régression Lasso.</p>
<div class="proof property admonition" id="property-5">
<p class="admonition-title"><span class="caption-number">Property 7 </span></p>
<div class="property-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(\mathbf X\in\mathcal{M}_{np}(\mathbb R)\)</span> la matrice des variables explicatives, et <span class="math notranslate nohighlight">\(\mathbf Y\in\mathbb{R}^n\)</span> le vecteur des valeurs de la variable expliquée. Soient <span class="math notranslate nohighlight">\(\lambda_1,\lambda_2\in\mathbb{R}^+\)</span>. On pose</p>
<p><span class="math notranslate nohighlight">\(\mathbf X^*\in\mathcal{M}_{(n+p)p}(\mathbb R) = \frac{1}{\sqrt{1+\lambda_2}}\begin{pmatrix}\mathbf X\\\sqrt{\lambda_2 }\mathbb{I}\end{pmatrix}\quad\text{et}\quad \mathbf Y^*=\begin{pmatrix}\mathbf Y\\0\end{pmatrix}\)</span></p>
<p>et on note <span class="math notranslate nohighlight">\(\gamma=\lambda_1/(\lambda_1+\lambda_2)\)</span>.</p>
<p>Alors la fonction objectif de la régression Elasticnet s’écrit <span class="math notranslate nohighlight">\(\|\mathbf Y^*-\mathbf X^*\boldsymbol\beta^*\|_2^2+\gamma\|\boldsymbol\beta^*\|_1\)</span>.
Si <span class="math notranslate nohighlight">\(\hat{\boldsymbol\beta}\)</span> minimise cette fonction, alors l’estimateur naïf de la régression Elasticnet est</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_e = \frac{1}{\sqrt{1+\lambda_2}}\hat{\boldsymbol\beta}\)</span></p>
</div>
</div><p>Puisque <span class="math notranslate nohighlight">\(\mathbf X^*\)</span> est de rang <span class="math notranslate nohighlight">\(p\)</span>, la solution peut sélectionner <span class="math notranslate nohighlight">\(p\)</span> variables contrairement à la régression Lasso.</p>
<p>En pratique, cet estimateur naïf ne donne satisfaction que lorsqu’il est proche de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> ou de <span class="math notranslate nohighlight">\(\boldsymbol\beta_l\)</span>. On retient généralement l’estimateur rééchelonné <span class="math notranslate nohighlight">\((1+\lambda_2)\boldsymbol\beta_e = \sqrt{1+\lambda_2}\hat{\boldsymbol\beta}\)</span> (Elasticnet peut être vu comme un Lasso où la matrice de variance-covariance est proche de la matrice Identité, et on montre que le facteur <span class="math notranslate nohighlight">\(1+\lambda_2\)</span> intervient alors).</p>
<p>La figure suivante compare les différentes méthodes de régression sur la fonction</p>
<p><span class="math notranslate nohighlight">\(f(x) = x-\frac35 x^2+\frac15x^3 + 18sin(x)\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(p=8\)</span> et <span class="math notranslate nohighlight">\(n=20\)</span>. Les <span class="math notranslate nohighlight">\(n=20\)</span> points  échantillonnés sur la courbe <span class="math notranslate nohighlight">\(y=f(x)\)</span> sont utilisés pour faire la régression sur l’intervalle [-10,10].</p>
<p><img alt="" src="_images/comparregression.png" /></p>
</div>
</div>
<div class="section" id="regression-logistique">
<h3>Régression logistique<a class="headerlink" href="#regression-logistique" title="Permalink to this headline">#</a></h3>
<p id="index-9">Dans les sections précédentes, nous n’avons pas abordé les cas où les prédicteurs exhibent des dépendances non linéaires ou lorsque la variable à prédire n’est pas quantitative.</p>
<p>La régression logistique est un modèle linéaire généralisé utilisé pour prédire une variable binaire, ou catégorielle, à partir de prédicteurs quantitatifs ou catégoriels.</p>
<div class="section" id="regression-logistique-binaire">
<h4>Régression logistique binaire<a class="headerlink" href="#regression-logistique-binaire" title="Permalink to this headline">#</a></h4>
<p>Dans un premier temps, la variable à prédire est binaire : elle ne prend donc que deux valeurs 0/1 (ou -1/1). Dans le chapitre suivant, nous étudierons des algorithmes permettant d’aborder ce problème sous un angle classification. Ici, nous nous intéressons à une modélisation probabiliste, permettant notamment de prendre en compte le bruit dans les données.</p>
<div class="section" id="id2">
<h5>Modèle<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h5>
<p>On recherche une distribution conditionnelle <span class="math notranslate nohighlight">\(P(Y|X)\)</span> de la variable à prédire sachant les prédicteurs. Si le problème est en 0/1, alors <span class="math notranslate nohighlight">\(Y\)</span> est une variable indicatrice et on a <span class="math notranslate nohighlight">\(P(Y=1)=\mathbb{E}(Y)\)</span> et <span class="math notranslate nohighlight">\(P(Y=1|X=x)=\mathbb{E}(Y|X=x)\)</span>. La probabilité conditionnelle est donc l’espérance conditionnelle de l’indicatrice.</p>
<p>Supposons que <span class="math notranslate nohighlight">\(P(Y=1|X=x)=p(x,\boldsymbol\theta)\)</span> avec <span class="math notranslate nohighlight">\(p\)</span> fonction paramétrée par <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span>. On suppose également que les observations sont indépendantes. La vraisemblance est alors donnée par</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n P(Y=y_i|X=x_i) = \displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)</span></p>
<div class="proof remark dropdown admonition" id="remark-6">
<p class="admonition-title"><span class="caption-number">Remark 18 </span></p>
<div class="remark-content section" id="proof-content">
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> tirages d’une variable de Bernoulli dont la probabilité de succès est constante et vaut <span class="math notranslate nohighlight">\(p\)</span>, la vraisemblance est <span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}\)</span>. Cette vraisemblance est maximisée lorsque
<span class="math notranslate nohighlight">\(p=n^{-1}\displaystyle\sum_{i=1}^n y_i\)</span>.</p>
</div>
</div><p>En notant <span class="math notranslate nohighlight">\(p_i=p(x_i,\boldsymbol\theta)\)</span>, maximiser la vraisemblance sans contrainte amène à la solution non informative <span class="math notranslate nohighlight">\(p_i=1\)</span> si <span class="math notranslate nohighlight">\(y_i=1\)</span> et 0 sinon. Si l’on essaye d’ajouter des contraintes (relations entre les <span class="math notranslate nohighlight">\(p_i\)</span>), alors l’estimation du maximum de vraisemblance devient difficile.</p>
<p>Ici le modèle  <span class="math notranslate nohighlight">\(p_i=p(x_i,\boldsymbol\theta)\)</span> suppose que si <span class="math notranslate nohighlight">\(p\)</span> est continue, alors des valeurs proches de <span class="math notranslate nohighlight">\(x_i\)</span> amènent à des valeurs proches de <span class="math notranslate nohighlight">\(p_i\)</span>. En supposant <span class="math notranslate nohighlight">\(p\)</span> connue comme fonction de <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span>, la vraisemblance est une fonction de <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> et on peut estimer ce paramètre en maximisant la vraisemblance.</p>
</div>
<div class="section" id="id3">
<h5>Régression logistique<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h5>
<p>On recherche un ‘’bon’’ modèle pour <span class="math notranslate nohighlight">\(p\)</span> :</p>
<ol class="simple">
<li><p>On peut dans un premier temps supposer que <span class="math notranslate nohighlight">\(p(\mathbf x)\)</span> est une fonction linéaire de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. Les fonctions linéaires étant non bornées, elles ne peuvent modéliser des probabilités.</p></li>
<li><p>On peut alors supposer que <span class="math notranslate nohighlight">\(log\ p(\mathbf x)\)</span> est une fonction linéaire de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. Là aussi, la fonction logarithme est non bornée supérieurement, et ne peut modéliser une probabilité.</p></li>
<li><p>Partant de cette idée, on borne le logarithme en utilisant la transformation logistique (ou logit) <span class="math notranslate nohighlight">\(log\frac{p(\mathbf x)}{1-p(\mathbf x)}\)</span>. Etant donné un événement ayant une probabilité <span class="math notranslate nohighlight">\(p\)</span> de réussir, le rapport <span class="math notranslate nohighlight">\(p/(1-p)\)</span> est appelé la côte de l’événement (rapport de la probabilité qu’il se produise sur celle qu’il ne se produise pas. Si vous avez <span class="math notranslate nohighlight">\(p\)</span>=3/4 de chances de réussir à votre examen de permis, cotre côte est <span class="math notranslate nohighlight">\(p/(1-p)=\frac{3/4}{1/4}\)</span>=3 contre un). On peut alors supposer que cette fonction de <span class="math notranslate nohighlight">\(p\)</span> est linéaire en <span class="math notranslate nohighlight">\(\mathbf x\)</span>.</p></li>
</ol>
<p>Le modèle de régression logistique s’écrit alors formellement</p>
<p><span class="math notranslate nohighlight">\(logit(p(\mathbf x)) = log \frac{p(\mathbf x)}{1-p(\mathbf x)} = \beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\)</span></p>
<p>En résolvant par rapport à <span class="math notranslate nohighlight">\(p\)</span> on trouve alors</p>
<p><span class="math notranslate nohighlight">\(p(\mathbf x,\boldsymbol\theta) = \frac{e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}{1+e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}=\frac{1}{1+e^{-(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x)}}\quad\text{avec }\boldsymbol\theta=(\beta_0,\boldsymbol\beta)^T\)</span></p>
<p>Pour minimiser les erreurs de prédiction, on doit prédire <span class="math notranslate nohighlight">\(Y=1\)</span> si <span class="math notranslate nohighlight">\(p\geq 0.5\)</span>, soit <span class="math notranslate nohighlight">\(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\geq 0\)</span> et <span class="math notranslate nohighlight">\(Y=0\)</span> sinon. La régression logistique est donc un classifieur linéaire, dont la frontière de décision est justement l’hyperplan <span class="math notranslate nohighlight">\(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x= 0\)</span>. On peut montrer que la distance de <span class="math notranslate nohighlight">\(\mathbf x\)</span> à cet hyperplan est <span class="math notranslate nohighlight">\(\beta_0/\|\boldsymbol\beta\| + \mathbf x^T\boldsymbol\beta/\|\boldsymbol\beta\|\)</span>. Les probabilités d’appartenance de <span class="math notranslate nohighlight">\(\mathbf x\)</span> aux classes décroissent donc d’autant plus vite que <span class="math notranslate nohighlight">\(\|\boldsymbol\beta\|\)</span> est grand.</p>
<p>Dans la figure suivante, la probabilité d’appartenance à la classe 1 (points rouges) est donnée en fausses couleurs.</p>
<p><img alt="" src="_images/regression.png" /></p>
</div>
</div>
<div class="section" id="regression-logistique-a-plusieurs-classes">
<h4>Régression logistique à plusieurs classes<a class="headerlink" href="#regression-logistique-a-plusieurs-classes" title="Permalink to this headline">#</a></h4>
<p>Dans ce cas, <span class="math notranslate nohighlight">\(Y\)</span> peut prendre <span class="math notranslate nohighlight">\(k\)</span> valeurs. Le modèle reste le même, chaque classe <span class="math notranslate nohighlight">\(c\in[\![0,k-1]\!]\)</span> ayant son jeu de paramètres <span class="math notranslate nohighlight">\(\boldsymbol\theta_c=(\beta^c_0,\boldsymbol\beta^c)^T\)</span>. Les probabilités conditionnelles prédites sont alors</p>
<p><span class="math notranslate nohighlight">\((\forall c\in[\![0,k-1]\!])\;\;P(Y=c|X=\mathbf x) = \frac{e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}{1+e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}\)</span></p>
</div>
<div class="section" id="interpretation">
<h4>Interprétation<a class="headerlink" href="#interpretation" title="Permalink to this headline">#</a></h4>
<p>Si <span class="math notranslate nohighlight">\(\mathbf x=\mathbf 0\)</span>, alors <span class="math notranslate nohighlight">\(p(\mathbf x)=\frac{1}{1+e^{-\beta_0}}\)</span>. L’ordonnée à l’origine fixe donc le taux d’événements “de base”.</p>
<p>Supposons <span class="math notranslate nohighlight">\(\boldsymbol\beta\in\mathbb{R}\)</span> (l’interprétation sera la même dans le cas général). Considérons l’effet sur la probabilité d’un évènement du changement de <span class="math notranslate nohighlight">\(x\in\mathbb{R}\)</span> d’une unité, passant de <span class="math notranslate nohighlight">\(x_0\)</span> à <span class="math notranslate nohighlight">\(x_0+1\)</span>. Alors :</p>
<p><span class="math notranslate nohighlight">\(logit(p(x_0+1))-logit(p(x_0)) = \beta_0+\beta(x_0+1)-(\beta_0+\beta(x_0)) = \beta\)</span>
et en utilisant la définition de la fonction logit :</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
log \frac{p( x_0+1)}{1-p(x_0+1)}-log \frac{p( x_0)}{1-p(x_0)} &amp;=&amp; \beta\\
log \left  [\frac{\frac{p( x_0+1)}{1-p(x_0+1)}}{\frac{p( x_0)}{1-p(x_0)}} \right ]&amp;=&amp; \beta\\
\end{eqnarray*}\)</span></p>
<p>En notant OR (Odds Ratio, ou rapport de côte) le terme en argument du log, et en prenant l’exponentielle, on trouve <span class="math notranslate nohighlight">\(OR=e^\beta\)</span>. Le coefficient <span class="math notranslate nohighlight">\(\beta\)</span> est donc tel que <span class="math notranslate nohighlight">\(e^\beta\)</span> est le rapport de côte pour un changement unitaire de l’entrée <span class="math notranslate nohighlight">\(x\)</span>. Si <span class="math notranslate nohighlight">\(x\)</span> est incrémenté de deux unités, alors le rapport de côte est de <span class="math notranslate nohighlight">\(e^{2\beta}=(e^\beta)^2\)</span>, que l’on généralise facilement au cas d’un changement de <span class="math notranslate nohighlight">\(n\)</span> unités à OR=<span class="math notranslate nohighlight">\((e^\beta)^n\)</span>.</p>
<p>Dans le cas où <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> est un vecteur, sa ième composante est une estimation du changement de la probabilité d’un évènement correspondant à une augmentation d’une unité de la ième composante de <span class="math notranslate nohighlight">\(\mathbf x\)</span>, les autres composantes étant constantes.</p>
</div>
<div class="section" id="estimation-des-coefficients-de-la-regression-logistique">
<h4>Estimation des coefficients de la régression logistique<a class="headerlink" href="#estimation-des-coefficients-de-la-regression-logistique" title="Permalink to this headline">#</a></h4>
<p>D’après le modèle probabiliste, la distribution associée à la régression logistique est la loi binomiale. Pour <span class="math notranslate nohighlight">\(n\)</span> échantillons <span class="math notranslate nohighlight">\((x_i,y_i),i\in[\![1,n]\!]\)</span>, la vraisemblance s’écrit</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)</span></p>
<p>Pour estimer les paramètres <span class="math notranslate nohighlight">\(\beta_0\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> à partir des données, on maximise cette vraisemblance. On prend son logarithme, on calcule son gradient et on en déduit un système d’équations à résoudre. Cette approche amène à des calculs complexes, la formulation analytique n’étant pas simple, et une approximation numérique est en pratique mise en oeuvre pour trouver l’optimal.</p>
</div>
</div>
<div class="section" id="analyse-des-resultats-d-une-regression">
<h3>Analyse des résultats d’une régression<a class="headerlink" href="#analyse-des-resultats-d-une-regression" title="Permalink to this headline">#</a></h3>
<div class="section" id="etude-des-residus">
<h4>Etude des résidus<a class="headerlink" href="#etude-des-residus" title="Permalink to this headline">#</a></h4>
<p>L’étude des résidus <span class="math notranslate nohighlight">\( y_i- y^*_i\)</span> permet de repérer les observations aberrantes ou au contraire qui jouent un rôle fondamental dans la détermination de la régression. Elle permet également de vérifier que  le modèle linéaire est justifié.</p>
<p>Comme <span class="math notranslate nohighlight">\(\mathbf Y = \mathbf Y -\mathbf X\boldsymbol \beta +\mathbf X\boldsymbol \beta\)</span> , où <span class="math notranslate nohighlight">\(\mathbf Y-\mathbf X\boldsymbol \beta \)</span> est orthogonal à <span class="math notranslate nohighlight">\(\mathbf X\boldsymbol \beta\)</span>, la matrice de variance des résidus s’écrit</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbb{V}(\mathbf Y) &amp;=&amp; \mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+\mathbb{V}(\mathbf X\boldsymbol \beta)\\
\sigma^2 \mathbf{I} &amp;=&amp;\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+ \sigma^2 \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\\ 
\text {soit }\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)&amp;=&amp;\sigma^2(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T)
\end{eqnarray*}
\)</span></p>
<p>et les résidus sont donc en général corrélés entre eux.</p>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 19 </span></p>
<div class="remark-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\)</span> est la projection orthogonale sur <span class="math notranslate nohighlight">\(Im(\mathbf X)^\perp\)</span></p>
</div>
</div><p>Si <span class="math notranslate nohighlight">\(p_i\)</span> est le <span class="math notranslate nohighlight">\(i^e\)</span> terme diagonal du projecteur <span class="math notranslate nohighlight">\(\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\(\mathbb{V}( y_i- y^*_i) = (1-p_i)\sigma^2\)</span></p>
<p>d’où l’estimation de la variance du résidu <span class="math notranslate nohighlight">\(\hat{\mathbb{V}}(y_i-y^*_i) = (1-p_i)\hat{\sigma}^2\)</span>.</p>
<p>Si le modèle linéaire est justifié, alors la distribution des résidus suit approximativement une loi normale. Un test statistique (par exemple le test de Jarque-Berra) viendra confirmer ou infirmer l’hypothèse selon laquelle la distribution peut être considérée comme telle.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/nuagelin.png" /></p></th>
<th class="head"><p><img alt="" src="_images/nuagepaslin.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="" src="_images/reslin.png" /></p></td>
<td><p><img alt="" src="_images/respaslin.png" /></p></td>
</tr>
</tbody>
</table>
<div class="proof definition admonition" id="definition-8">
<span id="index-10"></span><p class="admonition-title"><span class="caption-number">Definition 39 </span> (Résidu studentisé)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle résidu studentisé la quantité <span class="math notranslate nohighlight">\(\frac{y_i-y^*_i}{\hat{\sigma}\sqrt{1-hp}}\)</span></p>
</div>
</div><p>Lorsque <span class="math notranslate nohighlight">\(n\)</span> est grand, ces résidus doivent être compris dans l’intervalle [-2,2].</p>
<p>Un fort résidu peut indiquer une valeur aberrante, mais la réciproque n’est pas vraie. Il est donc nécessaire d’étudier l’influence de chaque observation sur les résultats.</p>
</div>
<div class="section" id="influence-des-observations">
<h4>Influence des observations<a class="headerlink" href="#influence-des-observations" title="Permalink to this headline">#</a></h4>
<p>Pour étudier l’influence des observations sur la prédiction, deux approches sont possibles (et complémentaires) :</p>
<ol class="simple">
<li><p>étudier l’influence d’une observation sur sa propre prédiction. On calcule le résidu prédit <span class="math notranslate nohighlight">\(y_i-y_{\bar{i}}^*\)</span>, où <span class="math notranslate nohighlight">\(y_{\bar{i}}^*\)</span> est la prévision obtenue avec les <span class="math notranslate nohighlight">\(n-1\)</span> autres observations que <span class="math notranslate nohighlight">\(y_i\)</span>. Il est facile de montrer que ce résidu vaut <span class="math notranslate nohighlight">\(\frac{y_i-y_i^*}{1-p_i}\)</span></p></li>
</ol>
<div class="proof remark dropdown admonition" id="remark-9">
<p class="admonition-title"><span class="caption-number">Remark 20 </span></p>
<div class="remark-content section" id="proof-content">
<p>Il convient de rester prudent lorsque <span class="math notranslate nohighlight">\(p_i\)</span> est grand, et la quantité
<span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n  \frac{(y_i-y_i^*)^2}{(1-p_i)^2}\)</span>
est une mesure du pouvoir prédictif du modèle.</p>
</div>
</div><ol class="simple">
<li><p>étudier l’influence d’une observation sur les estimations des paramètres de la régression <span class="math notranslate nohighlight">\(\beta_i\)</span>. On peut par exemple calculer une distance, dite de Cook, entre <span class="math notranslate nohighlight">\(\boldsymbol \beta\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol \beta_{\bar{i}}\)</span> :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}}) = \frac{(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})^T\mathbf X^T \mathbf X(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})}{\hat{\sigma}^2(p+1)}=\frac{\|\mathbf Y^*-\mathbf Y_{\bar{i}}^*\|^2}{\hat{\sigma}^2(p+1)}\]</div>
<p>où <span class="math notranslate nohighlight">\(\mathbf Y_{\bar{i}}^*=\mathbf X\boldsymbol\beta_{\bar{i}}\)</span>. Si <span class="math notranslate nohighlight">\(d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}})&gt;1\)</span>, alors en général l’observation <span class="math notranslate nohighlight">\(i\)</span> a une influence anormale.</p>
</div>
<div class="section" id="stabilite-des-coefficients-de-regression">
<h4>Stabilité des coefficients de régression<a class="headerlink" href="#stabilite-des-coefficients-de-regression" title="Permalink to this headline">#</a></h4>
<p>La source principale d’instabilité dans l’estimation des paramètres de régression réside dans le fait que les variables explicatives sont très corrélées entre elles. Comme <span class="math notranslate nohighlight">\(\mathbb{V}(\boldsymbol \beta)=\sigma^2(\mathbf X^T\mathbf X)^{-1}\)</span> alors si les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> sont corrélés, la matrice <span class="math notranslate nohighlight">\(\mathbf X^T\mathbf X\)</span> est mal conditionnée. Dans ce cas, les paramètres sont estimés avec imprécision et les prédictions sont entâchées d’erreur. Il est donc essentiel de mesurer les colinéarités entre prédicteurs. Par simplicité (sans que cela nuise à la généralité), on suppose ici que les variables sont centrées et réduites : <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X)\)</span> est donc une matrice de taille <span class="math notranslate nohighlight">\(p\)</span> (le fait de centrer les données supprime la constante) et <span class="math notranslate nohighlight">\(\boldsymbol\beta\in\mathbb{R}^p\)</span>. Ainsi <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X)=n\mathbf R\)</span> où <span class="math notranslate nohighlight">\(\mathbf R\)</span> est la matrice de corrélation entre les prédicteurs.</p>
<p>Deux stratégies sont classiquement proposées :</p>
<ol class="simple">
<li><p>Facteur d’inflation de la variance : on a <span class="math notranslate nohighlight">\(\mathbb{V}(\boldsymbol \beta) = \sigma^2\frac{\mathbf{R}^{-1}}{n}\)</span> et <span class="math notranslate nohighlight">\(\sigma^2_{\beta_j} = \frac{\sigma^2}{n}(\mathbf{R}^{-1})_{jj}\)</span>. Or le <span class="math notranslate nohighlight">\(j^e\)</span> terme de <span class="math notranslate nohighlight">\(\mathbf{R}^{-1}\)</span> est <span class="math notranslate nohighlight">\(\frac{1}{1-R^2_j}\)</span> où <span class="math notranslate nohighlight">\(R^2_j\)</span> est le carré du coefficient de corrélation multiple de <span class="math notranslate nohighlight">\(\mathbf X_j\)</span> et des <span class="math notranslate nohighlight">\(p-1\)</span> autres variables explicatives. Ce terme est le facteur d’inflation de la variance. La moyenne de ces <span class="math notranslate nohighlight">\(p\)</span> termes est parfois utilisée comme indice global de colinéarité multiple.</p></li>
</ol>
<div class="proof remark dropdown admonition" id="remark-10">
<p class="admonition-title"><span class="caption-number">Remark 21 </span></p>
<div class="remark-content section" id="proof-content">
<p>Si les variables explicatives sont orthogonales, la régression multiple revient à <span class="math notranslate nohighlight">\(p\)</span> régressions simples.</p>
</div>
</div><ol class="simple">
<li><p>La factorisation spectrale de <span class="math notranslate nohighlight">\(\mathbf R\)</span> s’écrit <span class="math notranslate nohighlight">\(\mathbf R = \mathbf U\boldsymbol\Lambda \mathbf U^T\)</span>. Donc <span class="math notranslate nohighlight">\(\mathbf{R}^{-1}=\mathbf U\Lambda^{-1}\mathbf U^T\)</span> et la variance de <span class="math notranslate nohighlight">\(\beta_j\)</span> s’écrit</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\mathbb{V}(\beta_j) = \frac{\sigma^2}{n}\displaystyle\sum_{i=1}^p \frac{u_{ji}^2}{\lambda_i}\)</span></p>
<p>et dépend donc des inverses des valeurs propres de <span class="math notranslate nohighlight">\(\mathbf R\)</span>. Dans le cas où les prédicteurs sont fortement corrélés, les dernières valeurs propres sont proches de 0 ce qui entraîne l’instabilité des paramètres de régression.</p>
<p>Pour améliorer la stabilité des paramètres de régression, on peut alors :</p>
<ul class="simple">
<li><p>rejeter certains termes de la somme précédente, par exemple en remplaçant les <span class="math notranslate nohighlight">\(p\)</span> prédicteurs par leurs <span class="math notranslate nohighlight">\(p\)</span> composantes principales (Ceci revient à effectuer <span class="math notranslate nohighlight">\(p\)</span> régressions simples).</p></li>
<li><p>régulariser la régression en utilisant des approche de type Ridge regression.</p></li>
</ul>
</div>
</div>
<div class="section" id="selection-des-variables">
<h3>Sélection des variables<a class="headerlink" href="#selection-des-variables" title="Permalink to this headline">#</a></h3>
<p>Plutôt que d’expliquer <span class="math notranslate nohighlight">\(\mathbf Y\)</span> par l’ensemble des prédicteurs, on peut chercher un sous-ensemble de ces <span class="math notranslate nohighlight">\(p\)</span> variables permettant d’obtenir quasiment le même résultat (régression). Nous avons déjà abordé la sélection de variables dans un chapitre précédent.</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<div class="section" id="donnees">
<h4>Données<a class="headerlink" href="#donnees" title="Permalink to this headline">#</a></h4>
<p>On s’intéresse aux données suivantes et on cherche s’il existe une relation entre la production <span class="math notranslate nohighlight">\(Y\)</span> et les deux variables prédictives <span class="math notranslate nohighlight">\(X_1\)</span> et <span class="math notranslate nohighlight">\(X_2\)</span>.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Usine</p></th>
<th class="head"><p>Travail (h) <span class="math notranslate nohighlight">\(X_1\)</span></p></th>
<th class="head"><p>Capital (machines/h) <span class="math notranslate nohighlight">\(X_2\)</span></p></th>
<th class="head"><p>Production (<span class="math notranslate nohighlight">\(10^2\)</span> T)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1100</p></td>
<td><p>300</p></td>
<td><p>60</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1200</p></td>
<td><p>400</p></td>
<td><p>120</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1430</p></td>
<td><p>420</p></td>
<td><p>190</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>1500</p></td>
<td><p>400</p></td>
<td><p>250</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>1520</p></td>
<td><p>510</p></td>
<td><p>300</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>1620</p></td>
<td><p>590</p></td>
<td><p>360</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>1800</p></td>
<td><p>600</p></td>
<td><p>380</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>1820</p></td>
<td><p>630</p></td>
<td><p>430</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>1800</p></td>
<td><p>610</p></td>
<td><p>440</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id4">
<h4>Modèle<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h4>
<p>On fait l’hypothèse d’un modèle linéaire</p>
<p><span class="math notranslate nohighlight">\(y = \beta_0+\beta_1 X_1 + \beta_2 X_2+\varepsilon = \mathbf X \boldsymbol\beta+\boldsymbol\varepsilon\)</span></p>
<p>On a alors <span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y = \begin{pmatrix} -437.714\\0.336\\0.410\end{pmatrix}\)</span> et l’équation du modèle linéaire (hyperplan) aux moindres carrés est</p>
<p><span class="math notranslate nohighlight">\(y = -437.714+0.336 X_1+0.41X_2\)</span></p>
<p>De plus
<span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1} = \frac{3194}{6} = 639\)</span></p>
<p>de sorte que la covariance des paramètres de régression vaut</p>
<p><span class="math notranslate nohighlight">\(\hat{\sigma}^2 (\mathbf X^T\mathbf X)^{-1} = \begin{pmatrix} 3355.56 &amp; -4.152 &amp; 6.184\\-4.152 &amp; 0.008 &amp; -0.016 \\ 6.184 &amp; -0.016 &amp; 0.038\end{pmatrix}\)</span></p>
<p>Dans la figure suivante, les points au-dessus du plan regresseur sont en bleu, les autres en vert.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/plan.png" /></p></th>
<th class="head"><p><img alt="" src="_images/plan2.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Un point de vue…</p></td>
<td><p>Un autre</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-clustering"></span><h1> Quelques méthodes de classification </h1>
<h2> Introduction </h2>
La classification automatique a pour but d'obtenir une représentation simplifiée des données initiales. Elle consiste à organiser un ensemble de données en classes homogènes ou classes naturelles. 
<p>Une définition formelle de la classification, qui puisse servir de base à un processus automatisé, amène à se poser les questions suivantes :</p>
<ul class="simple">
<li><p>Comment les objets à classer sont-ils définis ?</p></li>
<li><p>Comment définir la notion de ressemblance entre objets ?</p></li>
<li><p>Qu’est-ce qu’une classe ?</p></li>
<li><p>Comment sont structurées les classes ?</p></li>
<li><p>Comment juger une classification par rapport à une autre ?</p></li>
</ul>
<p>Pour effectuer cette classification, deux démarches sont généralement utilisées :</p>
<ul class="simple">
<li><p>on regroupe en classes les objets qui partagent certaines caractéristiques.</p></li>
<li><p>on regroupe en classes les objets qui possèdent des caractéristiques proches. C’est cette approche qui est étudiée ici</p></li>
</ul>
<div class="section" id="structures-de-classification">
<h2>Structures de classification<a class="headerlink" href="#structures-de-classification" title="Permalink to this headline">#</a></h2>
<div class="section" id="partition">
<h3>Partition<a class="headerlink" href="#partition" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="definition-0">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 40 </span> (Partition)</p>
<div class="definition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\Omega\)</span> étant un ensemble fini, un ensemble <span class="math notranslate nohighlight">\(P =(P_1 ,P_2 ,\cdots  P_g )\)</span> de parties non vides de   <span class="math notranslate nohighlight">\(\Omega\)</span> est une partition si :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall k\neq l) P_k \cap P_l=\emptyset\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\displaystyle\cup_{i=1}^gP_i=\Omega\)</span></p></li>
</ul>
</div>
</div><p>Dans un ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> partitionné en <span class="math notranslate nohighlight">\(g\)</span> classes, chaque élément de l’ensemble appartient à une classe et une seule. Une manière pratique de décrire cette partition <span class="math notranslate nohighlight">\(P\)</span> consiste à lui associer la matrice de classification <span class="math notranslate nohighlight">\({\bf C}=(c_{ij}), i\in [\![1,n]\!], j\in [\![1,g]\!]\)</span>, avec <span class="math notranslate nohighlight">\(c_{ij}=1\)</span> si l’individu <span class="math notranslate nohighlight">\(i\)</span> appartient à <span class="math notranslate nohighlight">\(P_j\)</span>, et <span class="math notranslate nohighlight">\(c_{ij}=0\)</span> sinon. Dans le cas où l’on accepte qu’un individu appartienne à plusieurs classes (avec des degrés d’appartenance), on autorise <span class="math notranslate nohighlight">\(c_{ij}\)</span> à couvrir l’intervalle [0,1] et on parle alors de classification floue.</p>
</div>
<div class="section" id="hierarchie-indicee">
<h3>Hiérarchie indicée<a class="headerlink" href="#hierarchie-indicee" title="Permalink to this headline">#</a></h3>
<div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 41 </span> (Hiérarchie)</p>
<div class="definition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\Omega\)</span> étant un ensemble fini, un ensemble <span class="math notranslate nohighlight">\(H\)</span> de parties non vides de <span class="math notranslate nohighlight">\(\Omega\)</span> est une hiérarchie sur <span class="math notranslate nohighlight">\(\Omega\)</span> si :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega \in H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall x\in \Omega) \{x\}\in H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall h,h'\in H) h\cap h'=\emptyset\)</span> ou <span class="math notranslate nohighlight">\(h\subset h'\)</span> ou <span class="math notranslate nohighlight">\(h'\subset h\)</span></p></li>
</ul>
</div>
</div><p>Une hiérarchie est souvent représentée par l’intermédiaire d’un indice, fonction <span class="math notranslate nohighlight">\(i\)</span> de <span class="math notranslate nohighlight">\(H\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}^+\)</span>, strictement croissante vis à vis de l’inclusion et de noyau l’ensemble des singletons de <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
</div>
<div class="section" id="partition-et-hierarchie">
<h3>Partition et hiérarchie<a class="headerlink" href="#partition-et-hierarchie" title="Permalink to this headline">#</a></h3>
<p>Si <span class="math notranslate nohighlight">\(P =(P_1 \cdots,P_g)\)</span> est une partition de <span class="math notranslate nohighlight">\(\Omega\)</span>, l’ensemble <span class="math notranslate nohighlight">\(H\)</span> formé des classes <span class="math notranslate nohighlight">\(P_k\)</span> de <span class="math notranslate nohighlight">\(P\)</span>, des singletons de   <span class="math notranslate nohighlight">\(\Omega\)</span> et de l’ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> lui-même forme une hiérarchie. Remarquons qu’inversement, il est possible d’associer à chaque niveau d’une hiérarchie indicée une partition. Une hiérarchie indicée correspond donc à un ensemble de partitions emboîtées.</p>
</div>
</div>
<div class="section" id="objectifs-de-la-classification">
<h2>Objectifs de la classification<a class="headerlink" href="#objectifs-de-la-classification" title="Permalink to this headline">#</a></h2>
<div class="section" id="difficultes-de-caracteriser-les-objectifs">
<h3>Difficultés de caractériser les objectifs<a class="headerlink" href="#difficultes-de-caracteriser-les-objectifs" title="Permalink to this headline">#</a></h3>
<p>L’objectif de la classification automatique est l’organisation en classes homogènes des éléments d’un ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span>. Pour définir cette notion de classes homogènes, on utilise le plus souvent une mesure de similarité (ou de dissimilarité) sur  <span class="math notranslate nohighlight">\(\Omega\)</span>. Par exemple, on peut imposer à un couple quelconque d’individus d’une même classe d’être plus “proches” que n’importe quel couple formé par un individu de la classe et un individu d’une autre classe. En pratique, cet objectif est inutilisable, et plusieurs démarches sont alors utilisées pour remplacer cet objectif trop difficile à atteindre.</p>
</div>
<div class="section" id="demarche-numerique">
<h3>Démarche numérique<a class="headerlink" href="#demarche-numerique" title="Permalink to this headline">#</a></h3>
<div class="section" id="id1">
<h4>Partition<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<p>On remplace cette condition trop exigeante par une fonction numérique (critère) qui mesure la qualité d’homogénéité d’une partition. Le problème peut paraître alors très simple. En effet, par exemple, dans le cas de la recherche d’une partition, il suffit de chercher parmi l’ensemble fini de toutes les partitions celle qui optimise le critère numérique. Malheureusement, le nombre de ces partitions étant très grand, leur énumération est impossible dans un temps raisonnable.
Le nombre de partitions en <span class="math notranslate nohighlight">\(g\)</span> classes d’un ensemble à <span class="math notranslate nohighlight">\(n\)</span> éléments, que l’on note <span class="math notranslate nohighlight">\(S_n^g\)</span> est le nombre de Stirling de deuxième espèce. En posant <span class="math notranslate nohighlight">\(S_0^0=1\)</span> et pour tout <span class="math notranslate nohighlight">\(n&gt;0\)</span>, <span class="math notranslate nohighlight">\(S_n^0=S_0^n=0\)</span>, il peut être calculé par récurrence grâce à la relation <span class="math notranslate nohighlight">\(S_n^g=S_{n-1}^{g-1}+gS_{n-1}^g\)</span>. On peut montrer que</p>
<div class="math notranslate nohighlight">
\[\begin{split}S_n^g = \frac{1}{g!}\displaystyle\sum_{i=1}^g (-1)^{g-i}\begin{pmatrix}g\\ i \end{pmatrix}i^n\end{split}\]</div>
<p>et donc <span class="math notranslate nohighlight">\(S_n^g\sim \frac{g^n}{g!}\)</span> lorsque <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>. En pratique, sur un ordinateur calculant <span class="math notranslate nohighlight">\(10^6\)</span> partitions par seconde, il faut 126 000 ans pour calculer l’ensemble des partitions d’un ensemble à <span class="math notranslate nohighlight">\(n=25\)</span> éléments.</p>
<p>On utilise alors des heuristiques qui donnent, non pas la meilleure solution, mais une “bonne solution”, proche de la solution optimale. On parle alors d’optimisation locale. Lorsqu’il existe une structure d’ordre sur l’ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> et que celle-ci doit être respectée par la partition, il existe un algorithme de programmation dynamique (algorithme de Fisher), qui fournit la solution optimale.</p>
</div>
<div class="section" id="hierarchie">
<h4>Hiérarchie<a class="headerlink" href="#hierarchie" title="Permalink to this headline">#</a></h4>
<p>Dans le cas d’une hiérarchie, on cherche à obtenir des classes d’autant plus homogènes qu’elles sont situées dans le bas de la hiérarchie. La définition d’un critère est moins facile. Nous verrons qu’il est possible de le faire en utilisant la
notion d’ultramétrique (ultramétrique optimale).</p>
</div>
</div>
<div class="section" id="demarche-algorithmique">
<h3>Démarche algorithmique<a class="headerlink" href="#demarche-algorithmique" title="Permalink to this headline">#</a></h3>
<p>Il s’agit cette fois de définir directement un algorithme qui construit des classes homogènes en tenant compte de la mesure de similarité. Il est relativement facile de proposer de tels algorithmes, le problème est de pouvoir vérifier que les résultats fournis sont intéressants et répondent au problème posé. En réalité, cette démarche rejoint assez souvent la précédente.</p>
</div>
<div class="section" id="mesure-de-dissimilarite-et-distance">
<h3>Mesure de dissimilarité et distance<a class="headerlink" href="#mesure-de-dissimilarite-et-distance" title="Permalink to this headline">#</a></h3>
<p>Les algorithmes de classification dépendent d’une métrique qui définit implicitement la forme des classes qui seront calculées. Si la distance euclidienne suppose une isotropie dans les axes (et donc une représentation sphérique des classes), d’autres distances ou indices de dissimilarité peuvent être utilisés.</p>
<div class="section" id="indice-de-dissimilarite">
<h4>Indice de dissimilarité<a class="headerlink" href="#indice-de-dissimilarite" title="Permalink to this headline">#</a></h4>
<p>On se place dans <span class="math notranslate nohighlight">\(\mathbb R^d\)</span>, et on considère <span class="math notranslate nohighlight">\(n\)</span> individus à classer <span class="math notranslate nohighlight">\({\bf x_1}\ldots {\bf x_n}\)</span>.</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 42 </span> (Dissimilarité - ultramétrique)</p>
<div class="definition-content section" id="proof-content">
<p>Une mesure de dissimilarité <span class="math notranslate nohighlight">\(\delta\)</span> est une fonction de</p>
<p><span class="math notranslate nohighlight">\(
 \delta: \begin{array}{ccc}
\mathbb{R}^d\times\mathbb{R}^d &amp;\rightarrow &amp;\mathbb{R}^+\\
(\mathbf x_i,\mathbf x_j)&amp;\mapsto &amp; \delta_{ij} = \delta(\mathbf x_i,\mathbf x_j)
\end{array}
\)</span></p>
<p>vérifiant :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall i,j\in[\![1, n]\!])\ \delta_{ij}=\delta_{ji}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall i\in[\![1, n]\!])\ \delta_{ii}= 0\)</span></p></li>
</ul>
<p>Si l’inégalité triangulaire <span class="math notranslate nohighlight">\(\delta_{ij}\leq \delta_{ik}+\delta_{kj}\)</span> est de plus vérifiée pour tout <span class="math notranslate nohighlight">\(i,j,k\)</span>, alors <span class="math notranslate nohighlight">\(\delta\)</span> est une distance.</p>
<p>Si enfin l’inégalité ultramétrique  <span class="math notranslate nohighlight">\(\delta_{ij}\leq max(\delta_{ik}+\delta_{jk})\)</span> est  vérifiée pour tout <span class="math notranslate nohighlight">\(i,j,k\)</span>, <span class="math notranslate nohighlight">\(\delta\)</span> est une ultramétrique.</p>
</div>
</div><p>A partir des mesures de dissimilarité, on déduit des mesures de similarité <span class="math notranslate nohighlight">\(s_{ij}\)</span> le passage de l’une à l’autre se faisant par exemple par <span class="math notranslate nohighlight">\(\delta_{ij} = s_{max}-s_{ij}\)</span>.</p>
</div>
<div class="section" id="cas-de-variables-qualitatives">
<h4>Cas de variables qualitatives<a class="headerlink" href="#cas-de-variables-qualitatives" title="Permalink to this headline">#</a></h4>
<p>On suppose que les <span class="math notranslate nohighlight">\(d\)</span> composantes des <span class="math notranslate nohighlight">\({\bf x_i}\)</span> sont qualitatives, et on se limite ici au cas de variables bimodales.
Étant donnés <span class="math notranslate nohighlight">\({\bf x_i}=\begin{pmatrix} x_i^1\ldots x_i^d\end{pmatrix}\)</span> et <span class="math notranslate nohighlight">\({\bf x_j}\)</span>, on note :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a_{ij}\)</span> le nombre de co-occurences entre les individus <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_{ij}\)</span> le nombre de co-absences entre les individus <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c_{ij}\)</span> le nombre d’attributs présents chez <span class="math notranslate nohighlight">\(i\)</span> et absents chez <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_{ij}\)</span> le nombre d’attributs absents chez <span class="math notranslate nohighlight">\(i\)</span> et présents chez <span class="math notranslate nohighlight">\(j\)</span></p></li>
</ul>
<p>les mesures suivantes sont des exemples de dissimilarité :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \sqrt{b_{ij}+c_{ij}}\)</span> [distance “euclidienne” binaire]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [différence binaire de taille]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}c_{ij})}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [différence binaire de motif]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(a_{ij}+b_{ij}+c_{ij}+d_{ij})(b_{ij}+c_{ij})-(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [différence binaire de forme]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{4(a_{ij}+b_{ij}+c_{ij}+d_{ij})}\)</span> [dissimilarité binaire de variance]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{2a_{ij}+b_{ij}+c_{ij}}\)</span> [dissimilarité binaire de Lance et Williams]</p></li>
</ul>
</div>
<div class="section" id="cas-de-variables-quantitatives">
<h4>Cas de variables quantitatives<a class="headerlink" href="#cas-de-variables-quantitatives" title="Permalink to this headline">#</a></h4>
<p>Dans le cas de variables quantitatives, les normes  <span class="math notranslate nohighlight">\(L_p\)</span> :</p>
<p><span class="math notranslate nohighlight">\(\|{\bf x_i}\|_p=\left (\displaystyle\sum_{j=1}^d|x_i^j|^p\right ) ^\frac{1}{p}\)</span></p>
<p>sont classiquement utilisées, et par exemple</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p=1\)</span> : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_1=\displaystyle\sum_{k=1}^d|x_i^k-x_j^k|\)</span> est la norme <span class="math notranslate nohighlight">\(L_1\)</span> (ou city block).</p></li>
<li><p><span class="math notranslate nohighlight">\(p=2\)</span> : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_2=\sqrt{\displaystyle\sum_{k=1}^d(x_i^k-x_j^k)^2}\)</span> est la norme <span class="math notranslate nohighlight">\(L_2\)</span> (ou norme euclidienne).</p></li>
<li><p>“<span class="math notranslate nohighlight">\(p=\infty\)</span>” : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_\infty = \displaystyle\max_{1\leq k\leq d}\{|x_i^k-x_j^k|\}\)</span> est la norme du max (ou norme de Tchebychev)</p></li>
</ul>
<p>Si les variables ne sont pas normalisées, on peut utiliser la distance de Mahalanobis</p>
<p><span class="math notranslate nohighlight">\(\delta_{ij} = \displaystyle\sum_{k=1}^d\displaystyle\sum_{l=1}^dw_{kl}(x_i^k-x_j^k)(x_i^l-x_j^l)\)</span></p>
<p>où la matrice des <span class="math notranslate nohighlight">\(w_{kl}\)</span> est l’inverse de la matrice de covariance empirique. Cette distance élimine également les corrélations entre variables.</p>
<p>Enfin, on peut utiliser une métrique issue du coefficient de corrélation, dite distance de Pearson : <span class="math notranslate nohighlight">\(\delta_{ij} =\sqrt{1-r^2_{ij}}\)</span>, avec</p>
<p><span class="math notranslate nohighlight">\(r^2_{ij} = \frac{\left (\displaystyle\sum_{k=1}^d (x_i^k-\bar{x_i})(x_j^k-\bar{x_j})\right )^2}{\displaystyle\sum_{k=1}^d(x_i^k-\bar{x_i})^2\displaystyle\sum_{k=1}^d(x_j^k-\bar{x_j})^2}\)</span></p>
</div>
<div class="section" id="variables-de-comptage">
<h4>Variables de comptage<a class="headerlink" href="#variables-de-comptage" title="Permalink to this headline">#</a></h4>
<p>Dans le cas particulier de variables de comptage (<span class="math notranslate nohighlight">\(x_i^k\)</span> effectif de la classe <span class="math notranslate nohighlight">\(k\)</span> pour l’individu <span class="math notranslate nohighlight">\(i\)</span>), une mesure naturelle de dissimilarité entre <span class="math notranslate nohighlight">\({\bf x_i}\)</span> et <span class="math notranslate nohighlight">\({\bf x_j}\)</span> est le <span class="math notranslate nohighlight">\(\chi^2\)</span> du tableau de contingence 2<span class="math notranslate nohighlight">\(\times d\)</span> associé.</p>
</div>
<div class="section" id="quelle-mesure-choisir">
<h4>Quelle mesure choisir ?<a class="headerlink" href="#quelle-mesure-choisir" title="Permalink to this headline">#</a></h4>
<p>Une réflexion  sur le type de dissimilarité à choisir est nécessaire. Il est en particulier intéressant de répondre aux questions suivantes:</p>
<ul class="simple">
<li><p>de quelles variables initiales (qualitatives et/ou quantitatives) doit dépendre la dissimilarité?</p></li>
<li><p>est-il souhaitable (et possible) d’obtenir des variables pertinentes supplémentaires? Si oui par mesure ? par analyse linéaire (ACP,…) ou non linéaire (manifold learning) ?</p></li>
<li><p>quelles doivent être les importances relatives des diverses variables retenues dans la constitution de la dissimilarité ?</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="classification-ascendante-hierarchique">
<h2>Classification ascendante hiérarchique<a class="headerlink" href="#classification-ascendante-hierarchique" title="Permalink to this headline">#</a></h2>
<p>L’objectif est de construire une hiérarchie indicée d’un ensemble <span class="math notranslate nohighlight">\(\Omega\)</span> sur lequel on connaît une mesure de dissimilarité <span class="math notranslate nohighlight">\(\delta\)</span> telle que les points les plus proches soient regroupés dans les classes de plus petit indice. La hiérarchie est alors construite en appliquant itérativement ce principe, et l’arbre obtenu sur l’ensemble des itérations est appelé un dendrogramme.</p>
<p>Il existe essentiellement
deux approches :</p>
<ul class="simple">
<li><p>la classification descendante : on divise <span class="math notranslate nohighlight">\(\Omega\)</span> en classes, puis on recommence sur chacune de ces classes itérativement jusqu’à ce que les classes soient réduites à des singletons.</p></li>
<li><p>la classification ascendante : cette fois on part de la partition de <span class="math notranslate nohighlight">\(\Omega\)</span>  où chaque classe est un singleton. On procède alors par fusions successives des classes jusqu’à obtenir une seule classe, c’est-à -dire l’ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> lui-même. Nous insistons sur ce type de classification dans la suite.</p></li>
</ul>
<div class="section" id="algorithme">
<h3>Algorithme<a class="headerlink" href="#algorithme" title="Permalink to this headline">#</a></h3>
<div class="section" id="construction-de-la-hierarchie">
<span id="index-2"></span><h4>Construction de la hiérarchie<a class="headerlink" href="#construction-de-la-hierarchie" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(\Omega\)</span>  étant l’ensemble à classifier et <span class="math notranslate nohighlight">\(\delta\)</span> une mesure de dissimilarité sur cet ensemble, on définit à partir de <span class="math notranslate nohighlight">\(\delta\)</span> une  distance <span class="math notranslate nohighlight">\(D\)</span> entre les parties de  <span class="math notranslate nohighlight">\(\Omega\)</span>. Cette distance est en réalité une mesure de dissimilarité qui ne vérifie pas nécessairement toutes les propriétés d’une distance sur l’ensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span>. En général, <span class="math notranslate nohighlight">\(D\)</span> est appelé critère d’agrégation.
L’algorithme est alors le suivant :</p>
<div class="proof algorithm admonition" id="algorithm-3">
<p class="admonition-title"><span class="caption-number">Algorithm 4 </span> (Algorithme de clustering hiérarchique ascendant)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> Les éléments de <span class="math notranslate nohighlight">\(\Omega\)</span></p>
<p><strong>Sortie :</strong> Une hiérarchie</p>
<ol class="simple">
<li><p>Initialisation : partition des singletons</p></li>
<li><p>Calcul des distances entre classes.</p></li>
<li><p>Tant que le nombre de classes est <span class="math notranslate nohighlight">\(&gt;\)</span>1</p>
<ol class="simple">
<li><p>Regroupement des 2 classes les plus proches au sens de <span class="math notranslate nohighlight">\(D\)</span></p></li>
<li><p>Calcul des distances entre la nouvelle classe et les anciennes classes non regroupées.</p></li>
</ol>
</li>
</ol>
</div>
</div><p>Il est facile de montrer que l’ensemble des classes définies au cours de cet algorithme forme une hiérarchie.</p>
</div>
<div class="section" id="construction-de-l-indice">
<h4>Construction de l’indice<a class="headerlink" href="#construction-de-l-indice" title="Permalink to this headline">#</a></h4>
<p>Après avoir défini une hiérarchie, il est nécessaire de lui associer un indice. Pour les classes du bas de la hiérarchie, c’est-à-dire les singletons, cet indice est nécessairement la valeur 0. Pour les autres classes, cet indice est généralement
défini en associant à chacune des classes construites au cours de l’algorithme la distance <span class="math notranslate nohighlight">\(D\)</span> qui séparait les deux classes fusionnées pour former cette nouvelle classe. Pour que cette définition conduise bien à un indice, il est nécessaire que
les indices obtenus soient strictement croissants avec le niveau de la hiérarchie. Plusieurs difficultés peuvent alors apparaître :</p>
<ul class="simple">
<li><p>pour certains critères d’agrégation, l’indice ainsi défini n’est pas nécessairement croissant. On parle alors d’inversion. Par exemple, si les données sont formées par trois points du plan situés au sommet d’un triangle équilatéral de côté 1 et si on prend comme distance <span class="math notranslate nohighlight">\(D\)</span> entre classes la distance entre les centres de gravité, on obtient une inversion.</p></li>
<li><p>lorsqu’il y a égalité de l’indice pour plusieurs niveaux emboîtés, il suffit de filtrer la hiérarchie, c’est-à-dire conserver une seule classe qui regroupe toutes les classes emboîtées ayant le même indice.</p></li>
</ul>
</div>
</div>
<div class="section" id="criteres-d-agregation">
<h3>Critères d’agrégation<a class="headerlink" href="#criteres-d-agregation" title="Permalink to this headline">#</a></h3>
<p>Il existe de nombreux critères d’agrégation, mais les plus utilisés sont les suivants :</p>
<ul class="simple">
<li><p>critère du lien commun : <span class="math notranslate nohighlight">\(D_{min}(A,B)=\displaystyle\min_{i\in A,j\in B}\delta_{ij}\)</span></p></li>
<li><p>critère du lien maximum: <span class="math notranslate nohighlight">\(D_{max}(A,B)=\displaystyle\max_{i\in A,j\in B}\delta_{ij}\)</span></p></li>
<li><p>critère du lien moyen : <span class="math notranslate nohighlight">\(D_{moy}(A,B)=\frac{\displaystyle\sum_{i\in A}\displaystyle\sum_{j\in B}\delta_{ij}}{|A||B|}\)</span></p></li>
</ul>
<p><img alt="" src="_images/agreg.png" /></p>
</div>
<div class="section" id="formule-de-recurrence-de-lance-et-williams">
<h3>Formule de récurrence de Lance et Williams<a class="headerlink" href="#formule-de-recurrence-de-lance-et-williams" title="Permalink to this headline">#</a></h3>
<p>Pour les trois critères d’agrégation précédents, il existe des relations de simplification du calcul des distances entre classes essentielles pour la mise en place pratique de l’algorithme de classification ascendante :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D_{min}(A,B\cup C)=min(D_{min}(A,B),D_{min}(A,C))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{max}(A,B\cup C)=max(D_{max}(A,B),D_{min}(A,C))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{moy}(A,B\cup C)=\frac{|B|D_{moy}(A,B)+|C|D_{moy}(A,C)}{|B|+|C|}\)</span></p></li>
</ul>
</div>
<div class="section" id="critere-de-ward">
<h3>Critère de Ward<a class="headerlink" href="#critere-de-ward" title="Permalink to this headline">#</a></h3>
<p id="index-3">Lorsque l’ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> à classifier est mesuré par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives, il est possible de lui associer un nuage de points pondérés dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> muni de la distance euclidienne. Généralement, les pondérations seront toutes égales à 1. Le critère d’agrégation le plus utilisé dans cette situation est alors le critère d’inertie de Ward :</p>
<p><span class="math notranslate nohighlight">\(D(A,B)=\frac{p_Ap_B}{p_A+p_B}\|({\bf g}(A),{\bf g}(B))\|_2^2\)</span></p>
<p>où <span class="math notranslate nohighlight">\(p_E\)</span> représente la somme des pondérations des éléments d’une classe <span class="math notranslate nohighlight">\(E\)</span> et <span class="math notranslate nohighlight">\({\bf g}(E)\)</span> est le centre de gravité d’une classe <span class="math notranslate nohighlight">\(E\)</span>.</p>
</div>
<div class="section" id="proprietes-d-optimalite">
<h3>Propriétés d’optimalité<a class="headerlink" href="#proprietes-d-optimalite" title="Permalink to this headline">#</a></h3>
<p>La notion de hiérarchie indicée est équivalente à la notion d’ultramétrique. La classification hiérarchique ascendante transforme donc la mesure de dissimilarité <span class="math notranslate nohighlight">\(d\)</span> initiale en une mesure de dissimilarité <span class="math notranslate nohighlight">\(\delta\)</span> qui possède la propriété d’être une ultramétrique.</p>
<p>Le problème de la classification hiérarchique peut donc également se poser en ces termes : trouver l’ultramétrique <span class="math notranslate nohighlight">\(\delta^*\)</span> la plus proche de <span class="math notranslate nohighlight">\(\delta\)</span>. Il reste à munir l’espace des mesures de dissimilarité sur  <span class="math notranslate nohighlight">\(\Omega\)</span> d’une distance. On pourra utiliser, par exemple :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}(\delta_{ij}-\delta^*_{ij})^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}|\delta_{ij}-\delta^*_{ij}|\)</span></p></li>
</ul>
</div>
<div class="section" id="critere-d-arret-et-partition">
<h3>Critère d’arrêt et partition<a class="headerlink" href="#critere-d-arret-et-partition" title="Permalink to this headline">#</a></h3>
<p id="index-4">L’ensemble des itérations peut être visualisé sous la forme d’un arbre, appelé dendrogramme. La figure suivante présente un exemple de dendrogramme en clustering hiérarchique descendant sur <span class="math notranslate nohighlight">\(X = \{a, b, c, d, e\}\)</span>. La distance <span class="math notranslate nohighlight">\(D\)</span> n’est pas reportée</p>
<p><img alt="" src="_images/dendro1.png" /></p>
<p>Le critère d’arrêt permet de déterminer la partition  de <span class="math notranslate nohighlight">\(X\)</span> la plus appropriée. Ici encore, plusieurs choix sont possibles :</p>
<ul class="simple">
<li><p>en fixant a priori un nombre de classes</p></li>
<li><p>en fixant une borne supérieure <span class="math notranslate nohighlight">\(r\)</span> pour <span class="math notranslate nohighlight">\(D\)</span>, et en stoppant les itérations dès que les distances calculées par les liens dépassent <span class="math notranslate nohighlight">\(r\)</span>. A noter que <span class="math notranslate nohighlight">\(r\)</span> peut être également calculé par <span class="math notranslate nohighlight">\(r=\alpha max\{\delta(x,y),x,y\in X\}\)</span> (critère dit “scale distance upper bound”).</p></li>
<li><p>en coupant le dendrogramme au saut de distance <span class="math notranslate nohighlight">\(D\)</span> maximal.</p></li>
</ul>
<p><img alt="" src="_images/dendro2.png" /></p>
</div>
<div class="section" id="utilisation-des-methodes">
<h3>Utilisation des méthodes<a class="headerlink" href="#utilisation-des-methodes" title="Permalink to this headline">#</a></h3>
<p>La première difficulté est le choix de la mesure de dissimilarité sur  <span class="math notranslate nohighlight">\(\Omega\)</span> et du critère d’agrégation. Généralement, lorsque l’on dispose de variables quantitatives, le critère conseillé est le critère d’inertie. Ensuite, il est souvent nécessaire de disposer d’outils d’aide à l’interprétation et d’outils permettant de diminuer le nombre de niveaux de hiérarchie. Il est d’autre part conseillé d’utiliser conjointement d’autres méthodes d’analyse des données comme l’Analyse en Composantes Principales.</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<p>On étudie ici un jeu de données correspondant aux achats dans un supermarché. On cherche à caractériser les comportements des acheteurs en fonction de leurs revenus</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./data/Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>Genre</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>On affiche les données</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Score/Revenu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;Revenu annuel (k$)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution des âges et des scores d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_3_0.png" src="_images/clustering_3_0.png" />
</div>
</div>
<p>L’objectif est de trouver des catégories de population ayant les mêmes comportements d’achat. Le nombre de classes étant inconnu, la classification héararchique va permettre de donner des indications sur le nombre de groupes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="k">as</span> <span class="nn">sch</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dendrogramme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Clients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Indice&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">190</span><span class="p">,</span><span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">xmax</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">220</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="s1">&#39;Cut&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">dendrogram</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">linkage</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;ward&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_5_0.png" src="_images/clustering_5_0.png" />
</div>
</div>
<p>On projette ensuite le résultat de la classification</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">linkage</span> <span class="o">=</span> <span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="n">y_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Radins&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Prudents&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Riches&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Dépensiers modestes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;magenta&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Conscients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Classification&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;revenu annuel (k$)&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score (1-100)&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_7_0.png" src="_images/clustering_7_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="recherche-de-partitions">
<h2>Recherche de partitions<a class="headerlink" href="#recherche-de-partitions" title="Permalink to this headline">#</a></h2>
<div class="section" id="methode-des-centres-mobiles">
<h3>Méthode des centres mobiles<a class="headerlink" href="#methode-des-centres-mobiles" title="Permalink to this headline">#</a></h3>
<span class="target" id="index-5"></span><p id="index-6">La méthode des centres mobiles est encore connue sous le nom de méthode de réallocation-centrage ou des k-means lorsque l’ensemble à classifier est mesuré par <span class="math notranslate nohighlight">\(d\)</span> variables. Ici, <span class="math notranslate nohighlight">\(\Omega \in \mathbb{R}^d\)</span> est muni de sa distance euclidienne <span class="math notranslate nohighlight">\(\delta\)</span>. Pour simplifier la présentation, les pondérations des individus seront toutes égales à 1, mais la généralisation à des pondérations quelconques ne pose aucun problème.</p>
<div class="section" id="id2">
<h4>Algorithme<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h4>
<p>L’algorithme des centres-mobiles peut se définir ainsi :</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 5 </span> (Algorithme des centres mobiles)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(\Omega\)</span>,<span class="math notranslate nohighlight">\(g\)</span>, métrique</p>
<p><strong>Sortie :</strong> Une partition de <span class="math notranslate nohighlight">\(\Omega\)</span></p>
<ol class="simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points de  <span class="math notranslate nohighlight">\(\Omega\)</span> (centres initiaux des <span class="math notranslate nohighlight">\(g\)</span> classes)</p></li>
<li><p>Tant que (non convergence)</p>
<ol class="simple">
<li><p>Étape E : Construction de la partition en affectant chaque point de <span class="math notranslate nohighlight">\(\Omega\)</span> à la classe dont il est le plus près du centre (en cas d’égalité, l’affectation se fait à la classe de plus petit indice).</p></li>
<li><p>Étape M : Les centres de gravité de la partition qui vient d’être calculée deviennent les nouveaux centres</p></li>
</ol>
</li>
</ol>
</div>
</div><p><img alt="" src="_images/kmeans1.png" /></p>
<p>L’initialisation des centres de classe étant aléatoire, il convient de répliquer l’algorithme plusieurs fois et de, par exemple, retenir la partition majoritaire. La figure suivante présente deux résultats des k-means, sur un même jeu de données (5 classes, 50 points par classes), avec une initialisation aléatoire différente.</p>
<p><img alt="" src="_images/kmeans2.png" /></p>
</div>
<div class="section" id="critere-et-convergence">
<h4>Critère et convergence<a class="headerlink" href="#critere-et-convergence" title="Permalink to this headline">#</a></h4>
<p>La qualité d’un couple partition-centres est mesurée par la somme des inerties des classes par rapport à leur centre. On peut montrer qu’à chacune des deux étapes de l’algorithme, on améliore ce critère.</p>
</div>
<div class="section" id="lien-avec-la-methode-de-ward">
<h4>Lien avec la méthode de Ward<a class="headerlink" href="#lien-avec-la-methode-de-ward" title="Permalink to this headline">#</a></h4>
<p>La méthode des centres mobiles et la méthode de Ward optimisent toutes deux, à leur façon, le critère d’inertie intra-classe. Cette situation conduit à proposer des stratégies utilisant les deux approches comme, par exemple :</p>
<ul class="simple">
<li><p>appliquer les centres-mobiles pour regrouper l’ensemble initial en un nombre “important” de classes</p></li>
<li><p>appliquer la méthode de Ward en partant de ces classes</p></li>
<li><p>rechercher quelques “bons” niveaux de la hiérarchie</p></li>
<li><p>éventuellement, appliquer de nouveau la méthode des centres-mobiles sur les partitions obtenues pour améliorer encore leur critère.</p></li>
</ul>
</div>
</div>
<div class="section" id="generalisation-les-nuees-dynamiques">
<h3>Généralisation : les nuées dynamiques<a class="headerlink" href="#generalisation-les-nuees-dynamiques" title="Permalink to this headline">#</a></h3>
<p id="index-7">L’idée de base consiste à remplacer les centres   qui étaient des éléments de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> jouant le rôle de représentant ou encore de noyau de la classe par des éléments de nature très diverse adaptés au problème que l’on cherche à résoudre.</p>
<div class="section" id="formalisation">
<h4>Formalisation<a class="headerlink" href="#formalisation" title="Permalink to this headline">#</a></h4>
<p>On note <span class="math notranslate nohighlight">\(L=\{\lambda_i\}\)</span> l’ensemble des noyaux, <span class="math notranslate nohighlight">\(D:\Omega\times L\rightarrow \mathbb{R}^+\)</span> une mesure de ressemblance entre éléments de <span class="math notranslate nohighlight">\(\Omega\)</span> et de <span class="math notranslate nohighlight">\(L\)</span>. L’objectif est alors de trouver la partition en <span class="math notranslate nohighlight">\(g\)</span> classes (<span class="math notranslate nohighlight">\(g\)</span> fixé a priori) de <span class="math notranslate nohighlight">\(\Omega\)</span> minimisant le critère <span class="math notranslate nohighlight">\(\displaystyle\sum_{k}\displaystyle\sum_{x\in P_k}D(x,\lambda_k)\)</span></p>
<p>Cette minimisation est réalisée de façon alternée, comme pour les centres mobiles.</p>
</div>
<div class="section" id="choix-du-nombre-de-classes">
<h4>Choix du nombre de classes<a class="headerlink" href="#choix-du-nombre-de-classes" title="Permalink to this headline">#</a></h4>
<p>En général, le critère n’est pas indépendant du nombre de classes. Par exemple, le critère de l’inertie s’annule pour la partition triviale pour laquelle chaque point forme une classe. Il s’agit donc de la meilleure partition. Il est donc
nécessaire de fixer a priori le nombre de classes. Pour résoudre ce problème très difficile, plusieurs solutions sont utilisées :</p>
<ul class="simple">
<li><p>on a une idée du nombre de classes désirées</p></li>
<li><p>on recherche la meilleure partition pour plusieurs nombres de classes et on étudie la décroissance du critère en fonction du nombre de classes (méthode du coude)</p></li>
<li><p>on définit une fonction <span class="math notranslate nohighlight">\(f(\Omega)\)</span> qui rend le critère indépendant du nombre de classes</p></li>
<li><p>on ajoute des contraintes supplémentaires (nombre d’individus par classe, volume d’une classe…). C’est l’option retenue par la méthode Isodata</p></li>
<li><p>on effectue des tests statistiques sur les classes</p></li>
</ul>
</div>
</div>
<div class="section" id="quelques-variantes">
<h3>Quelques variantes<a class="headerlink" href="#quelques-variantes" title="Permalink to this headline">#</a></h3>
<div class="section" id="k-means">
<h4>K-means++<a class="headerlink" href="#k-means" title="Permalink to this headline">#</a></h4>
<p>Plutôt que d’initialiser les centres de manière aléatoire, l’algorithme K-means++ propose de partitionner <span class="math notranslate nohighlight">\(\Omega=\{\mathbf x_1\cdots \mathbf x_n\}\)</span> selon l’algorithme suivant :</p>
<ol class="simple">
<li><p>Tirer uniformément le premier centre de classe <span class="math notranslate nohighlight">\(c_1\)</span> dans <span class="math notranslate nohighlight">\(\Omega\)</span>\</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i\in[\![2,g]\!]\)</span>, choisir <span class="math notranslate nohighlight">\(\mathbf{c_i}\)</span> à partir de <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> selon la probabilité <span class="math notranslate nohighlight">\(D(\mathbf{x}_i)^2\)</span> / <span class="math notranslate nohighlight">\(\displaystyle\sum\limits_{j=1}^{m}{D(\mathbf{x}_j)}^2\)</span> où  <span class="math notranslate nohighlight">\(D(\mathbf{x}_i)\)</span> est la distance entre <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> et le centre de classe le plus proche déjà choisi. Ceci assure de tirer des centres de classe éloignés avec forte probabilité.</p></li>
</ol>
</div>
<div class="section" id="acceleration-des-k-means">
<h4>Accélération des k-means<a class="headerlink" href="#acceleration-des-k-means" title="Permalink to this headline">#</a></h4>
<p>L’algorithme original peut être amélioré de manière significative en évitant les calculs de distances non nécessaires. En exploitant l’inégalité triangulaire, et en conservant les bornes inférieures et supérieures des distances entre les points et les centres de classe, l’algorithme correspondant est performant, y compris pour de grandes valeurs de <span class="math notranslate nohighlight">\(k\)</span> (<a class="reference internal" href="#km">Algorithm 6</a>)</p>
<div class="proof algorithm admonition" id="km">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span> (Accélération des k-means)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(\Omega, g\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(P\)</span> une partition de <span class="math notranslate nohighlight">\(X\)</span> en <span class="math notranslate nohighlight">\(g\)</span> classes</p>
<ol class="simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points <span class="math notranslate nohighlight">\(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(l(\mathbf x,\mathbf c)=0\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega\)</span></p>
<ol class="simple">
<li><p>Affecter <span class="math notranslate nohighlight">\(\mathbf x\)</span> à la classe du centre le plus proche : <span class="math notranslate nohighlight">\(\mathbf c(x) = Arg \displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p>A chaque calcul de <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)\)</span>,<span class="math notranslate nohighlight">\( l(\mathbf x,\mathbf c)=\delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(u(\mathbf x,\mathbf c)=\displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)</span></p></li>
</ol>
</li>
<li><p>Tant que (non convergence)</p>
<ol class="simple">
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c,\mathbf {c'}\in C\)</span> calculer <span class="math notranslate nohighlight">\(\delta (\mathbf c,\mathbf {c'})\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\)</span> <span class="math notranslate nohighlight">\(s(c)= \frac{1}{2}\displaystyle\min_{\mathbf {c'}\neq \mathbf c} \delta(\mathbf c,\mathbf {c'})\)</span></p></li>
<li><p>Identifier les <span class="math notranslate nohighlight">\(\mathbf x\)</span> tels que <span class="math notranslate nohighlight">\(u(\mathbf x)\leq s(\mathbf c(\mathbf x))\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span> tels que <span class="math notranslate nohighlight">\(\mathbf c\neq \mathbf c(\mathbf x)\)</span> et <span class="math notranslate nohighlight">\(u(\mathbf x)&gt;l(\mathbf x,\mathbf c)\)</span> et <span class="math notranslate nohighlight">\(u(\mathbf x)&gt;\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)</span></p>
<ol class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(r(\mathbf x)\)</span></p>
<ol class="simple">
<li><p>Calculer <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r(\mathbf x)=Faux\)</span></p></li>
</ol>
</li>
<li><p>Sinon</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)=u(\mathbf x)\)</span></p></li>
</ol>
</li>
<li><p>Si <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)&gt;l(\mathbf x,\mathbf c)\)</span>  ou <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)&gt;\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)</span></p>
<ol class="simple">
<li><p>Calculer <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)&lt;\delta(\mathbf c(\mathbf x),\mathbf x)\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c(\mathbf x)= \mathbf c\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\in C\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf m(\mathbf c)\)</span> : centre de masse des points de <span class="math notranslate nohighlight">\(\Omega\)</span> plus proches de <span class="math notranslate nohighlight">\(\mathbf c\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(l(\mathbf x,\mathbf c)=max\left (l(\mathbf x,\mathbf c)-\delta(\mathbf m(\mathbf c),\mathbf c),0 \right )\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(u(\mathbf x)=u(\mathbf x)+\delta(\mathbf m(\mathbf c(\mathbf x)),\mathbf c(\mathbf x))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r(\mathbf x)=Vrai\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\in C\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c = \mathbf m(\mathbf c)\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="k-means-a-mini-batchs">
<h4>k-means à mini batchs<a class="headerlink" href="#k-means-a-mini-batchs" title="Permalink to this headline">#</a></h4>
<p>Il est également possible d’appliquer une optimisation par mini-batchs dans l’algorithme des k-means (<a class="reference internal" href="#kmbatch">Algorithm 7</a>).</p>
<div class="proof algorithm admonition" id="kmbatch">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (Accélération des k-means)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(\Omega, g\)</span>, <span class="math notranslate nohighlight">\(b\)</span> taille des batchs</p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(P\)</span> une partition de <span class="math notranslate nohighlight">\(X\)</span> en <span class="math notranslate nohighlight">\(g\)</span> classes</p>
<ol class="simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points <span class="math notranslate nohighlight">\(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf v=0\in\mathbb{R}^g\)</span></p></li>
<li><p>Tant que non convergence</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\leftarrow\)</span> batch de <span class="math notranslate nohighlight">\(b\)</span> exemples tirés de <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \mathcal{B}\)</span></p>
<ol class="simple">
<li><p>Affecter <span class="math notranslate nohighlight">\(\mathbf x\)</span> à la classe du centre le plus proche <span class="math notranslate nohighlight">\(\mathbf T(\mathbf x)\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \mathcal{B}\)</span></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c = \mathbf T(\mathbf x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_c = v_c + 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\eta = \frac{1}{v_c}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf c = (1-\eta)\mathbf c + \eta \mathbf x\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
</div>
<div class="section" id="id3">
<h3>Exemple<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p>On génère des données</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">nb_classes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="p">[[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],[</span><span class="mi">1</span> <span class="p">,</span>  <span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]])</span>
<span class="n">cluster_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>    

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">centers</span><span class="o">=</span><span class="n">center</span><span class="p">,</span><span class="n">cluster_std</span> <span class="o">=</span> <span class="n">cluster_std</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_9_0.png" src="_images/clustering_9_0.png" />
</div>
</div>
<p>Puis on applique l’algorithme des <span class="math notranslate nohighlight">\(k\)</span>-means.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">nb_classes</span><span class="p">,</span><span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Vraies classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;K means à </span><span class="si">{0:d}</span><span class="s2"> classes&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb_classes</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_11_0.png" src="_images/clustering_11_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="modeles-de-melange">
<h2>Modèles de mélange<a class="headerlink" href="#modeles-de-melange" title="Permalink to this headline">#</a></h2>
<p>Les modèles de mélange supposent que les données proviennent d’un mélange de distributions (généralement gaussiennes), et l’objectif est alors d’estimer les paramètres du modèle de mélange en maximisant la fonction de vraisemblance pour les données.
L’optimisation directe de la fonction de vraisemblance dans ce cas n’est pas une tâche simple, en raison des contraintes nécessaires sur les paramètres et de la nature complexe de la fonction de vraisemblance, qui présente généralement un grand nombre de maxima locaux et de points de selle. Une méthode courante pour estimer les paramètres du modèle de mélange est l’algorithme EM.</p>
<div class="section" id="definition">
<h3>Définition<a class="headerlink" href="#definition" title="Permalink to this headline">#</a></h3>
<p>Soient <span class="math notranslate nohighlight">\(\mathcal S = \{\mathbf X_1\cdots X_n\}\)</span> <span class="math notranslate nohighlight">\(n\)</span> vecteurs aléatoires i.i.d. à valeur dans <span class="math notranslate nohighlight">\(\mathcal X\subset \mathbb{R}^d\)</span> , chaque <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> étant distribué selon</p>
<div class="math notranslate nohighlight">
\[g(\mathbf x|\boldsymbol \theta) = \displaystyle\sum_{i=1}^K w_i\Phi_i(\mathbf x)\]</div>
<p>où <span class="math notranslate nohighlight">\(\Phi_i,i\in[\![1,K]\!]\)</span> sont des densités de probabilité sur <span class="math notranslate nohighlight">\(\mathcal X\)</span> et les <span class="math notranslate nohighlight">\(w_i\)</span> sont des poids positifs, sommant à 1. <span class="math notranslate nohighlight">\(g\)</span> peut être interprétée comme suit : soit <span class="math notranslate nohighlight">\(Z\)</span> une variable aléatoire discrète prenant les valeurs <span class="math notranslate nohighlight">\(i\in[\![1,K]\!]\)</span> avec probabilité <span class="math notranslate nohighlight">\(w_i\)</span>, et soit <span class="math notranslate nohighlight">\(\mathbf X\)</span> un vecteur aléatoire dont la distribution conditionnelle, étant donnée <span class="math notranslate nohighlight">\(Z=z\)</span> est <span class="math notranslate nohighlight">\(\Phi_z\)</span>. Alors</p>
<div class="math notranslate nohighlight">
\[\Phi_{Z,\mathbf X}(z,\mathbf x) = \Phi_Z(z)\Phi_{\mathbf X|Z}(\mathbf x,z) = w_z(\mathbf x)\]</div>
<p>et la distribution marginale de <span class="math notranslate nohighlight">\(\mathbf X\)</span> est calculée en sommant sur <span class="math notranslate nohighlight">\(z\)</span> les probabilités jointes.</p>
<p>Un vecteur aléatoire <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(g\)</span> peut donc être simulé d’abord en tirant <span class="math notranslate nohighlight">\(Z\)</span> suivant <span class="math notranslate nohighlight">\(P(Z=z)=w_z,z\in[\![1,K]\!]\)</span>, puis en tirant <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(\Phi_Z\)</span>. La famille <span class="math notranslate nohighlight">\(\mathcal S\)</span> ne contenant que les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span>, les <span class="math notranslate nohighlight">\(Z_i\)</span> sont des variables latentes, interprétées comme les étiquettes cachées des classes auxquelles les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> appartiennent.</p>
<p>Typiquement, les <span class="math notranslate nohighlight">\(\Phi_k\)</span> sont des lois paramétriques. Classiquement ce sont des lois gaussiennes <span class="math notranslate nohighlight">\(\mathcal N(\boldsymbol \mu_k,\boldsymbol \Sigma_k)\)</span> et donc en rassemblant tous les paramètres des lois, incluant les <span class="math notranslate nohighlight">\(w_k\)</span>, dans un vecteur de paramètre <span class="math notranslate nohighlight">\(\boldsymbol \theta = (\mu_k,\boldsymbol \Sigma_k,w_k,k\in[\!1,K]\!])\)</span>, on peut écrire</p>
<div class="math notranslate nohighlight">
\[g(s|\boldsymbol \theta) = \prod_{i=1}^n g(\mathbf x_i|\boldsymbol \theta) = \prod_{i=1}^n \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k)\]</div>
<p>où <span class="math notranslate nohighlight">\(s=(\mathbf x_1\cdots \mathbf x_n)\)</span> dénote une réalisation de <span class="math notranslate nohighlight">\(\mathcal S\)</span>.</p>
<p>On estime alors <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> en maximisant la log vraisemblance</p>
<div class="math notranslate nohighlight">
\[\ell(\boldsymbol\theta|s) = \displaystyle\sum_{i=1}^n ln(g(\mathbf x_i|\boldsymbol \theta)) = \displaystyle\sum_{i=1}^n ln \left ( \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k) \right )\]</div>
<p>ce qui est en général complexe, la fonction <span class="math notranslate nohighlight">\(\ell\)</span> admettant de nombreux extrema locaux.</p>
</div>
<div class="section" id="algorithme-em">
<h3>Algorithme EM<a class="headerlink" href="#algorithme-em" title="Permalink to this headline">#</a></h3>
<p>Plutôt que d’optimiser <span class="math notranslate nohighlight">\(\ell\)</span> directement depuis les données <span class="math notranslate nohighlight">\(s\)</span>, l’algorithme EM (<a class="reference internal" href="#EM">Algorithm 8</a>) augmente d’abord les données des variables latentes (les étiquettes <span class="math notranslate nohighlight">\(\mathbf z=(z_1\cdots z_n)\)</span> des classes). L’idée est que <span class="math notranslate nohighlight">\(s\)</span> est uniquement la partie observée des données aléatoires <span class="math notranslate nohighlight">\((\mathcal S,\mathbf Z)\)</span> générées d’abord en tirant <span class="math notranslate nohighlight">\(Z\)</span> suivant <span class="math notranslate nohighlight">\(P(Z=z)\)</span>, puis en tirant <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(\Phi_z\)</span>, de sorte à avoir</p>
<div class="math notranslate nohighlight">
\[g(s,z|\boldsymbol \theta) = \displaystyle\prod_{i=1}^n w_{z_i} \Phi_{z_i}(\mathbf x_i)\]</div>
<p>Ainsi, la log vraisemblance des données complètes, en général plus facile à optimiser, est</p>
<div class="math notranslate nohighlight">
\[\bar\ell(\boldsymbol\theta|s,z) =\displaystyle\sum{i=1}^n ln(w_{z_i} \Phi_{z_i}(\mathbf x_i))\]</div>
<p>Cependant, les <span class="math notranslate nohighlight">\(z\)</span> ne sont pas observées et <span class="math notranslate nohighlight">\(\bar\ell\)</span> ne peut être évaluée. Dans l’étape E de l’algorithme EM, <span class="math notranslate nohighlight">\(\bar\ell\)</span> est remplacée par <span class="math notranslate nohighlight">\(\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\)</span>, où l’indice <span class="math notranslate nohighlight">\(p\)</span> indique que <span class="math notranslate nohighlight">\(\mathbf Z\)</span> est distribuée selon la distribution conditionnelle de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> étant donnée <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, soit</p>
<div class="math notranslate nohighlight">
\[p(z)=g(z|s,\boldsymbol \theta) \propto g(s,z|\boldsymbol \theta)\]</div>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 22 </span></p>
<div class="remark-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(p(z)\)</span> est de la forme <span class="math notranslate nohighlight">\(p_1(z_1)\cdots p_n(z_n)\)</span> de telle sorte que, étant donné <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, les composantes de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> sont deux à deux indépendantes.</p>
</div>
</div><div class="proof algorithm admonition" id="EM">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Algorithmes EM)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entrée :</strong> <span class="math notranslate nohighlight">\(s,\boldsymbol\theta^{(0)}\)</span></p>
<p><strong>Sortie :</strong> Approximation de la log vraisemblance maximale</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(i=1\)</span></p></li>
<li><p>Tant que (not stop)</p>
<ol class="simple">
<li><p>Etape E : Trouver <span class="math notranslate nohighlight">\(p^{(i)}(z) = g(s|s,\boldsymbol\theta^{(i-1)})\)</span> et <span class="math notranslate nohighlight">\(Q^{(i)}(\boldsymbol\theta)=\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\)</span></p></li>
<li><p>Etape M : <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i)} = Arg \displaystyle\max_{\boldsymbol\theta} Q^{(i)}(\boldsymbol\theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i = i+1\)</span></p></li>
</ol>
</li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\boldsymbol\theta{(i)}\)</span></p></li>
</ol>
</div>
</div><p>Dans l’<a class="reference internal" href="#EM">Algorithm 8</a>, un critère d’arrêt est par exemple</p>
<div class="math notranslate nohighlight">
\[\frac{\ell(\boldsymbol\theta{(i)}|s)-\ell(\boldsymbol\theta^{(i-1)}|s)}{\ell(\boldsymbol\theta{(i)}|s)}&lt;\epsilon\]</div>
<p>Sous certaines conditions, la suite des <span class="math notranslate nohighlight">\(\ell(\boldsymbol\theta{(i)}|s)\)</span> converge vers un maximum local de la log vraisemblance <span class="math notranslate nohighlight">\(\ell\)</span>. La convergence vers le maximum global dépend bien sûr du choix de <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(0)}\)</span>, de sorte qu’une stratégie possible est d’exécuter plusieurs fois l’algorithme avec des initialisations différentes.</p>
<p>Dans le cas d’un mélange gaussien, <span class="math notranslate nohighlight">\(\Phi_k=\mathcal N(\boldsymbol\mu_k,\boldsymbol\Sigma_k),k\in[\![1,K]\!]\)</span>. Si <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i-1)}\)</span> est le vecteur optimal à l’itération courante, constitué des poids <span class="math notranslate nohighlight">\(w_k^{(i-1)}\)</span>, des vecteurs moyenne <span class="math notranslate nohighlight">\((\boldsymbol\mu_k)^{(i-1)}\)</span> et des matrices de covariances <span class="math notranslate nohighlight">\((\boldsymbol\Sigma_k)^{(i-1)}\)</span>, alors on détermine <span class="math notranslate nohighlight">\(p^{(i)}\)</span>, la distribution de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> conditionnelement à <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, pour le paramètre <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i-1)}\)</span>. Puisque les composantes de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> étant donné <span class="math notranslate nohighlight">\(\mathcal S=s\)</span> sont indépendantes, il suffit de spécifier la distribution discrète <span class="math notranslate nohighlight">\(p_j^{(i)}\)</span> de chaque <span class="math notranslate nohighlight">\(Z_j\)</span>, étant données l’observation <span class="math notranslate nohighlight">\(\mathbf X_j=\mathbf x_j\)</span>, calculée à l’aide de la forule de Bayes</p>
<div class="math notranslate nohighlight">
\[p_j^{(i)}(k)\propto w_k^{(i-1)}\Phi_k(\mathbf x_j|\boldsymbol\mu_k^{(i-1)},\boldsymbol\Sigma_k^{(i-1)}),k\in[\![1,K]\!]\]</div>
<p>Alors</p>
<ol class="simple">
<li><p>Pour l’étape E</p></li>
</ol>
<div class="math notranslate nohighlight">
\[Q^{(i)}(\boldsymbol\theta) = \mathbb{E}_p \displaystyle\sum_{j=1}^n \left (ln w_{z_j} + ln \Phi_{z_j}(\mathbf x_j|\boldsymbol\mu_{Z_j},\boldsymbol\Sigma_{Z_j}) \right )\]</div>
<p>où les <span class="math notranslate nohighlight">\(Z_j\)</span> sont indépendants et distribués selon <span class="math notranslate nohighlight">\(p_j^{(i)}\)</span>.</p>
<ol class="simple">
<li><p>Pour l’étape M, on maximise</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \displaystyle\sum_{j=1}^n\displaystyle\sum_{k=1}^K p_j^{(i)}(k)\left (ln w_k + ln \Phi_k(\mathbf x_j|\boldsymbol\mu_{k},\boldsymbol\Sigma_{k})\right )\]</div>
<p>sous la contrainte <span class="math notranslate nohighlight">\(\displaystyle\sum_{k=1}^K w_k=1\)</span>. En utilisant une relxation lagrangienne, et le fait que <span class="math notranslate nohighlight">\(\displaystyle\sum_{k=1}^K p_j^{(i)}(k)=1\)</span> on trouve pour tout <span class="math notranslate nohighlight">\(k\in[\![1,K]\!]\)</span></p>
<div class="math notranslate nohighlight">
\[w_k = \frac1n\displaystyle\sum_{j=1}^n p_j^{(i)}(k)\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol\mu_k = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) \mathbf x_j}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol\Sigma_{k} = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) (\mathbf x_j-\boldsymbol\mu_k)(\mathbf x_j-\boldsymbol\mu_k)^T}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s1">&#39;./data/mixture.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Paramètres initiaux</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">]])</span> <span class="c1"># poids</span>
<span class="n">M</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span> <span class="c1"># Moyennes</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># Co</span>

<span class="n">C</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">C</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">300</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">):</span> 

    <span class="c1"># Etape E</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>  
        <span class="n">mvn</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span> <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:]</span> <span class="p">)</span>
        <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">mvn</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Etape M</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>   
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
        <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span>
        <span class="n">xm</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">xm</span> <span class="o">@</span> <span class="p">(</span><span class="n">xm</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="mf">180.0</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  
    <span class="n">ell</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">M</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="mf">180.0</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/clustering_13_0.png" src="_images/clustering_13_0.png" />
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-TP1_statsDescriptives"></span><div class="tex2jax_ignore mathjax_ignore section" id="tp-statistiques-descriptives">
<h2>TP Statistiques descriptives<a class="headerlink" href="#tp-statistiques-descriptives" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="statistique-monovariee">
<h3>Statistique monovariée<a class="headerlink" href="#statistique-monovariee" title="Permalink to this headline">#</a></h3>
<p>Le tableau suivant donne les notes (sur 10) obtenues à un examen par un groupe d’élèves.
<span class="math notranslate nohighlight">\(\begin{array}{c|c}
Nom &amp; Note \\
  \hline
Alain &amp;6\\
Raymond &amp;5\\
Jean-Joseph &amp;9\\
Eglantine &amp;3\\
Isidore &amp;3\\
Mauricette &amp;1\\
Sylvère &amp;9\\
Pétunia &amp;6\\
Philemon &amp;5\\
Archibald &amp;6\\
Théodule &amp;5\\
Marguerite &amp;6\\
Proserpine &amp;5\\
Alphonse &amp;7\\
Géraud &amp;5\\
Basile &amp;10\\
Fantine &amp;2\\
Sidonie &amp;1\\
Thérèse &amp;1\\
Yves &amp;1
\end{array}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">notes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Donner les différentes modalités possibles</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer les effectifs cumulés de chaque modalité.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer les effectifs par modalité, les fréquences et les fréquences cumulées des notes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Représenter avec un (ou plusieurs) graphique(s) adapté(s) la répartition des notes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>En déduire (et afficher) la fonction de répartition empirique des notes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>En utilisant la fonction <a class="reference external" href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.boxplot.html">boxplot</a>, tracer la boîte à moustache des notes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer les éléments caractéristiques (indicateurs de tendance, de dispersion) des notes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="statistique-bivariee">
<h3>Statistique bivariée<a class="headerlink" href="#statistique-bivariee" title="Permalink to this headline">#</a></h3>
<div class="section" id="cas-de-variables-quantitatives">
<h4>Cas de variables quantitatives<a class="headerlink" href="#cas-de-variables-quantitatives" title="Permalink to this headline">#</a></h4>
<p>On donne le fichier de données suivant</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;./bivariee.txt&#39;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ob&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nuage de points&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;+&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">1.25</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Boite à moustache pour X&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;+&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Boite à moustache pour Y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/TP1_statsDescriptives_19_0.png" src="_images/TP1_statsDescriptives_19_0.png" />
</div>
</div>
<p>Calculer la covariance entre les deux variables. Conclusion ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer la corrélation entre les deux variables. Conclusion ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="cas-de-variables-qualitatives">
<h4>Cas de variables qualitatives<a class="headerlink" href="#cas-de-variables-qualitatives" title="Permalink to this headline">#</a></h4>
<p>On s’intéresse à un fichier décrivant la réussite d’étudiants en mathématiques (données décrites <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Student+Performance">ici</a>)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./student.csv&#39;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \
0       GP   F   18       U     GT3       A     4     4   at_home   teacher   
1       GP   F   17       U     GT3       T     1     1   at_home     other   
2       GP   F   15       U     LE3       T     1     1   at_home     other   
3       GP   F   15       U     GT3       T     4     2    health  services   
4       GP   F   16       U     GT3       T     3     3     other     other   
..     ...  ..  ...     ...     ...     ...   ...   ...       ...       ...   
390     MS   M   20       U     LE3       A     2     2  services  services   
391     MS   M   17       U     LE3       T     3     1  services  services   
392     MS   M   21       R     GT3       T     1     1     other     other   
393     MS   M   18       R     LE3       T     3     2  services     other   
394     MS   M   19       U     LE3       T     1     1     other   at_home   

     ... famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  
0    ...      4        3      4     1     1      3        6   5   6   6  
1    ...      5        3      3     1     1      3        4   5   5   6  
2    ...      4        3      2     2     3      3       10   7   8  10  
3    ...      3        2      2     1     1      5        2  15  14  15  
4    ...      4        3      2     1     2      5        4   6  10  10  
..   ...    ...      ...    ...   ...   ...    ...      ...  ..  ..  ..  
390  ...      5        5      4     4     5      4       11   9   9   9  
391  ...      2        4      5     3     4      2        3  14  16  16  
392  ...      5        5      3     3     3      3        3  10   8   7  
393  ...      4        4      1     3     4      5        0  11  12  10  
394  ...      3        2      3     3     3      5        5   8   9   9  

[395 rows x 33 columns]
</pre></div>
</div>
</div>
</div>
<p>On regarde les deux premières colonnes des données (code de l’école et genre de l’étudiant)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Construire le tableau de contingence de ces deux variables</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Construire le tableau théorique associé en supposant l’indépendance des deux variables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span> entre les variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(g\)</span>. Ces deux variables sont elles liées ou sont elles indépendantes ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>On s’intéresse maintenant à la septième variable <span class="math notranslate nohighlight">\(ne\)</span> qui code le niveau d’éducation des mères de la manière suivante :</p>
<ul class="simple">
<li><p>0 - none,</p></li>
<li><p>1 - primary education (4th grade),</p></li>
<li><p>2 - 5th to 9th grade,</p></li>
<li><p>3 - secondary education</p></li>
<li><p>4 - higher education</p></li>
</ul>
<p>Récupérer cette variable et calculer les effectifs de chacune des modalités. Calculer le tableau de contingence  entre les variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(ne\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Les effectifs étant trop faibles, fusionner les deux premières colonnes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Construire le tableau théorique, en supposant l’indépendance des variables</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
<p>Calculer la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span> entre les variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(ne\)</span>. Ces deux variables sont elles liées ou sont elles indépendantes ? Que peut on en déduire sur le choix de l’école ?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#TODO</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-TP2_ACP"></span><div class="tex2jax_ignore mathjax_ignore section" id="tp-acp">
<h2>TP ACP<a class="headerlink" href="#tp-acp" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># n&#39;exécuter qu&#39;une fois</span>
<span class="o">!</span>pip3 install sklearn numpy matplotlib --q
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">23.2.1</span> -&gt; <span class=" -Color -Color-Green">23.3</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3.11 -m pip install --upgrade pip</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="definition-de-quelques-outils">
<h2>Définition de quelques outils<a class="headerlink" href="#definition-de-quelques-outils" title="Permalink to this headline">#</a></h2>
<p>On définit ici quelques fonctions utiles pour l’affichage des résultats, non présents dans sklearn :</p>
<ul class="simple">
<li><p>screeplot : variance expliquée en fonction des composantes principales</p></li>
<li><p>CercleCorrelation : affichage du cercle des corrélations</p></li>
<li><p>biplot : affichage simultané des individus et des variables dans un plan principal</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">screeplot</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">displayx</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o-&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">displayx</span> <span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;CP &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">],</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Variance&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;CP&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Scree Plot&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">CercleCorrelation</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="n">np1</span><span class="p">,</span><span class="n">np2</span><span class="p">,</span><span class="n">data</span><span class="p">,</span><span class="n">nom_features</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">circle1</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">radius</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">circle1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nom_features</span><span class="p">)):</span>
        <span class="n">str1</span> <span class="o">=</span> <span class="s2">&quot;CP&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np1</span><span class="p">)</span>
        <span class="n">str2</span> <span class="o">=</span> <span class="s2">&quot;CP&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">np1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">np2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="n">x</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="n">y</span><span class="p">],</span><span class="s1">&#39;k-&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;rx&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">nom_features</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">str1</span> <span class="o">+</span><span class="s2">&quot; (</span><span class="si">%s%%</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="n">np1</span><span class="p">])[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;0.&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">str2</span> <span class="o">+</span><span class="s2">&quot; (</span><span class="si">%s%%</span><span class="s2">)&quot;</span><span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="n">np2</span><span class="p">])[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;0.&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Cercle des corrélations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">biplot</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="n">np1</span><span class="p">,</span><span class="n">np2</span><span class="p">,</span><span class="n">data</span><span class="p">,</span><span class="n">nom_features</span><span class="p">):</span>  
    <span class="n">cp1</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">np1</span><span class="p">]</span>
    <span class="n">cp2</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">np2</span><span class="p">]</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)[:,</span><span class="n">np1</span><span class="p">]</span> 
    <span class="n">ys</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)[:,</span><span class="n">np2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cp1</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cp1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">cp2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="n">ys</span><span class="p">),</span>
                  <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.0025</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">cp1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">cp2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span>
                 <span class="n">nom_features</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;bo&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;CP&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np1</span><span class="p">)</span> <span class="o">+</span><span class="s2">&quot; (</span><span class="si">%s%%</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="n">np1</span><span class="p">])[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;0.&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;CP&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np2</span><span class="p">)</span> <span class="o">+</span><span class="s2">&quot; (</span><span class="si">%s%%</span><span class="s2">)&quot;</span><span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">[</span><span class="n">np2</span><span class="p">])[:</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s2">&quot;0.&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Biplot individus / variables sur les CP&quot;</span> <span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">np1</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot; et &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="exercice-1-etude-de-proprietes-de-vins">
<h2>Exercice 1 : étude de propriétés de vins<a class="headerlink" href="#exercice-1-etude-de-proprietes-de-vins" title="Permalink to this headline">#</a></h2>
<p>Le fichier vins.csv contient les résultats d’analyses chimiques de trois classes de vins italiens, provenant de la même région mais de viticulteurs différents. Les analyses portent sur la quantification de 13 indices quantitatifs continus reliés aux vins : degré d’alcool, acide malique, présence de cendres, alcalinité des cendres, magnésium, phénols totaux, flavonoïdes, phénols non flavanoïdes, proanthocyanidines, intensité de la couleur du vin, teinte, OD280/OD315 des vins dilués et proline. Le fichier décrit un vin par ligne (sa classe et ses 13 attributs).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">vins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/vins.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">cat_vins</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">features_vins</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">vins</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">nom_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">% a</span><span class="s1">lcool&#39;</span><span class="p">,</span> <span class="s1">&#39;acide malique&#39;</span><span class="p">,</span> <span class="s1">&#39;cendres&#39;</span><span class="p">,</span> <span class="s1">&#39;alcalinité&#39;</span><span class="p">,</span> <span class="s1">&#39;magnésium&#39;</span><span class="p">,</span> <span class="s1">&#39;phénols&#39;</span> <span class="p">,</span> 
                <span class="s1">&#39;flavonoïdes&#39;</span><span class="p">,</span> <span class="s1">&#39;non flavanoïdes&#39;</span><span class="p">,</span> <span class="s1">&#39;proanthocyanidines&#39;</span><span class="p">,</span> <span class="s1">&#39;couleur&#39;</span><span class="p">,</span> <span class="s1">&#39;teinte&#39;</span><span class="p">,</span> 
                <span class="s1">&#39;OD280/OD315&#39;</span><span class="p">,</span><span class="s1">&#39;proline&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="votre-travail">
<h3>===== Votre travail : =====<a class="headerlink" href="#votre-travail" title="Permalink to this headline">#</a></h3>
<div class="section" id="appliquer-une-acp-a-ces-donnees-de-deux-manieres">
<h4>Appliquer une <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">ACP</a> à ces données, de deux manières :<a class="headerlink" href="#appliquer-une-acp-a-ces-donnees-de-deux-manieres" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>sur les données brutes du tableau</p></li>
<li><p>sur les données normalisées, en utilisant la fonction <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">StandardScaler</a> de ScikitLearn</p></li>
</ul>
</div>
<div class="section" id="pour-chacune-de-ces-analyses">
<h4>Pour chacune de ces analyses :<a class="headerlink" href="#pour-chacune-de-ces-analyses" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>déterminer si l’analyse est pertinente et si oui pourquoi</p></li>
<li><p>Si elle l’est effectivement, déterminer le nombre de composantes principales à retenir, interpréter les axes principaux, afficher le(s) plan(s) principal(aux) et interpréter graphiquement les résultats</p></li>
</ul>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="exercice-2-etude-de-donnees-image">
<h2>Exercice 2 : étude de données image<a class="headerlink" href="#exercice-2-etude-de-donnees-image" title="Permalink to this headline">#</a></h2>
<p>Nous étudions des données image, représentant des chiffres manuscrits. Nous utilisons pour cela la base de données standard MNIST (Mixed National Institute of Standards and Technology), très employée pour le test de nouveaux algorithmes de reconnaissance de ces chiffres. Elle est composée de 60000 images d’apprentissage et 10000 images de test. Les images en noir et blanc, normalisées centrées de 28 pixels de côté.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">Xmnist</span><span class="p">,</span> <span class="n">ymnist</span> <span class="o">=</span> <span class="n">fetch_openml</span><span class="p">(</span><span class="s1">&#39;mnist_784&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">planche</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
    <span class="n">exemples</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">X</span><span class="p">[:</span><span class="mi">12000</span><span class="p">:</span><span class="mi">600</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">13000</span><span class="p">:</span><span class="mi">30600</span><span class="p">:</span><span class="mi">600</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">30600</span><span class="p">:</span><span class="mi">60000</span><span class="p">:</span><span class="mi">590</span><span class="p">]]</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">28</span>
    <span class="n">taille</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">taille</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">exemples</span><span class="p">),</span> <span class="n">taille</span><span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">exemple</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">exemple</span> <span class="ow">in</span> <span class="n">exemples</span><span class="p">]</span>
    <span class="n">n_rows</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">exemples</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">taille</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">row_images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_empty</span> <span class="o">=</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="n">taille</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">exemples</span><span class="p">)</span>
    <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="n">n_empty</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rows</span><span class="p">):</span>
        <span class="n">rimages</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">taille</span> <span class="p">:</span> <span class="p">(</span><span class="n">row</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">taille</span><span class="p">]</span>
        <span class="n">row_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">rimages</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">row_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">planche</span><span class="p">(</span><span class="n">Xmnist</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.9/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `&#39;liac-arff&#39;` to `&#39;auto&#39;` in 1.4. You can set `parser=&#39;auto&#39;` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml&#39;s API doc for details.
  warn(
</pre></div>
</div>
<img alt="_images/TP2_ACP_11_1.png" src="_images/TP2_ACP_11_1.png" />
</div>
</div>
<p>L’ACP est ici utilisée pour réduire la représentation des données (un point ici est un vecteur de 28*28=784, l’espace original est donc <span class="math notranslate nohighlight">\(\mathbb{R}^{784}\)</span>). Intuitivement, on comprend bien que, de nombreux pixels étant noirs et communs à toutes les images, 784 dimensions sont inutiles.</p>
<p>Pour ne pas surcharger la visualisation, vous utiliserez un sous-ensemble de <span class="math notranslate nohighlight">\({\tt nb\_digits}\)</span> images de MNIST (code donné)</p>
<div class="section" id="votre-travail-consiste-a">
<h3>===== Votre travail consiste à : =====<a class="headerlink" href="#votre-travail-consiste-a" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Calculer une ACP de ces données (normalisées ? non normalisées ? jusitifier)</p></li>
<li><p>Projeter les individus dans un  plan principal (ou dans un expace 3D de composantes principales) et commenter le résultat (codes de l’affichage fournis).</p></li>
<li><p>Inférer sur le nombre de composantes principales nécessaires pour expliquer les données initiales</p></li>
<li><p>Essayer de reconstruire une image donnée à partir de ses coordonnées sur les <span class="math notranslate nohighlight">\(n\)</span> premières composantes principales (faire varier <span class="math notranslate nohighlight">\(n\)</span>) et voir la qualité de la reconstruction</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Utilisation d&#39;un sous-ensemble de Xmnist (pour ne pas surcharger la visualisation)</span>
<span class="n">ymnist</span><span class="o">=</span><span class="n">ymnist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">nb_digits</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmnist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">X0</span><span class="o">=</span><span class="n">Xmnist</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="n">y0</span><span class="o">=</span><span class="n">ymnist</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="n">Xdata</span> <span class="o">=</span> <span class="n">X0</span><span class="p">[:</span><span class="n">nb_digits</span><span class="p">]</span>
<span class="n">ydata</span> <span class="o">=</span> <span class="n">y0</span><span class="p">[:</span><span class="n">nb_digits</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyError</span><span class="g g-Whitespace">                                  </span>Traceback (most recent call last)
<span class="n">Input</span> <span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xmnist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">6</span> <span class="n">X0</span><span class="o">=</span><span class="n">Xmnist</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="n">y0</span><span class="o">=</span><span class="n">ymnist</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="n">Xdata</span> <span class="o">=</span> <span class="n">X0</span><span class="p">[:</span><span class="n">nb_digits</span><span class="p">]</span>

<span class="nn">File /usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3813,</span> in <span class="ni">DataFrame.__getitem__</span><span class="nt">(self, key)</span>
<span class="g g-Whitespace">   </span><span class="mi">3811</span>     <span class="k">if</span> <span class="n">is_iterator</span><span class="p">(</span><span class="n">key</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">3812</span>         <span class="n">key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">3813</span>     <span class="n">indexer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">_get_indexer_strict</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="s2">&quot;columns&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">3815</span> <span class="c1"># take() does not accept boolean indexers</span>
<span class="g g-Whitespace">   </span><span class="mi">3816</span> <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">indexer</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>

<span class="nn">File /usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:6070,</span> in <span class="ni">Index._get_indexer_strict</span><span class="nt">(self, key, axis_name)</span>
<span class="g g-Whitespace">   </span><span class="mi">6067</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">6068</span>     <span class="n">keyarr</span><span class="p">,</span> <span class="n">indexer</span><span class="p">,</span> <span class="n">new_indexer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reindex_non_unique</span><span class="p">(</span><span class="n">keyarr</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">6070</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raise_if_missing</span><span class="p">(</span><span class="n">keyarr</span><span class="p">,</span> <span class="n">indexer</span><span class="p">,</span> <span class="n">axis_name</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">6072</span> <span class="n">keyarr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">indexer</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">6073</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">Index</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">6074</span>     <span class="c1"># GH 42790 - Preserve name from an Index</span>

<span class="nn">File /usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:6130,</span> in <span class="ni">Index._raise_if_missing</span><span class="nt">(self, key, indexer, axis_name)</span>
<span class="g g-Whitespace">   </span><span class="mi">6128</span>     <span class="k">if</span> <span class="n">use_interval_msg</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">6129</span>         <span class="n">key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">6130</span>     <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;None of [</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">] are in the [</span><span class="si">{</span><span class="n">axis_name</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">6132</span> <span class="n">not_found</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ensure_index</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="n">missing_mask</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="g g-Whitespace">   </span><span class="mi">6133</span> <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">not_found</span><span class="si">}</span><span class="s2"> not in index&quot;</span><span class="p">)</span>

<span class="ne">KeyError</span>: &quot;None of [Int64Index([61057, 20011,  8715, 25442, 56644,  3604, 43571, 17641, 31746,\n            42246,\n            ...\n            61896,  7238,  1778, 12093,  9107, 25342, 53399,  5636, 28991,\n            26795],\n           dtype=&#39;int64&#39;, length=70000)] are in the [columns]&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fonction d&#39;affichage sur le plan principal (n1,n2)</span>
<span class="c1"># x : coordonnées des individus projetés sur les composantes principales</span>
<span class="c1"># y : label de l&#39;individu (c&#39;est à dire le chiffre lu sur l&#39;image)</span>
<span class="c1"># n1,n2 : numéro des composantes principales (0 : première CP,  : 2e CP...)</span>
<span class="k">def</span> <span class="nf">AfficheCP2D</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">n1</span><span class="p">,</span><span class="n">n2</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">n1</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">n2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">[:],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;jet&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Composante principale &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n1</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Composante principale &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n2</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MNIST&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="k">def</span> <span class="nf">AfficheCP3D</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">n1</span><span class="p">,</span><span class="n">n2</span><span class="p">,</span><span class="n">n3</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">,</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">134</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">n1</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">n2</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">n3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;jet&#39;</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Composante principale &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n1</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Composante principale &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n2</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Composante principale &#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">n3</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;MNIST&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">w_xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">w_yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">w_zaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-TP_Regression_Lineaire"></span><div class="tex2jax_ignore mathjax_ignore section" id="tp-regression-lineaire">
<h2>TP Régression linéaire<a class="headerlink" href="#tp-regression-lineaire" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># n&#39;exécuter qu&#39;une fois</span>
<span class="o">!</span>pip3 install sklearn numpy matplotlib ipywidgets IPython --q
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> A new release of pip is available: <span class=" -Color -Color-Red">23.2.1</span> -&gt; <span class=" -Color -Color-Green">23.3</span>
<span class=" -Color -Color-Bold">[</span><span class=" -Color -Color-Blue">notice</span><span class=" -Color -Color-Bold">]</span> To update, run: <span class=" -Color -Color-Green">python3.11 -m pip install --upgrade pip</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">IntSlider</span><span class="p">,</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span><span class="n">Lasso</span><span class="p">,</span><span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="regression-lineaire-premier-exemple">
<h3>Régression linéaire : premier exemple<a class="headerlink" href="#regression-lineaire-premier-exemple" title="Permalink to this headline">#</a></h3>
<p>On s’intéresse ici à la régression d’une droite sur un nuage de points, bruité par un bruit normal <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma^2)\)</span>, dont l’amplitude peut varier</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_points</span><span class="o">=</span><span class="mi">50</span>
<span class="c1"># Paramètres de la &quot;vraie droite&quot; y=b0+b1.x</span>
<span class="n">b0</span><span class="p">,</span><span class="n">b1</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span><span class="mf">2.5</span>

<span class="c1"># Intervalle </span>
<span class="n">x1</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="n">nb_points</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nb_points</span><span class="p">)</span>



<span class="k">def</span> <span class="nf">F</span><span class="p">(</span><span class="n">test_size</span><span class="p">,</span><span class="n">amp_bruit</span><span class="p">,</span><span class="n">var_bruit</span><span class="p">,</span><span class="n">moyenne_bruit</span><span class="p">):</span>
    <span class="n">y</span><span class="o">=</span><span class="n">b0</span><span class="o">+</span><span class="n">b1</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">amp_bruit</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">moyenne_bruit</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">var_bruit</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nb_points</span><span class="p">)</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">b0</span><span class="o">+</span><span class="n">b1</span><span class="o">*</span><span class="n">x</span>


    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xmean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">ymean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>    

    
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span> 
    <span class="n">beta0</span><span class="p">,</span><span class="n">beta1</span><span class="o">=</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">train_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">train_score</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="n">RMSE_train</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">train_pred</span><span class="o">-</span><span class="n">y_train</span><span class="p">)))</span>
    <span class="n">RMSE_test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">test_pred</span><span class="o">-</span><span class="n">y_test</span><span class="p">)))</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Score Entrainement : </span><span class="si">{0:3.4f}</span><span class="s2">, Test : </span><span class="si">{1:3.4f}</span><span class="s2"> --  RMSE Entrainement : </span><span class="si">{2:3.4f}</span><span class="s2">, Test : </span><span class="si">{3:3.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span><span class="n">test_score</span><span class="p">,</span><span class="n">RMSE_train</span><span class="p">,</span><span class="n">RMSE_test</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">train_pred</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Régression&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_true</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Vraie droite&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Entrainement&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">test_pred</span><span class="p">,</span><span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;magenta&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;prédit&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;Droite : $y = </span><span class="si">{0:3.4f}</span><span class="s2">+</span><span class="si">{1:3.4f}</span><span class="s2">x$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">b0</span><span class="p">,</span><span class="n">b1</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Régression : $y = </span><span class="si">{0:3.4f}</span><span class="s2">+</span><span class="si">{1:3.4f}</span><span class="s2">x$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta0</span><span class="p">,</span><span class="n">beta1</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;$R=</span><span class="si">{:3.4f}</span><span class="s2">$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s2">&quot;Centre de masse $X=(</span><span class="si">{0:3.3f}</span><span class="s2">,</span><span class="si">{0:3.3f}</span><span class="s2">)^T$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">xmean</span><span class="p">,</span><span class="n">ymean</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xmean</span><span class="p">,</span><span class="n">ymean</span><span class="p">,</span><span class="s2">&quot;X&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Centre de masse&#39;</span><span class="p">)</span>    


    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;regression.png&#39;</span><span class="p">,</span><span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">interactive</span><span class="p">(</span><span class="n">F</span><span class="p">,</span><span class="n">amp_bruit</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">var_bruit</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">moyenne_bruit</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span>
               <span class="n">test_size</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">RadioButtons</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;10%&quot;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span><span class="s2">&quot;30%&quot;</span><span class="p">:</span><span class="mf">0.3</span><span class="p">,</span><span class="s2">&quot;50%&quot;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">},</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">),</span><span class="n">disabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "f1141f9e49a0450f86c263ff83fea916"}
</script></div>
</div>
</div>
<div class="section" id="lineaire-ne-veut-pas-dire-droite">
<h3>Linéaire ne veut pas dire droite…<a class="headerlink" href="#lineaire-ne-veut-pas-dire-droite" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_points</span><span class="o">=</span><span class="mi">50</span>
<span class="n">x1</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="o">*</span><span class="n">nb_points</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nb_points</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">func_fit</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span><span class="n">test_size</span><span class="p">,</span><span class="n">degree</span><span class="p">,</span><span class="n">amp_bruit</span><span class="p">,</span><span class="n">var_bruit</span><span class="p">,</span><span class="n">moyenne_bruit</span><span class="p">):</span>
    <span class="n">y</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mf">0.6</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.2</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="mi">18</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y1</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">x1</span><span class="o">-</span><span class="mf">0.6</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.2</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="mi">18</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">y</span><span class="o">=</span> <span class="n">y</span><span class="o">+</span><span class="n">amp_bruit</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">moyenne_bruit</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">var_bruit</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">nb_points</span><span class="p">)</span>
    
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">55</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">model_type</span><span class="o">==</span><span class="s1">&#39;Linéaire&#39;</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">LinearRegression</span><span class="p">())</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">model_type</span><span class="o">==</span><span class="s1">&#39;LASSO&#39;</span><span class="p">):</span>    
        <span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">Lasso</span><span class="p">())</span>
        
    <span class="k">if</span> <span class="p">(</span><span class="n">model_type</span><span class="o">==</span><span class="s1">&#39;Ridge&#39;</span><span class="p">):</span>    
        <span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">,</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">Ridge</span><span class="p">())</span>
    
    <span class="n">X_train</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X_test</span><span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="n">train_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">train_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

    <span class="n">test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    
    <span class="n">RMSE_train</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">train_pred</span><span class="o">-</span><span class="n">y_train</span><span class="p">)))</span>
    <span class="n">RMSE_test</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">test_pred</span><span class="o">-</span><span class="n">y_test</span><span class="p">)))</span>
       
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Score Entrainement : </span><span class="si">{0:3.4f}</span><span class="s2">, Test : </span><span class="si">{1:3.4f}</span><span class="s2"> --  RMSE Entrainement : </span><span class="si">{2:3.4f}</span><span class="s2">, Test : </span><span class="si">{3:3.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_score</span><span class="p">,</span><span class="n">test_score</span><span class="p">,</span><span class="n">RMSE_train</span><span class="p">,</span><span class="n">RMSE_test</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Entraînement&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">test_pred</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;magenta&#39;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;predit&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Vraie courbe&#39;</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Courbe prédite&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
       
    <span class="k">return</span> <span class="p">(</span><span class="n">train_score</span><span class="p">,</span><span class="n">test_score</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">interactive</span><span class="p">(</span><span class="n">func_fit</span><span class="p">,</span><span class="n">model_type</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">RadioButtons</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Linéaire&#39;</span><span class="p">,</span><span class="s1">&#39;LASSO&#39;</span><span class="p">,</span> <span class="s1">&#39;Ridge&#39;</span><span class="p">],</span><span class="n">description</span> <span class="o">=</span> <span class="s2">&quot;Choose Model&quot;</span><span class="p">,</span><span class="n">layout</span><span class="o">=</span><span class="n">Layout</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">&#39;250px&#39;</span><span class="p">)),</span>
                <span class="n">test_size</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">RadioButtons</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;10%&quot;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span><span class="s2">&quot;30% &quot;</span><span class="p">:</span><span class="mf">0.3</span><span class="p">,</span><span class="s2">&quot;50%&quot;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">},</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">),</span>
               <span class="n">degree</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">description</span><span class="o">=</span> <span class="s1">&#39;Degré&#39;</span><span class="p">,</span><span class="n">continuous_update</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">amp_bruit</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">var_bruit</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span><span class="n">moyenne_bruit</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "44893e8a3c244cbd9c39cc086ae65cb4"}
</script></div>
</div>
</div>
</div>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"98a9adc1af934d3ba958b02bbf2cb222": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21618509dd6641e29a78844db133f453": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "144e1bb4372d4de0abf57c71c071c840": {"model_name": "RadioButtonsModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "RadioButtonsModel", "_options_labels": ["10%", "30%", "50%"], "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "RadioButtonsView", "description": "Test", "description_tooltip": null, "disabled": false, "index": 0, "layout": "IPY_MODEL_98a9adc1af934d3ba958b02bbf2cb222", "style": "IPY_MODEL_21618509dd6641e29a78844db133f453"}}, "07925ab681ba43ef8fbe5d2d242c4ee2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f1141f9e49a0450f86c263ff83fea916": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_144e1bb4372d4de0abf57c71c071c840", "IPY_MODEL_05a9493cf01044e69c24ac648f66b3f7", "IPY_MODEL_5583cfb64f2a4c528a7434db875fe8d7", "IPY_MODEL_c108c073ed2e422d9a22e13a4b232ab8", "IPY_MODEL_c8633dbc2256408baaf5b19d11aba191"], "layout": "IPY_MODEL_07925ab681ba43ef8fbe5d2d242c4ee2"}}, "ba6ef6f8c3da4dd698c33abedf883614": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7bf288ccae964b6386e06026eb4be8a6": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "05a9493cf01044e69c24ac648f66b3f7": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": true, "description": "amp_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_ba6ef6f8c3da4dd698c33abedf883614", "max": 5, "min": 0, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_7bf288ccae964b6386e06026eb4be8a6", "value": 2}}, "b1cf44ae6ebd4d738573e661299d142b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5ef42aea80b7456c8ba00a2a9a320e78": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "5583cfb64f2a4c528a7434db875fe8d7": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": true, "description": "var_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_b1cf44ae6ebd4d738573e661299d142b", "max": 2.0, "min": 0.0, "orientation": "horizontal", "readout": true, "readout_format": ".2f", "step": 0.1, "style": "IPY_MODEL_5ef42aea80b7456c8ba00a2a9a320e78", "value": 1.0}}, "4dd2d589fd284d4f8cdb4b896dfa25ba": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "496f996d158c41d8a86cce747214995f": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "c108c073ed2e422d9a22e13a4b232ab8": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": true, "description": "moyenne_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_4dd2d589fd284d4f8cdb4b896dfa25ba", "max": 3.0, "min": -3.0, "orientation": "horizontal", "readout": true, "readout_format": ".2f", "step": 0.5, "style": "IPY_MODEL_496f996d158c41d8a86cce747214995f", "value": 0.0}}, "bfd042c6fe4b4adf89e6cc806d369510": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c8633dbc2256408baaf5b19d11aba191": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_bfd042c6fe4b4adf89e6cc806d369510", "msg_id": "", "outputs": [{"output_type": "stream", "name": "stderr", "text": "<ipython-input-3-1106102c60a4>:42: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  plt.scatter(X_test,y_test,edgecolors='k',marker='x',c='g',s=100,label='test')\n<ipython-input-3-1106102c60a4>:43: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  plt.scatter(X_test,test_pred,edgecolors='k',marker='x',c='magenta',s=100,label='pr\u00e9dit')\n<ipython-input-3-1106102c60a4>:55: UserWarning: Legend does not support handles for Text instances.\nSee: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#implementing-a-custom-legend-handler\n  plt.legend(loc='best')\n"}, {"output_type": "display_data", "metadata": {"needs_background": "light"}, "data": {"text/plain": "<Figure size 864x432 with 1 Axes>", "image/png": "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\n"}}]}}, "fb15674a97204077bceda97d1898c1f5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "250px"}}, "60dd7fa58fc54db1970cfa231a9be474": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "973378283aef43c990ef15d71084d9ff": {"model_name": "RadioButtonsModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "RadioButtonsModel", "_options_labels": ["Lin\u00e9aire", "LASSO", "Ridge"], "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "RadioButtonsView", "description": "Choose Model", "description_tooltip": null, "disabled": false, "index": 0, "layout": "IPY_MODEL_fb15674a97204077bceda97d1898c1f5", "style": "IPY_MODEL_60dd7fa58fc54db1970cfa231a9be474"}}, "4e571e5d80a5423283b62932be089915": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ad5bfaaca98245d29fecedfa04a55a36": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "bdf234530ba945ff8390541c539ff401": {"model_name": "RadioButtonsModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "RadioButtonsModel", "_options_labels": ["10%", "30% ", "50%"], "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "RadioButtonsView", "description": "Test", "description_tooltip": null, "disabled": false, "index": 0, "layout": "IPY_MODEL_4e571e5d80a5423283b62932be089915", "style": "IPY_MODEL_ad5bfaaca98245d29fecedfa04a55a36"}}, "5826cd15a30f4d71b6830d3769be5abf": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d2b395abf05f402cb0abd2d97e665caf": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "95c1afccc6dc4bac8febc1b9d61ab45f": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": false, "description": "Degr\u00e9", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_5826cd15a30f4d71b6830d3769be5abf", "max": 30, "min": 1, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_d2b395abf05f402cb0abd2d97e665caf", "value": 1}}, "40ef1d789b5a4f49b871cd5146f74be8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "44893e8a3c244cbd9c39cc086ae65cb4": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_973378283aef43c990ef15d71084d9ff", "IPY_MODEL_bdf234530ba945ff8390541c539ff401", "IPY_MODEL_95c1afccc6dc4bac8febc1b9d61ab45f", "IPY_MODEL_f37673afe8bf4d86affb5d0ea840d4e7", "IPY_MODEL_d7e60ac497a84dd1b5c1f11ab236c27e", "IPY_MODEL_338fd55376cd4ebfa863f48d294abbb1", "IPY_MODEL_cdc23c83502742c397bf9fd0f1e47c6d"], "layout": "IPY_MODEL_40ef1d789b5a4f49b871cd5146f74be8"}}, "aa2793bdf3de4071bebdef05c35bb37c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8635aef6ae9b433281441d30e691c4b0": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "f37673afe8bf4d86affb5d0ea840d4e7": {"model_name": "IntSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "IntSliderView", "continuous_update": true, "description": "amp_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_aa2793bdf3de4071bebdef05c35bb37c", "max": 5, "min": 0, "orientation": "horizontal", "readout": true, "readout_format": "d", "step": 1, "style": "IPY_MODEL_8635aef6ae9b433281441d30e691c4b0", "value": 2}}, "351a232e5c47441dba64afa6dac61e1b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "31b7d569f38d44c492ce0048c2a4b076": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "d7e60ac497a84dd1b5c1f11ab236c27e": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": true, "description": "var_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_351a232e5c47441dba64afa6dac61e1b", "max": 2.0, "min": 0.0, "orientation": "horizontal", "readout": true, "readout_format": ".2f", "step": 0.1, "style": "IPY_MODEL_31b7d569f38d44c492ce0048c2a4b076", "value": 1.0}}, "f70e61394f544d53ba524a3b4e2ef95d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "85b1bb6ebfbd49b1800bb7a2e7f1d66f": {"model_name": "SliderStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "SliderStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": "", "handle_color": null}}, "338fd55376cd4ebfa863f48d294abbb1": {"model_name": "FloatSliderModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatSliderModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "FloatSliderView", "continuous_update": true, "description": "moyenne_bruit", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_f70e61394f544d53ba524a3b4e2ef95d", "max": 3.0, "min": -3.0, "orientation": "horizontal", "readout": true, "readout_format": ".2f", "step": 0.5, "style": "IPY_MODEL_85b1bb6ebfbd49b1800bb7a2e7f1d66f", "value": 0.0}}, "32e33f88e7514a1abb4ab15c59e1cb7c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "cdc23c83502742c397bf9fd0f1e47c6d": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_32e33f88e7514a1abb4ab15c59e1cb7c", "msg_id": "", "outputs": [{"output_type": "display_data", "metadata": {"needs_background": "light"}, "data": {"text/plain": "<Figure size 1008x432 with 1 Axes>", "image/png": "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\n"}}]}}}, "version_major": 2, "version_minor": 0}
</script>

</div>
<div class="toctree-wrapper compound">
<span id="document-canonique"></span><div class="tex2jax_ignore mathjax_ignore section" id="analyse-canonique">
<h2>Analyse canonique<a class="headerlink" href="#analyse-canonique" title="Permalink to this headline">#</a></h2>
<p>On considère <span class="math notranslate nohighlight">\(n\)</span> individus décrits par deux ensembles de <span class="math notranslate nohighlight">\(p\)</span> et <span class="math notranslate nohighlight">\(q\)</span> variables quantitatives respectivement, où on suppose sans perte de généralité que <span class="math notranslate nohighlight">\(p\leq q\)</span>. L’analyse canonique se propose d’examiner les liens existants entre ces deux ensembles afin de savoir s’ils mesurent les mêmes propriétés. On se retreint ici au cas de deux groupes, la généralisation au cas de <span class="math notranslate nohighlight">\(g\)</span> groupes quelconques donnant lieu à l’analyse canonique généralisée.</p>
<p>Formellement on dispose donc de deux matrices <span class="math notranslate nohighlight">\({\bf {\bf X_1}}\in\mathcal{M}_{n,p}(\mathbb{R})\)</span> et <span class="math notranslate nohighlight">\({\bf {\bf X_2}}\in\mathcal{M}_{n,q}(\mathbb{R})\)</span>, rassemblées dans une matrice <span class="math notranslate nohighlight">\({\bf X}=\left[{\bf {\bf X_1}}\mid {\bf {\bf X_2}} \right ]\)</span>. On supposera <span class="math notranslate nohighlight">\({\bf {\bf X_1}}\)</span> et <span class="math notranslate nohighlight">\({\bf {\bf X_2}}\)</span> de rang plein.</p>
<p>L’étude de la position géométrique relative des sous-espaces <span class="math notranslate nohighlight">\(Im({\bf {\bf X_1}})\)</span> et <span class="math notranslate nohighlight">\(Im({\bf {\bf X_2}})\)</span> permet d’analyser les deux ensembles de variables.</p>
<div class="section" id="variables-canoniques">
<h3>Variables canoniques<a class="headerlink" href="#variables-canoniques" title="Permalink to this headline">#</a></h3>
<p>On munit <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> d’une métrique <span class="math notranslate nohighlight">\({\bf D}\)</span>.</p>
<div class="section" id="principe">
<h4>Principe<a class="headerlink" href="#principe" title="Permalink to this headline">#</a></h4>
<p>La recherche de variables canoniques consiste à rechercher <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\in Im({\bf {\bf X_1}}), \Vert {\bf {\bf u_1}}\Vert=1\)</span> et  <span class="math notranslate nohighlight">\({\bf v_1}\in Im({\bf {\bf X_2}})\)</span>, <span class="math notranslate nohighlight">\(\Vert {\bf v_1}\Vert=1\)</span> tels que l’angle <span class="math notranslate nohighlight">\(({\bf {\bf u_1}},{\bf v_1})\)</span> est le plus faible. On recherche ensuite <span class="math notranslate nohighlight">\({\bf {\bf u_2}}\in Im({\bf {\bf X_1}}), \Vert {\bf {\bf u_2}}\Vert=1\)</span> orthogonal à <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> au sens de <span class="math notranslate nohighlight">\({\bf D}\)</span>, et <span class="math notranslate nohighlight">\({\bf v_2}\in Im({\bf {\bf X_2}}), \Vert {\bf v_2}\Vert=1\)</span> orthogonal à <span class="math notranslate nohighlight">\({\bf v_1}\)</span> au sens de <span class="math notranslate nohighlight">\({\bf D}\)</span> tels que l’angle <span class="math notranslate nohighlight">\(({\bf {\bf u_2}},{\bf v_2})\)</span> soit minimal. On poursuit cette procédure itérativement pour arriver au dernier couple <span class="math notranslate nohighlight">\(({\bf u_p},{\bf v_p})\)</span>.</p>
</div>
<div class="section" id="formulation-matricielle-dans-mathbb-r-n">
<h4>Formulation matricielle dans <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span><a class="headerlink" href="#formulation-matricielle-dans-mathbb-r-n" title="Permalink to this headline">#</a></h4>
<p>On note <span class="math notranslate nohighlight">\({\bf {\bf {\bf P_i}}},i\in[\![1,2]\!]\)</span> la projection orthogonale (au sens de <span class="math notranslate nohighlight">\({\bf D}\)</span>) sur <span class="math notranslate nohighlight">\(Im({\bf {\bf {\bf X_i}}})\)</span>. On sait alors que :</p>
<div class="math notranslate nohighlight">
\[{\bf {\bf P_i}}={\bf {\bf X_i}}({\bf {\bf X_i}}^T{\bf D}{\bf {\bf X_i}})^{-1}{\bf {\bf X_i}}^T{\bf D}\]</div>
<p>La recherche de  <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> et <span class="math notranslate nohighlight">\({\bf v_1}\)</span> amène à maximiser <span class="math notranslate nohighlight">\(cos({\bf {\bf u_1}},{\bf v_1})\)</span>. En utilisant le théorème suivant</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 14 </span> (Théorème de projection)</p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(L\)</span> un sous-espace vectoriel de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Étant donné un point <span class="math notranslate nohighlight">\({\bf y}\in \mathbb{R}^n\)</span>, il existe un unique point <span class="math notranslate nohighlight">\({\bf p}\)</span> de <span class="math notranslate nohighlight">\(L\)</span>, appelé la projection orthogonale de <span class="math notranslate nohighlight">\({\bf y}\)</span> sur <span class="math notranslate nohighlight">\(L\)</span>, tel que <span class="math notranslate nohighlight">\(\|{\bf y}-{\bf p}\|\leq \|{\bf y}-{\bf x}\|,\forall {\bf x}\in L\)</span>. Une condition nécessaire et suffisante pour que <span class="math notranslate nohighlight">\({\bf p}\in L\)</span> soit la projection orthogonale de <span class="math notranslate nohighlight">\({\bf y}\)</span> sur <span class="math notranslate nohighlight">\(L\)</span> est <span class="math notranslate nohighlight">\({\bf y}-{\bf p}\in L^\bot\)</span></p>
</div>
</div><p>On en déduit que <span class="math notranslate nohighlight">\({\bf v_1}\)</span> doit être tel que <span class="math notranslate nohighlight">\({\bf P_1}{\bf v_1}\)</span> soit colinéaire à <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span>. De même, <span class="math notranslate nohighlight">\({\bf v_1}\)</span> doit être le vecteur de <span class="math notranslate nohighlight">\(Im({\bf {\bf X_2}})\)</span> le plus proche de <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> (ou de <span class="math notranslate nohighlight">\({\bf P_1}{\bf v_1}\)</span> qui lui est colinéaire), donc <span class="math notranslate nohighlight">\({\bf v_1}\)</span> doit être colinéaire à <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}{\bf v_1}\)</span>. Le problème se ramène donc à la recherche des éléments propres de <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}\)</span>.</p>
<p>Tout d’abord, les vecteurs propres de <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}\)</span> sont dans <span class="math notranslate nohighlight">\(Im({\bf {\bf X_2}})\)</span>. En effet en multipliant à gauche <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}{\bf v_1}=\lambda_1{\bf v_1}\)</span> par <span class="math notranslate nohighlight">\({\bf P_2}\)</span>, et puisque <span class="math notranslate nohighlight">\({\bf P_2^2}={\bf P_2}\)</span> (projection), on a immédiatement <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}{\bf v_1}=\lambda_1{\bf P_2}{\bf v_1}\)</span> et donc <span class="math notranslate nohighlight">\({\bf P_2}{\bf v_1}={\bf v_1}\in Im({\bf {\bf X_2}})\)</span></p>
<p>On montre ensuite que <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}\)</span> est symétrique (par rapport à <span class="math notranslate nohighlight">\({\bf D}\)</span>). Pour cela, en fonction de la remarque précédente, il suffit de démontrer que pour tout <span class="math notranslate nohighlight">\({\bf x},{\bf y}\in Im({\bf {\bf X_2}})\)</span> (symétrie du produit scalaire engendré par <span class="math notranslate nohighlight">\({\bf D}\)</span>)</p>
<div class="math notranslate nohighlight">
\[{\bf x}^T{\bf D}{\bf P_2}{\bf P_1}{\bf y} = ({\bf P_2}{\bf P_1}{\bf x})^T{\bf D}{\bf y}\]</div>
<p>On a en effet :</p>
<p><span class="math notranslate nohighlight">\({\bf x}^T{\bf D}{\bf P_2}{\bf P_1}{\bf y}\)</span> =<span class="math notranslate nohighlight">\(({\bf P_2}{\bf x})^T{\bf D}{\bf P_1}{\bf y}\)</span>car <span class="math notranslate nohighlight">\({\bf P_2}\)</span> symétrique</p>
<p>=<span class="math notranslate nohighlight">\({\bf x}^T{\bf D}{\bf P_1}{\bf y}\)</span> car <span class="math notranslate nohighlight">\({\bf x}\in Im({\bf {\bf X_2}})\)</span></p>
<p>=<span class="math notranslate nohighlight">\(({\bf P_1}{\bf x})^T{\bf D}{\bf y}\)</span> car <span class="math notranslate nohighlight">\({\bf P_1}\)</span> symétrique</p>
<p>=<span class="math notranslate nohighlight">\(({\bf P_1}{\bf x})^T{\bf D}{\bf P_2}{\bf y}\)</span> car <span class="math notranslate nohighlight">\({\bf y}\in Im({\bf {\bf X_2}})\)</span></p>
<p>=<span class="math notranslate nohighlight">\(({\bf P_2}{\bf P_1}{\bf x})^T{\bf D}{\bf y}\)</span> car <span class="math notranslate nohighlight">\({\bf P_2}\)</span> symétrique</p>
<p><span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}\)</span> est donc symétrique, réelle, elle est donc diagonalisable. Elle possède au plus <span class="math notranslate nohighlight">\(min(p,q)\)</span> valeurs propres non identiquement nulles, toutes positives car les <span class="math notranslate nohighlight">\({\bf {\bf P_i}}\)</span> sont des matrices semi définies positives.</p>
<p>On recherche donc maintenant <span class="math notranslate nohighlight">\({\bf v_1}\)</span> tel que <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}{\bf v_1}=\lambda_1{\bf v_1}\)</span>. Il est immédiat alors que <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf P_1}{\bf P_2}\)</span> associé à <span class="math notranslate nohighlight">\(\lambda_1\)</span>, qui représente le carré du cosinus de l’angle entre <span class="math notranslate nohighlight">\({\bf v_1}\)</span> et <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> (donc <span class="math notranslate nohighlight">\(\lambda_1\leq 1\)</span>).</p>
<p>Le cas <span class="math notranslate nohighlight">\(\lambda_1=1\)</span> correspond à <span class="math notranslate nohighlight">\({\bf {\bf u_1}}={\bf v_1}\)</span> et sa multiplicité donne la dimension de <span class="math notranslate nohighlight">\(Im({\bf {\bf X_1}})\bigcap Im({\bf {\bf X_2}})\)</span>.</p>
<p>Les vecteurs propres associés à des valeurs propres nulles de rang inférieur à <span class="math notranslate nohighlight">\(q\)</span> engendrent la partie de <span class="math notranslate nohighlight">\(Im({\bf {\bf X_2}})\)</span> orthogonale à <span class="math notranslate nohighlight">\(Im({\bf {\bf X_1}})\)</span>.
En résumé, dans <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, les vecteurs propres <span class="math notranslate nohighlight">\({\bf u_i},{\bf v_i}\)</span> des matrices <span class="math notranslate nohighlight">\({\bf P_1}{\bf P_2}\)</span> et <span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}\)</span> vérifient :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\bf P_2}{\bf P_1}{\bf v_i}=\lambda_i {\bf v_i}\)</span> et <span class="math notranslate nohighlight">\(\sqrt{\lambda_i}{\bf v_i} = {\bf P_2}{\bf u_i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\({\bf P_1}{\bf P_2}{\bf u_i}=\lambda_i {\bf u_i}\)</span> et <span class="math notranslate nohighlight">\(\sqrt{\lambda_i}{\bf u_i} = {\bf P_1}{\bf v_i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall i\neq j)\quad {\bf v_i}^T{\bf D}{\bf v_j} = {\bf u_i}^T{\bf D}{\bf u_j}={\bf v_i}^T{\bf D}{\bf u_j}=0\)</span></p></li>
</ul>
</div>
<div class="section" id="formulation-matricielle-dans-les-espaces-des-variables">
<h4>Formulation matricielle dans les espaces des variables<a class="headerlink" href="#formulation-matricielle-dans-les-espaces-des-variables" title="Permalink to this headline">#</a></h4>
<p>D’après la définition des sous-espaces image, il existe <span class="math notranslate nohighlight">\({\bf a_i}\in\mathbb{R}^p,{\bf b_i}\in\mathbb{R}^q\)</span> tels que  <span class="math notranslate nohighlight">\({\bf u_i}={\bf {\bf X_1}}{\bf a_i}\)</span> et <span class="math notranslate nohighlight">\({\bf v_i}={\bf {\bf X_2}}{\bf b_i}\)</span>. Les <span class="math notranslate nohighlight">\({\bf a_i}\)</span> et <span class="math notranslate nohighlight">\({\bf b_i}\)</span> sont les facteurs canoniques qui se calculent par</p>
<div class="math notranslate nohighlight">
\[{\bf P_1}{\bf P_2}{\bf u_i}=\lambda_i {\bf u_i} \Leftrightarrow {\bf P_1}{\bf P_2}{\bf X_1}{\bf a_i}=\lambda_i {\bf X_1} {\bf a_i}\]</div>
<p>soit</p>
<div class="math notranslate nohighlight">
\[({\bf X_1}({\bf X_1}^T{\bf D}{\bf X_1})^{-1}{\bf X_1}^T{\bf D})({\bf X_2}({\bf X_2}^T{\bf D}{\bf X_2})^{-1}{\bf X_2}^T{\bf D}){\bf X_1}{\bf a_i}=\lambda_i {\bf X_1} {\bf a_i}\]</div>
<p>Or <span class="math notranslate nohighlight">\(Rg({\bf X_1})=p\)</span> donc l’équation se simplifie en</p>
<div class="math notranslate nohighlight">
\[(({\bf X_1}^T{\bf D}{\bf X_1})^{-1}{\bf X_1}^T{\bf D})({\bf X_2}({\bf X_2}^T{\bf D}{\bf X_2})^{-1}{\bf X_2}^T{\bf D}){\bf X_1}{\bf a_i}=\lambda_i {\bf a_i}\]</div>
<p>et de même</p>
<div class="math notranslate nohighlight">
\[(({\bf X_2}^T{\bf D}{\bf X_2})^{-1}{\bf X_2}^T{\bf D})({\bf X_1}({\bf X_1}^T{\bf D}{\bf X_1})^{-1}{\bf X_1}^T{\bf D}){\bf X_2}{\bf b_i}=\lambda_i {\bf b_i}\]</div>
<p>Si les variables sont centrées (<span class="math notranslate nohighlight">\({\bf {\bf X_i}}^T{\bf D}{\bf 1} = 0\)</span>), les matrices <span class="math notranslate nohighlight">\({\bf {\bf X_i}}^T{\bf D}{\bf X_j}\)</span> s’interprètent comme des matrices de covariance que l’on note <span class="math notranslate nohighlight">\({\bf V_{ij}} = {\bf {\bf X_i}}^T{\bf D}{\bf X_j}\)</span> et les équations des facteurs canoniques s’écrivent finalement</p>
<p><span class="math notranslate nohighlight">\( {\bf V_{11}^{-1}}{\bf V_{12}}{\bf V_{22}^{-1}}{\bf V_{21}}{\bf a_i}\)</span>=<span class="math notranslate nohighlight">\(\lambda_i {\bf a_i}\)</span></p>
<p><span class="math notranslate nohighlight">\( {\bf V_{22}^{-1}}{\bf V_{21}}{\bf V_{11}^{-1}}{\bf V_{12}}{\bf b_i}\)</span>=<span class="math notranslate nohighlight">\(\lambda_i {\bf b_i}\)</span></p>
<p>et les <span class="math notranslate nohighlight">\(\lambda_i\)</span> sont les carrés des coefficients de corrélation canonique entre les variables canoniques. Si on impose que les variables canoniques soient de variance unité, on normalise les facteurs par <span class="math notranslate nohighlight">\({\bf a_i}^T{\bf V_{11}}{\bf a_i}=1\)</span> et  <span class="math notranslate nohighlight">\({\bf b_i}^T{\bf V_{22}}{\bf b_i}=1\)</span>, d’où</p>
<div class="math notranslate nohighlight">
\[{\bf b_i}=\frac{1}{\sqrt{\lambda_i}}{\bf V_{22}^{-1}}{\bf V_{21}}{\bf a_i}\textrm{  et } {\bf a_i}=\frac{1}{\sqrt{\lambda_i}}{\bf V_{11}^{-1}}{\bf V_{12}}{\bf b_i}\]</div>
</div>
</div>
<div class="section" id="representation-graphique">
<h3>Représentation graphique<a class="headerlink" href="#representation-graphique" title="Permalink to this headline">#</a></h3>
<div class="section" id="representation-des-variables">
<h4>Représentation des variables<a class="headerlink" href="#representation-des-variables" title="Permalink to this headline">#</a></h4>
<p>On peut représenter indifféremment les variables canoniques de <span class="math notranslate nohighlight">\(Im({\bf {\bf X_1}})\)</span> ou <span class="math notranslate nohighlight">\(Im({\bf {\bf X_2}})\)</span> : on représente pour cela les colonnes de <span class="math notranslate nohighlight">\({\bf X_1}\)</span> et <span class="math notranslate nohighlight">\({\bf X_2}\)</span> en projection sur la base des <span class="math notranslate nohighlight">\({\bf u_i}\)</span> (resp <span class="math notranslate nohighlight">\({\bf v_i}\)</span>).</p>
<p>Par exemple, si l’on s’intéresse à la représentation sur <span class="math notranslate nohighlight">\(Im({\bf {\bf X_1}})\)</span>,  la projection sur <span class="math notranslate nohighlight">\(Lin({\bf {\bf u_1}},{\bf {\bf u_2}})\)</span> est appelée cercle des corrélations, car, si les colonnes de <span class="math notranslate nohighlight">\({\bf X_1}\)</span> et <span class="math notranslate nohighlight">\({\bf X_2}\)</span> sont normées par rapport à <span class="math notranslate nohighlight">\({\bf D}\)</span>, les composantes sur la base des <span class="math notranslate nohighlight">\(({\bf u_i})\)</span> sont les coefficients de corrélation entre variables initiales et variables canoniques. En effet, si on note pour <span class="math notranslate nohighlight">\(k\in[\![1,p]\!]\)</span> <span class="math notranslate nohighlight">\({\bf X_{\bullet,k}}\)</span> la <span class="math notranslate nohighlight">\(k^e\)</span> colonne de <span class="math notranslate nohighlight">\({\bf X_1}\)</span> alors <span class="math notranslate nohighlight">\({\bf X_{\bullet,k}}^T{\bf D}{\bf {\bf u_1}}={\bf X_{.k}}^T{\bf D}{\bf X_1}{\bf a_1}\)</span>. Le coefficient de corrélation entre <span class="math notranslate nohighlight">\({\bf X_{\bullet,k}}\)</span> et <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> est la <span class="math notranslate nohighlight">\(k^e\)</span> composante de <span class="math notranslate nohighlight">\({\bf V_{11}}{\bf a_1}\)</span> car <span class="math notranslate nohighlight">\({\bf X_{\bullet,k}}={\bf X_1}{\boldsymbol \delta_k}\)</span>, où <span class="math notranslate nohighlight">\({\boldsymbol \delta_k}\in\mathbb{R}^p\)</span> est le vecteur nul sauf la <span class="math notranslate nohighlight">\(k^e\)</span> composante qui vaut 1.</p>
<p>De même si on note pour <span class="math notranslate nohighlight">\(l\in[\![1,q]\!]\)</span> <span class="math notranslate nohighlight">\({\bf X_{\bullet,l}}\)</span> la <span class="math notranslate nohighlight">\(l^e\)</span> colonne de <span class="math notranslate nohighlight">\({\bf X_2}\)</span> alors <span class="math notranslate nohighlight">\({\bf X_{\bullet,l}}^T{\bf D}{\bf {\bf u_1}}={\boldsymbol \delta_l^T}{\bf X_2}^T{\bf D}{\bf X_1}a_1\)</span>, et la corrélation entre <span class="math notranslate nohighlight">\({\bf X_{\bullet,l}}\)</span> et <span class="math notranslate nohighlight">\({\bf u_i}\)</span> est la <span class="math notranslate nohighlight">\(l^e\)</span> composante de <span class="math notranslate nohighlight">\({\bf V_{21}}{\bf a_i}\)</span> ou encore la <span class="math notranslate nohighlight">\(l^e\)</span> composante de <span class="math notranslate nohighlight">\(\sqrt{\lambda_1}{\bf V_{22}}{\bf b_i}\)</span>.</p>
</div>
<div class="section" id="representation-des-individus">
<h4>Représentation des individus<a class="headerlink" href="#representation-des-individus" title="Permalink to this headline">#</a></h4>
<p>Là encore, on peut représenter les individus selon les variables canoniques <span class="math notranslate nohighlight">\({\bf u_i}\)</span> ou <span class="math notranslate nohighlight">\({\bf v_i}\)</span>. Dans le plan <span class="math notranslate nohighlight">\(Lin({\bf {\bf u_1}},{\bf {\bf u_2}})\)</span>, par exemple, les coordonnées du <span class="math notranslate nohighlight">\(k^e\)</span> point sont les <span class="math notranslate nohighlight">\(k^e\)</span> composantes des variables canoniques <span class="math notranslate nohighlight">\({\bf {\bf u_1}}\)</span> et <span class="math notranslate nohighlight">\({\bf {\bf u_2}}\)</span>.</p>
</div>
</div>
<div class="section" id="quelques-reflexions-autour-de-l-analyse-canonique">
<h3>Quelques réflexions autour de l’analyse canonique<a class="headerlink" href="#quelques-reflexions-autour-de-l-analyse-canonique" title="Permalink to this headline">#</a></h3>
<div class="section" id="interet-theorique">
<h4>Intérêt théorique<a class="headerlink" href="#interet-theorique" title="Permalink to this headline">#</a></h4>
<p>L’analyse canonique présente un intérêt théorique certain car plusieurs méthodes statistiques très utilisées en sont des cas particuliers.</p>
</div>
<div class="section" id="cas-q-1">
<h4>Cas <span class="math notranslate nohighlight">\(q\)</span>=1<a class="headerlink" href="#cas-q-1" title="Permalink to this headline">#</a></h4>
<p>Si l’on cherche à expliquer <span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span>  par <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> :</p>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(p\)</span>=1, c’est un problème de régression linéaire</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(p&gt;\)</span>1, c’est un problème de régression linéaire multiple</p></li>
<li><p>Si  <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> est constitué d’une ou plusieurs variables qualitatives, on aboutit à un modèle d’analyse de la variance</p></li>
<li><p>Si  <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> est constitué de variables qualitatives et quantitatives, on aboutit à un modèle d’analyse de la covariance</p></li>
</ul>
<p>Dans tous ces cas, le problème est de maximiser le coefficient de corrélation entre une variable quantitative et un ensemble de variables, c’est donc bien un problème d’analyse canonique.</p>
</div>
<div class="section" id="cas-general">
<h4>Cas général<a class="headerlink" href="#cas-general" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>L’analyse Factorielle Discriminante est le cas particulier de l’analyse canonique  pour lequel <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> décrit un ensemble de variables quantitatives et <span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span> une variable qualitative</p></li>
<li><p>L’Analyse Factorielle des correspondances est l’instantiation d’une analyse canonique dans laquelle <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span>  et <span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span> décrivent les modalités d’une variable qualitative.</p></li>
</ul>
</div>
<div class="section" id="limites">
<h4>Limites<a class="headerlink" href="#limites" title="Permalink to this headline">#</a></h4>
<p>L’analyse canonique décrit les relations linéaires existant entre deux ensembles de variables : les premières étapes mettent en évidence les directions de l’espace des variables selon lesquelles les deux ensembles sont les plus proches. Mais il est possible que les variables canoniques soient faiblement corrélées aux variables des tableaux <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span>  et <span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span>, elles deviennent dans ce cas  difficilement interprétables. Les variables d’origine n’interviennent en effet pas dans les calculs de détermination des composantes canoniques, seuls comptent les projecteurs sur les espaces engendrés par ces variables.</p>
</div>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<p>On s’intéresse au jeu de données <a class="reference external" href="https://gist.github.com/seankross/a412dfbd88b3db70b74b">mtcars</a>, qui décrit 32 voitures à l’aide de 11 paramètres quantitatifs. Ces paramètres peuvent être classés en deux catégories, l’une  (<span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span>) relative à des performances de conduite (puissance, consommation, …), l’autre  (<span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span>) plus reliée aux caractéristiques de l’automobile (nombre de cylindres, nombre de carburateurs,…). L’objectif est de voir s’il existe un lien entre ces deux ensembles de variables.</p>
<p>La projection des individus (les voitures) dans le plan <span class="math notranslate nohighlight">\(\mathbf u_1,\mathbf u_2\)</span> est donnée dans la figure suivante. A gauche, la projection des individus dans (<span class="math notranslate nohighlight">\(\mathbf u_1,\mathbf u_2\)</span>), à droite la projection des variables sur le cercle des corrélations. Les  variables de  <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> sont représentées en rouge, celles de  <span class="math notranslate nohighlight">\(\mathbf{X_2}\)</span>  en bleu</p>
<p><img alt="" src="_images/cca1.png" /><img alt="" src="_images/cca2.png" /></p>
<p>Ce plan identifie  deux extrêmes : les grandes voitures (en bas à gauche) et les petites  voitures (en haut à droite). La corrélation des données sur ce plan est de 0.98, une relation linéaire semble donc évidente entre ces deux groupes.</p>
<p>La projection des variables sur le cercle des corrélations montre  que toutes les variables sont expressives (près du cercle).</p>
</div>
</div>
<span id="document-afc"></span><div class="tex2jax_ignore mathjax_ignore section" id="analyse-factorielle-des-correspondances">
<h2>Analyse Factorielle des correspondances<a class="headerlink" href="#analyse-factorielle-des-correspondances" title="Permalink to this headline">#</a></h2>
<p id="index-0">On cherche à expliquer la liaison entre deux variables qualitatives <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, caractérisées par un ensemble de couples de modalités <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>. On note <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span> et <span class="math notranslate nohighlight">\(y_1\cdots y_K\)</span> les modalités distinctes de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> respectivement.</p>
<p>Plus précisément, l’analyse factorielle des correspondances (AFC) vise à définir un modèle statistique permettant de fournir des paramètres dont la représentation graphique illustre les correspondances entre les modalités de ces variables. Dans sa version “analyse de données”, l’AFC cherche à réduire la dimension des données en effectuant la décomposition factorielle des nuages de points associés aux profils lignes et aux profils colonnes du tableau de contingence croisant les modalités des deux variables (L’AFC est une double ACP sur les deux tableaux de profils). On aborde à la fin du chapitre une modélisation statistique de l’AFC, en supposant que les fréquences d’observation correspondent à l’observation d’une probabilité théorique, dont la distribution modélise le tableau de contingence des deux variables.</p>
<div class="section" id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">#</a></h3>
<p>Le tableau de contingence <span class="math notranslate nohighlight">\({\bf T}\)</span> entre les <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, vu comme une matrice,  est défini par</p>
<span class="target" id="index-1"></span><table class="colwidths-auto docutils align-default" id="index-2">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_k\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_K\)</span></p></th>
<th class="head"><p>total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{11}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1K}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_j\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{j1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{jk}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{jK}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{j.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_J\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{J1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{Jk}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{JK}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{J.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>total</p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.K}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
</tbody>
</table>
<p>où <span class="math notranslate nohighlight">\(n_{j.}\)</span> (resp <span class="math notranslate nohighlight">\(n_{.k}\)</span> )sont les effectifs marginaux représentant le nombre de fois où <span class="math notranslate nohighlight">\(x_j\)</span> (resp. <span class="math notranslate nohighlight">\(y_k\)</span>) apparaît, et <span class="math notranslate nohighlight">\(n_{jk}\)</span> le nombre d’apparitions du couple <span class="math notranslate nohighlight">\((x_j,y_k)\)</span>.</p>
<p>Les fréquences conjointes <span class="math notranslate nohighlight">\(f_{jk}=\frac{n_{jk}}{n}\)</span> et les fréquences marginales sont stockées dans des vecteurs <span class="math notranslate nohighlight">\({\bf g_J}=\begin{pmatrix}f_{1.}\ldots f_{J.} \end{pmatrix}^T\)</span> et <span class="math notranslate nohighlight">\({\bf g_K}=\begin{pmatrix}f_{.1}\ldots f_{.K} \end{pmatrix}^T\)</span>.</p>
<p>On note aussi <span class="math notranslate nohighlight">\({\bf D_J}=diag\left (f_{1.}\ldots f_{J.}\right )\)</span> et <span class="math notranslate nohighlight">\({\bf D_K}=diag\left (f_{.1}\ldots f_{.K} \right )\)</span>.</p>
<p>Dans le tableau de contingence <span class="math notranslate nohighlight">\(\mathbf T\)</span>, on lit le <span class="math notranslate nohighlight">\(j^e\)</span> profil ligne <span class="math notranslate nohighlight">\([\frac{n_{j1}}{n_{j.}}\ldots \frac{n_{jK}}{n_{j.}}]\)</span>, considéré comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span> et le <span class="math notranslate nohighlight">\(k^e\)</span> profil colonne <span class="math notranslate nohighlight">\([\frac{n_{1k}}{n_{.k}}\ldots \frac{n_{Jk}}{n_{.k}}]\)</span> considéré comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span>. Ces profils sont rangés dans des matrices de profils lignes <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R})\)</span> et de colonnes <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\)</span> définies par <span class="math notranslate nohighlight">\({\bf A}=\frac{1}{n}{\bf T^TD_J^{-1}}\textrm{  et  } {\bf B}=\frac{1}{n}{\bf T D_K^{-1}}\)</span></p>
</div>
<div class="section" id="double-acp">
<h3>Double ACP<a class="headerlink" href="#double-acp" title="Permalink to this headline">#</a></h3>
<p>L’analyse factorielle des correspondances peut être considérée comme le résultat d’une double ACP :</p>
<ul class="simple">
<li><p>une effectuée sur les profils colonnes dans <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span></p></li>
<li><p>une effectuée sur les profils lignes dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span></p></li>
</ul>
<p>relativement à la métrique du <span class="math notranslate nohighlight">\(\chi^2\)</span> de matrice <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> pour l’analyse en lignes et <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span> pour l’analyse en colonnes.</p>
<p>Ainsi, par exemple, la distance entre deux modalités <span class="math notranslate nohighlight">\(x_l\)</span> et <span class="math notranslate nohighlight">\(x_p\)</span> de <span class="math notranslate nohighlight">\(X\)</span> est donnée par :</p>
<p><span class="math notranslate nohighlight">\(\Vert {\bf A_{.l}}-{\bf A_{.p}}\Vert^2_{{\bf D_K^{-1}}} = \displaystyle\sum_{i=1}^K\frac{1}{f_{.i}}\left (A_{i,l}-A_{i,p} \right )^2\)</span></p>
<p>où <span class="math notranslate nohighlight">\({\bf A_{.l}}\)</span> est la <span class="math notranslate nohighlight">\(l^e\)</span> colonne de <span class="math notranslate nohighlight">\({\bf A}\)</span>. La métrique du <span class="math notranslate nohighlight">\(\chi^2\)</span> introduit les inverses
des fréquences marginales des modalités de <span class="math notranslate nohighlight">\(Y\)</span> comme pondérations des écarts
entre éléments de deux profils relatifs à <span class="math notranslate nohighlight">\(X\)</span> (et réciproquement). Elle attribue
donc plus de poids aux écarts correspondants à des modalités de faible effectif
(rares) pour <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="section" id="acp-dans-mathbb-r-j">
<h4>ACP dans <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span><a class="headerlink" href="#acp-dans-mathbb-r-j" title="Permalink to this headline">#</a></h4>
<p>L’ACP sur les profils colonnes est réalisée en recherchant les éléments propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span>, symétrique par rapport à la métrique <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span> et semi définie positive. On note <span class="math notranslate nohighlight">\(\bf U\)</span> la matrice des vecteurs propres. Cette ACP fournit une représentation des modalités de <span class="math notranslate nohighlight">\(Y\)</span>, réalisée au moyen des lignes de la matrice des composantes principales <span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TD_J^{-1}U}\)</span>.</p>
</div>
<div class="section" id="acp-dans-mathbb-r-k">
<h4>ACP dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span><a class="headerlink" href="#acp-dans-mathbb-r-k" title="Permalink to this headline">#</a></h4>
<p>L’ACP sur les profils lignes est réalisée en recherchant les éléments propres de <span class="math notranslate nohighlight">\({\bf AB}\)</span>, symétrique par rapport à la métrique <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> et semi définie positive. On note <span class="math notranslate nohighlight">\(\bf V\)</span> la matrice des vecteurs propres. Cette ACP fournit une représentation des modalités de <span class="math notranslate nohighlight">\(X\)</span>, réalisée au moyen des lignes de la matrice des composantes principales <span class="math notranslate nohighlight">\({\bf C_J}={\bf A^TD_K^{-1}V}\)</span>.</p>
<p>Puisque <span class="math notranslate nohighlight">\(\bf U\)</span> contient les vecteurs propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span> et <span class="math notranslate nohighlight">\(\bf V\)</span> ceux de <span class="math notranslate nohighlight">\({\bf AB}\)</span>, il suffit de réaliser en fait une seule ACP, les résultats de l’autre s’en déduisant simplement : si <span class="math notranslate nohighlight">\(\bf \Lambda\)</span> est la matrice des valeurs propres (hors <span class="math notranslate nohighlight">\(\lambda_0=0\)</span>) communes aux deux ACP :</p>
<p><span class="math notranslate nohighlight">\({\bf V} ={\bf AU\Lambda^{-\frac{1}{2}}}\textrm {  et  } {\bf U} ={\bf BV\Lambda^{-\frac{1}{2}}}\)</span>
Alors</p>
<p><span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TD_J^{-1}U }= {\bf B^TD_J^{-1}BV\Lambda^{-\frac{1}{2}}} = {\bf D_K^{-1}ABV \Lambda^{-\frac{1}{2}}} =  {\bf D_K^{-1}V \Lambda^{\frac{1}{2}}}\)</span>
et</p>
<p><span class="math notranslate nohighlight">\({\bf C_J}={\bf A^TD_K^{-1}V}= {\bf D_J^{-1}U \Lambda^{\frac{1}{2}}}\)</span>
d’où</p>
<p><span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TC_J\Lambda^{-\frac{1}{2}}} \textrm{    et    } {\bf C_J}={\bf A^TC_K\Lambda^{-\frac{1}{2}}}\)</span></p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 23 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soit deux matrices <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R)}\)</span>  et <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\)</span>. Les valeurs propres non nulles de <span class="math notranslate nohighlight">\({\bf AB}\)</span> et <span class="math notranslate nohighlight">\({\bf BA}\)</span> sont identiques avec le même degré de multiplicité. De plus, si <span class="math notranslate nohighlight">\(\bf u\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf BA}\)</span> associé à la valeur propre <span class="math notranslate nohighlight">\(\lambda\neq 0\)</span>, alors <span class="math notranslate nohighlight">\({\bf v} = {\bf Au} \)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf AB}\)</span> associé à la même valeur
propre.</p>
</div>
</div></div>
</div>
<div class="section" id="representation-graphique">
<h3>Représentation graphique<a class="headerlink" href="#representation-graphique" title="Permalink to this headline">#</a></h3>
<p>La décomposition de <span class="math notranslate nohighlight">\(\mathbf T/n\)</span> donne :</p>
<div class="math notranslate nohighlight">
\[\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}} = \displaystyle\sum_{i=0}^{min(J-1,K-1)}\sqrt{\lambda_i}\frac{u^i_jv^i_k}{f_{j.}f_{.k}}\]</div>
<p>Cette quantité est appelée taux de liaison entre les modalités <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span>. En se limitant au rang <span class="math notranslate nohighlight">\(q\)</span> on obtient pour chaque couple de modalité <span class="math notranslate nohighlight">\((j,k)\)</span> de <span class="math notranslate nohighlight">\(\mathbf T\)</span> une approximation de son écart relatif à l’indépendance, comme produit scalaire des deux vecteurs <span class="math notranslate nohighlight">\(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{j.}}u_j\)</span> et <span class="math notranslate nohighlight">\(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{.k}}v_k\)</span>, termes génériques des matrices <span class="math notranslate nohighlight">\({\bf D_J^{-1}U\Lambda^{\frac{1}{4}}}\)</span> et <span class="math notranslate nohighlight">\({\bf D_K^{-1}V\Lambda^{\frac{1}{4}}}\)</span>.</p>
<p>La représentation graphique de ces vecteurs (par exemple avec <span class="math notranslate nohighlight">\(q=2\)</span>), appelée biplot, donne la correspondance entre les deux modalités <span class="math notranslate nohighlight">\(x_j\)</span> et <span class="math notranslate nohighlight">\(y_k\)</span>. Lorsque ces deux modalités, éloignées de l’origine, sont
voisines (resp. opposées), leur produit scalaire est de valeur absolue importante ; leur cellule conjointe contribue alors fortement et de manière positive
(resp. négative) à la dépendance entre les deux variables. L’analyse factorielle des correspondances apparaît ainsi comme la meilleure reconstitution des fréquences <span class="math notranslate nohighlight">\(f_{jk}\)</span>, ou encore la meilleure représentation des écarts relatifs à l’indépendance.</p>
</div>
<div class="section" id="interpretation">
<h3>Interprétation<a class="headerlink" href="#interpretation" title="Permalink to this headline">#</a></h3>
<p>Les qualités de représentation dans la dimension choisie et les contributions
des modalités de <span class="math notranslate nohighlight">\(X\)</span> ou de <span class="math notranslate nohighlight">\(Y\)</span> se déduisent facilement de celles de l’ACP. Ces
quantités sont utilisées à la fois pour choisir la dimension de l’analyse factorielle des correspondances  et pour interpréter ses résultats dans la dimension choisie.</p>
<div class="section" id="inertie-et-test-d-independance">
<h4>Inertie et test d’indépendance<a class="headerlink" href="#inertie-et-test-d-independance" title="Permalink to this headline">#</a></h4>
<p>En analyse en composantes principales centrée réduite, l’inertie totale du nuage de points est  égale au nombre de variables. En AFC,  l’inertie totale du nuage des profils lignes est  égale à l’inertie totale du nuage des profils colonnes, égale au <span class="math notranslate nohighlight">\(\chi^2\)</span> d’indépendance entre les deux variables qualitatives.<br />
La valeur de l’inertie est donc un indicateur de la dispersion des nuages de points et une mesure de liaison entre les deux variables qualitatives,  appelée mesure d’écart à l’indépendance.</p>
</div>
<div class="section" id="interpretation-des-valeurs-propres">
<h4>Interprétation des valeurs propres<a class="headerlink" href="#interpretation-des-valeurs-propres" title="Permalink to this headline">#</a></h4>
<p>Les valeurs propres des ACP renseignent sur la dispersion des nuages de profils lignes et colonnes :</p>
<ul class="simple">
<li><p>Une valeur propre proche de 1 indique une dichotomie parfaite du tableau <span class="math notranslate nohighlight">\(\mathbf T\)</span>, qui peut être décomposé après reclassement des modalités en deux blocs distincts</p></li>
<li><p>Plus généralement <span class="math notranslate nohighlight">\(p\)</span> valeurs propres proches amènent à <span class="math notranslate nohighlight">\(k+1\)</span> blocs distincts</p></li>
<li><p>Si toutes les valeurs propres sont proches de 1, on aboutit à l’effet Guttman : il existe une correspondance entre chaque modalité ligne et une modalité colonne “associée”. Avec une réorganisation des modalités, les effectifs importants se trouvent alors le long de la diagonale.</p></li>
</ul>
</div>
<div class="section" id="qualite-globale">
<h4>Qualité globale<a class="headerlink" href="#qualite-globale" title="Permalink to this headline">#</a></h4>
<p>A <span class="math notranslate nohighlight">\(q\)</span> fixé, la qualité globale de la représentation se mesure comme dans le cadre de l’ACP, comme le rapport entre les <span class="math notranslate nohighlight">\(q\)</span> premières valeurs propres <span class="math notranslate nohighlight">\(\lambda_i\)</span> et la somme sur tout le spectre.</p>
<p>On montre que la qualité de la représentation dans la <span class="math notranslate nohighlight">\(i^e\)</span> dimension s’écrit <span class="math notranslate nohighlight">\(\frac{n\lambda_i}{\chi^2}\)</span></p>
</div>
<div class="section" id="qualite-de-chaque-modalite">
<h4>Qualité de chaque modalité<a class="headerlink" href="#qualite-de-chaque-modalite" title="Permalink to this headline">#</a></h4>
<p>Comme dans l’ACP également, la qualité d’une modalité de <span class="math notranslate nohighlight">\(X\)</span> (resp. <span class="math notranslate nohighlight">\(Y\)</span>) se quantifie par le carré du cosinus de l’angle entre le vecteur représentant cette modalité dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span> (resp. <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span>)  et sa projection orthogonale au sens de <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span>) dans le sous-espace principal de dimension <span class="math notranslate nohighlight">\(q\)</span>. Ces cosinus se calculent très simplement en faisant le rapport des sommes appropriées des carrés des coordonnées extraites des lignes de <span class="math notranslate nohighlight">\({\bf C_J}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf C_K}\)</span>).</p>
</div>
<div class="section" id="inertie-expliquee">
<h4>Inertie expliquée<a class="headerlink" href="#inertie-expliquee" title="Permalink to this headline">#</a></h4>
<p>L’inertie totale du nuage des profils lignes (resp. colonnes) est égale à la somme de toutes les valeurs propres <span class="math notranslate nohighlight">\(\lambda_i\)</span>. La part due au <span class="math notranslate nohighlight">\(j^e\)</span> profil ligne (resp. <span class="math notranslate nohighlight">\(k^e\)</span> profil colonne) est <span class="math notranslate nohighlight">\(f_{j.}\displaystyle\sum_i \left (\mathbf{C_J}(ji) \right )^2\)</span> (resp. <span class="math notranslate nohighlight">\(f_{.k}\displaystyle\sum_i \left (\mathbf{C_K}(ik) \right )^2\)</span>).</p>
<p>Les contributions à l’inertie selon chaque axe se calculent de la même manière, sans sommation sur <span class="math notranslate nohighlight">\(i\)</span>. Elles sont utilisées pour sélectionner les modalités les plus importantes (i.e. celles qui importent le plus dans la définition de la liaison entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>).</p>
</div>
<div class="section" id="choix-de-q">
<h4>Choix de q<a class="headerlink" href="#choix-de-q" title="Permalink to this headline">#</a></h4>
<p>Comme dans le cas de l’ACP, le choix de l’espace de représentation est important. On peut estimer <span class="math notranslate nohighlight">\(q\)</span> comme en ACP (pourcentage de l’inertie expliquée, décroissance des valeurs propres), ou utiliser une approche probabiliste : si</p>
<div class="math notranslate nohighlight">
\[\nu_{jk}^q = n f_{j.}f_{.k} + n\displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\]</div>
<p>est l’estimation d’ordre <span class="math notranslate nohighlight">\(q\)</span> de <span class="math notranslate nohighlight">\(n_{jk}\)</span> alors sous certaines conditions (<span class="math notranslate nohighlight">\(n\)</span> grand, modèle multinomial…), on montre que</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{j=1}^J\displaystyle\sum_{k=1}^K\frac{\left (n_{jk}-\nu_{jk}^q \right )^2}{\nu_{jk}^q}\approx \displaystyle\sum_{i\geq q+1} \lambda_i\)</span></p>
<p>suit approximativement une loi <span class="math notranslate nohighlight">\(\chi^2\)</span> à <span class="math notranslate nohighlight">\((J-q-1)(K-q-1)\)</span> degrés de liberté. On peut donc retenir <span class="math notranslate nohighlight">\(q\)</span> comme étant la plus petite dimension telle que cette quantité est inférieure à la valeur limite de cette loi.</p>
</div>
</div>
<div class="section" id="modele-statistique">
<h3>Modèle statistique<a class="headerlink" href="#modele-statistique" title="Permalink to this headline">#</a></h3>
<p>On suppose que chaque fréquence <span class="math notranslate nohighlight">\(f_{jk}\)</span> correspond à l’observation d’une probabilité   théorique <span class="math notranslate nohighlight">\(\pi_{jk}\)</span> et on modélise donc <span class="math notranslate nohighlight">\(\bf T\)</span> par la distribution correspondante. Le modèle décrivant cette distribution permet d’expliciter la probabilité.</p>
<div class="section" id="modele-log-lineaire">
<h4>Modèle log linéaire<a class="headerlink" href="#modele-log-lineaire" title="Permalink to this headline">#</a></h4>
<p>Souvent, le nombre <span class="math notranslate nohighlight">\(n\)</span> est fixé a priori. La distribution conjointe des effectifs <span class="math notranslate nohighlight">\(n_{jk}\)</span> est alors conditionnée par <span class="math notranslate nohighlight">\(n\)</span> et est une loi multinomiale de paramètre <span class="math notranslate nohighlight">\(\pi_{jk}\)</span> et d’espérance <span class="math notranslate nohighlight">\(n\pi_{jk}\)</span>.</p>
<p>Par définition, les variables <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont indépendantes si <span class="math notranslate nohighlight">\(\pi_{jk}=\pi_{j.}\pi_{.k}\)</span>. Dans le cas contraire, on peut écrire</p>
<p><span class="math notranslate nohighlight">\(\pi_{jk} = \pi_{j.}\pi_{.k}\frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\)</span></p>
<p>En passant au log, on linéarise en</p>
<p><span class="math notranslate nohighlight">\(ln\left (\pi_{jk}\right ) =  ln (\pi_{j.}) + ln (\pi_{.k})  + ln \left( \frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\right )\)</span></p>
<p>Ce modèle est saturé car il comporte autant de paramètres que de données.  L’indépendance est vérifiée si le dernier terme de couplage est nul pour tout <span class="math notranslate nohighlight">\((j,k)\)</span>. Les paramètres du modèle sont estimés en maximisant la log vraisemblance.</p>
</div>
<div class="section" id="modele-de-correlation">
<h4>Modèle de corrélation<a class="headerlink" href="#modele-de-correlation" title="Permalink to this headline">#</a></h4>
<p>Dans ce modèle, on écrit</p>
<p><span class="math notranslate nohighlight">\(\pi_{jk} = \pi_{j.}\pi_{.k} + \displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\)</span></p>
<p>où <span class="math notranslate nohighlight">\({\bf u^i}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf v^i}\)</span>) sont les vecteurs propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf AB}\)</span>),  <span class="math notranslate nohighlight">\(\lambda_i\)</span> les valeurs propres associées (qui sont identiques pour les deux matrices), et <span class="math notranslate nohighlight">\(q\leq min(J-1,K-1)\)</span>.</p>
<p>Les contraintes <span class="math notranslate nohighlight">\(\displaystyle\sum_{j=1}^J u^i_j =  \displaystyle\sum_{k=1}^K v^i_k = 0\)</span>  et <span class="math notranslate nohighlight">\({\bf (u^i)^TD_J^{-1} u^l }= {\bf (v^i)^TD_K^{-1} v^l} = \delta_{il}\)</span> (vecteurs propres orthonormés) permettent d’identifier les paramètres du modèle.</p>
<p>Les estimations des paramètres <span class="math notranslate nohighlight">\(\pi_{j.}\pi_{.k} ,\lambda_i,u^i,v^i\)</span> peuvent être réalisées par maximum de vraisemblance ou par moindres carrés.</p>
</div>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<p>On utilise ici des données open source du <a class="reference external" href="https://www.data.gouv.fr/fr/posts/les-donnees-des-elections/">gouvernement</a>, présentant le résultat du premier tour des élections présidentielles de 2017.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>HAMON</p></th>
<th class="head"><p>MACRON</p></th>
<th class="head"><p>ASSELINEAU</p></th>
<th class="head"><p>FILLON</p></th>
<th class="head"><p>CHEMINADE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>151296</p></td>
<td><p>615775</p></td>
<td><p>30223</p></td>
<td><p>586390</p></td>
<td><p>6078</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>240175</p></td>
<td><p>851372</p></td>
<td><p>26667</p></td>
<td><p>602884</p></td>
<td><p>6264</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>256620</p></td>
<td><p>1026255</p></td>
<td><p>41352</p></td>
<td><p>846252</p></td>
<td><p>7602</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>87382</p></td>
<td><p>338187</p></td>
<td><p>14330</p></td>
<td><p>304387</p></td>
<td><p>2842</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>180827</p></td>
<td><p>581076</p></td>
<td><p>13419</p></td>
<td><p>380815</p></td>
<td><p>3400</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>83552</p></td>
<td><p>323724</p></td>
<td><p>12075</p></td>
<td><p>300324</p></td>
<td><p>2882</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>5780</p></td>
<td><p>28528</p></td>
<td><p>965</p></td>
<td><p>39453</p></td>
<td><p>253</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>430404</p></td>
<td><p>1612816</p></td>
<td><p>64406</p></td>
<td><p>1249770</p></td>
<td><p>9796</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>216362</p></td>
<td><p>740037</p></td>
<td><p>28603</p></td>
<td><p>566045</p></td>
<td><p>5524</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>166640</p></td>
<td><p>630300</p></td>
<td><p>26043</p></td>
<td><p>521389</p></td>
<td><p>5688</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>113744</p></td>
<td><p>423075</p></td>
<td><p>14303</p></td>
<td><p>370188</p></td>
<td><p>3544</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>143491</p></td>
<td><p>575832</p></td>
<td><p>15529</p></td>
<td><p>516428</p></td>
<td><p>3731</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>113344</p></td>
<td><p>520909</p></td>
<td><p>25948</p></td>
<td><p>615455</p></td>
<td><p>4569</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>101948</p></td>
<td><p>389440</p></td>
<td><p>18725</p></td>
<td><p>314017</p></td>
<td><p>3425</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>MELENCHON</p></th>
<th class="head"><p>LASSALLE</p></th>
<th class="head"><p>FILLON</p></th>
<th class="head"><p>DUPONT-AIGNAN</p></th>
<th class="head"><p>POUTOU</p></th>
<th class="head"><p>LEPEN</p></th>
<th class="head"><p>ARTHAUD</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>484810</p></td>
<td><p>30508</p></td>
<td><p>586390</p></td>
<td><p>182200</p></td>
<td><p>34468</p></td>
<td><p>825600</p></td>
<td><p>24272</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>703505</p></td>
<td><p>91915</p></td>
<td><p>602884</p></td>
<td><p>155600</p></td>
<td><p>49649</p></td>
<td><p>640228</p></td>
<td><p>21442</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>805846</p></td>
<td><p>53282</p></td>
<td><p>846252</p></td>
<td><p>215951</p></td>
<td><p>43530</p></td>
<td><p>867874</p></td>
<td><p>24670</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>276954</p></td>
<td><p>15843</p></td>
<td><p>304387</p></td>
<td><p>87263</p></td>
<td><p>18529</p></td>
<td><p>387658</p></td>
<td><p>11492</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>385736</p></td>
<td><p>19097</p></td>
<td><p>380815</p></td>
<td><p>87928</p></td>
<td><p>27092</p></td>
<td><p>306644</p></td>
<td><p>14296</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>252307</p></td>
<td><p>13570</p></td>
<td><p>300324</p></td>
<td><p>82060</p></td>
<td><p>16282</p></td>
<td><p>329470</p></td>
<td><p>11365</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>21314</p></td>
<td><p>8711</p></td>
<td><p>39453</p></td>
<td><p>4462</p></td>
<td><p>1374</p></td>
<td><p>43041</p></td>
<td><p>495</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>1225311</p></td>
<td><p>36358</p></td>
<td><p>1249770</p></td>
<td><p>226266</p></td>
<td><p>45715</p></td>
<td><p>708340</p></td>
<td><p>23592</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>734223</p></td>
<td><p>75483</p></td>
<td><p>566045</p></td>
<td><p>135405</p></td>
<td><p>35219</p></td>
<td><p>762104</p></td>
<td><p>16777</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>633322</p></td>
<td><p>22411</p></td>
<td><p>521389</p></td>
<td><p>160722</p></td>
<td><p>33653</p></td>
<td><p>1003221</p></td>
<td><p>29194</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>362535</p></td>
<td><p>13900</p></td>
<td><p>370188</p></td>
<td><p>98957</p></td>
<td><p>23816</p></td>
<td><p>452702</p></td>
<td><p>15196</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>403454</p></td>
<td><p>16988</p></td>
<td><p>516428</p></td>
<td><p>109842</p></td>
<td><p>26340</p></td>
<td><p>364267</p></td>
<td><p>16018</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>515419</p></td>
<td><p>29551</p></td>
<td><p>615455</p></td>
<td><p>119025</p></td>
<td><p>21316</p></td>
<td><p>774791</p></td>
<td><p>10439</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>256149</p></td>
<td><p>7748</p></td>
<td><p>314017</p></td>
<td><p>29505</p></td>
<td><p>17599</p></td>
<td><p>213553</p></td>
<td><p>13180</p></td>
</tr>
</tbody>
</table>
<p>On décide dans l’analyse d’enlever le candidat LASSALLE, dont les votes sont concentrés dans les Pyrenées et en Corse (et qui introduit un biais dans l’étude).</p>
<p>A partir de ce tableau de données <span class="math notranslate nohighlight">\(\bf T\)</span>, on calcule les tableaux de fréquences en lignes et en colonnes, ainsi que les profils ligne et colonne moyens.
Comme en ACP, on s’intéresse à l’inertie du nuage de points, mais pour ce faire on utilise la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span>. Avec cette métrique, la distance entre deux lignes (ou deux colonnes) ne dépend pas des poids respectifs des colonnes (ou lignes). Par exemple, les différents candidats obtiennent des scores très différents et l’usage de la métrique euclidienne aurait donné trop de poids aux candidats qui ont obtenu des scores élevés. De plus, la métrique du <span class="math notranslate nohighlight">\(\chi^2\)</span> possède la propriété d’équivalence distributionnelle : si on regroupe deux modalités lignes (colonnes), les distances entre les profils-colonne (lignes), ou entre les autres profils-lignes (colonnes) restent inchangées.</p>
<p><img alt="" src="_images/presdist.png" /></p>
<p>On peut également calculer les taux de liaisons, définis pour deux individus <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span> par <span class="math notranslate nohighlight">\(\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}}\)</span>. Par exemple, le taux de liaison entre HAMON et Grand-Est est égal à  -0.2003, tandis que le taux de liaison entre CHEMINADE et Nouvelle-Aquitaine est égal à 0.2068. Le taux de liaison s’interpréte comme suit : le score du candidat dans la région est 20% moins élevé (ou 20.6% moins élevé) que le score théorique que l’on observerait si les votes étaient indépendants des régions.</p>
<p>Notons que <span class="math notranslate nohighlight">\(f_{j.}f_{.k}\)</span> représente le poids théorique de chaque case du tableau des fréquences. La somme de ces coefficients vaut 1. La moyenne de la série des taux de liaisons pondérée par les <span class="math notranslate nohighlight">\(f_{j.}f_{.k}\)</span> est nulle. De même, la variance de cette série avec la même pondération vaut <span class="math notranslate nohighlight">\(\chi^2\)</span>, et ici est égale à 0.0301.</p>
<p>On réalise ensuite une AFC, par analyse spectrale des matrices <span class="math notranslate nohighlight">\(\bf X^T\bf X\)</span> et <span class="math notranslate nohighlight">\(\bf X\bf X^T\)</span>, où <span class="math notranslate nohighlight">\(\bf X\)</span> est la matrice de terme général <span class="math notranslate nohighlight">\(f_{jk}/\sqrt{f_{j.}f_{.k}}\)</span>.</p>
<p>Le nombre de valeurs propres produites par la recherche des facteurs principaux est égal au minimum du nombre de lignes et du nombre de colonnes du tableau de contingence. La première valeur propre est systématiquement égale à 1, et n’est pas utilisée dans les résultats. Les autres valeurs propres sont des nombres positifs inférieurs à 1 et leur somme est égale à <span class="math notranslate nohighlight">\(\chi^2\)</span>.</p>
<p><img alt="" src="_images/spectralelig.png" /> <img alt="" src="_images/spectralecol.png" /></p>
<p>On utilise alors les vecteurs propres (axes factoriels) pour analyser les données lignes et colonnes. Pour chaque analyse, on reporte (illustré ici sur l’analyse en lignes) :</p>
<ul class="simple">
<li><p>La masse, qui rappelle les fréquences marginales des lignes c’est-à-dire le profil colonne moyen. Contrairement à l’ACP normée, dans laquelle chaque individu était affecté du même poids, les régions ont ici un poids dépendant de l’effectif total d’électeurs inscrits dans la région.</p></li>
<li><p>La qualité qui indique les qualités de représentation des individus ligne sur les deux premiers axes factoriels : c’est la somme des carrés des composantes de chaque individu sur les 2 axes, normalisée par la somme des carrés des composantes sur tous les axes.</p></li>
<li><p>La contribution de chaque individu à la formation de chaque axe factoriel</p></li>
</ul>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>Masse</p></th>
<th class="head"><p>Coord1</p></th>
<th class="head"><p>Coord2</p></th>
<th class="head"><p>Qualité</p></th>
<th class="head"><p>contrib1</p></th>
<th class="head"><p>contrib2</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>0.082561</p></td>
<td><p>-0.081478</p></td>
<td><p>0.030469</p></td>
<td><p>0.033810</p></td>
<td><p>0.1031</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>0.092573</p></td>
<td><p>0.022322</p></td>
<td><p>-0.009848</p></td>
<td><p>0.002607</p></td>
<td><p>0.0129</p></td>
<td><p>0.1404</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>0.116102</p></td>
<td><p>0.002463</p></td>
<td><p>0.020096</p></td>
<td><p>0.000032</p></td>
<td><p>0.0005</p></td>
<td><p>0.0074</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>0.042922</p></td>
<td><p>-0.049796</p></td>
<td><p>0.021913</p></td>
<td><p>0.012884</p></td>
<td><p>0.0203</p></td>
<td><p>0.0048</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>0.055616</p></td>
<td><p>0.074398</p></td>
<td><p>-0.000143</p></td>
<td><p>0.028325</p></td>
<td><p>0.0727</p></td>
<td><p>0.00599</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>0.039694</p></td>
<td><p>-0.027564</p></td>
<td><p>0.034140</p></td>
<td><p>0.003982</p></td>
<td><p>0.0057</p></td>
<td><p>0.026</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>0.004089</p></td>
<td><p>-0.089963</p></td>
<td><p>0.097463</p></td>
<td><p>0.036278</p></td>
<td><p>0.0061</p></td>
<td><p>0.0528</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>0.157099</p></td>
<td><p>0.099907</p></td>
<td><p>0.022705</p></td>
<td><p>0.048773</p></td>
<td><p>0.3461</p></td>
<td><p>0.0099</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>0.090960</p></td>
<td><p>-0.024738</p></td>
<td><p>-0.020736</p></td>
<td><p>0.003126</p></td>
<td><p>0.0066</p></td>
<td><p>0.2115</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>0.090114</p></td>
<td><p>-0.113101</p></td>
<td><p>-0.015944</p></td>
<td><p>0.061802</p></td>
<td><p>0.2215</p></td>
<td><p>0.1268</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>0.052720</p></td>
<td><p>-0.035129</p></td>
<td><p>0.014011</p></td>
<td><p>0.006437</p></td>
<td><p>0.0116</p></td>
<td><p>0.0004</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>0.061053</p></td>
<td><p>0.049348</p></td>
<td><p>0.050417</p></td>
<td><p>0.012418</p></td>
<td><p>0.0311</p></td>
<td><p>0.1115</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>0.076388</p></td>
<td><p>-0.085523</p></td>
<td><p>0.044826</p></td>
<td><p>0.035070</p></td>
<td><p>0.1061</p></td>
<td><p>0.1766</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>0.038108</p></td>
<td><p>0.073346</p></td>
<td><p>0.038647</p></td>
<td><p>0.026537</p></td>
<td><p>0.051</p></td>
<td><p>0.0222</p></td>
</tr>
</tbody>
</table>
<p>On représente alors graphiquement les individus ligne et colonne sur le premier plan factoriel.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/planlig.png" /></p></th>
<th class="head"><p><img alt="" src="_images/plancol.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Individus ligne</p></td>
<td><p>Individus colonne</p></td>
</tr>
</tbody>
</table>
<p>On en déduit alors l’analyse suivante (ici proposée sur les candidats) :</p>
<ul class="simple">
<li><p>Premier axe : Le Pen représente 67% de l’inertie de cet axe. Macron, à l’opposé en représente 19%. Clairement, cet axe oppose Le Pen à Macron, mais les autres candidats “les plus importants” ont un score du même signe que celui de Macron.</p></li>
<li><p>Fillon représente 63% de son inertie et s’oppose à Hamon, Poutou et Mélenchon. Macron et Le Pen sont insignifiants sur cet axe. Cet axe représente l’opposition classique droite / gauche.</p></li>
<li><p>Le Pen est placée assez loin de l’origine sur la représentation graphique:  l’inertie de la modalité Le Pen (54%) est bien plus importante que sa masse (21%). Dit autrement, les scores de Le Pen présentent une grande variabilité selon les régions, plus élevée que celle des scores de Macron (inertie 16% pour une masse de 24,3%) ou encore Mélenchon (inertie 5%, inertie 19,8%). Les électeurs de Le Pen, même s’ils sont plus nombreux que lors des scrutins précédents, restent inégalement répartis sur le territoire.</p></li>
<li><p>Enfin, la première source de variation dans les votes est une opposition Le Pen / Macron, indépendante des oppositions  droite / gauche traditionnels.</p></li>
</ul>
</div>
</div>
<span id="document-acm"></span><div class="tex2jax_ignore mathjax_ignore section" id="analyse-des-correspondances-multiples">
<h2>Analyse des correspondances multiples<a class="headerlink" href="#analyse-des-correspondances-multiples" title="Permalink to this headline">#</a></h2>
<p id="index-0">Tandis que l’analyse factorielle des correspondances permet d’expliquer la liaison entre deux variables qualitatives, l’analyse des correspondances multiples (ACM) s’intéresse au cas où l’on dispose de <span class="math notranslate nohighlight">\(p\geq 2\)</span> variables. C’est l’équivalent de l’ACP pour les variables qualitatives.</p>
<div class="section" id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this headline">#</a></h3>
<p>On dispose d’un tableau de données <span class="math notranslate nohighlight">\(\mathbf{H}=(h_{i,j})\)</span> à <span class="math notranslate nohighlight">\(n\)</span> lignes et <span class="math notranslate nohighlight">\(p\)</span> colonnes, où <span class="math notranslate nohighlight">\(n\)</span> est le nombre d’individus, <span class="math notranslate nohighlight">\(p\)</span> le nombre de variables qualitatives mesurées et pour <span class="math notranslate nohighlight">\(i\in[\![1,n]\!],j\in[\![1,p]\!],h_{ij}\in\mathcal{M}_j\)</span>, <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span> étant l’ensemble des modalités de la j<span class="math notranslate nohighlight">\(^e\)</span> variable. Si <span class="math notranslate nohighlight">\(m_j\)</span> est le cardinal de <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>, alors <span class="math notranslate nohighlight">\(m=\sum_{k=1}^p m_k\)</span> est le nombre total de modalités.</p>
<div class="proof definition admonition" id="definition-0">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 43 </span> (Tableau disjonctif complet)</p>
<div class="definition-content section" id="proof-content">
<p>Le tableau disjonctif complet <span class="math notranslate nohighlight">\(\mathbf T\)</span> des données est un tableau <span class="math notranslate nohighlight">\(n\times m\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\((\forall i\in[\![1,n]\!],j\in[\![1, m]\!])\; \mathbf T_{ij} = \left \{ \begin{array}{cl} 1&amp;\textrm{si l'individu i possède la modalité j}\\0 &amp; \textrm{sinon}\end{array}\right .\)</span></p>
</div>
</div><span class="target" id="index-2"></span><p id="index-3">On déduit de ce tableau disjonctif le tableau de Burt correspondant, <span class="math notranslate nohighlight">\(\mathbf B=\mathbf T^T \mathbf T\)</span>, qui rassemble les croisements deux à  deux de toutes les variables, i.e tous les tableaux de contingence des variables deux à deux. Sur la diagonale de <span class="math notranslate nohighlight">\(\mathbf B\)</span> se trouvent les coefficients <span class="math notranslate nohighlight">\(B_{ii}=n_i\)</span>, donnant le nombre d’individus possédant la modalité <span class="math notranslate nohighlight">\(i\)</span>.  Les autres coefficients <span class="math notranslate nohighlight">\(B_{ij} = \mathbf{T_{\bullet i}}^T \mathbf {T_{\bullet j}}\)</span> quantifient le nombre d’individus ayant les modalités <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="section" id="analyse">
<h3>Analyse<a class="headerlink" href="#analyse" title="Permalink to this headline">#</a></h3>
</div>
<div class="section" id="tableau-de-contingence-de-l-acm">
<h3>Tableau de contingence de l’ACM<a class="headerlink" href="#tableau-de-contingence-de-l-acm" title="Permalink to this headline">#</a></h3>
<p>En analyse des correspondances multiples, on traite <span class="math notranslate nohighlight">\(\mathbf T\)</span> comme un tableau de contingence. Les totaux en ligne sont alors égaux au nombre de variables <span class="math notranslate nohighlight">\(p\)</span>, les totaux en colonne correspondent au nombre d’individus ayant la modalité correspondant à la colonne traitée. Pour une colonne <span class="math notranslate nohighlight">\(j\)</span>, on note ce total <span class="math notranslate nohighlight">\(n_j\)</span> Le total de tous les coefficients de <span class="math notranslate nohighlight">\(\mathbf T\)</span> vaut donc <span class="math notranslate nohighlight">\(np\)</span>.</p>
<p>Comme dans le cas de l’AFC, l’ACM considère les fréquences, les profils ligne et les profils colonne.</p>
<p>Pour les fréquences :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{ij}=T_{ij}/np\)</span> est la fréquence conjointe et vaut donc <span class="math notranslate nohighlight">\(1/np\)</span> si l’individu <span class="math notranslate nohighlight">\(i\)</span> possède la modalité <span class="math notranslate nohighlight">\(j\)</span> et 0 sinon. On range ces coefficients dans une matrice <span class="math notranslate nohighlight">\(\mathbf{F}\in\mathcal{M}_{nm}(\mathbb{R})\)</span></p></li>
<li><p>le poids des lignes est constant et vaut <span class="math notranslate nohighlight">\(1/n\)</span>. On note alors <span class="math notranslate nohighlight">\(\mathbf{a} = (\frac{1}{n}\cdots \frac{1}{n})^T\in\mathbb{R}^n\)</span> le vecteur des poids des individus.</p></li>
<li><p>le poids des colonnes vaut <span class="math notranslate nohighlight">\(n_j/np\)</span>, et est d’autant plus fort que la modalité <span class="math notranslate nohighlight">\(j\)</span> est fréquente. On note alors <span class="math notranslate nohighlight">\(\mathbf{b} = (\frac{n_1}{np}\cdots \frac{n_m}{np})^T\in\mathbb{R}^m\)</span> le vecteur des poids des modalités.</p></li>
</ul>
<p>Comme en analyse factorielle des correspondances, on note <span class="math notranslate nohighlight">\({\bf D_n}=diag\left ({\bf a}\right )\)</span> et <span class="math notranslate nohighlight">\({\bf D_m}=diag\left ({\bf b} \right )\)</span>.</p>
<p>Pour les profils ligne et colonne :</p>
<ul class="simple">
<li><p>on lit dans <span class="math notranslate nohighlight">\(\mathbf T\)</span> le i<span class="math notranslate nohighlight">\(^e\)</span> profil ligne, considéré comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, de composantes <span class="math notranslate nohighlight">\(T_{ij}/p,j\in[\![1,m]\!]\)</span>. Ces profils sont rangés dans une matrice <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{nm}(\mathbb{R})\)</span> et on a <span class="math notranslate nohighlight">\({\bf A}={\bf D_n^{-1}F}\)</span>.</p></li>
<li><p>on lit dans <span class="math notranslate nohighlight">\(\mathbf T\)</span> le j<span class="math notranslate nohighlight">\(^e\)</span> profil colonne, considéré comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, de composantes <span class="math notranslate nohighlight">\(T_{ij}/n_j,i\in[\![1,n]\!]\)</span>. Ces profils sont rangés dans <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{nm}(\mathbb{R})\)</span> et on a <span class="math notranslate nohighlight">\({\bf B}={\bf FD_m^{-1}}\)</span>.</p></li>
</ul>
<p>L’ACM considère, comme l’AFC, deux nuages de points centrés :</p>
<ol class="simple">
<li><p>le nuage des <span class="math notranslate nohighlight">\(n\)</span> individus dans <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, i.e. les <span class="math notranslate nohighlight">\(n\)</span> lignes de la matrice <span class="math notranslate nohighlight">\({\bf D_n^{-1}(F-ab^T})\)</span>. Chaque individu est pondéré par <span class="math notranslate nohighlight">\(1/n\)</span></p></li>
<li><p>le nuage des <span class="math notranslate nohighlight">\(m\)</span> modalités dans <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, i.e. les <span class="math notranslate nohighlight">\(n\)</span> lignes de la matrice <span class="math notranslate nohighlight">\({\bf (F-ab^T)D_m^{-1}}\)</span>. Chaque modalité <span class="math notranslate nohighlight">\(j\)</span> est pondérée par <span class="math notranslate nohighlight">\(n_j/np\)</span>.</p></li>
</ol>
</div>
<div class="section" id="distances-entre-individus-et-entre-modalites">
<h3>Distances entre individus et entre modalités<a class="headerlink" href="#distances-entre-individus-et-entre-modalites" title="Permalink to this headline">#</a></h3>
<p>En analyse des correspondances multiples, on utilise la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> et <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> :</p>
<ul class="simple">
<li><p>dans l’espace des individus, la métrique est <span class="math notranslate nohighlight">\(\mathbf {D_m}^{-1}\)</span> :</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\chi^2(i,i') = (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet})^T\mathbf {D_m}^{-1} (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet}) = \displaystyle\sum_{j=1}^m\frac{1}{f_{\bullet j}}\left (\frac{T_{ij}-T_{i'j}}{p} \right )^2 = \frac {n}{p}\displaystyle\sum_{j=1}^m\frac{1}{{n_j}}\left (T_{ij}-T_{i'j}\right )^2\)</span>
Deux individus sont proches s’ils possèdent les mêmes modalités, sachant que l’on donne plus de poids au fait que ces deux individus ont en commun une modalité rare (<span class="math notranslate nohighlight">\(n_s\)</span> petit).</p>
<ul class="simple">
<li><p>dans l’espace des modalités, la métrique est <span class="math notranslate nohighlight">\(\mathbf {D_n}^{-1}\)</span> :</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\chi^2(j,j') = (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'})^T\mathbf {D_n}^{-1} (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'}) = n\displaystyle\sum_{i=1}^n\left (\frac{T_{ij}}{n_j} -\frac{T_{ij'}}{n_{j'}}\right )^2\)</span>
et deux modalités sont proches si elles sont possédées par les mêmes individus.</p>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 24 </span></p>
<div class="remark-content section" id="proof-content">
<p>On a de plus  <span class="math notranslate nohighlight">\(\chi^2(j,\mathbf{a}) = \frac{n}{n_j}-1\)</span> et  <span class="math notranslate nohighlight">\(f_{\bullet j}\chi^2(j,\mathbf{a}) = \frac{1}{p}\left( 1-\frac{n_j}{n}\right )\)</span>. Donc la distance d’une modalité au centre du nuage est d’autant plus grande que la modalité est rare et la part de l’inertie totale, due à une modalité est d’autant plus grande que la modalité est rare.  On  évite donc en pratique de conserver dans l’analyse les modalités trop rares.</p>
<p>De même, puisque <span class="math notranslate nohighlight">\(\displaystyle\sum_{k\in \mathcal{M}_j}f_{\bullet k}\chi^2(k,\mathbf{a}) = \frac{1}{p}\left( m_j-1\right )\)</span>, la part de l’inertie totale, due à une variable  <span class="math notranslate nohighlight">\(j\)</span> est d’autant plus grande que le nombre de modalités  de cette variable est grand. Là aussi, on  évite en pratique de conserver dans l’analyse des variables ayant des nombres de modalités trop différents.</p>
</div>
</div></div>
<div class="section" id="principe-de-l-acm">
<h3>Principe de l’ACM<a class="headerlink" href="#principe-de-l-acm" title="Permalink to this headline">#</a></h3>
<p>L’analyse en composantes multiples consiste alors à appliquer l’analyse factorielle des correspondances  du tableau des  contingences <span class="math notranslate nohighlight">\(\mathbf T\)</span>, c’est-à-dire effectuer une ACP pondérée des nuages des point-individus et des point-modalités .</p>
<p>Une différence notable vient cependant de l’interprétation de l’inertie de ces nuages de points  individus (<span class="math notranslate nohighlight">\(I(\mathbf{A})\)</span>) et modalités (<span class="math notranslate nohighlight">\(I(\mathbf{B})\)</span>). En AFC, on pouvait interpréter statistiquement cette inertie en terme de <span class="math notranslate nohighlight">\(\chi^2/n\)</span> mesurant l’indépendance entre les deux variables qualitatives. Ici, ce n’est plus le cas puisque l’on peut montrer que <span class="math notranslate nohighlight">\(I(\mathbf{A}) = I(\mathbf{B})= m/p-1\)</span>. L’inertie dépend donc du nombre moyen <span class="math notranslate nohighlight">\(m/p\)</span> de catégories par variable.</p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 25 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les anglo-saxons considère que l’ACM consiste à effectuer l’analyse factorielle des correspondances  du tableau de Burt <span class="math notranslate nohighlight">\(\mathbf T^T \mathbf T\)</span>, matrice symétrique de taille <span class="math notranslate nohighlight">\(m\)</span>. Les profils ligne et colonne sont alors identiques et correspondent aux modalités que l’on veut analyser. On ne peut donc pas effectuer d’analyse des individus.</p>
</div>
</div></div>
<div class="section" id="interpretation-des-resultats">
<h3>Interprétation des résultats<a class="headerlink" href="#interpretation-des-resultats" title="Permalink to this headline">#</a></h3>
</div>
<div class="section" id="inertie-expliquee">
<h3>Inertie expliquée<a class="headerlink" href="#inertie-expliquee" title="Permalink to this headline">#</a></h3>
<p>L’inertie totale, égale comme nous l’avons vu à <span class="math notranslate nohighlight">\(m/p-1\)</span> se calcule  également comme la somme des valeurs propres <span class="math notranslate nohighlight">\(\lambda_1+\cdots +\lambda_r\)</span>, où <span class="math notranslate nohighlight">\(r=min(n-1,m-p)\)</span> est le nombre de valeurs propres non nulles issues de l’ACP. La part d’inertie expliquée par l’axe <span class="math notranslate nohighlight">\(z\)</span> est alors <span class="math notranslate nohighlight">\(\lambda_z/(\lambda_1+\cdots +\lambda_r)\)</span>. En revanche, point important, le nombre d’axes retenus pour l’interprétation ou le recodage ne peut pas être choisi à partir de ces pourcentages d’inertie expliquées, contrairement à l’ACP.</p>
</div>
<div class="section" id="contributions-et-representation">
<h3>Contributions et représentation<a class="headerlink" href="#contributions-et-representation" title="Permalink to this headline">#</a></h3>
<p>En reprenant les résultats de l’AFC, on montre que :</p>
<ul class="simple">
<li><p>les individus les plus excentrés sur les plans factoriels sont ceux qui contribuent le plus</p></li>
<li><p>les modalités les plus excentrées ne sont pas nécessairement celles qui
contribuent le plus. En effet, leur contribution dépend de leur fréquence.</p></li>
<li><p>la contribution d’une variable qualitative <span class="math notranslate nohighlight">\(j\)</span> à un axe <span class="math notranslate nohighlight">\(z\)</span> donne une idée de la liaison entre cette variable et la composante principale correspondant à <span class="math notranslate nohighlight">\(z\)</span></p></li>
<li><p>une représentation graphique consiste alors à représenter les variables qualitatives sur un plan factoriel <span class="math notranslate nohighlight">\((z,z')\)</span> : on propose en abscisses (respectivement ordonnées) les contributions des variables à l’axe <span class="math notranslate nohighlight">\(z\)</span> (resp. <span class="math notranslate nohighlight">\(z'\)</span>)</p></li>
<li><p>on évalue la qualité de la représentation de la même manière qu’en ACP, à l’aide des cosinus carrés. Si deux individus sont bien projetés alors s’ils sont proches en projections, ils sont effectivement proches dans leur espace d’origine et on peut alors interpréter leur proximité :
deux individus se ressemblent (au sens de la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span>) s’ils ont choisis les mêmes modalités et ; deux modalités se ressemblent (en terme de <span class="math notranslate nohighlight">\(\chi^2\)</span>) si elles sont possédées par les mêmes individus.</p></li>
</ul>
</div>
<div class="section" id="cas-particulier-p-2">
<h3>Cas particulier <span class="math notranslate nohighlight">\(p\)</span>=2<a class="headerlink" href="#cas-particulier-p-2" title="Permalink to this headline">#</a></h3>
<p>Dans le cas <span class="math notranslate nohighlight">\(p=2\)</span>, on observe <span class="math notranslate nohighlight">\(2\)</span> variables ayant respectivement <span class="math notranslate nohighlight">\(m_1\)</span> et <span class="math notranslate nohighlight">\(m_2\)</span> modalités. On se retrouve donc dans le cas où l’AFC s’applique et on peut :</p>
<ul class="simple">
<li><p>soit analyser le tableau <span class="math notranslate nohighlight">\(\mathbf{T}\in\mathcal{M}_{n,m_1+m_2}(\mathbb{R})\)</span> par analyse en composantes multiples,</p></li>
<li><p>soit analyser le tableau de contingence <span class="math notranslate nohighlight">\(\mathbf{K}\in\mathcal{M}_{m_1,m_2}(\mathbb{R})\)</span> par AFC.</p></li>
</ul>
<p>Si on note <span class="math notranslate nohighlight">\(Sp(\mathbf{K}) = (\mu_i)\)</span> et <span class="math notranslate nohighlight">\(Sp(\mathbf{T}) = (\lambda_i)\)</span> alors on montre  que  <span class="math notranslate nohighlight">\(\mu_r = (2\lambda_r - 1)^2\)</span>
On en déduit qu’à chaque valeur  propre de l’AFC correspondent deux valeurs propres de l’ACM <span class="math notranslate nohighlight">\(\lambda_r = \frac{1\pm \sqrt{\mu_r}}{2}\)</span>, et une relation entre les coordonnées factorielles des deux analyses</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbf{c}_1 = \begin{pmatrix} \mathbf{x_K}\\\mathbf{y_K}\end{pmatrix}\ &amp;\textrm{pour}&amp; \lambda_r = \frac{1+ \sqrt{\mu_r}}{2}\\
\mathbf{c}_2 = \begin{pmatrix} \mathbf{x_K}\\-\mathbf{y_K}\end{pmatrix}\ &amp;\textrm{pour}&amp; \lambda_r = \frac{1- \sqrt{\mu_r}}{2}
\end{eqnarray*}\)</span></p>
<p>où <span class="math notranslate nohighlight">\(\mathbf{x_K},\mathbf{y_K}\)</span> sont les composantes principales des profils ligne et colonne de <span class="math notranslate nohighlight">\(K\)</span>.
De là viennent deux constats :</p>
<ul class="simple">
<li><p>dans l’analyse en composantes multiples de 2 variables, on ne retient que les valeurs propres supérieures strictement à 1/2, les composantes correspondant  aux valeurs propres inférieures se déduisant facilement</p></li>
<li><p>Les pourcentages d’inertie expliqués par les axes en ACM sont souvent très faibles et
ne peuvent donc pas être interprétés comme en AFC et en ACP.</p></li>
</ul>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this headline">#</a></h3>
<p>On souhaite évaluer l’effet de l’espèce de chêne sur des vins rouges vieillis en barrique. Un même vin a été vieilli dans six barriques différentes fabriquées avec deux types de chêne : les vins <span class="math notranslate nohighlight">\(V_1\)</span>, <span class="math notranslate nohighlight">\(V_5\)</span> et <span class="math notranslate nohighlight">\(V_6\)</span> ont été élevés avec le premier type de chêne, tandis que les vins <span class="math notranslate nohighlight">\(V_2\)</span>, <span class="math notranslate nohighlight">\(V_3\)</span> et <span class="math notranslate nohighlight">\(V_4\)</span> ont été élevés avec le second. Trois experts <span class="math notranslate nohighlight">\(E_1,E_2,E_3\)</span> ont ensuite choisi entre deux et cinq variables pour décrire les vins. Pour chaque vin et pour chaque variable, l’expert évalue l’intensité, codée soit comme une réponse binaire (i.e. fruité vs. non fruité), soit comme une réponse ternaire (i.e. pas fruité, un peu fruité, très fruité). On code le tout par un tableau disjonctif complet :</p>
<p><img alt="" src="_images/tab0.png" /></p>
<p>L’objectif de l’étude est d’une part de proposer une typologie des vins et d’autre part de savoir s’il y a un accord entre les échelles utilisées par les experts.</p>
<p>La figure suivante présente le résultat de l’analyse spectrale en lignes. Les tableaux qui suivent donnent les coordonnées des individus (<span class="math notranslate nohighlight">\(S\)</span>), la qualité de leur représentation et leur contribution  (C<span class="math notranslate nohighlight">\(\times 10^3\)</span>) sur les trois premiers axes factoriels.</p>
<p><img alt="" src="_images/spectralACM.png" /></p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Vin</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_4\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_5\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_6\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(S\)</span></p></td>
<td><p>1</p></td>
<td><p>-0.951</p></td>
<td><p>0.787</p></td>
<td><p>1.018</p></td>
<td><p>0.951</p></td>
<td><p>-1.018</p></td>
<td><p>-0.787</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>-0.316</p></td>
<td><p>0.632</p></td>
<td><p>-0.316</p></td>
<td><p>-0.316</p></td>
<td><p>-0.316</p></td>
<td><p>0.632</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>3</p></td>
<td><p>0.43</p></td>
<td><p>0.387</p></td>
<td><p>0.103</p></td>
<td><p>-0.43</p></td>
<td><p>-0.103</p></td>
<td><p>-0.387</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(cos^2\)</span></p></td>
<td><p>1</p></td>
<td><p>0.754</p></td>
<td><p>0.516</p></td>
<td><p>0.863</p></td>
<td><p>0.754</p></td>
<td><p>0.863</p></td>
<td><p>0.516</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>2</p></td>
<td><p>0.083</p></td>
<td><p>0.333</p></td>
<td><p>0.083</p></td>
<td><p>0.083</p></td>
<td><p>0.083</p></td>
<td><p>0.333</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>3</p></td>
<td><p>0.154</p></td>
<td><p>0.125</p></td>
<td><p>0.009</p></td>
<td><p>0.154</p></td>
<td><p>0.009</p></td>
<td><p>0.125</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(C\)</span></p></td>
<td><p>1</p></td>
<td><p>176.68</p></td>
<td><p>120.986</p></td>
<td><p>202.334</p></td>
<td><p>176.68</p></td>
<td><p>202.334</p></td>
<td><p>120.986</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>83.333</p></td>
<td><p>333.333</p></td>
<td><p>83.333</p></td>
<td><p>83.333</p></td>
<td><p>83.333</p></td>
<td><p>333.333</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>3</p></td>
<td><p>267.821</p></td>
<td><p>216.945</p></td>
<td><p>15.234</p></td>
<td><p>267.821</p></td>
<td><p>15.234</p></td>
<td><p>216.945</p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/tab.png" /></p>
<p>On peut alors projeter les individus lignes ou colonnes sur le premier plan factoriel.</p>
<p><img alt="" src="_images/plan12lig.png" /><img alt="" src="_images/plan12col.png" /></p>
<p>On peut alors par exemple utiliser le type de chêne (MONCHENE) comme une variable supplémentaire (ou illustrative) à projeter sur l’analyse après coup. On peut également projeter après analyse un nouveau vin (MONVIN, donc une observation supplémentaire), testé par les experts. Lorsque ces derniers n’étaient pas sûrs de la façon d’utiliser un descripteur, un modèle de réponse (1/2, 1/2) est utilisé pour représenter la réponse.</p>
<p><img alt="" src="_images/plan12lig2.png" /><img alt="" src="_images/plan12col2.png" /></p>
</div>
</div>
<span id="document-genindex"></span><div class="tex2jax_ignore mathjax_ignore section" id="index">
<h2>Index<a class="headerlink" href="#index" title="Permalink to this headline">#</a></h2>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Vincent BARRA<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>