

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Analyse de donn√©es</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="#" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="#">
  
  
  
  
    
    
      
    
    
    <img src="_static/isimainp.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/isimainp.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cours</span></p>
<ul class="nav bd-sidenav">







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Annexes</span></p>
<ul class="nav bd-sidenav">



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cours</span></p>
<ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-Rappels">Rappels de probabilit√©</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-elemstats">Elements de statistiques</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-statsdescriptives">Statistique descriptive</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-selection">S√©lection de variables</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-acp">Analyse en composantes principales</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-regression">R√©gression</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-clustering">Quelques m√©thodes de classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Annexes</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-afc">Analyse Factorielle des correspondances</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-acm">Analyse des correspondances multiples</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-genindex">Index</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h1>
<div class="section" id="elements-de-vocabulaire">
<h2>El√©ments de vocabulaire<a class="headerlink" href="#elements-de-vocabulaire" title="Permalink to this heading">#</a></h2>
<p>On d√©finit ici de mani√®re informelle les termes utilis√©s dans la suite :</p>
<ul class="simple" id="index-0">
<li><p><strong>Population</strong> : ensemble de cardinalit√© finie, not√©e <span class="math notranslate nohighlight">\(N\)</span>, ou infinie ;</p></li>
</ul>
<ul class="simple" id="index-1">
<li><p><strong>Echantillon</strong> : sous-ensemble de la population, de cardinalit√© <span class="math notranslate nohighlight">\(n\)</span> ;</p></li>
</ul>
<ul class="simple" id="index-2">
<li><p><strong>Individu</strong> : sous-ensemble de la population ou de l‚Äô√©chantillon, de cardinalit√© 1 ;</p></li>
</ul>
<ul class="simple" id="index-3">
<li><p><strong>Caract√®re</strong> : nature de la caract√©ristique √† laquelle on s‚Äôint√©resse statistiquement. Il peut √™tre <strong>qualitatif</strong> (nominal ou ordinal) ou <strong>quantitatif</strong> (discret ou continu).</p></li>
</ul>
</div>
<div class="section" id="probabilites-et-statistiques">
<h2>Probabilit√©s et statistiques<a class="headerlink" href="#probabilites-et-statistiques" title="Permalink to this heading">#</a></h2>
<p>La question qui se pose est la suivante : comment d√©finir ou estimer la valeur de probabilit√© associ√©e √† un  √©v√®nement ?
Plusieurs points de vue ont √©t√© propos√©s et adopt√©s que nous synth√©tisons tr√®s bri√®vement.</p>
<div class="section" id="approche-frequentiste">
<h3>Approche fr√©quentiste<a class="headerlink" href="#approche-frequentiste" title="Permalink to this heading">#</a></h3>
<p>Ce point de vue historique, souvent pr√©sent√© comme le plus ‚Äúnaturel‚Äù ou ‚Äúobjectif‚Äù, consiste √† d√©finir une probabilit√© comme la limite de la fr√©quence d‚Äôobservation de la caract√©ristique lorsque la taille de l‚Äôechantillon devient infinie. On suppose ici que les probabilit√©s sont une loi de la nature qu‚Äôil faut mesurer par l‚Äôexp√©rience. En pratique, la probabilit√© d‚Äôun  √©v√®nement est donc estim√©e/approxim√©e en r√©p√©tant un tr√®s grand nombre de fois l‚Äôexp√©rience dans les m√™mes conditions. C‚Äôest de ces exp√©riences r√©p√©t√©es que sont n√©s les outils de la statistique tels que la r√©gression lin√©aire ou le test du <span class="math notranslate nohighlight">\(\chi^2\)</span>.
On rencontre n√©anmoins tr√®s rapidement des limitations avec ce type d‚Äôapproche. D‚Äôune part, il est impossible d‚Äôun point de vue fr√©quentiste de traiter de petits √©chantillons de donn√©es. De plus, certains types de donn√©es ne sont
pas exploitables en raison de leur caract√®re non exp√©rimental (par exemple, quelle probabilit√© associer √† un √©v√®nement du type ‚Äúnombre de votants aux prochaines √©lections‚Äù qui n‚Äôest pas r√©p√©table). Enfin, il est parfois difficile de d√©finir un mod√®le  permettant de mod√©liser une erreur de mesure ou la variation observ√©e d‚Äôun caract√®re dans une population.</p>
</div>
<div class="section" id="approche-bayesienne">
<h3>Approche bay√©sienne<a class="headerlink" href="#approche-bayesienne" title="Permalink to this heading">#</a></h3>
<div class="margin sidebar">
<p class="sidebar-title">T. Bayes</p>
<p><img alt="" src="_images/bayes.jpeg" /></p>
</div>
<p>Un point de vue bien diff√©rent consiste √† d√©finir les probabilit√©s comme une mesure subjective de l‚Äôincertitude. Dans ce cadre, tout √©v√©nement peut √™tre probabilis√© √† partir d‚Äôun a priori de l‚Äôobservateur. Ce point de vue est appel√© Bay√©sien (fait appel √† la r√®gle de Bayes) pour calculer la loi de probabilit√© des √©v√®nements √† partir des √©chantillons de donn√©es a posteriori. L‚Äôint√©r√™t majeur de ce type de d√©marche est que tout est probabilisable (jusqu‚Äôaux param√®tres du mod√®le utilis√©) et qu‚Äôil s‚Äôappuie sur des r√©sultats de la th√©orie des probabilit√©s, comme le th√©or√®me central limite.
Ce point de vue ‚Äúsubjectif‚Äù a longtemps √©t√© d√©nonc√© par les adeptes de l‚Äôapproche fr√©quentiste qui rejettent l‚Äôid√©e que l‚Äôon puisse d√©finir un tel a priori sur les √©v√®nements. En effet, l‚Äôobjection majeure que l‚Äôon oppose souvent √† la m√©thodologie bay√©sienne est que deux observateurs diff√©rents, ayant des a priori diff√©rents, donneront des r√©sultats ou des interpr√©tations diff√©rentes.</p>
</div>
</div>
<div class="section" id="demarche-generale">
<h2>D√©marche g√©n√©rale<a class="headerlink" href="#demarche-generale" title="Permalink to this heading">#</a></h2>
<p>De mani√®re assez g√©n√©rale, une √©tude statistique consiste √† obtenir des informations sur un caract√®re concernant une population de grande taille en s‚Äôappuyant sur celles d‚Äôun sous-ensemble de taille r√©duite (l‚Äô√©chantillon), afin le plus souvent d‚Äôorienter une d√©cision. Le choix de l‚Äô√©chantillon se fera par tirage avec ou sans remise, par tirage uniforme (‚Äúau hasard‚Äù) ou non (tirage stratifi√© dans le cas d‚Äôun sondage par exemple, ou selon une loi de probabilit√© pr√©cise si une information a priori est disponible).
On estime alors des propri√©t√©s d‚Äôun caract√®re de l‚Äô√©chantillon. A partir de ces estimations, on cherche √† donner ‚Äúau mieux‚Äù des valeurs aux param√®tres correspondant de la population (la moyenne, la variance,‚Ä¶). L‚Äôestimation pourra √™tre ponctuelle ou par intervalle de confiance. On pourra √©galement s‚Äôint√©resser √† des tests d‚Äôajustement (ou d‚Äôad√©quation) √† une loi de type donn√©. La d√©cision, √©tape finale de l‚Äôanalyse statistique, se fera par des tests statistiques.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 1 </span></p>
<div class="example-content section" id="proof-content">
<p>Un cuisinier cherche √† savoir si sa sauce est suffisamment sal√©e. Apr√®s avoir remu√© sa casserole, il pr√©l√®ve une cuill√©r√©e de sauce (l‚Äô√©chantillon). Il la go√ªte (il estime le caract√®re sal√© de l‚Äô√©chantillon). En fonction du r√©sultat, il d√©cide que la casserole de sauce (la population) est suffisamment sal√©e ou pas.</p>
</div>
</div><div class="admonition important">
<p class="admonition-title">Important</p>
<span class="target" id="index-4"></span><p id="index-5">La science des statistiques se d√©compose donc en :</p>
<ul class="simple">
<li><p>la <strong>statistique descriptive</strong>, dont l‚Äôobjectif est de d√©crire le caract√®re d‚Äôun √©chantillon en r√©sumant l‚Äôinformation qu‚Äôil contient ;</p></li>
<li><p>la <strong>statistique inf√©rentielle</strong>, dont l‚Äôobjectif est d‚Äôinf√©rer, √† partir de l‚Äôinformation recueillie sur l‚Äô√©chantillon, des propri√©t√©s valables sur la population, de mani√®re la plus fiable possible.</p></li>
</ul>
</div>
<p id="index-6">A cette dichotomie s‚Äôajoute la <strong>statistique exploratoire</strong>, branche de l‚Äôanalyse de donn√©es, qui cherche √† comprendre l‚Äôorganisation des individus de l‚Äô√©chantillon (existe-t-il des groupes d‚Äôindividus semblables ? les caract√®res mesur√©s sont-ils les plus pertinents ?‚Ä¶)</p>
<p>Nous n‚Äôabordons pas dans ce cours la statistique inf√©rentielle.</p>
</div>
<div class="toctree-wrapper compound">
<span id="document-Rappels"></span><div class="tex2jax_ignore mathjax_ignore section" id="rappels-de-probabilite">
<h2>Rappels de probabilit√©<a class="headerlink" href="#rappels-de-probabilite" title="Permalink to this heading">#</a></h2>
<div class="section" id="experience-aleatoire">
<h3>Exp√©rience al√©atoire<a class="headerlink" href="#experience-aleatoire" title="Permalink to this heading">#</a></h3>
<div class="section" id="definitions">
<h4>D√©finitions<a class="headerlink" href="#definitions" title="Permalink to this heading">#</a></h4>
<div class="proof definition admonition" id="expalea">
<p class="admonition-title"><span class="caption-number">Definition 1 </span> (Exp√©rience al√©atoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une <strong>exp√©rience al√©atoire</strong> est une exp√©rience dont on ne peut pr√©voir le r√©sultat a priori. R√©p√©t√©e dans des conditions identiques, elle peut donner lieu √† des r√©sultats diff√©rents.</p>
</div>
</div><div class="proof example admonition" id="example-1">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<div class="example-content section" id="proof-content">
<ul class="simple">
<li><p>Le lanc√© de d√©</p></li>
<li><p>Les c√¥tes exactes d‚Äôune pi√®ce fabriqu√©e dans un atelier</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="issue">
<p class="admonition-title"><span class="caption-number">Definition 2 </span> (Issue)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle <strong>issue</strong> d‚Äôune exp√©rience al√©atoire l‚Äôun des r√©sultats possibles de cette exp√©rience</p>
</div>
</div><div class="proof definition admonition" id="univers">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (Univers des possibles)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle <strong>univers des possibles</strong> d‚Äôune exp√©rience al√©atoire l‚Äôensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> des issues de cette exp√©rience.</p>
</div>
</div><div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 3 </span></p>
<div class="example-content section" id="proof-content">
<p>Lorsque l‚Äôon joue √† pile ou face avec une pi√®ce de monnaie, l‚Äôexp√©rience a deux issues possibles et <span class="math notranslate nohighlight">\(\Omega = \{P,F\}\)</span>.</p>
</div>
</div><p>L‚Äôunivers des possibles n‚Äôest pas d√©fini de mani√®re unique, mais d√©pend de l‚Äôusage de l‚Äôexperience. Par exemple, pour le lancer de deux d√©s, on peut √™tre int√©ress√© par :</p>
<ul class="simple">
<li><p>le r√©sultat du lancer, dans ce cas <span class="math notranslate nohighlight">\(\Omega = \{(1,1), (1,2), \cdots (6,6)\}\)</span></p></li>
<li><p>la somme des deux faces et <span class="math notranslate nohighlight">\(\Omega = [\![2,12]\!]\)</span></p></li>
</ul>
<div class="proof definition admonition" id="evenement">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 4 </span> (Ev√®nement)</p>
<div class="definition-content section" id="proof-content">
<p>Etant donn√©e une exp√©rience al√©atoire, un <strong>√©v√®nement</strong> est une assertion vraie ou fausse suivant l‚Äôissue de l‚Äôexp√©rience. C‚Äôest donc un sous-ensemble <span class="math notranslate nohighlight">\(E\)</span> de <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
</div>
</div><div class="proof example admonition" id="example-6">
<p class="admonition-title"><span class="caption-number">Example 4 </span></p>
<div class="example-content section" id="proof-content">
<ul class="simple">
<li><p>Dans l‚Äôexp√©rience du lancer de deux d√©s, on peut s‚Äôint√©resser √† l‚Äô√©v√®nement ‚Äúla somme des deux faces est paire‚Äù ou encore  ‚Äúla somme est sup√©rieure √† 7‚Äù.</p></li>
<li><p>Si l‚Äôexp√©rience consid√©r√©e concerne les jobs effectu√©s sur une machine on peut consid√©rer :</p></li>
</ul>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\Omega=\mathbb{N}\)</span> et l‚Äô√©v√®nement ‚Äúle nombre de jobs ne d√©passe pas 10‚Äù : <span class="math notranslate nohighlight">\(E=[\![0,10]\!]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Omega=\mathbb{R}^*\)</span> et  l‚Äô√©v√®nement ‚Äúle job dure plus de 15 s‚Äù et <span class="math notranslate nohighlight">\(E=]15,+\infty[\)</span></p></li>
</ol>
</div>
</div><p>Il existe certains √©v√®nements particuliers :</p>
<ul class="simple">
<li><p>l‚Äô√©v√®nement dit certain : c‚Äôest l‚Äôunivers des possibles (par exemple ‚Äúla somme des deux faces d‚Äôun lancer de deux d√©s est inf√©rieure ou √©gale √† 12‚Äù)</p></li>
<li><p>l‚Äô√©v√®nement impossible (‚Äúla somme des deux faces d‚Äôun lancer de deux d√©s est sup√©rieure ou √©gale √† 20‚Äù)</p></li>
<li><p>l‚Äô√©v√®nement simple : tout singleton de <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p>l‚Äô√©v√®nement compos√© : tout sous-ensemble de <span class="math notranslate nohighlight">\(\Omega\)</span> de cardinalit√© au moins √©gale √† 2.</p></li>
</ul>
</div>
<div class="section" id="notation-et-operations-sur-les-evenements">
<h4>Notation et op√©rations sur les √©v√®nements<a class="headerlink" href="#notation-et-operations-sur-les-evenements" title="Permalink to this heading">#</a></h4>
<p>Les √©v√®nements peuvent √™tre interpr√©t√©s soit d‚Äôun point de vue ensembliste (Diagrammes de Venn), soit de mani√®re √©quivalente d‚Äôun point de vue probabiliste.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Notation</strong></p></th>
<th class="head"><p><strong>Interpr√©tation probabiliste</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\omega\)</span></p></td>
<td><p>issue possible, √©v√®nement √©l√©mentaire</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\)</span></p></td>
<td><p>√©v√®nement</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\omega\in A\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\omega\)</span> r√©alise <span class="math notranslate nohighlight">\(A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\subset B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> implique <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\cup B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> ou <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cap B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\bar A\)</span></p></td>
<td><p>contraire de <span class="math notranslate nohighlight">\(A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\emptyset\)</span></p></td>
<td><p>√©v√®nement impossible</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\Omega\)</span></p></td>
<td><p>√©v√®nement certain</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cap B=\emptyset\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> incompatibles</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\setminus B = A\cap \bar B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span> et pas <span class="math notranslate nohighlight">\(B\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="probabilites">
<h3>Probabilit√©s<a class="headerlink" href="#probabilites" title="Permalink to this heading">#</a></h3>
<div class="section" id="objectif">
<h4>Objectif<a class="headerlink" href="#objectif" title="Permalink to this heading">#</a></h4>
<p>L‚Äôobjectif des probabilit√©s est de donner une <strong>mesure</strong> √† la chance qu‚Äôa un √©v√®nement de se r√©aliser lors d‚Äôune exp√©rience al√©atoire. Pour ce faire, on d√©finit une fonction <span class="math notranslate nohighlight">\(P:\Omega\rightarrow [0,1]\)</span> v√©rifiant certains axiomes et propri√©t√©s.</p>
<div class="proof definition admonition" id="tribu">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 5 </span> (Tribu)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(T\)</span> une famille d‚Äô√©v√®nements. Pour que <span class="math notranslate nohighlight">\(T\)</span> soit probabilisable, il faut que :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\emptyset\in T, \Omega\in T\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A_i\)</span> est une suite dans <span class="math notranslate nohighlight">\(T\)</span> alors <span class="math notranslate nohighlight">\(\cup_iA_i\in T\)</span> et <span class="math notranslate nohighlight">\(\cap_iA_i\in T\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(A\in T\)</span> alors <span class="math notranslate nohighlight">\(\bar A\in T\)</span></p></li>
</ul>
<p><span class="math notranslate nohighlight">\(T\)</span> est une <strong>tribu</strong> et <span class="math notranslate nohighlight">\((\Omega,T)\)</span> est un <strong>espace probabilisable</strong>.</p>
</div>
</div><p>En pratique, on choisit souvent la tribu engendr√©e par une famille de <span class="math notranslate nohighlight">\(n\)</span> √©v√®nements <span class="math notranslate nohighlight">\(A_i\)</span>, qui est l‚Äôensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span> obtenues en effectuant l‚Äôunion de <span class="math notranslate nohighlight">\(k\)</span> √©v√®nements <span class="math notranslate nohighlight">\(A_i,i\in [\![1,n]\!]\)</span>.</p>
<div class="proof example admonition" id="example-8">
<p class="admonition-title"><span class="caption-number">Example 5 </span></p>
<div class="example-content section" id="proof-content">
<p>Dans le cas du lancer d‚Äôun d√©, <span class="math notranslate nohighlight">\(\Omega = \{1,2,3,4,5,6\}\)</span>, et :</p>
<ul class="simple">
<li><p>la tribu engendr√©e par la famille d‚Äô√©v√®nements <span class="math notranslate nohighlight">\(\{\{1,3,5\},\{2,4,6\}\}\)</span> est <span class="math notranslate nohighlight">\(\{\emptyset,\{1,3,5\},\{2,4,6\},\Omega\}\)</span>.</p></li>
<li><p>la tribu engendr√©e par la famille d‚Äô√©v√®nements <span class="math notranslate nohighlight">\(\{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}\)</span> est l‚Äôensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span>. Plus g√©n√©ralement, si <span class="math notranslate nohighlight">\(\Omega\)</span> est d√©nombrable, cette tribu est appel√©e <strong>tribu discr√®te</strong>.</p></li>
</ul>
</div>
</div><p>On peut √©galement s‚Äôint√©resser, si <span class="math notranslate nohighlight">\(\Omega=\mathbb R\)</span>, √† la tribu engendr√©e par les ouverts de <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>, on parle alors de <strong>tribu bor√©lienne</strong>.</p>
</div>
<div class="section" id="probabilite">
<h4>Probabilit√©<a class="headerlink" href="#probabilite" title="Permalink to this heading">#</a></h4>
<div class="margin sidebar">
<p class="sidebar-title">A. Kolmogorov</p>
<p><img alt="" src="_images/kolmogorov.jpeg" /></p>
</div>
<div class="proof axiom admonition" id="axiomKolmo">
<span id="index-2"></span><p class="admonition-title"><span class="caption-number">Axiom 1 </span> (Axiomatique de Kolmogorov)</p>
<div class="axiom-content section" id="proof-content">
<p>On appelle <strong>probabilit√©</strong> sur <span class="math notranslate nohighlight">\((\Omega,T)\)</span> une application <span class="math notranslate nohighlight">\(P\)</span> de <span class="math notranslate nohighlight">\(T\)</span> dans [0,1] v√©rifiant :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall A\in T)\; P(A)\in[0,1]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\Omega)=1\)</span></p></li>
<li><p>Pour toute famille d√©nombrable <span class="math notranslate nohighlight">\((A_i)\)</span> d‚Äô√©v√®nements disjoints <span class="math notranslate nohighlight">\(P(\displaystyle\bigcup_i A_i) = \displaystyle\sum_iP(A_i)\)</span></p></li>
</ul>
</div>
</div><p><span class="math notranslate nohighlight">\((\Omega,T,P)\)</span> est un <strong>espace probabilis√©</strong>.</p>
<div class="proof property admonition" id="property-10">
<p class="admonition-title"><span class="caption-number">Property 1 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\emptyset)=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A)\; P(\bar A)=1-P(A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\; P(A\setminus B) = P(A)-P(A\bigcap B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\; P(A\bigcup B) = P(A)+P(B)-P(A\bigcap B)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall A,B)\)</span> si <span class="math notranslate nohighlight">\(A\subset B\)</span> alors <span class="math notranslate nohighlight">\(P(A)\leq P(B)\)</span></p></li>
<li><p>Pour toute famille d√©nombrable <span class="math notranslate nohighlight">\((A_i)\)</span> d‚Äô√©v√®nements quelconques <span class="math notranslate nohighlight">\(P(\displaystyle\bigcup_i A_i) \leq \displaystyle\sum_iP(A_i)\)</span></p></li>
</ul>
</div>
</div></div>
<div class="section" id="conditionnement">
<h4>Conditionnement<a class="headerlink" href="#conditionnement" title="Permalink to this heading">#</a></h4>
<p>Les probabilit√©s <strong>conditionnelles</strong> int√®grent une information suppl√©mentaire sous la forme de l‚Äôobservation de la r√©alisation d‚Äôun √©v√®nement donn√©.</p>
<div class="proof definition admonition" id="definition-11">
<span id="index-3"></span><p class="admonition-title"><span class="caption-number">Definition 6 </span> (Probabilit√© conditionnelle)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(B\)</span> un √©v√®nement de probabilit√© non nulle. On appelle <strong>probabilit√© conditionnelle</strong> de <span class="math notranslate nohighlight">\(A\)</span> sachant <span class="math notranslate nohighlight">\(B\)</span> le rapport</p>
<p><span class="math notranslate nohighlight">\(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\)</span></p>
</div>
</div><p><span class="math notranslate nohighlight">\(P(A\mid B)\)</span> repr√©sente la probabilit√© que <span class="math notranslate nohighlight">\(A\)</span> se r√©alise sachant que <span class="math notranslate nohighlight">\(B\)</span> est r√©alis√©.</p>
<p>Remarquons que l‚Äôon peut √©crire <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A\mid B)P(B) = P(B\mid A)P(A)\)</span>.</p>
<div class="proof example admonition" id="example-12">
<p class="admonition-title"><span class="caption-number">Example 6 </span></p>
<div class="example-content section" id="proof-content">
<p>7% des fran√ßais sont atteints d‚Äôun cancer du poumon. 70% des malades sont des fumeurs et 50% des fran√ßais fument. On recherche la probabilit√© d‚Äô√™tre atteint d‚Äôun cancer du poumon lorsque l‚Äôon est fumeur.
L‚Äô√©v√®nement <span class="math notranslate nohighlight">\(A\)</span> est ‚Äúavoir un cancer du poumon‚Äù, et <span class="math notranslate nohighlight">\(B\)</span> est ‚Äú√™tre fumeur‚Äù. D‚Äôapr√®s les donn√©es on a <span class="math notranslate nohighlight">\(P(A)\)</span>=0.07, <span class="math notranslate nohighlight">\(P(B)\)</span> = 0.5 et <span class="math notranslate nohighlight">\(P(B\mid A)\)</span> = 0.7.
On a alors <span class="math notranslate nohighlight">\(P(A\mid B) = \frac{P(A\bigcap B)}{P(B)}\)</span> avec <span class="math notranslate nohighlight">\(P(A\bigcap B)=P(B\mid A)P(A)\)</span> d‚Äôo√π</p>
<p><span class="math notranslate nohighlight">\(P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B}\)</span> = 0.098</p>
</div>
</div></div>
<div class="section" id="independance">
<h4>Ind√©pendance<a class="headerlink" href="#independance" title="Permalink to this heading">#</a></h4>
<div class="proof definition admonition" id="definition-13">
<span id="index-4"></span><p class="admonition-title"><span class="caption-number">Definition 7 </span> (Ind√©pendance)</p>
<div class="definition-content section" id="proof-content">
<p>Deux √©v√®nements <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> sont dits <strong>ind√©pendants</strong> si et seulement si <span class="math notranslate nohighlight">\(P(A\mid B) = P(A)\)</span>.</p>
</div>
</div><p>On a alors bien √©videmment <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A)P(B)\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-14">
<p class="admonition-title"><span class="caption-number">Remark 1 </span></p>
<div class="remark-content section" id="proof-content">
<p>La notion d‚Äôind√©pendance est directement rattach√©e √† <span class="math notranslate nohighlight">\(P\)</span> : <span class="math notranslate nohighlight">\(A\)</span> et <span class="math notranslate nohighlight">\(B\)</span> peuvent √™tre ind√©pendants pour une probabilit√© donn√©e, mais pas pour une autre.</p>
</div>
</div><p>On peut g√©n√©raliser la notion d‚Äôind√©pendance √† une famille d‚Äô√©v√®nements <span class="math notranslate nohighlight">\((A_i)_{i\in[\![1,n]\!]}\)</span> : on dira que les <span class="math notranslate nohighlight">\(A_i\)</span> sont <strong>mutuellement ind√©pendants</strong> si pour tout <span class="math notranslate nohighlight">\(I\subset [\![1,n]\!]\)</span></p>
<p><span class="math notranslate nohighlight">\(P\left (\displaystyle\bigcap_{i\in I} A_i\right ) = \displaystyle\prod_{i\in I} P(A_i)\)</span></p>
<p>L‚Äôind√©pendance mutuelle est plus forte que l‚Äôind√©pendance deux √† deux.</p>
<div class="proof remark dropdown admonition" id="remark-15">
<p class="admonition-title"><span class="caption-number">Remark 2 </span></p>
<div class="remark-content section" id="proof-content">
<p>La notion d‚Äôind√©pendance n‚Äôest pas une notion purement ensembliste. Deux √©v√®nements peuvent √™tre ind√©pendants pour une loi de probabilit√© et pas pour une autre.</p>
</div>
</div></div>
<div class="section" id="theoreme-des-probabilites-totales">
<h4>Th√©or√®me des probabilit√©s totales<a class="headerlink" href="#theoreme-des-probabilites-totales" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-16">
<p class="admonition-title"><span class="caption-number">Theorem 1 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(\{B_i\}\)</span> un syst√®me complet d‚Äô√©v√®nements (qui forment donc une partition de <span class="math notranslate nohighlight">\(\Omega\)</span>). Pour tout √©v√®nement <span class="math notranslate nohighlight">\(A\)</span>, on peut √©crire</p>
<p><span class="math notranslate nohighlight">\(P(A) = \displaystyle\sum_i P(A\bigcap B_i) = \displaystyle\sum_i P(A| B_i) P(B_i)\)</span></p>
</div>
</div></div>
<div class="section" id="regle-de-bayes">
<h4>R√®gle de Bayes<a class="headerlink" href="#regle-de-bayes" title="Permalink to this heading">#</a></h4>
<p id="index-5">A partir de l‚Äô√©galit√© <span class="math notranslate nohighlight">\(P(A\bigcap B) = P(A|B)P(B)=P(B|A)P(A)\)</span>, on d√©finit la r√®gle de Bayes</p>
<p><span class="math notranslate nohighlight">\((\forall A,B)\quad P(B|A)=\frac{P(A|B)P(B)}{P(A)}\)</span></p>
<p>Si <span class="math notranslate nohighlight">\(B_i\)</span> est un syst√®me complet d‚Äô√©v√®nements, on a de plus</p>
<p><span class="math notranslate nohighlight">\(P(B_i|A)= \frac{P(A|B_i)P(B_i)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\displaystyle\sum_k P(A|B_k)P(B_k)}\)</span></p>
<div class="proof example admonition" id="example-17">
<p class="admonition-title"><span class="caption-number">Example 7 </span></p>
<div class="example-content section" id="proof-content">
<p>Un fabricant de boulons a trois usines de fabrication situ√©es √† Amiens, Besan√ßon et Clermont-Ferrand. Amiens fournit 25% de la production, Besan√ßon 20% et Clermont-Ferrand 55%. Les boulons de 5mm repr√©sentent 20% des boulons produits √† Amiens, 30% √† Besan√ßon et 15% √† Clermont-Ferrand. On r√©pond √† la question suivante : sachant que le boulon achet√© a une taille de 5mm, quelle est la probabilit√© qu‚Äôil soit produit √† Clermont-Ferrand ?</p>
<p>On note <span class="math notranslate nohighlight">\(B_1\)</span> (respectivement <span class="math notranslate nohighlight">\(B_2,B_3\)</span>) l‚Äô√©v√®nement ‚ÄúLe boulon est produit √† Amiens (resp. Besan√ßon, Clermont-Ferrand)‚Äù. On note √©galement <span class="math notranslate nohighlight">\(A\)</span> l‚Äô√©v√®nement ‚ÄúLe boulon fait 5mm‚Äù. On cherche donc</p>
<p><span class="math notranslate nohighlight">\(P(B_3|A) = \frac{P(A|B_3)P(B_3)}{P(A)}= \frac{0.15*0.55}{0.1925}=0.428\)</span>.</p>
</div>
</div><p>On a calcul√© dans l‚Äôexemple une <strong>probabilit√© a posteriori</strong>, c‚Äôest √† dire sachant une information suppl√©mentaire (le boulon fait 5mm). La prise en compte de cette information modifie la valeur de la probabilit√© associ√©e √† <span class="math notranslate nohighlight">\(B_3\)</span>. La th√©orie des probabilit√©s au travers de l‚Äôapproche bay√©sienne est adapt√©e pour prendre en compte toute information nouvelle.</p>
</div>
</div>
<div class="section" id="variable-aleatoire">
<h3>Variable al√©atoire<a class="headerlink" href="#variable-aleatoire" title="Permalink to this heading">#</a></h3>
<div class="section" id="concept-de-variable-aleatoire">
<h4>Concept de variable al√©atoire<a class="headerlink" href="#concept-de-variable-aleatoire" title="Permalink to this heading">#</a></h4>
<p>Soit un espace probabilis√© <span class="math notranslate nohighlight">\((\Omega, T,P)\)</span>.</p>
<div class="proof definition admonition" id="definition-18">
<span id="index-6"></span><p class="admonition-title"><span class="caption-number">Definition 8 </span> (Variable al√©atoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable al√©atoire est une application <span class="math notranslate nohighlight">\(X:\Omega\rightarrow E\)</span> (on prendra <span class="math notranslate nohighlight">\(E=\mathbb R\)</span>)</p>
</div>
</div><p>Pour obtenir la probabilit√© d‚Äôune valeur quelconque image par <span class="math notranslate nohighlight">\(X\)</span>, il suffit de d√©nombrer les <span class="math notranslate nohighlight">\(\omega\)</span> qui r√©alisent cette valeur.</p>
<div class="proof example admonition" id="example-19">
<p class="admonition-title"><span class="caption-number">Example 8 </span></p>
<div class="example-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(\Omega\)</span> = (Pile,Face), on consid√®re la loi de probabilit√© <span class="math notranslate nohighlight">\(P\)</span> telle que : <span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; P(\omega)=\frac12\)</span>.
<span class="math notranslate nohighlight">\(P(X=1)= P(\{Pile\}) = \frac12\)</span>.</p>
</div>
</div><p>On dit que l‚Äôon transporte la loi de probabilit√© de <span class="math notranslate nohighlight">\(\Omega\)</span> sur <span class="math notranslate nohighlight">\(E\)</span> par l‚Äôapplication <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Les √©l√©ments de <span class="math notranslate nohighlight">\(E\)</span> sont les <strong>r√©alisations</strong> de la variable al√©atoire.</p>
<div class="proof example admonition" id="example-20">
<p class="admonition-title"><span class="caption-number">Example 9 </span></p>
<div class="example-content section" id="proof-content">
<p>Si l‚Äôexp√©rience consiste √† observer le r√©sultat du tirage de deux d√©s √† 6 faces, <span class="math notranslate nohighlight">\(\Omega = \{(1,1), (1,2), \cdots (6,5), (6,6)\}\)</span>, on consid√®re la loi de probabilit√© telle que <span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; P(\omega)=\frac{1}{36}\)</span>.</p>
<p>Si l‚Äôapplication <span class="math notranslate nohighlight">\(X\)</span> r√©alise la somme des deux √©l√©ments de <span class="math notranslate nohighlight">\(\omega\in\Omega\)</span>, alors on a par exemple <span class="math notranslate nohighlight">\(P(X=3)= P(\{(1,2),(2,1)\}) = \frac{2}{36}\)</span>, ou encore <span class="math notranslate nohighlight">\(P(X=5)= P(\{(1,4),(2,3),(3,2),(4,1)\}) = \frac{4}{36}\)</span>.</p>
</div>
</div></div>
<div class="section" id="variable-aleatoire-mesurable">
<h4>Variable al√©atoire mesurable<a class="headerlink" href="#variable-aleatoire-mesurable" title="Permalink to this heading">#</a></h4>
<p>On d√©finit sur <span class="math notranslate nohighlight">\(E\)</span> une tribu <span class="math notranslate nohighlight">\(T'\)</span>.  <span class="math notranslate nohighlight">\((E,T')\)</span> est alors un espace probabilisable, et tout √©l√©ment <span class="math notranslate nohighlight">\(B\)</span> de <span class="math notranslate nohighlight">\(T'\)</span> est un √©v√®nement. On note alors <span class="math notranslate nohighlight">\(X^{-1}(B) = \{\omega\in\Omega,\; X(\omega)\in B\}\)</span></p>
<div class="proof definition admonition" id="definition-21">
<span id="index-7"></span><p class="admonition-title"><span class="caption-number">Definition 9 </span> (Variable al√©atoire mesurable)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> est dite mesurable  si et seulement si : <span class="math notranslate nohighlight">\((\forall B\in T')\; X^{-1}(B)\in T\)</span></p>
</div>
</div><p>Dans les deux exemples pr√©c√©dents, on a par exemple <span class="math notranslate nohighlight">\(X^{-1}(1)= \{Pile\}\)</span> ou encore <span class="math notranslate nohighlight">\(X^{-1}(3) = \{(1,2),(2,1)\}\)</span> et <span class="math notranslate nohighlight">\(P(X=3)=P(X^{-1}(3)) = \frac{2}{36}\)</span>.</p>
<span class="target" id="index-8"></span><p id="index-9">On note souvent <span class="math notranslate nohighlight">\(P_X(B) = P(X^{-1}(B))=P(\{\omega / X(\omega)\in B\})\)</span> et on l‚Äôappelle <strong>probabilit√© image</strong> de <span class="math notranslate nohighlight">\(P\)</span> par <span class="math notranslate nohighlight">\(X\)</span>. En calculant la probabilit√© de chaque r√©alisation de la variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span>, on peut en d√©duire la <strong>loi de probabilit√©</strong> (ou <strong>distribution</strong>) de <span class="math notranslate nohighlight">\(X\)</span>.</p>
<ul class="simple">
<li><p>Pour une variable al√©atoire discr√®te <span class="math notranslate nohighlight">\(X\)</span>, la loi de probabilit√© est donc <span class="math notranslate nohighlight">\(P_X(x_i)= P(X=x_i) = P(\{\omega / X(\omega)=x_i\})\)</span>. <span class="math notranslate nohighlight">\(P_X\)</span> est appel√©e <strong>masse ponctuelle</strong>.</p></li>
</ul>
<span class="target" id="index-10"></span><ul class="simple" id="index-11">
<li><p>Pour une variable al√©atoire continue <span class="math notranslate nohighlight">\(X\)</span>, la loi de probabilit√© est donc <span class="math notranslate nohighlight">\(f_X(x)dx = P(x\leq X\leq x+dx) = P(\{\omega /x\leq X(\omega)\leq x+dx\})\)</span>. <span class="math notranslate nohighlight">\(f_X\)</span> est appel√©e <strong>densit√© de probabilit√©</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">floor</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">tirage</span><span class="p">():</span>
    <span class="n">d1</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>   
    <span class="n">d2</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">d1</span><span class="o">+</span><span class="n">d2</span> <span class="o">-</span><span class="mi">1</span>          

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10000</span>                       
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>        
    <span class="n">f</span><span class="p">[</span><span class="n">tirage</span><span class="p">()</span> <span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="o">/</span><span class="n">n</span>                      

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loi de probabilit√© d&#39;une variable al√©atoire discr√®te&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2b40be8386aeb598154a1505ed1c5096ec0556bb45e947d1dc4f76ee5f9fec04.png" src="_images/2b40be8386aeb598154a1505ed1c5096ec0556bb45e947d1dc4f76ee5f9fec04.png" />
</div>
</div>
<div class="proof definition admonition" id="definition-22">
<span id="index-12"></span><p class="admonition-title"><span class="caption-number">Definition 10 </span> (Fonction de r√©partition)</p>
<div class="definition-content section" id="proof-content">
<p>La fonction de r√©partition d‚Äôune variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> est l‚Äôapplication <span class="math notranslate nohighlight">\(F_X\)</span> de <span class="math notranslate nohighlight">\(\mathbb R\)</span> dans [0,1] telle que <span class="math notranslate nohighlight">\(F_X(x) = P(X\leq x)\)</span>.</p>
</div>
</div><p><span class="math notranslate nohighlight">\(F_X\)</span> est donc monotone croissante, continue √† droite et on a en particulier :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(a\leq X\leq b) = F_X(b)-F_X(a)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(X&gt;x) = 1-P(X\leq x) = 1-F_X(x)\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">floor</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">tirage</span><span class="p">():</span>
    <span class="n">d1</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>   
    <span class="n">d2</span><span class="o">=</span><span class="n">floor</span><span class="p">(</span><span class="mi">6</span><span class="o">*</span><span class="n">random</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  
    <span class="k">return</span> <span class="n">d1</span><span class="o">+</span><span class="n">d2</span> <span class="o">-</span><span class="mi">1</span>          

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10000</span>                       
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>        
    <span class="n">f</span><span class="p">[</span><span class="n">tirage</span><span class="p">()</span> <span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="o">/</span><span class="n">n</span>     

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>          
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="p">)</span>   
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">fn</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">xmax</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
          <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymin</span><span class="o">=</span><span class="n">fn</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ymax</span><span class="o">=</span><span class="n">fn</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>
          <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f8cb77e0cb4ee1d410b20d28e1a812f534de338ae185b2af4a99c78ba7175763.png" src="_images/f8cb77e0cb4ee1d410b20d28e1a812f534de338ae185b2af4a99c78ba7175763.png" />
</div>
</div>
<p>La notion de variable al√©atoire est ainsi une formalisation de la notion de grandeur variant selon le r√©sultat d‚Äôune exp√©rience al√©atoire. On peut alors pr√©ciser et formaliser la d√©finition pr√©c√©dente.</p>
<div class="proof definition admonition" id="definition-23">
<p class="admonition-title"><span class="caption-number">Definition 11 </span> (Variable al√©atoire)</p>
<div class="definition-content section" id="proof-content">
<p>Une variable al√©atoire est une application mesurable <span class="math notranslate nohighlight">\(X:(\Omega,T,P) \rightarrow (E,T')\)</span></p>
</div>
</div><div class="proof remark dropdown admonition" id="remark-24">
<p class="admonition-title"><span class="caption-number">Remark 3 </span></p>
<div class="remark-content section" id="proof-content">
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb N\)</span>, on parle de variable al√©atoire (r√©elle) discr√®te</p></li>
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb R\)</span>, on parle de variable al√©atoire (r√©elle) continue. <span class="math notranslate nohighlight">\(T'\)</span> est alors la tribu <strong>bor√©lienne</strong></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(E=\mathbb N^n\)</span> ou <span class="math notranslate nohighlight">\(E=\mathbb R^n\)</span>, on parle de <strong>vecteur al√©atoire</strong> de dimension <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ul>
</div>
</div></div>
<div class="section" id="caracteristiques-des-variables-aleatoires">
<h4>Caract√©ristiques des variables al√©atoires<a class="headerlink" href="#caracteristiques-des-variables-aleatoires" title="Permalink to this heading">#</a></h4>
<p>Une loi de probabilit√© est caract√©ris√©e par un certain nombre de grandeurs :</p>
<ul class="simple">
<li><p>sa valeur centrale</p></li>
<li><p>sa dispersion</p></li>
<li><p>sa forme</p></li>
</ul>
<div class="section" id="esperance-mathematique-d-une-variable-aleatoire">
<h5>Esp√©rance math√©matique d‚Äôune variable al√©atoire<a class="headerlink" href="#esperance-mathematique-d-une-variable-aleatoire" title="Permalink to this heading">#</a></h5>
<div class="proof definition admonition" id="definition-25">
<span id="index-13"></span><p class="admonition-title"><span class="caption-number">Definition 12 </span> (Esp√©rance)</p>
<div class="definition-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(X\)</span> une variable al√©atoire. On d√©finit l‚Äôesp√©rance math√©matique de <span class="math notranslate nohighlight">\(X\)</span>, et on note <span class="math notranslate nohighlight">\(\mathbb E(X)\)</span> par :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X = \displaystyle\sum_{x_i} x_iP(X=x_i)= \displaystyle\sum_{x_i} x_i P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discr√®te et si la somme converge.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X =\int_x xdP(x) = \int_x x f_X(x) dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue et si l‚Äôint√©grale converge.</p></li>
</ul>
</div>
</div><p><span class="math notranslate nohighlight">\(\mathbb E(X)\)</span> est la moyenne arithm√©tique (√©galement not√©e <span class="math notranslate nohighlight">\(\mu_X\)</span>) des diff√©rentes valeurs prises par <span class="math notranslate nohighlight">\(X\)</span> pond√©r√©es par leur probabilit√©.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_discrete</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">rv_discrete</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Esp√©rance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">expect</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Esp√©rance :  23.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_continuous</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">3.5</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mf">5.5</span> 
<span class="k">class</span> <span class="nc">distribution_gen</span><span class="p">(</span><span class="n">rv_continuous</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Esp√©rance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">expect</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lb</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">ub</span><span class="o">=</span><span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Esp√©rance :  7.904816400226159
</pre></div>
</div>
</div>
</div>
<p>On dira que <span class="math notranslate nohighlight">\(X\)</span> est <strong>centr√©e</strong> si <span class="math notranslate nohighlight">\(\mathbb{E}(X)=0\)</span>.</p>
<div class="proof example admonition" id="example-26">
<p class="admonition-title"><span class="caption-number">Example 10 </span></p>
<div class="example-content section" id="proof-content">
<p>Pour l‚Äôexp√©rience d‚Äôun lancer de d√© √† 6 faces  : <span class="math notranslate nohighlight">\(\mathbb E(X) = \mu_X = \displaystyle\sum_{i=1}^6 i\frac16 = \frac72\)</span></p>
</div>
</div><div class="proof property admonition" id="property-27">
<p class="admonition-title"><span class="caption-number">Property 2 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(a)= a\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(aX)= a\mathbb{E}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}(X+a) = \mathbb{E}(X) +a\)</span></p></li>
</ul>
</div>
</div></div>
<div class="section" id="moment-d-une-fonction-d-une-variable-aleatoire">
<h5>Moment d‚Äôune fonction d‚Äôune variable al√©atoire<a class="headerlink" href="#moment-d-une-fonction-d-une-variable-aleatoire" title="Permalink to this heading">#</a></h5>
<p id="index-14">Soit <span class="math notranslate nohighlight">\(\phi\)</span> l‚Äôapplication qui associe √† toute variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> la variable al√©atoire <span class="math notranslate nohighlight">\(Y=\phi(X)\)</span>.</p>
<div class="proof definition admonition" id="definition-28">
<p class="admonition-title"><span class="caption-number">Definition 13 </span> (Moment)</p>
<div class="definition-content section" id="proof-content">
<p>Le moment  <span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)]\)</span> de la fonction <span class="math notranslate nohighlight">\(\phi\)</span> de la variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> est √©gal √†</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)] = \displaystyle\sum_{x_i} \phi(x_i)P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discr√®te</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\phi(X)] = \int_x \phi(x) f_X(x)dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="definition-29">
<span id="index-15"></span><p class="admonition-title"><span class="caption-number">Definition 14 </span> (Moment d‚Äôordre <span class="math notranslate nohighlight">\(k\)</span>)</p>
<div class="definition-content section" id="proof-content">
<p>Le moment d‚Äôordre <span class="math notranslate nohighlight">\(k\)</span> d‚Äôune variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> est √©gal √† :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(X^k) = \displaystyle\sum_{x_i} x_i^k P_X(x_i)\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est discr√®te</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbb{E}(X^k) = \int_x x^k f_X(x)dx\)</span> si <span class="math notranslate nohighlight">\(X\)</span> est continue</p></li>
</ul>
</div>
</div><p>Le moment d‚Äôordre <span class="math notranslate nohighlight">\(k\)</span> est donc un cas particulier avec <span class="math notranslate nohighlight">\(Y=X^k\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-30">
<p class="admonition-title"><span class="caption-number">Remark 4 </span></p>
<div class="remark-content section" id="proof-content">
<p>L‚Äôesp√©rance <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> est le moment d‚Äôordre 1.</p>
</div>
</div><div class="proof definition admonition" id="definition-31">
<span id="index-16"></span><p class="admonition-title"><span class="caption-number">Definition 15 </span> (Moment centr√© d‚Äôordre <span class="math notranslate nohighlight">\(k\)</span>)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle moment centr√© d‚Äôordre <span class="math notranslate nohighlight">\(k\)</span> la quantit√© <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right]\)</span></p>
</div>
</div><p>Ainsi :</p>
<ul class="simple">
<li><p>pour une variable al√©atoire discr√®te <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \displaystyle\sum_{x_i} (x_i-\mathbb{E}(X))^k P_X(x_i) = \displaystyle\sum_{x_i} (x_i-\mu_X)^k P_X(x_i)\)</span></p></li>
<li><p>pour une variable al√©atoire continue <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mathbb{E}(X))^k \right] = \int_x (x-\mathbb{E}(X))^k f_X(x)dx = \int_x (x-\mu_X)^k f_X(x)dx\)</span></p></li>
</ul>
</div>
<div class="section" id="variance-d-une-variable-aleatoire">
<h5>Variance d‚Äôune variable al√©atoire<a class="headerlink" href="#variance-d-une-variable-aleatoire" title="Permalink to this heading">#</a></h5>
<span class="target" id="index-17"></span><p id="index-18">Pour <span class="math notranslate nohighlight">\(k\)</span>=2, le moment centr√© d‚Äôordre 2 est appel√© la <strong>variance</strong> et est not√© <span class="math notranslate nohighlight">\(\mathbb{V}(X)\)</span>. La racine carr√©e de la variance est <strong>l‚Äô√©cart type</strong> et est not√© <span class="math notranslate nohighlight">\(\sigma_X\)</span>. On a donc <span class="math notranslate nohighlight">\(\sigma_X^2=\mathbb{V}(X)\)</span>.</p>
<div class="proof proposition admonition" id="proposition-32">
<p class="admonition-title"><span class="caption-number">Proposition 1 </span> (Formule de Koenig)</p>
<div class="proposition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\mathbb{V}(X) = \mathbb{E}(X^2)-\mu_X^2\)</span></p>
</div>
</div><p>En effet, <span class="math notranslate nohighlight">\(\mathbb{E}\left [(X-\mu_X)^2 \right] = \mathbb{E}\left [(X^2-2\mu_XX+\mu_X^2 \right] = \mathbb{E}(X^2)-2\mu_X\mathbb{E}(X)+\mu_X^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_discrete</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">rv_discrete</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ecart-type : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variance :  61.0
Ecart-type :  7.810249675906654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">rv_continuous</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">3.5</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mf">5.5</span> 
<span class="k">class</span> <span class="nc">distribution_gen</span><span class="p">(</span><span class="n">rv_continuous</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">6</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">()</span>
<span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_gen</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ecart-type : &quot;</span><span class="p">,</span> <span class="n">distribution</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Variance:  0.029595913310970445
Ecart-type :  0.1720346282321395
</pre></div>
</div>
</div>
</div>
<div class="proof property admonition" id="property-33">
<p class="admonition-title"><span class="caption-number">Property 3 </span></p>
<div class="property-content section" id="proof-content">
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall a,b\in\mathbb{R})\; \mathbb{V}(aX+b)= a^2\mathbb{V}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall a\in\mathbb{R})\; \mathbb{E}\left [(X-a)^2\right ] = \mathbb V(X) +(\mathbb{E}(X)-a)^2\)</span> (th√©or√®me de Huygens)</p></li>
<li><p><span class="math notranslate nohighlight">\(\forall k&gt;0\; P(|X-\mathbb{E}(X)|\geq k\sigma_X)\leq \frac{1}{k^2}\)</span> (in√©galit√© de Bienaym√©-Tchebychev)</p></li>
</ul>
</div>
</div><p>On dira que la variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> est <strong>r√©duite</strong> (ou <strong>norm√©e</strong>) si <span class="math notranslate nohighlight">\(\mathbb{V}(X)=1\)</span>.</p>
</div>
<div class="section" id="moments-d-ordre-superieur">
<h5>Moments d‚Äôordre sup√©rieur<a class="headerlink" href="#moments-d-ordre-superieur" title="Permalink to this heading">#</a></h5>
<span class="target" id="index-19"></span><p id="index-20">On consid√®re √©galement souvent les moments d‚Äôordre 3 (coefficient d‚Äôasym√©trie ou skewness) et 4 (coefficient d‚Äôapplatissement ou kurtosis).</p>
</div>
</div>
</div>
</div>
<span id="document-elemstats"></span><div class="tex2jax_ignore mathjax_ignore section" id="elements-de-statistiques">
<h2>Elements de statistiques<a class="headerlink" href="#elements-de-statistiques" title="Permalink to this heading">#</a></h2>
<p>Dans l‚Äôexpression ‚Äú√©tude statistique‚Äù, il faut distinguer :</p>
<ol class="arabic simple">
<li><p><strong>les donn√©es statistiques</strong> : suivant l‚Äô√©tude, plusieurs probl√®mes peuvent √™tre pos√©s :</p>
<ul class="simple">
<li><p>Recueil des donn√©es (brutes) avec notamment le probl√®me des sondages</p></li>
<li><p>Nature des donn√©es avec √©ventuellement la transformation des donn√©es brutes, notamment pour les s√©ries chronologiques (s√©rie corrig√©e des variations saisonni√®res)</p></li>
<li><p>Organisation des donn√©es : il s‚Äôagit le plus souvent de r√©sumer l‚Äôinformation par les techniques de la statistique descriptive</p></li>
</ul>
</li>
<li><p><strong>le mod√®le math√©matique</strong> : une analyse du ph√©nom√®ne √©tudi√© doit permettre de traduire les probl√®mes pos√©s par l‚Äô√©tude dans un langage formel, celui des probabilit√©s. Apr√®s avoir fait des choix, des hypoth√®ses sur la loi de probabilit√© et sur les param√®tres de cette loi, on s‚Äôefforce de se placer dans un mod√®le statistique dans lequel des outils th√©oriques permettent de r√©soudre un certain nombre de probl√®mes th√©oriques. Dans ce mod√®le th√©orique, il s‚Äôagit de donner une interpr√©tation aux donn√©es exp√©rimentales et, souvent, des hypoth√®ses implificatrices de ‚Äúm√™me loi‚Äù et d‚Äôind√©pendance sont faites.</p></li>
<li><p><strong>l‚Äôanalyse statistique</strong> : l‚Äôutilisation d‚Äôoutils statistiques adapt√©s au mod√®le retenu permet de faire l‚Äôinterface entre les donn√©es statistiques et le mod√®le th√©orique choisi pour d√©crire le ph√©nom√®ne √©tudi√©.</p></li>
</ol>
<p>L‚Äô√©tude statistique peut alors se traduire sous diverses formes :</p>
<ul class="simple">
<li><p>pr√©ciser le mod√®le choisi, en estimant les param√®tres intervenant dans celui-ci</p></li>
<li><p>juger la validit√© d‚Äôhypoth√®ses faites sur ces param√®tres qui se traduira non pas en ‚Äò‚Äôconfirmation d‚Äôhypoth√®ses‚Äô‚Äô, mais en ‚Äò‚Äôd√©tecteur d‚Äôhypoth√®ses fausses‚Äô‚Äô</p></li>
<li><p>juger l‚Äôad√©quation du mod√®le retenu en termes de lois de probabilit√© avec la m√™me r√©serve que ci-dessus</p></li>
</ul>
<p>Les r√©sultats th√©oriques devront √™tre interpr√©t√©s dans le contexte de l‚Äô√©tude en consid√©rant que ces r√©sultats ont √©t√© obtenus dans le cadre d‚Äôun mod√®le th√©orique pr√©cis, d‚Äôo√π la n√©cessit√© d‚Äôune analyse correcte et d‚Äôune bonne formalisation. De plus, il faudra prendre en compte les techniques utilis√©es, qui ne permettent de r√©pondre qu‚Äô√† des questions pr√©cises. Enfin, dans le cas d‚Äôune application pratique, il faudra garder √† l‚Äôesprit que les conclusions auront des cons√©quences √©conomiques (ou autres).</p>
<div class="section" id="echantillon-d-une-variable-aleatoire">
<h3>Echantillon d‚Äôune variable al√©atoire<a class="headerlink" href="#echantillon-d-une-variable-aleatoire" title="Permalink to this heading">#</a></h3>
<div class="section" id="definition">
<h4>D√©finition<a class="headerlink" href="#definition" title="Permalink to this heading">#</a></h4>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 16 </span> (Echantillon)</p>
<div class="definition-content section" id="proof-content">
<p>Soit une variable al√©atoire <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \mathbb{R}\)</span>. On appelle <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon de la variable al√©atoire parente <span class="math notranslate nohighlight">\(X\)</span> la donn√©e de <span class="math notranslate nohighlight">\(n\)</span> variables al√©atoires <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span>, d√©finies sur le m√™me espace, ind√©pendantes, ayant m√™me loi que <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
</div><p>On a donc pour tout <span class="math notranslate nohighlight">\((x_1\cdots x_n)^T\in\mathbb{R}^n\)</span></p>
<p><span class="math notranslate nohighlight">\(P(X_1&lt;x_1\cdots X_n&lt;x_n)=P(X_1&lt;x_1)\cdots P(X_n&lt;x_n)=P(X&lt;x_1)\cdots P(X&lt;x_n)\)</span></p>
<p>On consid√®re alors une exp√©rience al√©atoire <span class="math notranslate nohighlight">\(\mathcal E\)</span> d√©crite par l‚Äôinterm√©diaire de la variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span>. Consid√©rer un <span class="math notranslate nohighlight">\(n\)</span> √©chantillon de <span class="math notranslate nohighlight">\(X\)</span> consiste √† supposer la possibilit√© de <span class="math notranslate nohighlight">\(n\)</span> r√©p√©titions de l‚Äôexp√©rience <span class="math notranslate nohighlight">\(\mathcal E\)</span> dans des conditions identiques, sans interactions entre elles.</p>
<p>Chaque r√©p√©tition conduit √† l‚Äôobservation d‚Äôune valeur prise par <span class="math notranslate nohighlight">\(X\)</span>, d‚Äôo√π l‚Äôobservation de <span class="math notranslate nohighlight">\(n\)</span> valeurs <span class="math notranslate nohighlight">\(x_1\cdots x_n\)</span> √† la suite des <span class="math notranslate nohighlight">\(n\)</span> r√©p√©titions, consid√©r√©es comme une valeur effectivement prise par le <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> de <span class="math notranslate nohighlight">\(X\)</span>. Les valeurs <span class="math notranslate nohighlight">\((x_1\cdots x_n)\)</span>  rel√®vent de l‚Äôobservation : ce sont les donn√©es statistiques recueillies √† la suite des <span class="math notranslate nohighlight">\(n\)</span> exp√©riences : elles sont appel√©es r√©alisation du <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon.</p>
<p>A noter que les hypoth√®ses de m√™me loi et d‚Äôind√©pendance sont simplificatrices.</p>
</div>
<div class="section" id="schema-de-bernoulli-et-modele-binomial">
<h4>Sch√©ma de Bernoulli et mod√®le binomial<a class="headerlink" href="#schema-de-bernoulli-et-modele-binomial" title="Permalink to this heading">#</a></h4>
<p>Si <span class="math notranslate nohighlight">\(\mathcal E\)</span> n‚Äôa que deux √©ventualit√©s possibles (r√©alisation ou non d‚Äôun √©v√®nement <span class="math notranslate nohighlight">\(A\)</span>), alors l‚Äôexp√©rience peut √™tre d√©crite par l‚Äôinterm√©diaire d‚Äôune variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> (<span class="math notranslate nohighlight">\(\mathbb{1}_A\)</span>, fonction indicatrice de <span class="math notranslate nohighlight">\(A\)</span>), de Bernoulli <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \{0,1\}\)</span> avec <span class="math notranslate nohighlight">\(P(X=1)=P(A)=p\in]0,1[\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(\mathcal E\)</span> est r√©p√©t√©e <span class="math notranslate nohighlight">\(n\)</span> fois dans des conditions identiques, sans interaction entre elles, on consid√®re un <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> de variable al√©atoire parente <span class="math notranslate nohighlight">\(X\)</span>. Les valeurs prises par la variable al√©atoire <span class="math notranslate nohighlight">\(S_n=X_1+\cdots X_n\)</span> repr√©sentent le nombre de r√©alisations de <span class="math notranslate nohighlight">\(A\)</span> √† la suite des <span class="math notranslate nohighlight">\(n\)</span> r√©p√©titions. Une telle situation est dite relever du sch√©ma de Bernoulli.</p>
<div class="proof property admonition" id="property-1">
<p class="admonition-title"><span class="caption-number">Property 4 </span></p>
<div class="property-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(S_n:(\Omega,\mathcal A,P)\mapsto [\![0,n]\!]\)</span> a une loi binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\forall k\in[\![0,n]\!]\; P(S_n=k)=\begin{pmatrix}n\\k\end{pmatrix} p^k (1-p)^{n-k}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(S_n)=np,\; \mathbb{V}(S_n)=np(1-p)\)</span></p></li>
</ul>
</div>
</div><p>En effet, d‚Äôapr√®s l‚Äôind√©pendance pour toute suite (<span class="math notranslate nohighlight">\(\delta_1\cdots \delta_n\)</span>) avec pour tout <span class="math notranslate nohighlight">\(k\in[\![1,n]\!]\)</span> <span class="math notranslate nohighlight">\(\delta_k\in\{0,1\}\)</span>, on a :</p>
<p><span class="math notranslate nohighlight">\(P(X_1=\delta_1\cdots X_n=\delta_n) = \displaystyle\prod_{k=1}^n P(X_k=\delta_k) = \displaystyle\prod_{k=1}^n P(X=\delta_k)=p^{s_n}(1-p)^{(n-s_n)}\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\delta_1+\cdots+ \delta_n=s_n\)</span> , les variables al√©atoires ayant m√™me loi de Bernoulli que <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Le nombre de solutions de <span class="math notranslate nohighlight">\(\delta_1+\cdots+ \delta_n=s_n\)</span> avec <span class="math notranslate nohighlight">\(s_n\in[\![0,n]\!]\)</span> et <span class="math notranslate nohighlight">\(\delta_k\in\{0,1\}\)</span> est <span class="math notranslate nohighlight">\(\begin{pmatrix}s_n\\n\end{pmatrix}\)</span>, d‚Äôo√π le r√©sultat.</p>
<p>D‚Äôapr√®s la lin√©arit√© de l‚Äôesp√©rance et l‚Äô√©galit√© de Bienaym√©, on a de plus
<span class="math notranslate nohighlight">\(\mathbb{E}(S_n) = \displaystyle\sum_{k=1}^n \mathbb{E}(X_k)=n\mathbb{E}(X)=np\quad \mathbb{V}(S_n)=\displaystyle\sum_{k=1}^n \mathbb{V}(X_k)=n\mathbb{V}(X)=np(1-p)\)</span></p>
</div>
<div class="section" id="moyenne-et-variances-empiriques-d-un-n-echantillon">
<h4>Moyenne et variances empiriques d‚Äôun <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon<a class="headerlink" href="#moyenne-et-variances-empiriques-d-un-n-echantillon" title="Permalink to this heading">#</a></h4>
<p>Etant donn√© un <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span> d‚Äôune variable al√©atoire parente <span class="math notranslate nohighlight">\(X\)</span>, on appelle :</p>
<span class="target" id="index-0"></span><ul id="index-1">
<li><p>moyenne empirique du <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon la variable al√©atoire</p></li>
<li><div class="math notranslate nohighlight">
\[\bar{X}_n=\frac1n \displaystyle\sum_{k=1}^n X_k\]</div>
</li>
<li><p>variance empirique biais√©e du <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon la variable al√©atoire (Ne pas confondre avec la variable <span class="math notranslate nohighlight">\(S_n\)</span> du sch√©ma de Bernoulli)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[S_n^2=\frac1n \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2=\frac1n \displaystyle\sum_{k=1}^n X_k^2 -\bar{X}_n^2\]</div>
<ul class="simple">
<li><p>variance empirique non biais√©e du <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon la variable al√©atoire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[{S'}_n^2=\frac{1}{n-1} \displaystyle\sum_{k=1}^n (X_k-\bar{X}_n)^2\]</div>
<p>On a bien s√ªr <span class="math notranslate nohighlight">\((n-1){S'}_n^2=nS_n^2\)</span>.</p>
<p>Les valeurs prises par <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> co√Øncident avec la moyenne exp√©rimentale <span class="math notranslate nohighlight">\(\bar{x}_n\)</span> des donn√©es exp√©rimentales <span class="math notranslate nohighlight">\((x_1\cdots x_n)\)</span>, r√©alisation du <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon. De m√™me pour <span class="math notranslate nohighlight">\(S_n^2\)</span> pour la variance exp√©rimentale.</p>
<div class="proof property admonition" id="property-2">
<p class="admonition-title"><span class="caption-number">Property 5 </span></p>
<div class="property-content section" id="proof-content">
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(\bar{X}_n)= \mathbb{E}(X)=m\; ;\; \mathbb{V}(\bar{X}_n) = \frac{\mathbb{V}(X)}{n}=\frac{\sigma^2}{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(S_n^2) = \frac{n-1}{n}\sigma^2\; ;\;  \mathbb{E}({S'}_n^2)=\sigma^2\)</span></p></li>
<li><p>Sous l‚Äôhypoth√®se de normalit√©, <span class="math notranslate nohighlight">\(\mathbb{V}({S'}_n^2)=\frac{2\sigma^4}{n-1}\)</span></p></li>
</ol>
</div>
</div><p>En effet :</p>
<ol class="arabic simple">
<li><p>Imm√©diat d‚Äôapr√®s la lin√©arit√© de l‚Äôesp√©rance, l‚Äô√©galit√© de Bienaym√© et la propri√©t√© <span class="math notranslate nohighlight">\(\mathbb{V}(\alpha X)=\alpha^2\mathbb{V}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((n-1){S'}_n^2=\displaystyle\sum_{k=1}^n X_k^2-n\bar{X_n^2}\)</span> d‚Äôo√π</p></li>
</ol>
<p><span class="math notranslate nohighlight">\((n-1)\mathbb{E}({S'}_n^2)=\displaystyle\sum_{k=1}^n\mathbb{E}(X_k^2)-n\mathbb{E}(\bar{X_n^2})=n(\sigma^2+m^2)-n\left (\frac{\sigma^2}{n}+m^2 \right )\)</span>
et le r√©sultat.</p>
<p>Le dernier point est admis.</p>
</div>
<div class="section" id="echantillons-de-variables-aleatoires-normales">
<h4>Echantillons de variables al√©atoires normales<a class="headerlink" href="#echantillons-de-variables-aleatoires-normales" title="Permalink to this heading">#</a></h4>
<p>Les lois de probabilit√© usuelles sont rappel√©es en fin de ce chapitre ({ref}`loisusuelles).</p>
<div class="section" id="etude-d-un-n-echantillon">
<h5>Etude d‚Äôun <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon<a class="headerlink" href="#etude-d-un-n-echantillon" title="Permalink to this heading">#</a></h5>
<p>Soit un <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> de variable al√©atoire parente <span class="math notranslate nohighlight">\(X\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>. On a les r√©sultats suivants :</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\sqrt{n} \frac{\bar{X}_n-m}{\sigma}\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{nS_n^2}{\sigma^2} = \frac{(n-1)S'^2_n}{\sigma^2}\)</span> suit une loi <span class="math notranslate nohighlight">\(\chi^2_{n-1}\)</span></p></li>
<li><p>les variables al√©atoires <span class="math notranslate nohighlight">\(\bar{X}_n\)</span> et <span class="math notranslate nohighlight">\(S_n^2\)</span> sont ind√©pendantes</p></li>
<li><p><span class="math notranslate nohighlight">\(T=\sqrt{n}\frac{\bar{X}_n-m}{S'_n}=\sqrt{n-1}\frac{\bar{X}_n-m}{S_n}\)</span> suit une loi de Student √† <span class="math notranslate nohighlight">\(n-1\)</span> degr√©s de libert√©.</p></li>
</ol>
</div>
<div class="section" id="etude-de-deux-echantillons-independants">
<h5>Etude de deux √©chantillons ind√©pendants<a class="headerlink" href="#etude-de-deux-echantillons-independants" title="Permalink to this heading">#</a></h5>
<p>Soient un <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> de <span class="math notranslate nohighlight">\(X\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_1,\sigma_1)\)</span>, un <span class="math notranslate nohighlight">\(m\)</span>-√©chantillon <span class="math notranslate nohighlight">\(Y_1\cdots Y_m\)</span> de <span class="math notranslate nohighlight">\(Y\)</span> de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_2,\sigma_2)\)</span>, les √©chantillons √©tant ind√©pendants. Avec des notations √©videntes, on a les r√©sultats suivants :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F = \frac{\sigma_2^2 S'^2_n(X)}{\sigma_1^2 S'^2_m(Y)} = \frac{(m-1)n}{(n-1)m}\frac{\sigma_2^2S_n^2(X)}{\sigma_1^2S_m^2(Y)}\)</span> admet une loi de Fisher-Sn√©d√©cor FS(<span class="math notranslate nohighlight">\(n-1\)</span>,<span class="math notranslate nohighlight">\(m-1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(T = \sqrt{\frac{(n+m-2)mn}{m+n}}\frac{(\bar{X}_n-\bar{Y}_m)-(m_1-m_2)}{\sqrt{nS_n^2(X)+mS_m^2(Y)}}\)</span> admet, sous l‚Äôhypoth√®se <span class="math notranslate nohighlight">\(\sigma_1=\sigma_2\)</span>, une loi de Student √† <span class="math notranslate nohighlight">\((n+m-2)\)</span> degr√©s de libert√©.</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 5 </span></p>
<div class="remark-content section" id="proof-content">
<p>Sous l‚Äôhypoth√®se <span class="math notranslate nohighlight">\(\sigma_1=\sigma_2=\sigma\)</span> :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{X}_n-\bar{Y}_m\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(m_1-m_2,\sigma\sqrt{\frac1n+\frac1m})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{nS_n^2(X)}{\sigma^2}+\frac{mS_m^2(Y)}{\sigma^2}\)</span> a une loi <span class="math notranslate nohighlight">\(\chi^2_{n-1+m-1}\)</span>.</p></li>
</ul>
</div>
</div></div>
</div>
</div>
<div class="section" id="loi-des-grands-nombres">
<h3>Loi des grands nombres<a class="headerlink" href="#loi-des-grands-nombres" title="Permalink to this heading">#</a></h3>
<div class="section" id="inegalite-de-tchebychev">
<h4>In√©galit√© de Tchebychev<a class="headerlink" href="#inegalite-de-tchebychev" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-4">
<p class="admonition-title"><span class="caption-number">Theorem 2 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit une variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> de moyenne <span class="math notranslate nohighlight">\(m\)</span> et d‚Äô√©cart-type <span class="math notranslate nohighlight">\(\sigma\)</span>. Alors :</p>
<p><span class="math notranslate nohighlight">\((\forall t&gt;0)\; P(|X-m|\geq t)\leq \frac{\sigma^2}{t^2}\quad\textrm{et}\quad (\forall u&gt;0)\; P(\frac{|X-m|}{\sigma}\geq u)\leq \frac{1}{u^2}\)</span></p>
</div>
</div><p>En effet :
Soit <span class="math notranslate nohighlight">\(A=\left \{|X-m|\geq t\right \}\)</span> et <span class="math notranslate nohighlight">\(\mathbb{1}_A(\omega)\)</span> = 1 si <span class="math notranslate nohighlight">\(\omega\in A\)</span>, 0 sinon. Alors :</p>
<p><span class="math notranslate nohighlight">\((\forall \omega\in\Omega)\; |X(\omega)-m|^2\geq |X(\omega)-m|^2\mathbb{1}_A(\omega) \geq t^2\mathbb{1}_A(\omega)\)</span></p>
<p>L‚Äôesp√©rance √©tant croissante et v√©rifiant <span class="math notranslate nohighlight">\(\mathbb{E}(\mathbb{1}_A)=P(A)\)</span>, on a
<span class="math notranslate nohighlight">\(\sigma^2=\mathbb{E}(|X-m|^2)\geq t^2P(A) = t^2P(|X-m|\geq t)\)</span> et le r√©sultat.</p>
<div class="proof remark dropdown admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 6 </span></p>
<div class="remark-content section" id="proof-content">
<p>Ces in√©galit√©s, souvent tr√®s grossi√®res et d‚Äôint√©ret essentiellement th√©orique, n‚Äôont d‚Äôutilit√© que pour <span class="math notranslate nohighlight">\(t&gt;\sigma\)</span> ou <span class="math notranslate nohighlight">\(u&gt;1\)</span> (une probabilit√© est toujours inf√©rieure √† 1). La seconde donne un majorant de la probabilit√© d‚Äôobserver des valeurs prises par <span class="math notranslate nohighlight">\(X\)</span> √† l‚Äôext√©rieur de l‚Äôintervalle <span class="math notranslate nohighlight">\([m-u\sigma,m+u\sigma]\)</span></p>
</div>
</div></div>
<div class="section" id="phenomene-de-regularite-statistique">
<h4>Ph√©nom√®ne de r√©gularit√© statistique<a class="headerlink" href="#phenomene-de-regularite-statistique" title="Permalink to this heading">#</a></h4>
<p>Consid√©rons plusieurs s√©quences de 100 lancers d‚Äôune pi√®ce de monnaie et notons, pour chaque s√©quence, la suite <span class="math notranslate nohighlight">\((f_n)_{n\geq 1}\)</span> des fr√©quences des piles obtenus. Un exemple de simulation avec <span class="math notranslate nohighlight">\(p=0.4\)</span> est propos√© dans la figure suivante avec le code ayant servi √† la produire.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span>  <span class="nn">random</span>  <span class="kn">import</span>  <span class="n">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">experience</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">0.4</span>
    <span class="n">f</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span><span class="n">p</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">+=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">f</span><span class="o">+=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">f</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">nb_sequences</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_sequences</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">experience</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$n$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f_n$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0cea81a801c984e24621e903cb2edc97e70a88d552ca3e93915f8d74de36b8f4.png" src="_images/0cea81a801c984e24621e903cb2edc97e70a88d552ca3e93915f8d74de36b8f4.png" />
</div>
</div>
<p>La fluctuation de la fr√©quence est importante pour des petites valeurs de <span class="math notranslate nohighlight">\(n\)</span>, puis elle s‚Äôatt√©nue, pour se stabiliser autour d‚Äôune valeur voisine de <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Cette constatation exp√©rimentale conduit aux remarques suivantes, qui sont pr√©cis√©es dans la suite dans le cadre th√©orique :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_n\)</span> donne une id√©e de la valeur de <span class="math notranslate nohighlight">\(p\)</span> avec une plus ou moins grande pr√©cision</p></li>
<li><p>la probabilit√© appara√Æt comme une fr√©quence limite.</p></li>
</ul>
</div>
<div class="section" id="loi-faible-des-grands-nombres">
<h4>Loi faible des grands nombres<a class="headerlink" href="#loi-faible-des-grands-nombres" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 3 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> une suite de variables al√©atoires ind√©pendantes, identiquement distribu√©es (i.i.d) de m√™me loi qu‚Äôune variable <span class="math notranslate nohighlight">\(X\)</span>, admettant une moyenne <span class="math notranslate nohighlight">\(m\)</span> et un √©cart-type <span class="math notranslate nohighlight">\(\sigma\)</span>. Si <span class="math notranslate nohighlight">\((\bar{X}_n)_{n\geq 1}\)</span> est la suite des moyennes empiriques associ√©e √† <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> alors</p>
<p><span class="math notranslate nohighlight">\((\forall t&gt;0)\; \displaystyle\lim_{n\rightarrow\infty} P(|\bar{X}_n-m|\geq t) = 0\)</span></p>
<p>On dit que la suite converge en probatilit√© vers <span class="math notranslate nohighlight">\(m\)</span> et on note <span class="math notranslate nohighlight">\(\bar{X}_n\xrightarrow[n\rightarrow\infty]{P} m\)</span></p>
</div>
</div><p>C‚Äôest une cons√©quence imm√©diate de l‚Äôin√©galit√© de Tchebychev : <span class="math notranslate nohighlight">\(P(|\bar{X}_n-m|\geq t)\leq\frac{\sigma^2}{nt^2}\)</span> puisque <span class="math notranslate nohighlight">\(\mathbb{V}(\bar{X}_n)=\frac{\sigma^2}{n}\)</span></p>
<p>L‚Äôobservation des valeurs prises par la moyenne empirique donne une bonne information sur la moyenne th√©orique <span class="math notranslate nohighlight">\(m\)</span> de <span class="math notranslate nohighlight">\(X\)</span>. La pr√©cision, au sens ci-dessus, est d‚Äôautant meilleure que <span class="math notranslate nohighlight">\(n\)</span> est grand.</p>
</div>
<div class="section" id="loi-forte-des-grands-nombres">
<h4>Loi forte des grands nombres<a class="headerlink" href="#loi-forte-des-grands-nombres" title="Permalink to this heading">#</a></h4>
<p>avec les hypoth√®ses pr√©c√©dentes, on peut montrer que</p>
<p><span class="math notranslate nohighlight">\(P(\{\omega\in\Omega, \displaystyle\lim_{n\rightarrow\infty} \bar{X}_n(\omega)=m\})=1\)</span></p>
<p>Sauf cas tr√®s improbable (avec probabilit√© nulle), la suite des r√©alisations <span class="math notranslate nohighlight">\((\bar{x}_n)_{n\geq 1}\)</span> des moyennes exp√©rimentales des mesures converge vers la moyenne th√©orique <span class="math notranslate nohighlight">\(m\)</span>. On dit que la suite <span class="math notranslate nohighlight">\((\bar{X}_n)_{n\geq 1}\)</span> converge presque s√ªrement vers <span class="math notranslate nohighlight">\(m\)</span> et on note <span class="math notranslate nohighlight">\(\bar{X}_n\xrightarrow[n\rightarrow\infty]{p.s.} m\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 7 </span></p>
<div class="remark-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X=\mathbb{1}_A\)</span> alors <span class="math notranslate nohighlight">\(m=p=P(A)\)</span> et la probabilit√© de l‚Äô√©v√®nement <span class="math notranslate nohighlight">\(A\)</span> appara√Æt comme une fr√©quence limite.</p>
</div>
</div></div>
</div>
<div class="section" id="approximation-de-mathcal-b-n-p-par-la-loi-de-poisson-mathcal-p-lambda">
<h3>Approximation de <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> par la loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(\lambda)\)</span><a class="headerlink" href="#approximation-de-mathcal-b-n-p-par-la-loi-de-poisson-mathcal-p-lambda" title="Permalink to this heading">#</a></h3>
<div class="section" id="theoreme-d-analyse">
<h4>Th√©or√®me d‚Äôanalyse<a class="headerlink" href="#theoreme-d-analyse" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-8">
<p class="admonition-title"><span class="caption-number">Theorem 4 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(p\)</span> est une fonction de <span class="math notranslate nohighlight">\(n\)</span> telle que <span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty}np(n)=\lambda&gt;0\)</span>, alors pour tout <span class="math notranslate nohighlight">\(k\geq 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty}\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k} = e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p>
</div>
</div><p>En effet</p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{n(n-1)\cdots (n-k+1)}{k!}p^k(1-p)^{n-k}\)</span></p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\p\end{pmatrix} p^k(1-p)^{n-k}=\frac{(np)^k}{k!}\displaystyle\prod_{j=0}^k\left (1-\frac{j}{n}\right )(1-p)^{n-k}\)</span></p>
<p>et le r√©sultat est d√©montr√© en remarquant que <span class="math notranslate nohighlight">\(\displaystyle\lim_{n\rightarrow\infty} p(n)=0\)</span>.</p>
</div>
<div class="section" id="application">
<h4>Application<a class="headerlink" href="#application" title="Permalink to this heading">#</a></h4>
<p>Soit <span class="math notranslate nohighlight">\(S_n\)</span> une variable al√©atoire de loi <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span>. Lorsque <span class="math notranslate nohighlight">\(n\)</span> est grand (&gt;50) et <span class="math notranslate nohighlight">\(p\)</span> petite (<span class="math notranslate nohighlight">\(np\)</span>&lt;10), on peut approcher la loi de <span class="math notranslate nohighlight">\(S_n\)</span> par une loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(np)\)</span>. On lit alors la valeur correspondante dans la table de la loi de Poisson, pour tout <span class="math notranslate nohighlight">\(k\in[\![0,n]\!]\)</span>
<span class="math notranslate nohighlight">\(P(S_n=k)\approx e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p>
<p>De plus, en remarquant que <span class="math notranslate nohighlight">\(\Sigma_n=n-S_n\)</span> suit <span class="math notranslate nohighlight">\(\mathcal{B}(n,1-p)\)</span>, on a</p>
<div class="math notranslate nohighlight">
\[\begin{split}P(\Sigma_n=k)=P(S_n=n-k)=\begin{pmatrix}n\\p\end{pmatrix} p^{n-k}(1-p)^{k} \end{split}\]</div>
<p>et quand <span class="math notranslate nohighlight">\(n\)</span> est grand (&gt;50) et <span class="math notranslate nohighlight">\(p\)</span> voisin de 1 (<span class="math notranslate nohighlight">\(n(1-p)&lt;10\)</span>) on peut approcher la loi de <span class="math notranslate nohighlight">\(\Sigma_n\)</span> par une loi de Poisson <span class="math notranslate nohighlight">\(\mathcal P(n(1-p))\)</span>.</p>
</div>
</div>
<div class="section" id="theoreme-central-limite">
<h3>Th√©or√®me central limite<a class="headerlink" href="#theoreme-central-limite" title="Permalink to this heading">#</a></h3>
<div class="section" id="le-t-c-l">
<h4>Le T.C.L.<a class="headerlink" href="#le-t-c-l" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-9">
<p class="admonition-title"><span class="caption-number">Theorem 5 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit une suite <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> de variables al√©atoires, i.i.d. de m√™me loi qu‚Äôune variable parente <span class="math notranslate nohighlight">\(X\)</span>, d√©finies sur le m√™me espace <span class="math notranslate nohighlight">\((\Omega,\mathcal A,P)\)</span>. On consid√®re la suite des moyennes empiriques <span class="math notranslate nohighlight">\((X_n)_{n\geq 1}\)</span> des <span class="math notranslate nohighlight">\(n\)</span>-√©chantillons <span class="math notranslate nohighlight">\((X_1\cdots X_n)\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(X\)</span> admet une moyenne <span class="math notranslate nohighlight">\(m\)</span> et un √©cart-type <span class="math notranslate nohighlight">\(\sigma\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\((\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\sqrt{n}\frac{\bar X_n-m}{\sigma} &lt;x\right) = \phi(x)\)</span>
o√π <span class="math notranslate nohighlight">\( \phi(x)\)</span> est la fonction de r√©partition de la loi normale centr√©e r√©duite <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
<p>On dit que <span class="math notranslate nohighlight">\(\left (\sqrt{n}\frac{\bar X_n-m}{\sigma}\right )_{n\geq 1}\)</span> converge en loi vers <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
</div>
</div><p>La figure suivante illustre ce mod√®le dans le cas o√π la variable al√©atoire parente <span class="math notranslate nohighlight">\(X\)</span> suit un sch√©ma de Bernoulli avec <span class="math notranslate nohighlight">\(P(X = 1)=0.1, P(X=0)=0.9\)</span>.</p>
<p><img alt="" src="_images/tcl.png" /></p>
</div>
<div class="section" id="commentaires">
<h4>Commentaires<a class="headerlink" href="#commentaires" title="Permalink to this heading">#</a></h4>
<p>Pour mesurer une grandeur de valeur inconnue <span class="math notranslate nohighlight">\(m\)</span>, il suffit d‚Äôune seule mesure lorsqu‚Äôil n‚Äôy a pas d‚Äôerreur exp√©rimentale. Mais les mesures sont toujours ent√¢ch√©es d‚Äôerreur et une exp√©rience ou mesure peut √™tre mod√©lis√©e par une variable al√©atoire <span class="math notranslate nohighlight">\(X\)</span> dnot la moyenne th√©orique <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span> est la valeur cherch√©e <span class="math notranslate nohighlight">\(m\)</span> si les mesures ne sont pas biais√©es, c‚Äôest-√†-dire affect√©es d‚Äôune erreur syst√©matique.</p>
<p>Ayant effectu√© <span class="math notranslate nohighlight">\(n\)</span> mesures, on a une r√©alisation d‚Äôun <span class="math notranslate nohighlight">\(n\)</span>-√©chantillon de <span class="math notranslate nohighlight">\(X\)</span> et une valeur observ√©e <span class="math notranslate nohighlight">\(\bar x_n\)</span> de la moyenne empirique <span class="math notranslate nohighlight">\(\bar X_n\)</span>. On peut prendre cette valeur comme estimation de <span class="math notranslate nohighlight">\(m\)</span>, l‚Äô√©cart <span class="math notranslate nohighlight">\(|\bar x_n-m|\)</span> √©tant une r√©alisation de <span class="math notranslate nohighlight">\(|\bar X_n-m|\)</span>.</p>
<ul class="simple">
<li><p>La loi forte des grands nombres justifie cette estimation en supposant  <span class="math notranslate nohighlight">\(\mathbb{E}(X)=m\)</span></p></li>
<li><p>L‚Äôin√©galit√© de Tchebychev donne une id√©e grossi√®re de l‚Äô√©cart en terme de probabilit√©</p></li>
<li><p>le th√©or√®me central limite donne une √©valuation asymptotique de cet √©cart al√©atoire</p></li>
</ul>
<p>Dans la pratique, pour <span class="math notranslate nohighlight">\(n\)</span> grand, dans le cadre de ce th√©or√®me, on a l‚Äôapproximation suivante :</p>
<p><span class="math notranslate nohighlight">\((\forall a&lt;b)\;\;\;\; P\left (a\sqrt{n}\frac{\bar X_n-m}{\sigma} &lt;b\right)\approx \phi(b)-\phi(a)\)</span></p>
</div>
<div class="section" id="cas-particulier-theoreme-de-moivre-laplace">
<h4>Cas particulier : th√©or√®me de Moivre-Laplace<a class="headerlink" href="#cas-particulier-theoreme-de-moivre-laplace" title="Permalink to this heading">#</a></h4>
<div class="proof theorem admonition" id="theorem-10">
<p class="admonition-title"><span class="caption-number">Theorem 6 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(X=\mathbb{1}_A\)</span>  une variable al√©atoire de Bernoulli avec <span class="math notranslate nohighlight">\(P(A)=p\)</span>. Dans les conditions du th√©or√®me central limite la variable <span class="math notranslate nohighlight">\(S_n=\displaystyle\sum_{k=1}^n X_k=n\bar X_n\)</span> suit une loi binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p)\)</span> et</p>
<p><span class="math notranslate nohighlight">\( (\forall x\in\mathbb{R})\; \displaystyle\lim_{n\rightarrow\infty}P\left (\frac{S_n-np}{\sqrt{np(1-p)}} &lt;x\right) = \phi(x)\)</span></p>
</div>
</div><p>On peut donc approcher une loi binomiale par une loi normale.</p>
</div>
</div>
<div class="section" id="modeles-probabilistes-usuels">
<span id="loisusuelles"></span><h3>Mod√®les probabilistes usuels<a class="headerlink" href="#modeles-probabilistes-usuels" title="Permalink to this heading">#</a></h3>
<p>On donne ici un catalogue non exhaustif des principaux mod√®les probabilistes, et leurs principales propri√©t√©s. Une illustration graphique des lois correspondantes est propos√©e dans les figures suivantes.</p>
<div class="section" id="lois-discretes">
<h4>Lois discr√®tes<a class="headerlink" href="#lois-discretes" title="Permalink to this heading">#</a></h4>
<p>On consid√®re une variable al√©atoire <span class="math notranslate nohighlight">\(X:(\Omega,\mathcal A,P)\mapsto \mathcal D\)</span></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Mod√®le</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathcal D}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{P(X=k)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{E}(X)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{V}(X)}\)</span></p></th>
<th class="head"><p>Utilisation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bernoulli</p></td>
<td><p><span class="math notranslate nohighlight">\(\{0,1\}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X=1)=p,P(X=0)=1-p=q\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(pq\)</span></p></td>
<td><p>Exp√©rience ayant 2 √©ventualit√©s possibles</p></td>
</tr>
<tr class="row-odd"><td><p>Binomiale <span class="math notranslate nohighlight">\(\mathcal{B}(n,p) \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([\![0,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\begin{pmatrix}n\\k\end{pmatrix}p^k q^{n-k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(np\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(npq\)</span></p></td>
<td><p>Tirage avec remise</p></td>
</tr>
<tr class="row-even"><td><p>Hyperg√©om√©trique, <span class="math notranslate nohighlight">\(\mathcal{H}(m,N,n), m&lt;N\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([\![0,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\begin{pmatrix}m\\k\end{pmatrix}\begin{pmatrix}N-m\\n-k\end{pmatrix}}{\begin{pmatrix}N\\n\end{pmatrix}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\frac{m}{M}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{N-n}{N-1}n\frac{m}{N}\frac{N-m}{N}\)</span></p></td>
<td><p>Tirage sans remise</p></td>
</tr>
<tr class="row-odd"><td><p>Uniforme</p></td>
<td><p><span class="math notranslate nohighlight">\([\![1,n]\!]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac1n\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{n+1}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{n2-1}{12}\)</span></p></td>
<td><p>Equiprobabilit√© des r√©sultats</p></td>
</tr>
<tr class="row-even"><td><p>Poisson <span class="math notranslate nohighlight">\(\mathcal{P}(\lambda), \lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{N}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(e^{-\lambda}\frac{\lambda^k}{k!}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda\)</span></p></td>
<td><p>Files d‚Äôattente, Ev√®nements rares</p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/discretes.png" /></p>
<div class="section" id="modele-de-bernoulli">
<h5>Mod√®le de Bernoulli<a class="headerlink" href="#modele-de-bernoulli" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">bernoulli</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1">#nombre de r√©p√©titions de l&#39;exp√©rience</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># probabilit√© de succ√®s</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.3
Variance:  0.21
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-binomiale">
<h5>Loi binomiale<a class="headerlink" href="#loi-binomiale" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span> 
<span class="n">x</span> <span class="o">=</span> <span class="mi">7</span> 
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">binom</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance:  1.6
Densit√© de probabilit√© :  0.000786432
Fonction de r√©partition :  0.9999220736
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-hypergeometrique">
<h5>Loi hyperg√©om√©trique<a class="headerlink" href="#loi-hypergeometrique" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hypergeom</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> 
<span class="n">M</span> <span class="o">=</span> <span class="mi">15</span> 
<span class="n">m</span> <span class="o">=</span> <span class="mi">9</span> 
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  3.0
Variance:  0.8571428571428571
Densit√© de probabilit√© :  0.23976023976023975
Fonction de r√©partition :  0.28671328671328666
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="distribution-de-poisson">
<h5>Distribution de Poisson<a class="headerlink" href="#distribution-de-poisson" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">poisson</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">Lambda</span> <span class="o">=</span> <span class="mi">2</span><span class="o">/</span><span class="mi">3</span> 

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">poisson</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.6666666666666666
Variance:  0.6666666666666666
Densit√© de probabilit√© :  0.3422780793550613
Fonction de r√©partition :  0.8556951983876534
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="lois-absolument-continues">
<h4>Lois absolument continues<a class="headerlink" href="#lois-absolument-continues" title="Permalink to this heading">#</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Mod√®le</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathcal D}\)</span></p></th>
<th class="head"><p>Densit√©</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{E}(X)}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\boldsymbol{\mathbb{V}(X)}\)</span></p></th>
<th class="head"><p>Utilisation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Uniforme</p></td>
<td><p><span class="math notranslate nohighlight">\([a,b]\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{b-a}\mathbb{1}_{]a,b[}(x)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{b+a}{2}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{(b-a)^2}{12}\)</span></p></td>
<td><p>Pas d‚Äôa priori sur la distribution</p></td>
</tr>
<tr class="row-odd"><td><p>Exponentiel <span class="math notranslate nohighlight">\(Exp(\lambda)\)</span><span class="math notranslate nohighlight">\(\lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}^+\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x) =\lambda e^{-\lambda x} \mathbb{1}_{x&gt;0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{\lambda^2}\)</span></p></td>
<td><p>Files d‚Äôattente, Dur√©e de vie sans usure</p></td>
</tr>
<tr class="row-even"><td><p>Pareto  <span class="math notranslate nohighlight">\(\alpha&gt;1,x_0&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([x_0,+\infty[\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{\alpha-1}{x_0}\left (\frac{x_0}{x} \right )^\alpha \mathbb{1}_{x\geq x_0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\alpha-1}{\alpha-2}x_0\)</span> <span class="math notranslate nohighlight">\(\alpha&gt;2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{(\alpha-1)x_0^2}{(\alpha-3)(\alpha-2)^2}\)</span> <span class="math notranslate nohighlight">\(\alpha&gt;3\)</span></p></td>
<td><p>Revenu des m√©nages</p></td>
</tr>
<tr class="row-odd"><td><p>Normale <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-m)^2}{2\sigma^2}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(m\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma^2\)</span></p></td>
<td><p>voir T.C.L.</p></td>
</tr>
<tr class="row-even"><td><p>Gamma <span class="math notranslate nohighlight">\(\gamma(a,\lambda)\)</span><span class="math notranslate nohighlight">\(a&gt;0,\lambda&gt;0\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((\mathbb{R}^+)^*\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x) = \frac{\lambda^a}{\Gamma(a)}e^{-\lambda x}x^{a-1}\mathbb{1}_{x&gt;0}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{a}{\lambda}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{a}{\lambda^2}\)</span></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Khi-deux <span class="math notranslate nohighlight">\(\chi_n^2\)</span> <span class="math notranslate nohighlight">\(n\)</span> degr√©s libert√©</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x, k)=\frac{1}{2^\frac{k}{2}\Gamma(\frac{k}{2})} x^{\frac{k}{2} - 1} e^{-\frac{x}{2}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(2n\)</span></p></td>
<td><p>Test du khi-deux</p></td>
</tr>
<tr class="row-even"><td><p>Student <span class="math notranslate nohighlight">\(n\)</span> degr√©s libert√©</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(f(x)=\frac{1}{\sqrt{\pi n}}\frac{\Gamma((n+1)/2)}{\Gamma(n/2)} \left (1+\frac{t^2}{n} \right )^{-\frac{n+1}{2}}\)</span> t&gt;0</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p>Test √©galit√© moyenne</p></td>
</tr>
<tr class="row-odd"><td><p>Fisher-Sn√©d√©cor <span class="math notranslate nohighlight">\(n\)</span> et <span class="math notranslate nohighlight">\(m\)</span> degr√©s libert√©</p></td>
<td><p><span class="math notranslate nohighlight">\((\mathbb{R}^+)^*\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}n^{\frac{n}{2}}m^{\frac{m}{2}}\frac{x^{\frac{n-2}{2}}}{(nx+m)^{\frac{n+m}{2}}}\)</span></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/continues.png" /></p>
<div class="section" id="modele-uniforme">
<h5>Mod√®le uniforme<a class="headerlink" href="#modele-uniforme" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">b</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">uniform</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Esp√©rance: &quot;</span><span class="p">,</span> <span class="n">mean</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance: &quot;</span><span class="p">,</span> <span class="n">var</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">uniform</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">b</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Esp√©rance:  3.0
Variance:  1.3333333333333333
Densit√© de probabilit√© :  0.25
Fonction de r√©partition :  0.375
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-normale">
<h5>Loi normale<a class="headerlink" href="#loi-normale" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">x</span> <span class="o">=</span> <span class="mf">1.3</span> 
<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span> 
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span> 

<span class="n">mean</span><span class="p">,</span><span class="n">var</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span> <span class="o">=</span> <span class="n">m</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.0
Variance :  1.0
Densit√© de probabilit√© :  0.17136859204780736
Fonction de r√©partition :  0.9031995154143897
</pre></div>
</div>
</div>
</div>
<p>Sous l‚Äôhypoth√®se de normalit√©, de nombreux outils statistiques sont disponibles. Souvent, l‚Äôhypoth√®se de normalit√© est justifi√©e par l‚Äôinterm√©diaire du th√©or√®me centrale limite. Des consid√©rations, parfois abusives, permettent de se placer dans le cadre d‚Äôutilisation de ce th√©or√®me et de choisir un mod√®le normal alors qu‚Äôune √©tude des donn√©es statistiques met en d√©faut le choix de ce mod√®le (probl√®me dit d‚Äôad√©quation).</p>
<div class="proof property admonition" id="property-11">
<p class="admonition-title"><span class="caption-number">Property 6 </span></p>
<div class="property-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> est une variable al√©atoire de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span> alors la variable <span class="math notranslate nohighlight">\(Z=\frac{X-m}{\sigma}\)</span> est la variable centr√©e r√©duite associ√©e, et suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> dite aussi loi de Gauss-Laplace.</p>
</div>
</div><p>La fonction de r√©partition de <span class="math notranslate nohighlight">\(Z\)</span> est <span class="math notranslate nohighlight">\(\phi(Z) = P(Z&lt;z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^z e^{-\frac{t^2}{2}}dt\)</span>, dont les valeurs peuvent √™tre lues dans une table.</p>
<div class="proof theorem admonition" id="theorem-12">
<p class="admonition-title"><span class="caption-number">Theorem 7 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(X_1\)</span> et <span class="math notranslate nohighlight">\(X_2\)</span> deux variables al√©atoires ind√©pendantes, de loi respective <span class="math notranslate nohighlight">\(\mathcal{N}(m_1,\sigma_1)\)</span> et <span class="math notranslate nohighlight">\(\mathcal{N}(m_2,\sigma_2)\)</span>. Alors la variable al√©atoire <span class="math notranslate nohighlight">\(X=\alpha_1X_1+\alpha_2X_2\)</span> admet une loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span> avec</p>
<p><span class="math notranslate nohighlight">\(m = \alpha_1 m_1+\alpha_2 m_2\quad \textrm{et}\quad \sigma_2^2 = \alpha_1^2 \sigma_1+\alpha_2^2 \sigma_2^2\)</span></p>
<p>En particulier, √©tant donn√©es <span class="math notranslate nohighlight">\(n\)</span> variables al√©atoires <span class="math notranslate nohighlight">\(X_1\cdots X_n\)</span> i.i.d. de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>, alors la variable al√©atoire <span class="math notranslate nohighlight">\(\bar X_n = \frac1n \displaystyle\sum_{k=1}^nX_k\)</span> suit une loi normale <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma/\sqrt{n})\)</span>.</p>
</div>
</div><div class="proof remark admonition" id="remark-13">
<p class="admonition-title"><span class="caption-number">Remark 8 </span></p>
<div class="remark-content section" id="proof-content">
<p>Dans ce cas, <span class="math notranslate nohighlight">\(\sqrt{n}\frac{\bar X_n-m}{\sigma}\)</span> suit une loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>.</p>
</div>
</div></div>
<div class="section" id="loi-exponentielle">
<h5>Loi exponentielle<a class="headerlink" href="#loi-exponentielle" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">expon</span>

<span class="n">Lambda</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> 
<span class="n">mean</span><span class="p">,</span><span class="n">var</span> <span class="o">=</span> <span class="n">expon</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">Lambda</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Esp√©rance : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">expon</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">expon</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Esp√©rance :  0.5
Variance :  0.25
Densit√© de probabilit√© :  0.2706705664732254
Fonction de r√©partition :  0.8646647167633873
</pre></div>
</div>
</div>
</div>
<p>On parle de loi de probabilit√© sans m√©moire car elle v√©rifie :
<span class="math notranslate nohighlight">\( (\forall s,t\in(\mathbb{R}^+)^*\; P(X&gt;s+t |X&gt;t) = P(X&gt;s)\)</span></p>
</div>
<div class="section" id="distribution-gamma">
<h5>Distribution Gamma<a class="headerlink" href="#distribution-gamma" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="n">x</span> <span class="o">=</span> <span class="mi">3</span> 
<span class="n">a</span> <span class="o">=</span> <span class="mi">3</span> 
<span class="n">Lambda</span> <span class="o">=</span> <span class="mf">1.8</span> 

<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>  <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span>  <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">Lambda</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  1.6666666666666667
Variance :  0.925925925925926
Densit√© de probabilit√© :  0.11853315025792688
Fonction de r√©partition :  0.9052421318239862
</pre></div>
</div>
</div>
</div>
<p>Les propri√©t√©s de cette loi reposent sur celles de la fonction <span class="math notranslate nohighlight">\(\Gamma(a) = \int_0^{+\infty} x-{a-1}e^{-x}dx\)</span>, int√©grale convergente pour tout <span class="math notranslate nohighlight">\(a&gt;0\)</span>.</p>
<div class="proof theorem admonition" id="theorem-14">
<p class="admonition-title"><span class="caption-number">Theorem 8 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont des variables al√©atoires ind√©pendantes de loi respective <span class="math notranslate nohighlight">\(\gamma(a,\lambda)\)</span> et <span class="math notranslate nohighlight">\(\gamma(b,\lambda)\)</span>, alors <span class="math notranslate nohighlight">\(X=X_1+X_2\)</span> est de loi <span class="math notranslate nohighlight">\(\gamma(a+b,\lambda)\)</span></p>
</div>
</div><div class="proof theorem admonition" id="theorem-15">
<p class="admonition-title"><span class="caption-number">Theorem 9 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Si <span class="math notranslate nohighlight">\(X\)</span> est de loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> alors la variable al√©atoire <span class="math notranslate nohighlight">\(Y=X^2\)</span> admet une loi <span class="math notranslate nohighlight">\(\gamma(\frac12,\frac12)\)</span>.</p>
<p>Etant donn√©es plus g√©n√©ralement <span class="math notranslate nohighlight">\(n\)</span> variables al√©atoires i.i.d. de loi <span class="math notranslate nohighlight">\(\mathcal{N}(m,\sigma)\)</span>, alors  la variable al√©atoire <span class="math notranslate nohighlight">\(V=\displaystyle\sum_{k=1}^n \left (\frac{X_k-m}{\sigma}\right )^2\)</span> admet une loi <span class="math notranslate nohighlight">\(\gamma(\frac{n}{2},\frac12)\)</span>. C‚Äôest la loi du khi-deux √† <span class="math notranslate nohighlight">\(n\)</span> degr√©s de libert√©.</p>
</div>
</div></div>
<div class="section" id="loi-du-khi-deux">
<h5>Loi du Khi-deux<a class="headerlink" href="#loi-du-khi-deux" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">n</span><span class="o">=</span><span class="mi">2</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">chi2</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">chi2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">chi2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance :  4.0
Densit√© de probabilit√© :  0.11156508007421491
Fonction de r√©partition :  0.7768698398515702
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loi-de-student">
<h5>Loi de Student<a class="headerlink" href="#loi-de-student" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">t</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  0.0
Variance :  inf
Densit√© de probabilit√© :  0.027410122234342152
Fonction de r√©partition :  0.9522670168666454
</pre></div>
</div>
</div>
</div>
<p>L‚Äôutilisation pratique de cette loi est √©nonc√©e par le th√©or√®me suivant :</p>
<div class="proof theorem admonition" id="theorem-16">
<p class="admonition-title"><span class="caption-number">Theorem 10 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient deux variables al√©atoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> ind√©pendantes, de loi respective <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> et <span class="math notranslate nohighlight">\(\chi_n^2\)</span>. Alors la variable al√©atoire <span class="math notranslate nohighlight">\(T=\frac{X}{\sqrt{Y/n}}\)</span> admet une loi de Student √† <span class="math notranslate nohighlight">\(n\)</span> degr√©s de libert√©.</p>
</div>
</div></div>
<div class="section" id="loi-de-fisher-snedecor">
<h5>Loi de Fisher-Sn√©d√©cor<a class="headerlink" href="#loi-de-fisher-snedecor" title="Permalink to this heading">#</a></h5>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">f</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">m</span><span class="o">=</span><span class="mi">4</span>
<span class="n">x</span><span class="o">=</span><span class="mi">3</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span><span class="n">moments</span><span class="o">=</span><span class="s1">&#39;mv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Moyenne : &quot;</span><span class="p">,</span>  <span class="n">mean</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variance : &quot;</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Densit√© de probabilit√© : &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fonction de r√©partition : &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Moyenne :  2.0
Variance :  inf
Densit√© de probabilit√© :  0.06399999999999996
Fonction de r√©partition :  0.84
</pre></div>
</div>
</div>
</div>
<p>L‚Äôutilisation pratique de cette loi est √©nonc√©e par le th√©or√®me suivant :</p>
<div class="proof theorem admonition" id="theorem-17">
<p class="admonition-title"><span class="caption-number">Theorem 11 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soient deux variables al√©atoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> ind√©pendantes, de loi respective <span class="math notranslate nohighlight">\(\chi_n^2\)</span> et <span class="math notranslate nohighlight">\(\chi_m^2\)</span>. Alors la variable al√©atoire <span class="math notranslate nohighlight">\(T=\frac{X/n}{Y/m}\)</span> admet une loi de Fisher-Sn√©d√©cor √† <span class="math notranslate nohighlight">\(n\)</span> et <span class="math notranslate nohighlight">\(m\)</span> degr√©s de libert√©.</p>
</div>
</div></div>
</div>
</div>
</div>
<span id="document-statsdescriptives"></span><div class="tex2jax_ignore mathjax_ignore section" id="statistique-descriptive">
<h2>Statistique descriptive<a class="headerlink" href="#statistique-descriptive" title="Permalink to this heading">#</a></h2>
<div class="section" id="definitions">
<h3>D√©finitions<a class="headerlink" href="#definitions" title="Permalink to this heading">#</a></h3>
<p>Dans la suite, nous nous int√©ressons √† des unit√©s statistiques ou individus statistiques ou unit√©s d‚Äôobservation (individus,  entreprises,  m√©nages, donn√©es abstraites‚Ä¶). Bien que le cas infini soit envisageable, nous nous restreignons ici √† l‚Äô√©tude d‚Äôun nombre fini de ces unit√©s. Un ou plusieurs caract√®res (ou variables) est mesur√© sur chaque unit√©. Les variables sont d√©sign√©es par simplicit√© par une lettre. Leurs valeurs possibles sont appel√©es modalit√©s et l‚Äôensemble des valeurs possibles ou des modalit√©s est appel√© le domaine. L‚Äôensemble des individus statistiques forme la population.</p>
<div class="section" id="typologie-des-variables">
<h4>Typologie des variables<a class="headerlink" href="#typologie-des-variables" title="Permalink to this heading">#</a></h4>
<p>La typologie des variables d√©finit le type de probl√®me statistique que l‚Äôon doit aborder :</p>
<div class="proof definition admonition" id="definition-0">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 17 </span> (Variable qualitative)</p>
<div class="definition-content section" id="proof-content">
<p>La variable est dite qualitative lorsque les modalit√©s sont des cat√©gories. Suivant qu‚Äôil existe une relation d‚Äôordre sur les cat√©gories, on distingue :</p>
<ul class="simple">
<li><p>la variable qualitative nominale, si les modalit√©s  ne peuvent pas √™tre ordonn√©es</p></li>
<li><p>la variable qualitative ordinale, si les modalit√©s peuvent √™tre ordonn√©es</p></li>
</ul>
</div>
</div><div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 18 </span> (Variable quantitative)</p>
<div class="definition-content section" id="proof-content">
<p>La variable est dite quantitative lorsque les modalit√©s sont des valeurs num√©riques (scalaires ou vectorielles) :</p>
<ul class="simple">
<li><p>la variable est quantitative discr√®te si les modalit√©s forment un ensemble d√©nombrable</p></li>
<li><p>la variable quantitative est continue si les modalit√©s vivent dans un espace continu.</p></li>
</ul>
</div>
</div><p>Dans certains cas (l‚Äô√¢ge par exemple), une variable d‚Äôun type (quantitative continue ici) peut √™tre exprim√©e d‚Äôune autre mani√®re pour des raisons pratiques de collecte ou de mesure. De m√™me, les variables qualitatives ordinales peuvent √™tre cod√©es, par exemple selon une √©chelle de satisfaction.</p>
<div class="proof definition admonition" id="definition-2">
<span id="index-2"></span><p class="admonition-title"><span class="caption-number">Definition 19 </span> (S√©rie statistique)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle s√©rie statistique une suite de <span class="math notranslate nohighlight">\(n\)</span> valeurs prises par une variable <span class="math notranslate nohighlight">\(X\)</span> sur les unit√©s d‚Äôobservation, not√©es <span class="math notranslate nohighlight">\(x_1\cdots x_n\)</span>.</p>
</div>
</div></div>
<div class="section" id="variable-qualitative-nominale">
<h4>Variable qualitative nominale<a class="headerlink" href="#variable-qualitative-nominale" title="Permalink to this heading">#</a></h4>
<p id="index-3">Une variable qualitative nominale a des valeurs distinctes qui ne peuvent pas √™tre ordonn√©es. On note <span class="math notranslate nohighlight">\(J\)</span> le nombre de valeurs distinctes ou de modalit√©s, not√©es <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span>. On appelle effectif d‚Äôune modalit√© ou d‚Äôune valeur distincte le nombre de fois que cette modalit√© (ou valeur distincte) appara√Æt dans la s√©rie statistique. On note <span class="math notranslate nohighlight">\(n_j\)</span> l‚Äôeffectif de la modalit√© <span class="math notranslate nohighlight">\(x_j\)</span>. La fr√©quence d‚Äôune modalit√© <span class="math notranslate nohighlight">\(j\)</span> est  alors √©gale √† <span class="math notranslate nohighlight">\(f_j=\frac{n_j}{n}\)</span>.</p>
<p>Le tableau statistique d‚Äôune variable qualitative nominale peut √™tre repr√©sent√© par deux types de graphiques. Les effectifs sont repr√©sent√©s par un diagramme en tuyau d‚Äôorgue et les fr√©quences par un diagramme en secteurs. Pour ce dernier, si le nombre de modalit√©s devient trop important, la repr√©sentation perd de son int√©r√™t.</p>
<p><img alt="" src="_images/baton.png" /></p>
</div>
<div class="section" id="variable-qualitative-ordinale">
<h4>Variable qualitative ordinale<a class="headerlink" href="#variable-qualitative-ordinale" title="Permalink to this heading">#</a></h4>
<p id="index-4">Le domaine peut √™tre muni d‚Äôune relation d‚Äôordre.  Les valeurs distinctes d‚Äôune variable ordinale peuvent donc √™tre ordonn√©es <span class="math notranslate nohighlight">\(x_1\leq x_2\cdots\leq  x_J\)</span>, √† permutation pr√®s dans l‚Äôordre croissant des indices. L‚Äôeffectif cumul√© <span class="math notranslate nohighlight">\(N_j\)</span> et la fr√©quence cumul√©e <span class="math notranslate nohighlight">\(F_j\)</span> des variables sont alors d√©finis par
<span class="math notranslate nohighlight">\((\forall j\in[\![1,J]\!])\quad N_j=\displaystyle\sum_{i=1}^j n_i\quad \textrm {et}\quad F_j=\displaystyle\sum_{i=1}^j f_i\)</span></p>
<p>Les fr√©quences et les effectifs (cumul√©s ou non) peuvent √™tre repr√©sent√©s sous la forme d‚Äôun diagramme en tuyaux d‚Äôorgue.</p>
</div>
<div class="section" id="variable-quantitative-discrete">
<h4>Variable quantitative discr√®te<a class="headerlink" href="#variable-quantitative-discrete" title="Permalink to this heading">#</a></h4>
<p id="index-5">Le domaine d‚Äôune telle variable est d√©nombrable. Comme pour les variables qualitatives ordinales, on peut calculer les effectifs (cumul√©s ou non) et les fr√©quences (cumul√©es ou non).</p>
<p>La r√©partition des valeurs de la variable peut √™tre repr√©sent√©e par un diagramme en b√¢tonnets. Les fr√©quences cumul√©es  sont visualis√©es par la fonction de r√©partition de la variable , d√©finie par</p>
<p><span class="math notranslate nohighlight">\(F(x) = \left \{
\begin{eqnarray}
0&amp;\textrm{ si} &amp;x&lt;x_1\\
F_j &amp;\textrm{ si}&amp;  x\in[x_j,x_{j+1}[\\
1&amp; \textrm{ si}&amp;  x_J\leq x
\end{eqnarray}\right .\)</span></p>
<p><img alt="" src="_images/baton2.png" /></p>
</div>
<div class="section" id="variable-quantitative-continue">
<h4>Variable quantitative continue<a class="headerlink" href="#variable-quantitative-continue" title="Permalink to this heading">#</a></h4>
<p id="index-6">Le domaine d‚Äôune  variable quantitative continue est infini et est assimil√© √† <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> ou √† un intervalle de <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Cependant, la mesure √©tant limit√©e en pr√©cision, on peut traiter ces variables comme des variables discr√®tes.</p>
<p>La repr√©sentation graphique de ces variables (et la construction du tableau statistique) passe par le regroupement des modalit√©s ou valeurs en classes. Le tableau ainsi construit est souvent appel√© distribution group√©e. La classe <span class="math notranslate nohighlight">\(j\)</span> est l‚Äôensemble des valeurs incluses dans <span class="math notranslate nohighlight">\([c^-_j,c^+_j[\)</span>, o√π <span class="math notranslate nohighlight">\(c^-_j\)</span> et <span class="math notranslate nohighlight">\(c^+_j\)</span> sont les bornes inf√©rieure et sup√©rieure de la classe. Sur cet intervalle, on peut calculer la fr√©quence <span class="math notranslate nohighlight">\(f_j\)</span> de la classe, la fr√©quence cumul√©e, l‚Äôeffectif <span class="math notranslate nohighlight">\(n_j\)</span>‚Ä¶ La r√©partition en classes n√©cessite de d√©finir a priori le nombre de classes <span class="math notranslate nohighlight">\(J\)</span> et l‚Äôamplitude <span class="math notranslate nohighlight">\(a_j\)</span> des intervalles. Si elles peuvent √™tre d√©finies de mani√®re empirique, quelques r√®gles permettent d‚Äô√©tablir <span class="math notranslate nohighlight">\(J\)</span> et l‚Äôamplitude pour une s√©rie statistique de <span class="math notranslate nohighlight">\(n\)</span> observations. Par exemple :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(J=1+3.3log_{10}(n)\)</span> (r√®gle de Sturge)</p></li>
<li><p><span class="math notranslate nohighlight">\(J=2.5\sqrt[4\,]{n}\)</span> (r√®gle de Yule)</p></li>
</ul>
<p>La repr√©sentation graphique se fait par exemple par histogramme.
Les histogrammes sont des repr√©sentations de la distribution des donn√©es, agr√©g√©es par intervalles. A partir de l‚Äô√©tendue des donn√©es, on subdivise l‚Äôintervalle en <span class="math notranslate nohighlight">\(k\)</span> bins, de tailles <span class="math notranslate nohighlight">\(t_k\)</span> non n√©cessairement identiques, et on compte le nombre d‚Äôindividus <span class="math notranslate nohighlight">\(n_k\)</span> rentrant dans chaque bin. L‚Äôhistogramme peut alors √™tre :</p>
<ul class="simple">
<li><p>non normalis√© : <span class="math notranslate nohighlight">\(h_k = n_k\)</span></p></li>
<li><p>normalis√©: <span class="math notranslate nohighlight">\(h_k = n_k/t_k\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Comptage des individus</span>
<span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">findBin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="nb">bin</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bins</span><span class="p">):</span>
            <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="nb">bin</span>
            <span class="k">if</span> <span class="n">left</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">i</span>
        <span class="k">return</span> <span class="kc">None</span>
    
    <span class="n">count</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">bins</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">findBin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">count</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">count</span>

        
<span class="c1"># Affichage de l&#39;histogramme</span>
<span class="k">def</span> <span class="nf">plot_hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span>  <span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">,</span><span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">bins</span> <span class="o">=</span><span class="p">[</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="n">bin_width</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">)</span> <span class="p">]</span>
    <span class="n">bin_left</span> <span class="o">=</span> <span class="p">[</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">bins</span> <span class="p">]</span>
    <span class="n">bin_widths</span> <span class="o">=</span> <span class="p">[</span> <span class="n">r</span><span class="o">-</span><span class="n">l</span>  <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">r</span> <span class="ow">in</span> <span class="n">bins</span> <span class="p">]</span>
    <span class="n">bin_height</span> <span class="o">=</span> <span class="p">[</span> 
        <span class="nb">float</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span> <span class="k">if</span> <span class="n">normed</span> <span class="k">else</span> <span class="n">c</span> 
        <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">count</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bins</span><span class="p">),</span> <span class="n">bin_widths</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">bin_left</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="n">bin_width</span><span class="p">,</span><span class="n">height</span><span class="o">=</span><span class="n">bin_height</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">bin_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">bin_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">subplot</span><span class="p">,</span> <span class="n">binsize</span> <span class="ow">in</span> <span class="p">((</span><span class="mi">141</span><span class="p">,</span> <span class="mi">5</span><span class="p">),(</span><span class="mi">142</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">143</span><span class="p">,</span> <span class="mi">80</span><span class="p">),</span> <span class="p">(</span><span class="mi">144</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Taille des bins : &#39;</span><span class="p">,</span> <span class="n">binsize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">subplot</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plot_hist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">bin_min</span><span class="p">,</span> <span class="n">bin_max</span><span class="p">,</span> <span class="n">binsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/68753ea9f1244fce844333bff92b83bb3502b3dae3e8026b3dd5d3e6598c8a48.png" src="_images/68753ea9f1244fce844333bff92b83bb3502b3dae3e8026b3dd5d3e6598c8a48.png" />
</div>
</div>
<p>Le choix de la largeur <span class="math notranslate nohighlight">\(t\)</span> des bins d√©pend des donn√©es, et par exemple on a :</p>
<ul class="simple">
<li><p>Loi de Scott : <span class="math notranslate nohighlight">\(t = \frac{3.5 \sigma}{Card(X)^{1/3}}\)</span>, o√π <span class="math notranslate nohighlight">\(\sigma\)</span> est l‚Äô√©cart type des donn√©es.</p></li>
<li><p>Loi de Freedman‚ÄìDiaconis : <span class="math notranslate nohighlight">\( t = \frac{2 IQR}{Card(X)^{1/3}}\)</span>, o√π <span class="math notranslate nohighlight">\(IQR\)</span> est la distance interquartile.</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 9 </span></p>
<div class="remark-content section" id="proof-content">
<p>Toutes les classes n‚Äôont pas n√©cessairement la m√™me amplitude</p>
</div>
</div><p>Les effectifs (ou les fr√©quences) sont repr√©sent√©(e)s par un histogramme. Si l‚Äôon s‚Äôint√©resse √† la repr√©sentation des effectifs (resp. des fr√©quences), la densit√© d‚Äôeffectif <span class="math notranslate nohighlight">\(h_j\)</span> (resp. de fr√©quence <span class="math notranslate nohighlight">\(d_j\)</span>),  d√©finie par <span class="math notranslate nohighlight">\(h_j=\frac{n_j}{a_j}\)</span> (resp. <span class="math notranslate nohighlight">\(d_j=\frac{f_j}{a_j}\)</span>), d√©termine la hauteur du rectangle repr√©sentant la classe <span class="math notranslate nohighlight">\(j\)</span>. L‚Äôaire de l‚Äôhistogramme est √©gale √† l‚Äôeffectif total <span class="math notranslate nohighlight">\(n\)</span> pour l‚Äôhistogramme des effectifs, et √† 1 pour l‚Äôhistogramme des fr√©quences.</p>
<p>Comme dans le cas discret, la fonction de r√©partition peut √™tre calcul√©e de la mani√®re suivante :</p>
<p><span class="math notranslate nohighlight">\(F(x) = \left \{
\begin{eqnarray}
0&amp;\textrm{ si}&amp; x&lt;c^-_1\\
F_{j-1}+\frac{f_j}{c^+_j-c^-_j}(x-c^-_j) &amp;\textrm{ si}&amp; x\in[c^-_j,c^+_j[\\
1&amp; \textrm{ si}&amp;c^+_J\leq x
\end{eqnarray}\right .\)</span></p>
</div>
</div>
<div class="section" id="pre-traitement-des-donnees">
<h3>Pr√©-traitement des donn√©es<a class="headerlink" href="#pre-traitement-des-donnees" title="Permalink to this heading">#</a></h3>
<p>Faire une analyse de donn√©es, c‚Äôest traiter un tableau de taille <span class="math notranslate nohighlight">\(n\times d\)</span> o√π <span class="math notranslate nohighlight">\(n\)</span> est le nombre d‚Äôindividus et <span class="math notranslate nohighlight">\(d\)</span> le nombre de variables (caract√®res) mesur√©es sur ces individus. En raison de la colecte des donn√©es, des erreurs de mesure ou d‚Äôautres facteurs, ce tableau est parfois incomplet et il convient de le pr√©traiter pour pouvoir effectuer l‚Äôanalyse.</p>
<div class="section" id="points-aberrants">
<h4>Points aberrants<a class="headerlink" href="#points-aberrants" title="Permalink to this heading">#</a></h4>
<p>Une anomalie (ou point aberrant, ou outlier) est une observation (ou un sous-ensemble d‚Äôobservations) qui semble incompatible avec le reste de l‚Äôensemble de donn√©es.</p>
<p>S‚Äôil est parfois possible d‚Äôidentifier graphiquement ces points aberrants √† l‚Äôaide de bo√Ætes √† moustaches (voir <a class="reference internal" href="#boxplot"><span class="std std-ref">Pour r√©sumer</span></a>), il existe une vaste litt√©rature sur la d√©tection d‚Äôanomalies qu‚Äôil n‚Äôest pas possible d‚Äôaborder ici. De plus, suivant le type de donn√©es manipul√©es (donn√©es s√©quentielles ou non), le type de m√©thode peut √™tre diff√©rent. On mentionne donc ici quelques techniques simples :</p>
<ul class="simple">
<li><p>le d√©tecteur de Hampel : on consid√®re que <span class="math notranslate nohighlight">\(x_i\)</span> est un point aberrant si</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|x_i-x_{\frac12}|&gt;3.MADM\]</div>
<p>o√π <span class="math notranslate nohighlight">\(MADM = 1.4826.|x_i-x_{\frac12}|_\frac12\)</span>, et o√π <span class="math notranslate nohighlight">\(y_{\frac12}\)</span> est la m√©diane des donn√©es <span class="math notranslate nohighlight">\(y\)</span></p>
<ul class="simple">
<li><p>la r√®gle empirique de l‚Äô√©cart-type : on consid√®re que <span class="math notranslate nohighlight">\(x_i\)</span> est un point aberrant si</p></li>
</ul>
<div class="math notranslate nohighlight">
\[|x_i-\bar x|&gt;3.\sigma\]</div>
<p>o√π  <span class="math notranslate nohighlight">\(\bar x\)</span> (respectivement <span class="math notranslate nohighlight">\(\sigma\)</span>) est la moyenne (resp. l‚Äô√©cart-type ) des donn√©es.</p>
<ul class="simple">
<li><p>la m√©thode LOF (Local Outlier Factor) qui repose sur le concept de densit√© locale, o√π la localit√© est donn√©e par les <span class="math notranslate nohighlight">\(k\)</span> voisins les plus proches, dont la distance est utilis√©e pour estimer la densit√©. En comparant la densit√© locale d‚Äôun objet aux densit√©s locales de ses voisins, il est possible d‚Äôidentifier des r√©gions de densit√© similaire et des points dont la densit√© est nettement inf√©rieure √† celle de leurs voisins. Ces derniers sont consid√©r√©s comme des valeurs aberrantes. La densit√© locale est estim√©e par la distance typique √† laquelle un point peut √™tre atteint √† partir de ses voisins.</p></li>
<li><p>la m√©thode COF (Connectivity based Outlier Factor) bas√©e sur le m√™me principe que LOF, √† ceci pr√®s que l‚Äôestimation de densit√© est effectu√©e en utilisant le minimum de la somme des distances reliant tous les voisins d‚Äôun point donn√©.</p></li>
</ul>
</div>
<div class="section" id="donnees-manquantes">
<h4>Donn√©es manquantes<a class="headerlink" href="#donnees-manquantes" title="Permalink to this heading">#</a></h4>
<p>Lors de la collecte des donn√©es, il arrive que certaines d‚Äôentre elles ne soient pas disponibles ou enregistr√©es. On distingue trois types de donn√©es manquantes :</p>
<ol class="arabic simple">
<li><p>les donn√©es manquant de mani√®re compl√®tement al√©atoire :  la probabilit√© qu‚Äôune donn√©e soit manquante ne d√©pend pas des valeurs connues ni de la valeur manquante elle-m√™me.</p></li>
<li><p>les donn√©es manquant de mani√®re al√©atoire :  la probabilit√© qu‚Äôune donn√©e soit manquante peut d√©pendre de valeurs connues (d‚Äôautres variables parmi les <span class="math notranslate nohighlight">\(d\)</span>), mais pas de la variable dont les valeurs sont manquantes.</p></li>
<li><p>les donn√©es manquant de mani√®re non al√©atoire : la probabilit√© qu‚Äôune donn√©e soit manquante d√©pend d‚Äôautres variables qui ont √©galement des valeurs manquantes, ou elle d√©pend de la variable elle-m√™me.</p></li>
</ol>
<p>Pour r√©soudre ce probl√®me de donn√©es manquantes, dans la mesure o√π ces derni√®res ne sont pas trop nombreuses, on a recours √† des techniques d‚Äô<strong>imputation</strong>.</p>
<p>Dans le cas d‚Äôune imputation simple (une seule donn√©e manquante), on peut par exemple remplacer la valeur manquante dans une colonne <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span> par :</p>
<ul class="simple">
<li><p>une valeur fixe</p></li>
<li><p>une statistique sur la colonne <span class="math notranslate nohighlight">\(j\)</span> (la plus petite ou la plus grande valeur, la moyenne de la colonne, la valeur la plus fr√©quente‚Ä¶)</p></li>
<li><p>une valeur issue des <span class="math notranslate nohighlight">\(k\)</span> plus proches voisins de la ligne du tableau o√π la valeur en position <span class="math notranslate nohighlight">\(j\)</span> est manquante</p></li>
<li><p>une valeur calcul√©e par r√©gression (voir chapitre correspondant) sur l‚Äôensemble du tableau</p></li>
<li><p>la valeur pr√©c√©dente (ou suivante) dans le cas o√π la colonne est une s√©rie ordonn√©e ou temporelle.</p></li>
</ul>
<p>Dans le cas d‚Äôune imputation multiple, o√π un sous-ensemble de valeurs doit √™tre combl√©, on peut adopter la strat√©gie suivante :</p>
<ol class="arabic simple">
<li><p>Effectuer une imputation simple pour toutes les valeurs manquantes de l‚Äôensemble de donn√©es.</p></li>
<li><p>Remettre les valeurs manquantes d‚Äôune variable <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span> √† ‚Äúmanquante‚Äù.</p></li>
<li><p>Former un mod√®le pour pr√©dire les valeurs manquantes de <span class="math notranslate nohighlight">\(j\)</span> en utilisant les valeurs disponibles de la variable <span class="math notranslate nohighlight">\(j\)</span> en tant que variable d√©pendante et les autres variables de l‚Äôensemble de donn√©es comme ind√©pendantes.</p></li>
<li><p>Pr√©dire les valeurs manquantes dans la colonne <span class="math notranslate nohighlight">\(j\)</span> en utilisant le mod√®le entra√Æn√© √† l‚Äô√©tape 3.</p></li>
<li><p>R√©p√©ter les √©tapes 2 √† 4 pour toutes les autres colonnes pr√©sentant des valeurs manquantes.</p></li>
<li><p>R√©p√©ter l‚Äô√©tape 2-5 jusqu‚Äô√† convergence (ou un nombre maximal d‚Äôit√©rations)</p></li>
<li><p>R√©p√©ter les √©tapes 1-6 plusieurs fois avec diff√©rentes initialisations de nombres al√©atoires pour cr√©er diff√©rentes versions de l‚Äôensemble de donn√©es complet/imput√©.</p></li>
</ol>
</div>
<div class="section" id="transformation-des-donnees-qualitatives">
<h4>Transformation des donn√©es qualitatives<a class="headerlink" href="#transformation-des-donnees-qualitatives" title="Permalink to this heading">#</a></h4>
<p>Pour pouvoir √™tre trait√©es num√©riquement, les donn√©es qualitatives doivent √™tre transform√©es. Plusieurs techniques existent parmi lesquelles :</p>
<ul class="simple">
<li><p>pour le cas des variables ordinales : on utilise le rang pour encoder les modalit√©s de la variable. Par exemple, pour un niveau de diplomation Brevet<span class="math notranslate nohighlight">\(&lt;\)</span>Bac<span class="math notranslate nohighlight">\(&lt;\)</span>Licence<span class="math notranslate nohighlight">\(&lt;\)</span>Master<span class="math notranslate nohighlight">\(&lt;\)</span>Doctorat, on codera Licence par 3 et Doctorat par 5.</p></li>
<li><p>le one-hot encoding : pour une variable qualitative pr√©sentant <span class="math notranslate nohighlight">\(J\)</span> modalit√©s, on construit un vecteur de taille <span class="math notranslate nohighlight">\(J\)</span> dont les composantes sont toutes nulles sauf la <span class="math notranslate nohighlight">\(J\)</span>-√®me qui vaut 1. Par exemple, si <span class="math notranslate nohighlight">\(J\)</span>=3, on construit 1 vecteur de taille 3, et pour un individu ayant la modalit√© 2, on le code en (0 1 0). Lorsque <span class="math notranslate nohighlight">\(J\)</span> est √©lev√©, on se retrouve avec un jeu de donn√©es volumineux.</p></li>
<li><p>les m√©thodes de plongement (embedding) : utilis√©es principalement en apprentissage profond (Deep learning) pour le traitement du langage naturel, ces classes de m√©thodes construisent une repr√©sentation de chaque modalit√© d‚Äôune variable qualitative en un vecteur num√©rique de taille fixe et choisie. Pour le mot ‚Äúrouge‚Äù de la variable ‚Äúcouleur‚Äù, par exemple, l‚Äôencodage peut par exemple √™tre repr√©sent√© par le vecteur (0.31 0.57 0.12). En pratique, le calcul de ces repr√©sentations s‚Äôeffectue classiquement par l‚Äôentra√Ænement d‚Äôun r√©seau de neurones ayant pour entr√©e uniquement les variables qualitatives. Tout d‚Äôabord, un encodage one-hot est appliqu√© √† la variable afin d‚Äô√™tre mise en entr√©e du r√©seau, qui n‚Äôaccepte que les entr√©es num√©riques. La sortie d‚Äôune des couches cach√©es du r√©seau constitue alors le vecteur recherch√©. On concat√®ne ensuite ce vecteur aux donn√©es initiales, utilis√©es dans l‚Äôajustement du mod√®le final.</p></li>
</ul>
</div>
<div class="section" id="normalisation">
<h4>Normalisation<a class="headerlink" href="#normalisation" title="Permalink to this heading">#</a></h4>
<p>Il arrive que les donn√©es collect√©es ne soient pas du m√™me ordre de grandeur, notamment en raison des unit√©s de mesure (un individu mesur√© par sa taille en millim√®tres et son poids en tonnes par exemple). Cette diff√©rence de valeur absolue introduit un biais dans l‚Äôanalyse des donn√©es (<a class="reference internal" href="#biais"><span class="std std-ref">figure 1</span></a>) qu‚Äôil convient de corriger. C‚Äôest le processus de normalisation des donn√©es.</p>
<p>Pour une colonne <span class="math notranslate nohighlight">\(j\in[\![1,p]\!]\)</span>, on dispose de <span class="math notranslate nohighlight">\(n\)</span> valeurs <span class="math notranslate nohighlight">\(x_{ij},i\in[\![1,n]\!]\)</span>. On note : <span class="math notranslate nohighlight">\(x_{min} = \displaystyle\min_{i\in[\![1,n]\!]}x_{ij}\)</span>, <span class="math notranslate nohighlight">\(x_{max} = \displaystyle\max_{i\in[\![1,n]\!]}x_{ij}\)</span>,   <span class="math notranslate nohighlight">\(\bar x_j\)</span> la moyenne des <span class="math notranslate nohighlight">\(x_{ij}\)</span>, <span class="math notranslate nohighlight">\(\sigma_j\)</span> leur √©cart-type, <span class="math notranslate nohighlight">\(x_\frac14, x_\frac12\)</span> et <span class="math notranslate nohighlight">\(x_\frac34\)</span> les premier, deuxi√®me et troisi√®me quartiles. On distingue alors classiquement trois types de normalisation :</p>
<ol class="arabic simple">
<li><p>la normalisation min-max : <span class="math notranslate nohighlight">\(x_{ij} = \frac{x_{ij}-x_{min}}{x_{max}-x_{min}}\)</span></p></li>
<li><p>la normalisation standard : <span class="math notranslate nohighlight">\(x_{ij}=\frac{x_{ij}-\bar x_j}{\sigma_j}\)</span></p></li>
<li><p>la normalisation robuste : <span class="math notranslate nohighlight">\(x_{ij}=\frac{x_{ij}-x_\frac12}{x_\frac34-x_\frac14}\)</span></p></li>
</ol>
<p>La normalisation standard d√©pend de la pr√©sence de points aberrants (qui affectent la moyenne).</p>
<div class="figure align-default" id="biais">
<img alt="_images/normK.png" src="_images/normK.png" />
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">Effet de la normalisation sur un algorithme de classification (voir chapitre correspondant). En haut un jeu de donn√©es avec deux nuages de points allong√©s selon l‚Äôaxe des <span class="math notranslate nohighlight">\(x\)</span>, certainement en raison d‚Äôune diff√©rence d‚Äô√©chelle entre les unit√©s de mesure de <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>. Au milieu une classification par <span class="math notranslate nohighlight">\(k\)</span>-moyennes, <span class="math notranslate nohighlight">\(k\)</span>=2 sans normalisation, en utilisant la distance euclidienne. Les deux classes sont s√©par√©es suivant l‚Äôaxe des <span class="math notranslate nohighlight">\(x\)</span>, ne refl√©tant pas la r√©partition naturelle des points. En bas, apr√®s normalisation, les deux nuages de points sont correctement s√©par√©s</span><a class="headerlink" href="#biais" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
<div class="section" id="statistique-descriptive-univariee">
<h3>Statistique descriptive univari√©e<a class="headerlink" href="#statistique-descriptive-univariee" title="Permalink to this heading">#</a></h3>
<p id="index-7">La statistique descriptive univari√©e consiste √† √©tudier un ensemble d‚Äôunit√©s d‚Äôobservations, lorsque celles-ci sont d√©crites par une seule variable.</p>
<p>Soit donc <span class="math notranslate nohighlight">\(X\)</span> une variable et <span class="math notranslate nohighlight">\(x_j,j\in [\![1,n]\!]\)</span> l‚Äôensemble des valeurs prises par cette variable, <span class="math notranslate nohighlight">\(n_i\)</span> √©tant le nombre de fois o√π la valeur <span class="math notranslate nohighlight">\(x_i\)</span> est prise. <span class="math notranslate nohighlight">\(X\)</span> peut √™tre qualitative ou quantitative, les param√®tres de description d√©crits dans la suite s‚Äôappliqueront √† l‚Äôune de ces natures ou au deux.</p>
<div class="section" id="parametres-de-position">
<h4>Param√®tres de position<a class="headerlink" href="#parametres-de-position" title="Permalink to this heading">#</a></h4>
<p>Plusieurs param√®tres permettent de d√©crire la position ‚Äúla plus repr√©sentative‚Äù d‚Äôune variable :</p>
<div class="proof definition admonition" id="definition-4">
<p class="admonition-title"><span class="caption-number">Definition 20 </span> (Mode)</p>
<div class="definition-content section" id="proof-content">
<p>Le mode est la valeur distincte correspondant √† l‚Äôeffectif le plus √©lev√©. Il est not√© <span class="math notranslate nohighlight">\(x_M\)</span>.</p>
</div>
</div><p>Le mode peut √™tre calcul√© pour tout type de variable, n‚Äôest pas n√©cessairement unique. Lorsqu‚Äôune variable continue est d√©coup√©e en classes, il est possible de d√©finir une classe modale (classe correspondant √† l‚Äôeffectif le plus √©lev√©)</p>
<div class="proof definition admonition" id="definition-5">
<p class="admonition-title"><span class="caption-number">Definition 21 </span> (Moyennes)</p>
<div class="definition-content section" id="proof-content">
<p>Les moyennes ne peuvent √™tre d√©finies que sur des variables quantitatives. Plusieurs moyennes peuvent √™tre calcul√©es, parmi lesquelles :</p>
<ul class="simple">
<li><p>la moyenne <strong>arithm√©tique</strong>  <span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{n}{\displaystyle\sum_{i=1}^nx_i}=  \frac{1}{n}{\displaystyle\sum_{i=1}^J n_ix_i}\)</span>. C‚Äôest le moment √† l‚Äôorigine d‚Äôordre 1.</p></li>
<li><p>la moyenne <strong>g√©om√©trique</strong> : si les <span class="math notranslate nohighlight">\(x_i\)</span> sont positifs, la moyenne g√©om√©trique est la quantit√© <span class="math notranslate nohighlight">\(G=\left (\displaystyle\prod_{i=1}^n x_i\right )^\frac{1}{n}\)</span>. C‚Äôest donc l‚Äôexponentielle de la moyenne arithm√©tique des logarithmes des valeurs observ√©es.</p></li>
<li><p>la moyenne <strong>harmonique</strong> : si les <span class="math notranslate nohighlight">\(x_i\)</span> sont positifs, la moyenne harmonique est d√©finie par <span class="math notranslate nohighlight">\(H=\frac{n}{\displaystyle\sum_{i=1}^J 1/x_i}\)</span></p></li>
<li><p>la moyenne <strong>pond√©r√©e</strong> : dans certains cas, on n‚Äôaccorde pas la m√™me importance √† toutes les observations (fiabilit√©, confiance‚Ä¶). La moyenne pond√©r√©e est alors d√©finie par
<span class="math notranslate nohighlight">\(\bar{x}_w= \frac{\displaystyle\sum_{i=1}^n w_ix_i}{\displaystyle\sum_{i=1}^n w_i}\)</span></p></li>
</ul>
</div>
</div><p>Dans le cas o√π <span class="math notranslate nohighlight">\(\forall i,w_i=1/n\)</span>, la moyenne pond√©r√©e est la moyenne arithm√©tique. De plus, dans tous les cas, on peut montrer que <span class="math notranslate nohighlight">\(H\leq G\leq \bar{x}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">ArithmeticMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># calculable directement avec np.mean(X)</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">GeometricMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">p</span><span class="o">=</span><span class="mi">1</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">p</span><span class="o">*=</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">p</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">HarmonicMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">/</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">s</span>

<span class="k">def</span> <span class="nf">WeightedMean</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># Exemples de poids</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">weights</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="p">((</span><span class="n">ArithmeticMean</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;Arithm√©tique&#39;</span><span class="p">),(</span><span class="n">GeometricMean</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;G√©om√©trique&#39;</span><span class="p">),</span>
                             <span class="p">(</span><span class="n">HarmonicMean</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;Harmonique&#39;</span><span class="p">),(</span><span class="n">WeightedMean</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;Pond√©r√©e&#39;</span><span class="p">)):</span>
    <span class="n">m</span><span class="o">=</span><span class="n">method</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ArithmeticMean  :  1316.3086347078017
GeometricMean  :  1258.4787575642572
HarmonicMean  :  1198.219210728503
WeightedMean  :  1334.7912420657199
</pre></div>
</div>
<img alt="_images/2804fa9a53fac2c66af4fbfaabb6bf50e03c3e6bc0da88aab5fe60d0addf6a53.png" src="_images/2804fa9a53fac2c66af4fbfaabb6bf50e03c3e6bc0da88aab5fe60d0addf6a53.png" />
</div>
</div>
<div class="proof definition admonition" id="definition-6">
<p class="admonition-title"><span class="caption-number">Definition 22 </span> (M√©diane)</p>
<div class="definition-content section" id="proof-content">
<p>La m√©diane, not√©e <span class="math notranslate nohighlight">\(x_\frac{1}{2}\)</span> est la valeur centrale de la s√©rie statistique tri√©e par ordre croissant.</p>
</div>
</div><p>En d‚Äôautres termes, c‚Äôest la valeur de la s√©rie tri√©e telle qu‚Äôau moins 50% des effectifs soient inf√©rieurs √† <span class="math notranslate nohighlight">\(x_\frac{1}{2}\)</span>. Elle peut √™tre calcul√©e sur des variables quantitatives ou qualitatives ordinales (dans le cas o√π des √©chelles de valeur ont √©t√© d√©finies).</p>
<div class="proof definition admonition" id="definition-7">
<p class="admonition-title"><span class="caption-number">Definition 23 </span> (Quantiles)</p>
<div class="definition-content section" id="proof-content">
<p>Le quantile d‚Äôordre <span class="math notranslate nohighlight">\(p\)</span> est d√©fini par <span class="math notranslate nohighlight">\(x_p=F^{-1}(p)\)</span>, o√π <span class="math notranslate nohighlight">\(F\)</span> est la fonction de r√©partition.</p>
</div>
</div><p>La notion de quantile g√©n√©ralise la notion de m√©diane. Si la fonction de r√©partition √©tait continue et strictement croissante, la d√©finition de <span class="math notranslate nohighlight">\(x_p\)</span> serait unique. Or <span class="math notranslate nohighlight">\(F\)</span> est discontinue et d√©finie par paliers et les valeurs de quantiles varient suivant par exemple l‚Äôutilisation ou non d‚Äôune m√©thode d‚Äôinterpolation de <span class="math notranslate nohighlight">\(F\)</span>. Pour calculer <span class="math notranslate nohighlight">\(x_p\)</span>, on peut par exemple consid√©rer que si <span class="math notranslate nohighlight">\(np\)</span> est pair,
<span class="math notranslate nohighlight">\(x_p=\frac{x_{np}+x_{np+1}}{2}\)</span>
on remarque alors que la m√©diane est le quantile d‚Äôordre <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>
et sinon
<span class="math notranslate nohighlight">\(x_p=x_{\lceil{np}\rceil}\)</span>
En particulier, un quartile est chacune des 3 valeurs qui divisent les donn√©es tri√©es en 4 parts √©gales, de sorte que chaque partie repr√©sente 1/4 de l‚Äô√©chantillon de population. On note <span class="math notranslate nohighlight">\(Q_i\)</span> le <span class="math notranslate nohighlight">\(i^e\)</span> quartile.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
    
<span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">style</span>  <span class="ow">in</span> <span class="p">((</span><span class="mi">25</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">),(</span><span class="mi">50</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">),(</span><span class="mi">75</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">)):</span>
    <span class="n">m</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">q</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;quartile &quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>quartile  25  :  905.9190521240237
quartile  50  :  1399.66320800781
quartile  75  :  1626.326538085935
</pre></div>
</div>
<img alt="_images/e2dcdea760a58a0bc06ee4e52de94d5c9add4fc4e0d97921134b4fe12767c888.png" src="_images/e2dcdea760a58a0bc06ee4e52de94d5c9add4fc4e0d97921134b4fe12767c888.png" />
</div>
</div>
</div>
<div class="section" id="parametres-de-dispersion">
<h4>Param√®tres de dispersion<a class="headerlink" href="#parametres-de-dispersion" title="Permalink to this heading">#</a></h4>
<p>Il est tr√®s souvent utile d‚Äôappr√©cier la dispersion des mesures autour du param√®tre de position. Pour cela, sur des variables quantitatives uniquement, plusieurs outils sont disponibles :</p>
<div class="proof definition admonition" id="definition-8">
<p class="admonition-title"><span class="caption-number">Definition 24 </span> (Etendue)</p>
<div class="definition-content section" id="proof-content">
<p>L‚Äô√©tendue est la simple diff√©rence entre la plus grande et la plus petite valeur observ√©e.</p>
</div>
</div><div class="proof definition admonition" id="definition-9">
<p class="admonition-title"><span class="caption-number">Definition 25 </span> (D√©viation maximale)</p>
<div class="definition-content section" id="proof-content">
<p>La d√©viation maximale est d√©finie par
<span class="math notranslate nohighlight">\( maxdev(X) = max \{ |x_i - \bar{x}| \,|\, i\in[\![1,n]\!]\}\)</span></p>
</div>
</div><div class="proof definition admonition" id="definition-10">
<p class="admonition-title"><span class="caption-number">Definition 26 </span> (D√©viation moyenne absolue)</p>
<div class="definition-content section" id="proof-content">
<p>La d√©viation moyenne absolue est d√©finie par
<span class="math notranslate nohighlight">\( mad(X) = \frac{1}{n} \displaystyle\sum_{i=1}^n |x_i - \bar{x}|\)</span></p>
</div>
</div><div class="proof definition admonition" id="definition-11">
<p class="admonition-title"><span class="caption-number">Definition 27 </span> (Distance interquartile)</p>
<div class="definition-content section" id="proof-content">
<p>La distance interquartile <span class="math notranslate nohighlight">\(Q_3-Q_1\)</span> est la diff√©rence entre le troisi√®me et le premier quartile. C‚Äôest une statistique robuste aux points aberrants.</p>
</div>
</div><div class="proof definition admonition" id="definition-12">
<p class="admonition-title"><span class="caption-number">Definition 28 </span> (Variance)</p>
<div class="definition-content section" id="proof-content">
<p>La variance est la somme des carr√©s des √©carts √† la moyenne, normalis√©e par le nombre d‚Äôobservations
<span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )^2\)</span></p>
</div>
</div><p>Cette variance est dite biais√©e. La variance non biais√©e est obtenue en divisant non pas par <span class="math notranslate nohighlight">\(n\)</span>, mais par <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<div class="proof definition admonition" id="definition-13">
<p class="admonition-title"><span class="caption-number">Definition 29 </span> (Ecart type)</p>
<div class="definition-content section" id="proof-content">
<p>L‚Äô√©cart type est la racine carr√©e de la variance.</p>
</div>
</div><div class="proof definition admonition" id="definition-14">
<p class="admonition-title"><span class="caption-number">Definition 30 </span> (Ecart moyen absolu)</p>
<div class="definition-content section" id="proof-content">
<p>L‚Äô√©cart moyen absolu est la somme des valeurs absolues des √©carts √† la moyenne divis√©e par le nombre d‚Äôobservations.</p>
</div>
</div><p>Notons qu‚Äôil s‚Äôagit de la distance <span class="math notranslate nohighlight">\(L_1\)</span> du vecteur des observations au vecteur compos√© de la valeur moyenne, divis√© par le nombre d‚Äôobservations. La variance est la distance <span class="math notranslate nohighlight">\(L_2\)</span> entre ces deux vecteurs. Lorsque la distance est calcul√©e par rapport au vecteur compos√© de la valeur m√©diane, on parle d‚Äô√©cart m√©dian absolu.</p>
<p><img alt="" src="_images/dispersion.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;./data/data.csv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">max_dev</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mad</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigma</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">IQR</span><span class="p">(</span><span class="n">X</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">75</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;16&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Points&#39;</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">for</span> <span class="n">method</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span><span class="n">style</span><span class="p">,</span>  <span class="ow">in</span> <span class="p">((</span><span class="n">max_dev</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">),(</span><span class="n">mad</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">),(</span><span class="n">sigma</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">),(</span><span class="n">IQR</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">)):</span>
    <span class="n">s</span><span class="o">=</span><span class="n">method</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot; : &quot;</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="s2">&quot;+/-&quot;</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;black&#39;</span> <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">style</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">method</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">style</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">m</span><span class="o">-</span><span class="n">s</span><span class="p">,</span><span class="n">m</span><span class="o">+</span><span class="n">s</span><span class="p">],[</span><span class="n">pos</span><span class="p">,</span><span class="n">pos</span><span class="p">],</span><span class="n">style</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max_dev  :  1316.3086347078017 +/- 738.0729570890783
mad  :  1316.3086347078017 +/- 327.4656915004233
sigma  :  1316.3086347078017 +/- 374.5723639541368
IQR  :  1316.3086347078017 +/- 720.4074859619113
</pre></div>
</div>
<img alt="_images/6809a268f52e2a602de0d33de2c2d15f89d5a5a8b8b70550ee2f772ca674c7a2.png" src="_images/6809a268f52e2a602de0d33de2c2d15f89d5a5a8b8b70550ee2f772ca674c7a2.png" />
</div>
</div>
</div>
<div class="section" id="parametres-de-forme">
<h4>Param√®tres de forme<a class="headerlink" href="#parametres-de-forme" title="Permalink to this heading">#</a></h4>
<p>Les param√®tres de forme sont souvent calcul√©s en r√©f√©rence √† la forme de la loi normale, pour √©valuer la sym√©trie, l‚Äôaplatissement ou la d√©rive par rapport √† cette loi.</p>
<div class="proof definition admonition" id="definition-15">
<p class="admonition-title"><span class="caption-number">Definition 31 </span> (Skewness)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[g_1 = \frac{m_3}{\sigma^3}\]</div>
</div>
</div><p>Le skewness est √©galement appel√© coefficient d‚Äôasym√©trie de Fisher.</p>
<div class="proof definition admonition" id="definition-16">
<p class="admonition-title"><span class="caption-number">Definition 32 </span> (Kurtosis)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[K=\frac{m_4}{m_2^2}\]</div>
</div>
</div><p><span class="math notranslate nohighlight">\(K\)</span> permet de mesurer l‚Äôaplatissement.</p>
<div class="proof definition admonition" id="definition-17">
<p class="admonition-title"><span class="caption-number">Definition 33 </span> (Coefficient d‚Äôasym√©trie de Yule)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[A_Y = \frac{x_{3/4}+x_{1/4}-2x_{1/2}}{x_{3/4}-x_{1/4}}\]</div>
</div>
</div><p>Ce coefficient est fond√© sur les positions de trois quartiles (le premier, la m√©diane et le troisi√®me) et est normalis√© par la distance interquartile.</p>
<div class="proof definition admonition" id="definition-18">
<p class="admonition-title"><span class="caption-number">Definition 34 </span> (Coefficient d‚Äôasym√©trie de Pearson)</p>
<div class="definition-content section" id="proof-content">
<div class="math notranslate nohighlight">
\[A_P = \frac{\bar{x}-x_M}{\sigma}\]</div>
</div>
</div><p>Ce coefficient est fond√© sur la comparaison de la moyenne et du mode, et est normalis√© par l‚Äô√©cart type.</p>
<p>Tous les coefficients d‚Äôasym√©trie ont des propri√©t√©s similaires : ils sont nuls si la distribution est sym√©trique, n√©gatifs si la distribution est allong√©e √† gauche (left asymmetry), et positifs si la distribution est allong√©e √† droite (right asymmetry).</p>
<p>On peut aussi chercher √† mesurer l‚Äôaplatissement (ou kurtosis) d‚Äôune distribution de mesure. Dans ce cas, on utilise le coefficient d‚Äôaplatissement de Pearson ou de Fisher, respectivement donn√©s par
<span class="math notranslate nohighlight">\(\beta_2=\frac{m_4}{\sigma^4}\quad\textrm{et}\quad g_2=\beta_2-3\)</span></p>
<p>Une distribution est alors dite :</p>
<ul class="simple">
<li><p>m√©sokurtique si <span class="math notranslate nohighlight">\(g_2\)</span> est proche de 0</p></li>
<li><p>leptokurtique si <span class="math notranslate nohighlight">\(g_2&gt;0\)</span> (queues plus longues et distribution plus pointue)</p></li>
<li><p>platykyrtique si <span class="math notranslate nohighlight">\(g_2&lt;0\)</span> (queues plus courtes et distribution arrondie).</p></li>
</ul>
<div class="section" id="pour-resumer">
<span id="boxplot"></span><h5>Pour r√©sumer<a class="headerlink" href="#pour-resumer" title="Permalink to this heading">#</a></h5>
<p>Les principales statistiques d‚Äôune s√©rie statistique peuvent √™tre r√©sum√©es dans des <strong>bo√Ætes √† moustache</strong>, qui permettent de voir sur un m√™me graphique :</p>
<ul class="simple">
<li><p>la m√©diane</p></li>
<li><p>une bo√Æte entre les premier et le troisi√®me quartile</p></li>
<li><p>l‚Äô√©tendue</p></li>
<li><p>les points aberrants.</p></li>
</ul>
<p>Ce mode de repr√©sentation consiste √† dessiner une bo√Æte dont les extr√©mit√©s d√©pendent du premier et du troisi√®me quartiles <span class="math notranslate nohighlight">\(Q_1\)</span> et <span class="math notranslate nohighlight">\(Q_3\)</span> , en ajoutant une barre √† l‚Äôint√©rieur
mat√©rialisant le second quartile  <span class="math notranslate nohighlight">\(Q_2\)</span> (la valeur m√©diane de l‚Äô√©chantillon). A cette bo√Æte, on ajoute des ‚Äúmoustaches‚Äù dont les extr√©mit√©s d√©pendent :</p>
<ul class="simple">
<li><p>soit des valeurs extr√©males prises par l‚Äô√©chantillon (minimum et maximum);</p></li>
<li><p>soit de la plus petite et de la plus grande valeur de l‚Äô√©chantillon appartenant √† l‚Äôintervalle <span class="math notranslate nohighlight">\([Q_1 -\delta, Q_3+\delta ]\)</span>. La grandeur <span class="math notranslate nohighlight">\(\delta\)</span> est une mesure de la dispersion des donn√©es. G√©n√©ralement, on utilise <span class="math notranslate nohighlight">\(\delta = 1.5(Q_3-Q_1)\)</span>.</p></li>
</ul>
<p>Les valeurs de l‚Äô √©chantillon en dehors des moustaches sont parfois mat√©rialis√©es par des points et sont alors consid√©r√©es comme les points aberrants de l‚Äô√©chantillon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">annotate_boxplot</span><span class="p">(</span><span class="n">bpdict</span><span class="p">,</span> <span class="n">annotate_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">x_offset</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">x_loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                     <span class="n">text_offset_x</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span>
                     <span class="n">text_offset_y</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">annotate_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">annotate_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">text_offset_x</span><span class="p">,</span> <span class="n">text_offset_y</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;arrowstyle&#39;</span><span class="p">:</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">})</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;M√©diane&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;medians&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_1$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;boxes&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_3$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;boxes&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">2</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_1-1.5(Q_3-Q_1)$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;caps&#39;</span><span class="p">][</span><span class="n">x_loc</span><span class="o">*</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;$Q_3+1.5(Q_3-Q_1)$&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_loc</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">,</span> <span class="n">bpdict</span><span class="p">[</span><span class="s1">&#39;caps&#39;</span><span class="p">][(</span><span class="n">x_loc</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_ydata</span><span class="p">()[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">annotate_params</span><span class="p">)</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Donn√©es&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">150</span><span class="p">)})</span>

<span class="n">bpdict</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">whis</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">return_type</span><span class="o">=</span><span class="s1">&#39;dict&#39;</span><span class="p">)</span>
<span class="n">annotate_boxplot</span><span class="p">(</span><span class="n">bpdict</span><span class="p">,</span> <span class="n">x_loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fb54cd244877b2f8954d547e22f2771224f0e621141faef4d25862f5da2427f4.png" src="_images/fb54cd244877b2f8954d547e22f2771224f0e621141faef4d25862f5da2427f4.png" />
</div>
</div>
</div>
</div>
<div class="section" id="la-description-ne-fait-pas-tout">
<h4>La description ne fait pas tout‚Ä¶<a class="headerlink" href="#la-description-ne-fait-pas-tout" title="Permalink to this heading">#</a></h4>
<p>La description d‚Äôun ensemble de valeurx <span class="math notranslate nohighlight">\(x_j\)</span> par la moyenne, la variance, voire le comportement lin√©aire (coefficient de corr√©lation, voir plus loin) peut ne pas suffire √† comprendre la distribution des donn√©es. Un exemple classique (analyse bivari√©e, section suivante) est le quartet d‚ÄôAnscombe (figure ci-dessous), constitu√© de quatre ensembles de points  <span class="math notranslate nohighlight">\((x,y)\in\mathbb{R}^2\)</span> de m√™me propri√©t√©s statistiques (moyenne, variance, coefficient de r√©gression lin√©aire) mais qui sont distribu√©s de mani√®re totalement diff√©rente dans le plan.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">y1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">8.04</span><span class="p">,</span> <span class="mf">6.95</span><span class="p">,</span> <span class="mf">7.58</span><span class="p">,</span> <span class="mf">8.81</span><span class="p">,</span> <span class="mf">8.33</span><span class="p">,</span> <span class="mf">9.96</span><span class="p">,</span> <span class="mf">7.24</span><span class="p">,</span> <span class="mf">4.26</span><span class="p">,</span> <span class="mf">10.84</span><span class="p">,</span> <span class="mf">4.82</span><span class="p">,</span> <span class="mf">5.68</span><span class="p">]</span>
<span class="n">y2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">9.14</span><span class="p">,</span> <span class="mf">8.14</span><span class="p">,</span> <span class="mf">8.74</span><span class="p">,</span> <span class="mf">8.77</span><span class="p">,</span> <span class="mf">9.26</span><span class="p">,</span> <span class="mf">8.10</span><span class="p">,</span> <span class="mf">6.13</span><span class="p">,</span> <span class="mf">3.10</span><span class="p">,</span> <span class="mf">9.13</span><span class="p">,</span> <span class="mf">7.26</span><span class="p">,</span> <span class="mf">4.74</span><span class="p">]</span>
<span class="n">y3</span> <span class="o">=</span> <span class="p">[</span><span class="mf">7.46</span><span class="p">,</span> <span class="mf">6.77</span><span class="p">,</span> <span class="mf">12.74</span><span class="p">,</span> <span class="mf">7.11</span><span class="p">,</span> <span class="mf">7.81</span><span class="p">,</span> <span class="mf">8.84</span><span class="p">,</span> <span class="mf">6.08</span><span class="p">,</span> <span class="mf">5.39</span><span class="p">,</span> <span class="mf">8.15</span><span class="p">,</span> <span class="mf">6.42</span><span class="p">,</span> <span class="mf">5.73</span><span class="p">]</span>
<span class="n">x4</span> <span class="o">=</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="n">y4</span> <span class="o">=</span> <span class="p">[</span><span class="mf">6.58</span><span class="p">,</span> <span class="mf">5.76</span><span class="p">,</span> <span class="mf">7.71</span><span class="p">,</span> <span class="mf">8.84</span><span class="p">,</span> <span class="mf">8.47</span><span class="p">,</span> <span class="mf">7.04</span><span class="p">,</span> <span class="mf">5.25</span><span class="p">,</span> <span class="mf">12.50</span><span class="p">,</span> <span class="mf">5.56</span><span class="p">,</span> <span class="mf">7.91</span><span class="p">,</span> <span class="mf">6.89</span><span class="p">]</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;1.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span>
    <span class="s1">&#39;2.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span>
    <span class="s1">&#39;3.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">),</span>
    <span class="s1">&#39;4.&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">y4</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                        <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;wspace&#39;</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">,</span> <span class="s1">&#39;hspace&#39;</span><span class="p">:</span> <span class="mf">0.18</span><span class="p">})</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">datasets</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

    <span class="n">p1</span><span class="p">,</span> <span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># slope, intercept</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axline</span><span class="p">(</span><span class="n">xy1</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">p0</span><span class="p">),</span> <span class="n">slope</span><span class="o">=</span><span class="n">p1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">bar x$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">sigma$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
             <span class="sa">f</span><span class="s1">&#39;$r$ = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
            <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f4a58c4b2213da08a87d1a9464c3353ff17b764178a2fc7ef335d138cbc5e5f9.png" src="_images/f4a58c4b2213da08a87d1a9464c3353ff17b764178a2fc7ef335d138cbc5e5f9.png" />
</div>
</div>
</div>
</div>
<div class="section" id="statistique-descriptive-bivariee">
<h3>Statistique descriptive bivari√©e<a class="headerlink" href="#statistique-descriptive-bivariee" title="Permalink to this heading">#</a></h3>
<p>On s‚Äôint√©resse √† deux variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>, mesur√©es sur les <span class="math notranslate nohighlight">\(n\)</span> unit√©s d‚Äôobservation. La s√©rie statistique est alors une suite de <span class="math notranslate nohighlight">\(n\)</span> couples <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> des valeurs prises par les deux variables sur chaque individu.</p>
<div class="section" id="cas-de-deux-variables-quantitatives">
<h4>Cas de deux variables quantitatives<a class="headerlink" href="#cas-de-deux-variables-quantitatives" title="Permalink to this heading">#</a></h4>
<p>Le couple est un couple de valeurs num√©riques. C‚Äôest donc un point dans le plan <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>. Les variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> peuvent √™tre analys√©es s√©par√©ment, en op√©rant une statistique univari√©e sur chacune de ces variables. Les param√®tres calcul√©s (de position, de dispersion‚Ä¶) sont dits marginaux. Cependant, il est int√©ressant d‚Äô√©tudier le lien entre ces deux variables, par l‚Äôinterm√©diaire des valeurs des couples. On d√©finit pour cela un certain nombre d‚Äôoutils :</p>
<div class="proof definition admonition" id="definition-19">
<p class="admonition-title"><span class="caption-number">Definition 35 </span> (Covariance)</p>
<div class="definition-content section" id="proof-content">
<p>La covariance de <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> est d√©finie par :
<span class="math notranslate nohighlight">\(\sigma_{xy}=\frac{1}{n}\displaystyle\sum_{i=1}^n\left (x_i-\bar{x}\right )\left (y_i-\bar{y}\right )\)</span></p>
</div>
</div><span class="target" id="index-8"></span><div class="proof definition admonition" id="definition-20">
<span id="index-9"></span><p class="admonition-title"><span class="caption-number">Definition 36 </span> (Coefficient de corr√©lation)</p>
<div class="definition-content section" id="proof-content">
<p>Le coefficient de corr√©lation  de deux variables <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span> est d√©fini par
<span class="math notranslate nohighlight">\(r_{xy}=\frac{\sigma_{xy}}{\sigma_{x}\sigma_{y}}\)</span>.
Le coefficient de d√©termination est le carr√© du coefficient de corr√©lation.</p>
</div>
</div><p>Le coefficient de corr√©lation est donc la covariance normalis√©e par les √©carts types marginaux des variables. Il mesure la d√©pendance lin√©aire entre <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>. Il est compris dans l‚Äôintervalle [-1,1] est est positif (resp. n√©gatif) si les points sont align√©s le long d‚Äôune droite croissante (resp. d√©croissante), d‚Äôautant plus grand en valeur absolue que la d√©pendance lin√©aire est v√©rifi√©e. Dans le cas o√π le coefficient est nul, il n‚Äôexiste pas de d√©pendance lin√©aire.</p>
<p>Pour conna√Ætre plus pr√©cis√©ment la relation lin√©aire qui lie <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(y\)</span>, on effectue une r√©gression lin√©aire en calculant par exemple la droite de r√©gression : si <span class="math notranslate nohighlight">\(y=a+bx\)</span>, il est facile de montrer que
<span class="math notranslate nohighlight">\(b=\frac{\sigma_{xy}}{\sigma_x^2}\quad\textrm{et}\quad a=\bar{y}-b\bar{x}\)</span></p>
<p>et la droite de r√©gression s‚Äô√©crit <span class="math notranslate nohighlight">\(y-\bar{y}=\frac{\sigma_{xy}}{\sigma_x^2}\left ( x-\bar{x}\right )\)</span>.</p>
<p>A partir de cette droite, on peut calculer les valeurs ajust√©es, obtenues √† partir de la droite de r√©gression : <span class="math notranslate nohighlight">\(y^*_i=a+bx_i\)</span>. Ce sont les valeurs th√©oriques des <span class="math notranslate nohighlight">\(y_i\)</span> et les r√©sidus <span class="math notranslate nohighlight">\(e_i=y_i-y_i^*\)</span> repr√©sentent la partie inexpliqu√©e des <span class="math notranslate nohighlight">\(y_i\)</span> par la droite de r√©gression (ceux l√† m√™me que l‚Äôon essaye de minimiser par la m√©thode des moindres carr√©s). Nous reviendrons dans le chapitre sur la r√©gression sur l‚Äôanalyse de ces r√©sidus.</p>
</div>
<div class="section" id="cas-de-deux-variables-qualitatives">
<h4>Cas de deux variables qualitatives<a class="headerlink" href="#cas-de-deux-variables-qualitatives" title="Permalink to this heading">#</a></h4>
<p>Le couple est un couple de valeurs <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> o√π <span class="math notranslate nohighlight">\(x_i\)</span> et <span class="math notranslate nohighlight">\(y_i\)</span> prennent comme valeurs des modalit√©s qualitatives. Notons <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span> et <span class="math notranslate nohighlight">\(y_1\cdots y_K\)</span> les valeurs distinctes prises.</p>
<p>Les donn√©es peuvent √™tre regroup√©es sous la forme d‚Äôun <strong>tableau de contingence</strong> prenant la forme suivante :</p>
<span class="target" id="index-10"></span><p id="index-11"><span class="math notranslate nohighlight">\(\begin{array}{c|ccccc|c}
&amp;y_1&amp;\cdots&amp;y_k&amp;\cdots&amp;y_K&amp;total\\
\hline
x_1&amp;n_{11}&amp;\cdots&amp;n_{1k}&amp;\cdots&amp;n_{1K}&amp;n_{1.}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_j&amp;n_{j1}&amp;\cdots&amp;n_{jk}&amp;\cdots&amp;n_{jK}&amp;n_{j.}\\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
x_J&amp;n_{J1}&amp;\cdots&amp;n_{Jk}&amp;\cdots&amp;n_{JK}&amp;n_{J.}\\
\hline
total&amp;n_{.1}&amp;\cdots&amp;n_{.k}&amp;\cdots&amp;n_{.K}&amp;n\\
\end{array}
\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\(n_{j.}\)</span> (resp <span class="math notranslate nohighlight">\(n_{.k}\)</span> )sont les effectifs marginaux repr√©sentant le nombre de fois o√π <span class="math notranslate nohighlight">\(x_j\)</span> (resp. <span class="math notranslate nohighlight">\(y_k\)</span>) appara√Æt, et <span class="math notranslate nohighlight">\(n_{jk}\)</span> le nombre d‚Äôapparition du couple <span class="math notranslate nohighlight">\((x_j,y_k)\)</span>.</p>
<p>Le tableau des fr√©quences <span class="math notranslate nohighlight">\(f_{jk}\)</span> s‚Äôobtient en divisant tous les effectifs par la taille <span class="math notranslate nohighlight">\(n\)</span> dans ce tableau.</p>
<p>Un tel tableau s‚Äôinterpr√®te toujours en comparant les fr√©quences en lignes ou les fr√©quences en colonnes (profils lignes ou colonnes), d√©finies  respectivement par
<span class="math notranslate nohighlight">\(f_k^{(j)}= \frac{n_{jk}}{n_{j.}}=\frac{f_{jk}}{f_{j.}}\quad\textrm{ et }\quad f_j^{(k)}= \frac{n_{jk}}{n_{.k}}=\frac{f_{jk}}{f_{.k}}\)</span></p>
<p>Si l‚Äôon cherche un lien entre les variables, on construit un tableau d‚Äôeffectifs th√©oriques qui repr√©sente la situation o√π les variables ne sont pas li√©es (ind√©pendance). Ce tableau est constitu√© des effectifs
<span class="math notranslate nohighlight">\(n_{jk}^*=\frac{n_{j.}n_{.k}}{n}\)</span>
Les effectifs observ√©s <span class="math notranslate nohighlight">\(n_{jk}\)</span> ont les m√™mes marges que les <span class="math notranslate nohighlight">\(n_{jk}^*\)</span>, et les √©carts √† l‚Äôind√©pendance sont calcul√©s par la diff√©rence <span class="math notranslate nohighlight">\(e_{jk}=n_{jk}-n_{jk}^*\)</span></p>
<p id="index-12">La d√©pendance du tableau se mesure au moyen du khi-deux d√©fini par
<span class="math notranslate nohighlight">\(\chi^2_{obs}= \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{e_{jk}^2}{n_{jk}^*}\)</span>
qui peut √™tre normalis√© pour ne plus d√©pendre du nombre d‚Äôobservations :
<span class="math notranslate nohighlight">\(\phi^2=\frac{\chi^2_{obs}}{n}\)</span></p>
<p>La construction du tableau des effectifs th√©oriques et sa comparaison au tableau des observations permet dans un premier temps de mettre en √©vidence les associations significatives entre modalit√©s des deux variables. Pour cela, on calcule la contribution au <span class="math notranslate nohighlight">\(\chi^2\)</span> des modalit√©s <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span> :</p>
<div class="math notranslate nohighlight">
\[\frac{1}{\chi^2_{obs}}\frac{\left (n_{jk}-n_{j.}n_{.k}\right )^2}{n_{jk}^*}\]</div>
<p>Le signe de la diff√©rence <span class="math notranslate nohighlight">\(n_{jk}-n_{jk}^*\)</span> indique alors s‚Äôil y a une association positive ou n√©gative entre les modalit√©s <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Plus g√©n√©ralement, le <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est un indicateur de liaison entre les variables.  Dans le cas o√π <span class="math notranslate nohighlight">\(\chi^2_{obs}=0\)</span>, il y a ind√©pendance. Pour rechercher la borne sup√©rieure du khi-deux et voir dans quel cas elle est atteinte, on d√©veloppe le carr√© et on obtient</p>
<div class="math notranslate nohighlight">
\[\chi^2_{obs} = n\left [\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}^2}{n_{j.}n_{.k}} -1\right ]\]</div>
<p>Comme <span class="math notranslate nohighlight">\(\frac{n_{jk}}{n_{.k}}\leq 1\)</span> on a <span class="math notranslate nohighlight">\( \frac{n_{jk}^2}{n_{j.}n_{.k}} \leq \frac{n_{jk}}{n_{.k}}\)</span> d‚Äôo√π</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J\frac{n_{jk}^2}{n_{j.}n_{.k}}\leq \displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J \frac{n_{jk}}{n_{.k}} = \displaystyle\sum_{k=1}^K \frac{\displaystyle\sum_{j=1}^J n_{jk}}{n_{.k}}=\displaystyle\sum_{k=1}^K \frac{n_{.k}}{n_{.k}}=1\]</div>
<p>d‚Äôo√π <span class="math notranslate nohighlight">\(\chi^2_{obs}\leq n(K-1)\)</span>. On pourrait de m√™me montrer que <span class="math notranslate nohighlight">\(\chi^2_{obs}\leq n(J-1)\)</span> et donc <span class="math notranslate nohighlight">\(\phi^2\leq min(J-1,K-1)\)</span>.</p>
<p>La borne est atteinte dans le cas de la d√©pendance fonctionnelle (si <span class="math notranslate nohighlight">\(\forall j \frac{n_{jk}}{n_{j.}}=1\)</span>, i.e. il n‚Äôexiste qu‚Äôune case non nulle dans chaque ligne.)</p>
<p>A partir de ce khi-deux normalis√©, on calcule finalement plusieurs coefficients permettant de mesurer l‚Äôind√©pendance, et parmi ceux-ci citons :</p>
<ul class="simple">
<li><p>le coefficient de Cramer:
<span class="math notranslate nohighlight">\(V=\sqrt{\frac{\phi^2}{min(J-1,K-1)}}\)</span></p></li>
<li><p>le coefficient de contingence de Pearson :
<span class="math notranslate nohighlight">\(C = \sqrt{\frac{\phi^2}{\phi^2 + 1}}\)</span></p></li>
<li><p>le coefficient de Tschuprow :
<span class="math notranslate nohighlight">\(T = \sqrt{\frac{\phi^2}{\sqrt{(K-1)(J-1)}}}\)</span></p></li>
</ul>
<p>Ces coefficients sont tous compris entre 0 (ind√©pendance) et 1 (d√©pendance fonctionnelle). Pour estimer √† partir de quelle valeur la d√©pendance fonctionnelle est significative, on proc√®de de la mani√®re suivante : si les <span class="math notranslate nohighlight">\(n\)</span> observations √©taient pr√©lev√©es dans une population o√π les variables sont ind√©pendantes, on recherche les valeurs probables de <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>.</p>
<p>En s‚Äôappuyant sur la loi multinomiale et le test du <span class="math notranslate nohighlight">\(\chi^2\)</span>, on montre que <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est une r√©alisation d‚Äôune variable al√©atoire <span class="math notranslate nohighlight">\(Z\)</span> suivant approximativement une loi <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span>.</p>
<div class="proof remark dropdown admonition" id="remark-21">
<p class="admonition-title"><span class="caption-number">Remark 10 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(U_1\ldots U_p\)</span> <span class="math notranslate nohighlight">\(p\)</span> variables i.i.d de loi normale centr√©e r√©duite. On appelle loi du <span class="math notranslate nohighlight">\(\chi^2\)</span> √† <span class="math notranslate nohighlight">\(p\)</span> degr√©s de libert√© la loi de la variable <span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^pU_i^2\)</span>.</p>
</div>
</div><p>En effet, les <span class="math notranslate nohighlight">\(e_{jk}\)</span> sont li√©es par <span class="math notranslate nohighlight">\((K-1)(J-1)\)</span> relations lin√©aires puisqu‚Äôon estime les probabilit√©s de r√©alisation de <span class="math notranslate nohighlight">\(x_j\)</span> et <span class="math notranslate nohighlight">\(y_k\)</span> respectivement par <span class="math notranslate nohighlight">\(n_{j,.}/n\)</span> et <span class="math notranslate nohighlight">\(n_{.k}/n\)</span>. Il suffit alors de fixer un risque d‚Äôerreur <span class="math notranslate nohighlight">\(\alpha\)</span> (une valeur qui, s‚Äôil y avait ind√©pendance, n‚Äôaurait qu‚Äôune probabilit√© faible d‚Äô√™tre d√©pass√©e), et on rejette l‚Äôhypoth√®se d‚Äôind√©pendance si <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>  est sup√©rieur √† la valeur critique qu‚Äôune variable <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span> a une probabilit√© <span class="math notranslate nohighlight">\(\alpha\)</span> de d√©passer.
L‚Äôesp√©rance d‚Äôun <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span> √©tant √©gale √† son degr√© de libert√©, <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span> est d‚Äôautant plus grand que le nombre de modalit√©s <span class="math notranslate nohighlight">\(J\)</span> et/ou <span class="math notranslate nohighlight">\(K\)</span> est grand.</p>
<p>D‚Äôautres indices existent, qui ne d√©pendent pas de <span class="math notranslate nohighlight">\(\chi^2_{obs}\)</span>, comme par exemple</p>
<p><span class="math notranslate nohighlight">\(\begin{equation} G^2 = 2\displaystyle\sum_{k=1}^K\displaystyle\sum_{j=1}^J n_{jk} ln \left (\frac{ n_{jk}}{ n^*_{jk}} \right )\end{equation}\)</span></p>
<p>qui sous l‚Äôhypoth√®se d‚Äôind√©pendance suit une loi <span class="math notranslate nohighlight">\(\chi^2_{(K-1)(J-1)}\)</span>.</p>
</div>
<div class="section" id="cas-d-une-variable-quantitative-et-d-une-variable-qualitative">
<h4>Cas d‚Äôune variable quantitative et d‚Äôune variable qualitative<a class="headerlink" href="#cas-d-une-variable-quantitative-et-d-une-variable-qualitative" title="Permalink to this heading">#</a></h4>
<p>On s‚Äôint√©resse ici au cas o√π les modalit√©s <span class="math notranslate nohighlight">\(x_i\)</span> sont qualitatives, et o√π <span class="math notranslate nohighlight">\(y\)</span> est une variable quantitative, dont les modalit√©s sont des r√©alisations d‚Äôune variable al√©atoire <span class="math notranslate nohighlight">\(Y\)</span>.
Le rapport de corr√©lation th√©orique entre <span class="math notranslate nohighlight">\(x\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> est d√©fini par</p>
<div class="math notranslate nohighlight">
\[\eta^2_{Y\mid x} = \frac{\sigma^2_{\mathbb{E}_{Y\mid x}}}{\sigma^2_Y}\]</div>
<p>Si <span class="math notranslate nohighlight">\(n_j\)</span> est le nombre d‚Äôobservations de la modalit√© <span class="math notranslate nohighlight">\(x_j,j\in[\![1\,J]\!]\)</span>, <span class="math notranslate nohighlight">\(y_{ij}\)</span> la valeur de <span class="math notranslate nohighlight">\(Y\)</span> du <span class="math notranslate nohighlight">\(i^e\)</span> individu de la modalit√© <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(\bar{y}_1\ldots \bar{y}_J\)</span> sont les moyennes de <span class="math notranslate nohighlight">\(Y\)</span> pour ces modalit√©s et <span class="math notranslate nohighlight">\(\bar{y}\)</span> la moyenne totale sur les <span class="math notranslate nohighlight">\(n\)</span> individus, le rapport de corr√©lation empirique est d√©fini par</p>
<div class="math notranslate nohighlight">
\[e^2 = \frac{\frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2}{\sigma^2_y}\]</div>
<p>La quantit√©</p>
<p><span class="math notranslate nohighlight">\(\sigma^2_\cap = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\sigma_j^2\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\sigma_j^2 =  \frac{1}{n_j}\displaystyle\sum_{i=1}^{n_j}\left (y_{ij}-\bar{y}_j \right )^2\)</span>,  est appel√©e variance intra groupe (ou intra classe), et donne une id√©e de la variabilit√© √† l‚Äôint√©rieur de chaque modalit√©.
La quantit√©
<span class="math notranslate nohighlight">\(\sigma_\cup = \frac{1}{n}\displaystyle\sum_{j=1}^J n_j\left (\bar{y}_j-\bar{y}\right )^2\)</span>
est la variance inter groupes (ou inter classes), et mesure la variabilit√© entre les diff√©rentes modalit√©s.</p>
<p>Le th√©or√®me de d√©composition de la variance (ou th√©or√®me de Huygens) affirme que la variance totale <span class="math notranslate nohighlight">\(\sigma^2_y\)</span>, calcul√©e sans distinction de modalit√© s‚Äô√©crit :
<span class="math notranslate nohighlight">\(\sigma^2_y = \sigma^2_\cap + \sigma^2_\cup\)</span></p>
<p>De ces d√©finitions, on a alors :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(e^2=0\)</span> si toutes les moyennes de <span class="math notranslate nohighlight">\(Y\)</span> sont √©gales, d‚Äôo√π l‚Äôabsence de d√©pendance en moyenne</p></li>
<li><p><span class="math notranslate nohighlight">\(e^2=1\)</span> si tous les individus d‚Äôune modalit√© de <span class="math notranslate nohighlight">\(x\)</span> ont m√™me valeur de <span class="math notranslate nohighlight">\(Y\)</span> et ceci pour chaque modalit√©</p></li>
<li><p><span class="math notranslate nohighlight">\(e^2\)</span> permet de comprendre, via le th√©or√®me de Huygens,  quelle variation est pr√©dominante dans la variance totale. Ainsi par exemple, si la variable quantitative est la note d‚Äôun √©l√®ve √† un examen, et la variable qualitative son assiduit√© au cours correspondant, la variabilit√© entre les notes obtenues dans toute la promotion d√©pend de deux
facteurs : le fait que les √©tudiants assistent ou pas aux cours, et le fait qu‚Äô√† assiduit√©
√©gale (i.e. √† l‚Äôint√©rieur d‚Äôune m√™me modalit√© d‚Äôassiduit√©) les √©tudiants n‚Äôont pas le m√™me niveau. <span class="math notranslate nohighlight">\(e^2\)</span>  permet alors de savoir lequel de ces deux facteurs est pr√©dominant
pour expliquer la variabilit√© des notes dans toute la promotion.</p></li>
</ul>
<p>Pour d√©terminer √† partir de quelle valeur <span class="math notranslate nohighlight">\(e^2\)</span> est significatif, on compare donc <span class="math notranslate nohighlight">\(\sigma^2_\cap\)</span> √† <span class="math notranslate nohighlight">\(\sigma^2_\cup\)</span>. On peut montrer que si le rapport de corr√©lation th√©orique est nul, alors la variable <span class="math notranslate nohighlight">\(\frac{\left (\frac{e^2}{J-1}\right )}{\left (\frac{1-e^2}{n-J}\right )}\)</span> suit une loi de Fisher Snedecor, en supposant que les distributions conditionnelles de <span class="math notranslate nohighlight">\(Y\)</span> pour chaque modalit√© de <span class="math notranslate nohighlight">\(X\)</span> sont gaussiennes, de m√™me esp√©rance et de m√™me variance.</p>
<div class="proof remark dropdown admonition" id="remark-22">
<p class="admonition-title"><span class="caption-number">Remark 11 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soient <span class="math notranslate nohighlight">\(U\)</span> et <span class="math notranslate nohighlight">\(V\)</span> deux variables al√©atoires ind√©pendantes suivant respectivement des lois <span class="math notranslate nohighlight">\(\chi^2_n\)</span> et <span class="math notranslate nohighlight">\(\chi^2_p\)</span>. On d√©finit la loi de Fisher Snedecor par <span class="math notranslate nohighlight">\(F(n,p)=\frac{U/n}{V/P}\)</span>) <span class="math notranslate nohighlight">\(F(J-1,n-J)\)</span></p>
</div>
</div></div>
</div>
<div class="section" id="vers-une-analyse-multivariee">
<h3>Vers une analyse multivari√©e<a class="headerlink" href="#vers-une-analyse-multivariee" title="Permalink to this heading">#</a></h3>
<p>Bien √©videmment, dans la majorit√© des cas, un individu sera d√©crit par <span class="math notranslate nohighlight">\(p\geq 2\)</span> variables. Si certains algorithmes de statistique descriptive multidimensionnelle sont abord√©s dans ce cours, il est n√©anmoins possible d‚Äôavoir une premi√®re approche exploratoire de ce cas.</p>
<div class="section" id="matrices-de-covariance-et-de-correlation">
<h4>Matrices de covariance et de corr√©lation<a class="headerlink" href="#matrices-de-covariance-et-de-correlation" title="Permalink to this heading">#</a></h4>
<p>La premi√®re id√©e, lorsque l‚Äôon a observ√© <span class="math notranslate nohighlight">\(d\)</span> variables sur <span class="math notranslate nohighlight">\(n\)</span> individus, est de calculer les <span class="math notranslate nohighlight">\(d\)</span> variances de ces variables, et les <span class="math notranslate nohighlight">\(\frac{p(p-1)}{2}\)</span> covariances. Ces mesures sont regroup√©es dans une matrice <span class="math notranslate nohighlight">\(p\times p\)</span>, sym√©trique, semi d√©finie positive, appel√©e matrice de variance-covariance (ou matrice des covariances), et classiquement not√©e <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>.</p>
<p>De m√™me, on peut former la matrice des corr√©lations entre les variables, √† diagonale unit√© et sym√©trique. La matrice r√©sultante, not√©e <span class="math notranslate nohighlight">\(\mathbf R\)</span>, est √©galement semi d√©finie positive et sa repr√©sentation graphique en fausses couleurs permet d‚Äôappr√©cier les d√©pendances lin√©aires entre variables.</p>
<p><img alt="" src="_images/batiments.png" /></p>
<p>Dans le cas de variables qualitatives, les coefficients de corr√©lation peuvent √™tre remplac√©s par les coefficients de Cramer, de Tschuprow‚Ä¶</p>
</div>
<div class="section" id="tableaux-de-nuages">
<h4>Tableaux de nuages<a class="headerlink" href="#tableaux-de-nuages" title="Permalink to this heading">#</a></h4>
<p>On peut proposer √† partir de l√† des repr√©sentations entre sous-ensembles de variables. La figure suivante propose un exemple de tels tableaux, parfois appel√©s splom (Scatter PLOt Matrix) :</p>
<ul class="simple">
<li><p>la partie triangulaire sup√©rieure repr√©sente les nuages de points de couples de variables</p></li>
<li><p>la diagonale repr√©sente les histogrammes des variables</p></li>
<li><p>la partie trianglaire inf√©rieure donne le coefficient de corr√©lation entre les deux variables, et une estimation de la densit√© de la distribution 2D des donn√©es</p></li>
</ul>
<p><img alt="" src="_images/batiments2.png" /></p>
</div>
<div class="section" id="tableaux-de-burt">
<h4>Tableaux de Burt<a class="headerlink" href="#tableaux-de-burt" title="Permalink to this heading">#</a></h4>
<p>Le tableau de Burt est une g√©n√©ralisation particuli√®re de la table de contingence dans le cas o√π l‚Äôon √©tudie simultan√©ment <span class="math notranslate nohighlight">\(p\)</span> variables qualitatives <span class="math notranslate nohighlight">\(X_1\ldots X_p\)</span>. Notons <span class="math notranslate nohighlight">\(c_j\)</span> le nombre de modalit√©s de <span class="math notranslate nohighlight">\(X_j\)</span> et posons <span class="math notranslate nohighlight">\(c=\displaystyle\sum_{j=1}^p c_j\)</span>.</p>
<span class="target" id="index-13"></span><p id="index-14">Le tableau de Burt est une matrice carr√©e sym√©trique de taille <span class="math notranslate nohighlight">\(c\)</span>, constitu√©e de <span class="math notranslate nohighlight">\(p^2\)</span> sous-matrices. Chacune des <span class="math notranslate nohighlight">\(p\)</span> sous-matrices diagonales est relative √† l‚Äôune des <span class="math notranslate nohighlight">\(p\)</span> variables, la <span class="math notranslate nohighlight">\(j^e\)</span> √©tant carr√©e de taille <span class="math notranslate nohighlight">\(c_j\)</span>, diagonale, et de coefficients diagonaux les effectifs marginaux de <span class="math notranslate nohighlight">\(X_j\)</span>. La sous-matrice dans le bloc <span class="math notranslate nohighlight">\((k,l)\)</span> du tableau, <span class="math notranslate nohighlight">\(k\neq l\)</span>, est la table de contingence des variables <span class="math notranslate nohighlight">\(X_k\)</span> et <span class="math notranslate nohighlight">\(X_l\)</span>.</p>
</div>
</div>
</div>
<span id="document-selection"></span><div class="tex2jax_ignore mathjax_ignore section" id="selection-de-variables">
<h2>S√©lection de variables<a class="headerlink" href="#selection-de-variables" title="Permalink to this heading">#</a></h2>
<p>On s‚Äôint√©resse ici √† <span class="math notranslate nohighlight">\(n\)</span> individus  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> d√©crits par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives ou caract√©ristiques (features), <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^d\)</span>. Avec l‚Äôav√®nement des Big Data, et la g√©n√©ralisation des capteurs, <span class="math notranslate nohighlight">\(d\)</span> peut √™tre tr√®s grand (plusieurs milliers), et analyser telles quelles les donn√©es brutes devient difficile d‚Äôun point de vue calculatoire et interpr√©tation. De plus, il est rare que les caract√©ristiques soient totalement utiles et ind√©pendantes.</p>
<p>Une √©tape souvent utilis√©e en analyse de donn√©es consiste donc √† pr√©traiter cet espace, par exemple pour :</p>
<ul class="simple">
<li><p>le transformer en un format compatible avec des algorithmes qui seront utilis√©s</p></li>
<li><p>r√©duire la complexit√© temporelle des algorithmes qui seront utilis√©s</p></li>
<li><p>r√©duire la complexit√© spatiale du probl√®me trait√©</p></li>
<li><p>d√©coupler des variables et chercher les d√©pendances</p></li>
<li><p>introduire des a priori, ou des propri√©t√©s importantes pour les algorithmes (donn√©es centr√©es norm√©es, descripteurs √©pars‚Ä¶)</p></li>
<li><p>permettre une interpr√©tation plus intuitive et/ou graphique (<a class="reference internal" href="#tsne"><span class="std std-ref">figure 2</span></a>)</p></li>
</ul>
<div class="figure align-default" id="tsne">
<img alt="_images/tsne.png" src="_images/tsne.png" />
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Exemple de r√©duction de dimension (source: Maaten &amp; Hinton, 2008). Des images 28<span class="math notranslate nohighlight">\(\times\)</span> 28 de chiffres manuscrits sont repr√©sent√©es par un vecteur de 784 valeurs, puis transform√©s en vecteurs de <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> pour les projeter dans le plan. La m√©thode utilis√©e permet d‚Äôoptimiser la transformation de sorte √† ce que les images repr√©sentant le m√™me chiffre soient regroup√©es dans des nuages compacts.</span><a class="headerlink" href="#tsne" title="Permalink to this image">#</a></p>
</div>
<p>Deux strat√©gies peuvent alors √™tre utilis√©es :</p>
<ol class="arabic simple">
<li><p>s√©lectionner un sous-ensemble des variables initiales comme descripteurs des individus</p></li>
<li><p>calculer de nouveaux descripteurs √† partir des variables initiales.</p></li>
</ol>
<p>Nous nous int√©ressons ici √† la premi√®re approche, la seconde (extraction de caract√©ristiques) √©tant abord√©e pour une approche lin√©aire dans le chapitre sur l‚Äôanalyse en composantes principales.</p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 12 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les m√©thodes d‚Äôextraction de caract√©ristiques peuvent √™tre soit lin√©aires (on recherche des combinaisons lin√©aires des variables initiales  permettant d‚Äôoptimiser un cerrtain crit√®re), ou non lin√©aires (on parle √©galement de manifold learning)</p>
</div>
</div><div class="section" id="definitions">
<h3>D√©finitions<a class="headerlink" href="#definitions" title="Permalink to this heading">#</a></h3>
<p>La s√©lection de caract√©ristiques consiste √† choisir parmi les <span class="math notranslate nohighlight">\(d\)</span> descripteurs d‚Äôun ensemble d‚Äôindividus <span class="math notranslate nohighlight">\(\mathbf x_i,i\in[\![1,n]\!]\)</span>, un sous-ensemble de  <span class="math notranslate nohighlight">\(t&lt;d\)</span>  caract√©ristiques jug√©es ‚Äúles plus pertinentes‚Äù, les <span class="math notranslate nohighlight">\(d-t\)</span> restantes √©tant ignor√©es.</p>
<p>On note <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span> les <span class="math notranslate nohighlight">\(d\)</span> caract√©ristiques.  On note <span class="math notranslate nohighlight">\(Perf\)</span> une fonction qui permet d‚Äô√©valuer un sous-ensemble de caract√©ristiques, et on suppose que <span class="math notranslate nohighlight">\(Perf\)</span> atteint son maximum pour le meilleur sous-ensemble de caract√©ristiques (‚Äúle plus pertinent‚Äù). Le probl√®me de s√©lection se formule donc comme un probl√®me d‚Äôoptimisation</p>
<div class="math notranslate nohighlight">
\[\hat{F} = Arg\displaystyle\max_{U\subset F} Perf(U)\]</div>
<p>le cardinal <span class="math notranslate nohighlight">\(|\hat{F|}\)</span> de <span class="math notranslate nohighlight">\(\hat{F}\)</span> √©tant soit contr√¥l√© par l‚Äôutilisateur, soit d√©fini par l‚Äôalgorithme de s√©lection.</p>
<p>On distingue alors trois strat√©gies :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(|\hat{F|}\)</span> est d√©fini par l‚Äôutilisateur et l‚Äôoptimisation s‚Äôeffectue sur tous les sous-ensembles ayant ce cardinal</p></li>
<li><p>On conna√Æt une mesure minimale de performance <span class="math notranslate nohighlight">\(\gamma\)</span>  et la s√©lection recherche le plus petit sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> dont la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> est sup√©rieure ou √©gale √† <span class="math notranslate nohighlight">\(\gamma\)</span></p></li>
<li><p>On cherche un compromis entre l‚Äôam√©lioration de la performance <span class="math notranslate nohighlight">\(Perf(U)\)</span> et la r√©duction de la taille du sous ensemble.</p></li>
</ul>
<p>La mesure de pertinence d‚Äôune caract√©ristique est donc au centre des algorithmes de s√©lection. Plusieurs d√©finitions sont possibles, et nous dirons ici  qu‚Äôune caract√©ristique <span class="math notranslate nohighlight">\(f_i\)</span> est :</p>
<ul class="simple">
<li><p>pertinente si son absence entra√Æne une d√©t√©rioration significative de la performance de l‚Äôalgorithme utilis√© en aval (classification ou r√©gression)</p></li>
<li><p>peu pertinente si elle n‚Äôest pas pertinente et s‚Äôil existe un sous-ensemble <span class="math notranslate nohighlight">\(U\)</span> tel que la performance de <span class="math notranslate nohighlight">\(U\cup\{f_i\}\)</span> est significativement meilleure que la peformance de <span class="math notranslate nohighlight">\(U\)</span></p></li>
<li><p>non pertinente, si elle ne rentre pas dans les deux premi√®res d√©finitions. En g√©n√©ral, ces caract√©ristiques sont supprim√©es.</p></li>
</ul>
</div>
<div class="section" id="caracteristiques-des-methodes-de-selection">
<h3>Caract√©ristiques des m√©thodes de s√©lection<a class="headerlink" href="#caracteristiques-des-methodes-de-selection" title="Permalink to this heading">#</a></h3>
<p>Une m√©thode de s√©lection bas√©e sur l‚Äôoptimisation de <span class="math notranslate nohighlight">\(Perf\)</span> utilise g√©n√©ralement trois √©tapes. Les  deux derni√®res sont it√©r√©es jusqu‚Äô√† un test d‚Äôarr√™t.</p>
<div class="section" id="initialisation">
<h4>Initialisation<a class="headerlink" href="#initialisation" title="Permalink to this heading">#</a></h4>
<p>L‚Äôinitialisation consiste √† choisir l‚Äôensemble de d√©part des caract√©ristiques. Il peut s‚Äôagir de l‚Äôensemble vide, de <span class="math notranslate nohighlight">\(F\)</span> tout entier, ou un sous-ensemble quelconque <span class="math notranslate nohighlight">\(U\subset F\)</span>.</p>
</div>
<div class="section" id="exploration-des-sous-ensembles">
<h4>Exploration des sous-ensembles<a class="headerlink" href="#exploration-des-sous-ensembles" title="Permalink to this heading">#</a></h4>
<p>A partir de cette initialisation, les strat√©gies d‚Äôexploration des sous-ensembles de caract√©ristiques se d√©clinent en trois cat√©gories :</p>
<ol class="arabic simple">
<li><p>g√©n√©ration exhaustive : tous les sous-ensembles de caract√©ristiques sont √©valu√©s. Si elle garantit de trouver la valeur optimale, cette m√©thode n‚Äôest que peu applicable d√®s que <span class="math notranslate nohighlight">\(|F|\)</span> devient important (<span class="math notranslate nohighlight">\(2^{|F|}\)</span> sous-ensembles possibles)</p></li>
<li><p>g√©n√©ration heuristique : une g√©n√©ration it√©rative est effectu√©e, chaque it√©ration permettant de s√©lectionner ou de rejeter une ou plusieurs caract√©ristiques. La g√©n√©ration peut √™tre ascendante (ajout de caract√©ristiques √† partir de l‚Äôensemble vide), descendante (suppression de caract√©ristiques √† partir de <span class="math notranslate nohighlight">\(F\)</span>), ou mixte.</p></li>
<li><p>g√©n√©ration stochastique : pour un ensemble de donn√©es et une initialisation d√©finie, une strat√©gie de recherche heuristique retourne toujours le m√™me sous-ensemble, ce qui la rend tr√®s sensible au changement
de l‚Äôensemble de donn√©es. La g√©n√©ration stochastique g√©n√®re al√©atoirement un nombre fini de sous-ensembles de caract√©ristiques afin de s√©lectionner le meilleur. La convergence est sous-optimale mais peut s‚Äôav√©rer pr√©f√©rable dans des algorithmes d‚Äôapprentissage, par exemple pour √©viter le ph√©nom√®ne de surapprentissage.</p></li>
</ol>
</div>
<div class="section" id="evaluation-des-sous-ensembles">
<h4>Evaluation des sous-ensembles<a class="headerlink" href="#evaluation-des-sous-ensembles" title="Permalink to this heading">#</a></h4>
<div class="section" id="filtres">
<h5>Filtres<a class="headerlink" href="#filtres" title="Permalink to this heading">#</a></h5>
<p>Le crit√®re d‚Äô√©valuation utilis√© √©value la pertinence d‚Äôune caract√©ristique selon des mesures
qui reposent sur les propri√©t√©s de donn√©es d‚Äôapprentissage.</p>
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> exemples  <span class="math notranslate nohighlight">\(\mathbf x_i, i\in[\![1,n]\!]\)</span> , on note <span class="math notranslate nohighlight">\(\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d\)</span>  une donn√©e d‚Äôapprentissage (la <span class="math notranslate nohighlight">\(j^e\)</span> caract√©ristique <span class="math notranslate nohighlight">\(f_j\)</span> ayant donc pour valeur <span class="math notranslate nohighlight">\(x_{ij}\)</span>) , d‚Äô√©tiquette <span class="math notranslate nohighlight">\(y_i\)</span> (en classification ou r√©gression). Les m√©thodes de type filtres calculent un score pour √©valuer le degr√© de pertinence de chacune des caract√©ristiques <span class="math notranslate nohighlight">\(f_i\)</span> , parmi lesquelles on peut citer</p>
<ul class="simple">
<li><p>Le crit√®re de corr√©lation, utilis√© en classification binaire</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C_i =\frac{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )\left (y_{k} -\mu_k\right )}{\sqrt{\displaystyle\sum_{k=1}^n\left (x_{ki} -\mu_i\right )^2\displaystyle\sum_{k=1}^n\left (y_{k} -\mu_k\right )^2}}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mu_i\)</span> (resp. <span class="math notranslate nohighlight">\(\mu_k\)</span>) est la moyenne de la caract√©ristique <span class="math notranslate nohighlight">\(f_i\)</span> observ√©e sur <span class="math notranslate nohighlight">\(\mathbf x_1\cdots \mathbf x_n\)</span> (resp. moyenne des √©tiquettes)</p>
<ul class="simple">
<li><p>Le crit√®re de Fisher,  qui permet de mesurer dans un probl√®me de classification √† <span class="math notranslate nohighlight">\(C\)</span> classes le degr√© de s√©parabilit√© des classes √† l‚Äôaide
d‚Äôune caract√©ristique donn√©e</p></li>
</ul>
<div class="math notranslate nohighlight">
\[F_i = \frac{\displaystyle\sum_{c=1}^C n_c\left (\mu_c^i-\mu_i \right )^2}{\displaystyle\sum_{c=1}^C n_c(\Sigma_c^i)^2}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(n_c, \mu_c^i\)</span> et <span class="math notranslate nohighlight">\(\Sigma_c^i\)</span> sont l‚Äôeffectif, la moyenne et l‚Äô√©cart-type de la caract√©ristique  <span class="math notranslate nohighlight">\(f_i\)</span> dans la classe <span class="math notranslate nohighlight">\(c\)</span></p>
<ul class="simple">
<li><p>l‚Äôinformation mutuelle</p></li>
</ul>
<div class="math notranslate nohighlight">
\[I(i) = \displaystyle\sum_{\mathbf x_i} \displaystyle\sum_{y}P(X=\mathbf x_i,Y=y)log\left ( \frac{P(X=\mathbf x_i,Y=y)}{P(X=\mathbf x_i)P(Y=y)}\right )\]</div>
<p>qui mesure la d√©pendance entre les distributions de deux populations. Ici <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont deux variables al√©atoires dont les r√©alisations sont les valeurs de <span class="math notranslate nohighlight">\(f_i\)</span> et des √©tiquettes de classes. Les probabilit√©s sont estim√©es de mani√®re fr√©quentiste.</p>
<p>Dans l‚Äôexemple suivant, on choisit de garder <span class="math notranslate nohighlight">\(|\hat{F|}=2\)</span> descripteurs, en contr√¥lant la pertinence par l‚Äôinformation mutuelle en classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">mutual_info_classif</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des donn√©es avant : &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">mutual_info_classif</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des donn√©es apr√®s : &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables s√©lectionn√©es : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des donn√©es avant :  (150, 4)
Taille des donn√©es apr√®s :  (150, 2)
Variables s√©lectionn√©es :  [False False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="methodes-enveloppantes">
<h5>M√©thodes enveloppantes<a class="headerlink" href="#methodes-enveloppantes" title="Permalink to this heading">#</a></h5>
<p>Le principal inconv√©nient des approches pr√©c√©dentes est le fait qu‚Äôelles ignorent l‚Äôinfluence des caract√©ristiques s√©lectionn√©es sur la performance de l‚Äôalgorithme √† utiliser par la suite. Les m√©thodes de type enveloppantes (wrappers)  √©valuent un sous-ensemble de caract√©ristiques par sa performance
de classification en utilisant un algorithme d‚Äôapprentissage.  Les sous-ensembles de caract√©ristiques s√©lectionn√©s par cette m√©thode sont bien adapt√©s √† l‚Äôalgorithme de classification utilis√©, mais ils ne sont pas n√©cessairement pour un autre. De plus, la complexit√© de l‚Äôalgorithme d‚Äôapprentissage rend ces m√©thodes co√ªteuses.</p>
<p>Les principales diff√©rences entre les filtres et les m√©thodes enveloppantes pour la s√©lection des caract√©ristiques sont les suivantes :</p>
<ul class="simple">
<li><p>Les filtres mesurent la pertinence des caract√©ristiques par leur corr√©lation avec la variable d√©pendante, tandis que les m√©thodes enveloppantes mesurent l‚Äôutilit√© d‚Äôun sous-ensemble de caract√©ristiques en entra√Ænant un mod√®le sur celles-ci.</p></li>
<li><p>Les filtres sont beaucoup plus rapides que les m√©thodes enveloppantes car elles n‚Äôimpliquent pas l‚Äôapprentissage des mod√®les. D‚Äôun autre c√¥t√©, les m√©thodes enveloppantes sont √©galement tr√®s co√ªteuses en termes de calcul.</p></li>
<li><p>Les filtres utilisent des m√©thodes statistiques pour l‚Äô√©valuation d‚Äôun sous-ensemble de caract√©ristiques, tandis que les m√©thodes enveloppantes utilisent la validation crois√©e.</p></li>
<li><p>Les filtres peuvent √©chouer √† trouver le meilleur sous-ensemble de caract√©ristiques dans de nombreuses occasions, mais les m√©thodes enveloppantes peuvent toujours fournir le meilleur sous-ensemble de caract√©ristiques.</p></li>
<li><p>L‚Äôutilisation d‚Äôun sous-ensemble de caract√©ristiques √† partir des m√©thodes enveloppantes am√®ne plus facilement au ph√©nom√®ne de surapprentissage</p></li>
</ul>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 13 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les wrappers s√©lectionnent les caract√©ristiques en se fondant sur une estimation du risque r√©el.</p>
</div>
</div></div>
<div class="section" id="methodes-integrees">
<h5>M√©thodes int√©gr√©es<a class="headerlink" href="#methodes-integrees" title="Permalink to this heading">#</a></h5>
<p>Les m√©thodes int√©gr√©es incluent la s√©lection de variables lors du processus d‚Äôapprentissage. Un tel m√©canisme int√©gr√© pour la s√©lection des caract√©ristiques peut √™tre trouv√©, par
exemple, dans les algorithmes de type SVM,  AdaBoost  ou dans les
arbres de d√©cision.</p>
</div>
</div>
</div>
<div class="section" id="quelques-methodes-de-selection">
<h3>Quelques m√©thodes de s√©lection<a class="headerlink" href="#quelques-methodes-de-selection" title="Permalink to this heading">#</a></h3>
<div class="section" id="suppression-des-descripteurs-a-variance-faible">
<h4>Suppression des descripteurs √† variance faible<a class="headerlink" href="#suppression-des-descripteurs-a-variance-faible" title="Permalink to this heading">#</a></h4>
<p>Une premi√®re id√©e simple consiste √† supprimer les descripteurs ayant une faible variance, ces derniers n‚Äô√©tant pas discriminants dans la d√©finition des individus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">X2</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Avant s√©lection, &quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Apr√®s s√©lection, &quot;</span><span class="p">,</span><span class="n">X2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables s√©lectionn√©es : &quot;</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Avant s√©lection,  (150, 4)
Apr√®s s√©lection,  (150, 3)
Variables s√©lectionn√©es :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithmes-de-selection-sequentielle">
<h4>Algorithmes de s√©lection s√©quentielle<a class="headerlink" href="#algorithmes-de-selection-sequentielle" title="Permalink to this heading">#</a></h4>
<p>Les algorithmes SFS (Sequential Forward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>) et SBS (Sequential Backward Selection, <a class="reference internal" href="#SFS">Algorithm 1</a>-rouge) ont √©t√© les premiers √† √™tre propos√©s. Ils utilisent des approches heuristiques de recherche en partant, pour la premi√®re, d‚Äôun ensemble de caract√©ristiques vide et pour la seconde de  <span class="math notranslate nohighlight">\(F\)</span> tout entier.</p>
<div class="proof algorithm admonition" id="SFS">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algorithmes SFS et SBS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(F = \left (f_1\cdots f_d\right )\)</span>, taille de l‚Äôensemble final  <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\leftarrow F\)</span></span>)</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> √† <span class="math notranslate nohighlight">\( T\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(i=1\)</span> √† <span class="math notranslate nohighlight">\(d-T\)</span></span>)</p>
<ol class="arabic simple">
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> √† <span class="math notranslate nohighlight">\( |{F}|\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(j=1\)</span> √† <span class="math notranslate nohighlight">\(|\hat{F}|\)</span></span>)</p>
<ol class="arabic simple">
<li><p>Evaluer <span class="math notranslate nohighlight">\(\{f_j\}\cup \hat{F}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus \{f_j\}\)</span></span>)</p></li>
</ol>
</li>
<li><p><span class="math notranslate nohighlight">\(f_{max}\)</span> = meilleure caract√©ristique <span class="math notranslate nohighlight">\(\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(f_{min}\)</span>=moins bonne caract√©ristique</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow\hat{F}\cup\{f_{max}\}, F=F\setminus f_{max}\quad\)</span> (<span style="color:red"><span class="math notranslate nohighlight">\(\hat{F}\setminus\hat{F}f_{min}\)</span></span>)</p></li>
</ol>
</li>
</ol>
</div>
</div><p>L‚Äô√©tape d‚Äô√©valuation utilise des donn√©es d‚Äôapprentissage : une heuristique √©value, sur un crit√®re de performance, l‚Äôint√©r√™t d‚Äôajouter (ou de supprimer) le descripteur <span class="math notranslate nohighlight">\(f_i\)</span>.</p>
<p>Des variantes autour de ces algorithmes simples ont √©t√© propos√©es depuis et par exemple :</p>
<ul class="simple">
<li><p>il est possible √† chaque it√©ration d‚Äôinclure (ou d‚Äôexclure) un sous-ensemble de caract√©ristiques, plut√¥t qu‚Äôune seule (m√©thodes GSFS et GSBS)</p></li>
<li><p>on peut appliquer <span class="math notranslate nohighlight">\(p\)</span> fois SFS puis <span class="math notranslate nohighlight">\(q\)</span> fois SBS, de mani√®re it√©rative, avec <span class="math notranslate nohighlight">\(p,q\)</span> des param√®tres qui peuvent √©voluer au cours des it√©rations (algorithme SFFS et SFBS)</p></li>
</ul>
<p>Dans l‚Äôexemple suivant, l‚Äôheuristique choisie est l‚Äôalgorithme des 3 plus proches voisins et la mesure de performance sous-jacente est la mesure de validation crois√©e.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span> <span class="o">=</span> <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sfs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des donn√©es avant s√©lection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des donn√©es apr√®s s√©lection&quot;</span><span class="p">,</span><span class="n">sfs</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables s√©lectionn√©es : &quot;</span><span class="p">,</span> <span class="n">sfs</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des donn√©es avant s√©lection (150, 4)
Taille des donn√©es apr√®s s√©lection (150, 3)
Variables s√©lectionn√©es :  [ True False  True  True]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="algorithme-focus">
<h4>Algorithme Focus<a class="headerlink" href="#algorithme-focus" title="Permalink to this heading">#</a></h4>
<p>L‚Äôalgorithme de filtrage Focus (<a class="reference internal" href="#FOCUS">Algorithm 2</a>}) repose sur une recherche exhaustive sur <span class="math notranslate nohighlight">\(F\)</span> pour trouver le sous-ensemble le plus performant de taille optimale.</p>
<div class="proof algorithm admonition" id="FOCUS">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algorithme FOCUS)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , taille de l‚Äôensemble final  <span class="math notranslate nohighlight">\(T\)</span>, seuil <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(\hat{F}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow \emptyset\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> √† <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="arabic simple">
<li><p>chaque sous-ensemble <span class="math notranslate nohighlight">\(S_i\)</span> de taille <span class="math notranslate nohighlight">\(i\)</span></p>
<ol class="arabic simple">
<li><p>Si Inconsistance(A,<span class="math notranslate nohighlight">\(S_i\)</span>)&lt;<span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat{F}\leftarrow S_i\)</span></p></li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\hat{F}\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="algorithme-relief">
<h4>Algorithme relief<a class="headerlink" href="#algorithme-relief" title="Permalink to this heading">#</a></h4>
<p>La m√©thode relief en classification binaire (<a class="reference internal" href="#relief">Algorithm 3</a>), propose de calculer une mesure globale de la pertinence des caract√©ristiques en accumulant la diff√©rence des distances entre des exemples d‚Äôapprentissage choisis al√©atoirement et leurs plus proches voisins de la m√™me classe et de l‚Äôautre classe.</p>
<div class="proof algorithm admonition" id="relief">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Algorithme Relief)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(A= \{\mathbf x_i=\left (x_{i1} \cdots x_{id} \right )^T\in\mathbb{R}^d,1\leq i\leq n  \}\)</span> , nombre d‚Äôit√©rations <span class="math notranslate nohighlight">\(T\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> un vecteur de poids des caract√©ristiques, <span class="math notranslate nohighlight">\(w_i\in[-1,1],i\in[\![1,d]\!]\)</span></p>
<ol class="arabic simple">
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> √† <span class="math notranslate nohighlight">\( d\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(w_i\leftarrow 0\)</span></p></li>
</ol>
</li>
<li><p>Pour <span class="math notranslate nohighlight">\(i=1\)</span> √† <span class="math notranslate nohighlight">\( T\)</span></p>
<ol class="arabic simple">
<li><p>Choisir al√©atoirement un exemple <span class="math notranslate nohighlight">\(\mathbf x_k\)</span></p></li>
<li><p>Chercher deux plus proches voisins de <span class="math notranslate nohighlight">\(\mathbf x_k\)</span>, l‚Äôun (<span class="math notranslate nohighlight">\(\mathbf x_p\)</span>) dans sa  classe, l‚Äôautre (<span class="math notranslate nohighlight">\(\mathbf x_q\)</span>) dans l‚Äôautre classe</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(j=1\)</span> √† <span class="math notranslate nohighlight">\(d\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(w_j\leftarrow w_j+\frac{1}{nT}\left (|x_{kj} -x_{qj}|-|x_{kj} -x_{pj}| \right )\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="methode-sac">
<h4>M√©thode SAC<a class="headerlink" href="#methode-sac" title="Permalink to this heading">#</a></h4>
<p>L‚Äôalgorithme SAC (Selection Adaptative de Caract√©ristiques)  construit un ensemble de classifieurs (ou de r√©gresseurs) <span class="math notranslate nohighlight">\((M_1\cdots M_d)\)</span> appris sur chacun des descripteurs et s√©lectionne les meilleurs par discrimination lin√©aire de Fisher. Pour ce faire, l‚Äôalgorithme construit un vecteur dont les √©l√©ments sont les performances <span class="math notranslate nohighlight">\(Perf(M_i)\)</span> des mod√®les <span class="math notranslate nohighlight">\(M_i\)</span>, tri√©s par ordre d√©croissant. Deux moyennes <span class="math notranslate nohighlight">\(m_1(i)\)</span> et <span class="math notranslate nohighlight">\(m_2(i)\)</span> sont calcul√©es, qui repr√©sentent les deux moyennes de performance d‚Äôapprentissage qui ont une valeur respectivement plus grande (plus petite) que la performance du mod√®le <span class="math notranslate nohighlight">\(M_i\)</span> :</p>
<div class="math notranslate nohighlight">
\[m_1(i) = \frac{1}{i}\displaystyle\sum_{j=1}^i Perf (M_j)\textrm{ et } m_2(i) = \frac{1}{d-i}\displaystyle\sum_{j=i+1}^d Perf (M_j)\]</div>
<p>Deux variances des performances  <span class="math notranslate nohighlight">\(v_1^2(i)\)</span> et <span class="math notranslate nohighlight">\( v_2^2(i)\)</span> sont alors calcul√©es √† partir de ces moyennes, et le sous-ensemble de caract√©ristiques s√©lectionn√© est celui qui maximise le discriminant de Fisher</p>
<div class="math notranslate nohighlight">
\[\frac{|m_1(i)-m_2(i)|}{v_1^2(i)+v_2^2(i)}\]</div>
</div>
<div class="section" id="algorithme-rfe">
<h4>Algorithme RFE<a class="headerlink" href="#algorithme-rfe" title="Permalink to this heading">#</a></h4>
<p>L‚Äôalgorithme RLE (Recusrive Feature Elimination) trie les descripteurs en analysant, localement, la sensibilit√© de la performance.
√âtant donn√© un pr√©dicteur <span class="math notranslate nohighlight">\(f\)</span> qui attribue des poids aux caract√©ristiques (par exemple, les coefficients d‚Äôun mod√®le lin√©aire), l‚Äôobjectif de l‚Äôalgorithme est de s√©lectionner les caract√©ristiques en consid√©rant de mani√®re r√©cursive des ensembles de caract√©ristiques de plus en plus petits. Tout d‚Äôabord, le pr√©dicteur <span class="math notranslate nohighlight">\(f\)</span> est entra√Æn√© sur l‚Äôensemble initial de caract√©ristiques et l‚Äôimportance de chaque caract√©ristique est calcul√©e par un algorithme d√©di√© (crit√®re de Gini, entropie‚Ä¶). Les caract√©ristiques les moins importantes sont √©limin√©es de l‚Äôensemble actuel de caract√©ristiques. Cette proc√©dure est r√©p√©t√©e de mani√®re r√©cursive sur l‚Äôensemble √©lagu√© jusqu‚Äô√† ce que le nombre souhait√© de caract√©ristiques √† s√©lectionner soit finalement atteint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span> 
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taille des donn√©es avant s√©lection&quot;</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Variables s√©lectionn√©es : &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">get_support</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classement des variables : &quot;</span><span class="p">,</span><span class="n">s</span><span class="o">.</span><span class="n">ranking_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Taille des donn√©es avant s√©lection (150, 4)
Variables s√©lectionn√©es :  [False False  True  True]
Classement des variables :  [2 3 1 1]
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-acp"></span><p>Les m√©thodes factorielles ont pour but de traiter et visualiser des donn√©es multidimensionnelles. La prise en compte simultan√©e de l‚Äôensemble des variables est un probl√®me difficile, rendu parfois plus simple car l‚Äôinformation apport√©e par les variables est redondante. Les m√©thodes factorielles visent alors √† exploiter cette redondance pour tenter de remplacer les variables initiales par un nombre r√©duit de nouvelles variables, conservant au mieux l‚Äôinformation initiale.</p>
<p>Les principales m√©thodes de ce type incluent l‚Äôanalyse factorielle des correspondances, l‚Äôanalyse des correspondances multiples, l‚Äôanalyse factorielle d‚Äôun tableau de distance (pour les tableaux de proximit√©) ou encore l‚Äôanalyse factorielle discriminante. Ces m√©thodes sont propos√©es en annexe de ce cours.</p>
<p>Nous nous int√©ressons ici √† une m√©thode de r√©duction de dimension lin√©aire sur donn√©es quantitatives, l‚Äôanalyse en composantes principales.</p>
<div class="tex2jax_ignore mathjax_ignore section" id="analyse-en-composantes-principales">
<h2>Analyse en composantes principales<a class="headerlink" href="#analyse-en-composantes-principales" title="Permalink to this heading">#</a></h2>
<span class="target" id="index-0"></span><p id="index-1">Pour les donn√©es quantitatives, l‚ÄôAnalyse en Composantes Principales (ACP) est l‚Äôune des m√©thodes les plus utilis√©es. Elle consid√®re que les nouvelles variables sont des combinaisons lin√©aires des variables initiales, non corr√©l√©es.</p>
<p><img alt="" src="_images/acpintro.png" /></p>
<p>Dans la suite, les donn√©es seront des tableaux <span class="math notranslate nohighlight">\(n\times d\)</span> de variables quantitatives, une ligne √©tant un individu, et les colonnes d√©crivant les param√®tres mesur√©s. Les observations de <span class="math notranslate nohighlight">\(d\)</span> variables sur <span class="math notranslate nohighlight">\(n\)</span> individus sont donc rassembl√©es dans une matrice <span class="math notranslate nohighlight">\({\bf X}\in\mathcal{M}_{n,d}(\mathbb{R})\)</span> .  On notera <span class="math notranslate nohighlight">\(x^j\)</span> la j-√®me variable, identifi√©e par la j-√®me colonne <span class="math notranslate nohighlight">\({\bf X_{\bullet,j}}\)</span> de <span class="math notranslate nohighlight">\({\bf X}\)</span>, et <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> le i-√®me individu (i.e. <span class="math notranslate nohighlight">\({\bf X_{i,\bullet}^T}\)</span>).</p>
<div class="section" id="principe-de-la-methode">
<h3>Principe de la m√©thode<a class="headerlink" href="#principe-de-la-methode" title="Permalink to this heading">#</a></h3>
</div>
<div class="section" id="pre-traitement-du-tableau">
<h3>Pr√©-traitement du tableau<a class="headerlink" href="#pre-traitement-du-tableau" title="Permalink to this heading">#</a></h3>
<p>En analyse en composantes principales, on raisonne souvent sur des variables centr√©es et/ou r√©duites.</p>
<div class="section" id="donnees-centrees">
<h4>Donn√©es centr√©es<a class="headerlink" href="#donnees-centrees" title="Permalink to this heading">#</a></h4>
<p>Notons <span class="math notranslate nohighlight">\(\mathbf{g} = \left ( \bar{x}^1\cdots \bar{x}^d\right )\)</span> le vecteur des moyennes arithm√©tiques de chaque variable (centre de gravit√©) :</p>
<p><span class="math notranslate nohighlight">\(\mathbf{g}={\bf X^TD\mathbf{1}}\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\({\bf D}\)</span> est une matrice diagonale de poids,  chaque <span class="math notranslate nohighlight">\(d_{ii}\)</span> donnant l‚Äôimportance de l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span> dans les donn√©es (le plus souvent <span class="math notranslate nohighlight">\({\bf D}=\frac{1}{n}{ \mathbb{I}}\)</span>),  et <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> est le vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont toutes les composantes sont √©gales √† 1. Le tableau <span class="math notranslate nohighlight">\({\bf Y}={\bf X}-\mathbf{1}\mathbf{g}^T=({ \mathbb{I}}-\mathbf{1}\mathbf{1}^T{\bf D}){\bf X}\)</span> est le tableau centr√© associ√© √† <span class="math notranslate nohighlight">\({\bf X}\)</span>.</p>
</div>
<div class="section" id="donnees-reduites">
<h4>Donn√©es r√©duites<a class="headerlink" href="#donnees-reduites" title="Permalink to this heading">#</a></h4>
<p>La matrice de variance/covariance des donn√©es centr√©es est √©gale √†
<span class="math notranslate nohighlight">\({\bf V} = {\bf X^TDX} - \mathbf{g}\mathbf{g^T} = {\bf Y^TDY}\)</span>.</p>
<p>Si on note <span class="math notranslate nohighlight">\({\bf D_{1/\sigma}}\)</span> la matrice diagonale des inverses des √©carts-types des variables, alors  <span class="math notranslate nohighlight">\({\bf Z}={\bf YD_{1/\sigma}}\)</span>
est la matrice des donn√©es centr√©es r√©duites. La matrice <span class="math notranslate nohighlight">\({\bf R}={\bf D_{1/\sigma}VD_{1/\sigma}}={\bf Z^TDZ}\)</span>
est la matrice de corr√©lation des donn√©es et r√©sume la structure des d√©pendances lin√©aires entre les <span class="math notranslate nohighlight">\(d\)</span> variables.</p>
</div>
</div>
<div class="section" id="projection-des-individus-sur-un-sous-espace">
<h3>Projection des individus sur un sous-espace<a class="headerlink" href="#projection-des-individus-sur-un-sous-espace" title="Permalink to this heading">#</a></h3>
<p>Le principe de la m√©thode est d‚Äôobtenir une repr√©sentation approch√©e du nuage des <span class="math notranslate nohighlight">\(n\)</span> individus dans un sous-espace <span class="math notranslate nohighlight">\(F_k\)</span> de dimension faible. Ceci s‚Äôeffectue par un m√©canisme de projection.</p>
<p>Le choix de l‚Äôespace de projection est dict√© par le crit√®re suivant, qui revient √† d√©former le moins possible les distances en projection : le sous-espace de dimension <span class="math notranslate nohighlight">\(k\)</span> recherch√© est tel que la moyenne des carr√©s des distances entre projections soit la plus grande possible. En d√©finissant l‚Äôinertie d‚Äôun nuage de points comme la moyenne pond√©r√©e des carr√©s des distances au centre de gravit√©, le crit√®re revient alors √† maximiser l‚Äôinertie du nuage projet√© sur <span class="math notranslate nohighlight">\(F_k\)</span>.</p>
<p>Soit <span class="math notranslate nohighlight">\({\bf P}\)</span> la projection orthogonale sur <span class="math notranslate nohighlight">\(F_k\)</span>. Le nuage de points projet√© est associ√© au tableau <span class="math notranslate nohighlight">\({\bf XP^T}\)</span> puisque chaque individu <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> se projette sur <span class="math notranslate nohighlight">\(F_k\)</span> selon un vecteur colonne <span class="math notranslate nohighlight">\(\mathbf{Pe_i}\)</span> ou ligne <span class="math notranslate nohighlight">\(\mathbf{e_i P^T}\)</span>.</p>
<p>La matrice de variance du tableau <span class="math notranslate nohighlight">\({\bf XP^T}\)</span> est, dans le cas o√π les variables sont centr√©es :
<span class="math notranslate nohighlight">\({\bf (XP^T)^TD(XP^T) }= {\bf PVP^T}\)</span>.
L‚Äôinertie du nuage projet√© est donc √©gale √† <span class="math notranslate nohighlight">\(Tr({\bf PVP^TM})\)</span>, o√π <span class="math notranslate nohighlight">\({\bf M}\)</span> est une matrice sym√©trique d√©finie positive de taille <span class="math notranslate nohighlight">\(d\)</span>, d√©finissant la distance entre deux individus</p>
<p><span class="math notranslate nohighlight">\(d^2(\mathbf{e_i},\mathbf{e_j}) = (\mathbf{e_i}-\mathbf{e_j})^T{\bf M}(\mathbf{e_i}-\mathbf{e_j})\)</span></p>
<p>Mais</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
Tr({\bf PVP^TM})&amp;=&amp;Tr({\bf PVMP})\quad \textrm{car }{\bf P^TM}={\bf MP}\\
&amp;=&amp; Tr({\bf VMP^2})\quad \textrm{car }Tr({\bf AB})=Tr({\bf BA})\\
&amp;=&amp;Tr({\bf VMP})\quad \textrm{car } P\textrm{ est une projection}
\end{eqnarray*}\)</span></p>
<p>Le probl√®me pos√© est donc de trouver la projection <span class="math notranslate nohighlight">\({\bf P}\)</span>, de rang <span class="math notranslate nohighlight">\(k\)</span> maximisant <span class="math notranslate nohighlight">\(Tr({\bf VMP})\)</span>. La projection <span class="math notranslate nohighlight">\({\bf P}\)</span> r√©alisant cette optimisation donnera alors <span class="math notranslate nohighlight">\(F_k\)</span>.</p>
<p>L‚Äôanalyse en composantes principales consiste alors, de mani√®re it√©rative, √† chercher un sous-espace de dimension 1 d‚Äôinertie maximale, puis le sous-espace de dimension 1 orthogonal au pr√©c√©dent d‚Äôinertie maximale et ainsi de suite. Elle s‚Äôappuie sur le r√©sultat suivant :</p>
<div class="proof theorem admonition" id="theorem-0">
<p class="admonition-title"><span class="caption-number">Theorem 12 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(F_k\)</span> un sous-espace portant l‚Äôinertie maximale. Alors le sous-espace de dimension <span class="math notranslate nohighlight">\(k+1\)</span> portant l‚Äôinertie maximale est la somme directe de <span class="math notranslate nohighlight">\(F_k\)</span> et de la droite orthogonale √† <span class="math notranslate nohighlight">\(F_k\)</span> portant l‚Äôinertie maximale.</p>
</div>
</div></div>
<div class="section" id="elements-principaux">
<h3>Elements principaux<a class="headerlink" href="#elements-principaux" title="Permalink to this heading">#</a></h3>
<span class="target" id="index-2"></span><div class="section" id="axes-principaux">
<span id="index-3"></span><h4>Axes principaux<a class="headerlink" href="#axes-principaux" title="Permalink to this heading">#</a></h4>
<p>Rechercher un sous-espace de dimension 1 d‚Äôinertie maximale revient √† rechercher une droite de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> passant par le centre de gravit√© des donn√©es <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> maximisant l‚Äôinertie du nuage projet√© sur cet axe. Soit <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> un vecteur directeur de cette droite. La projection orthogonale sur la droite est d√©finie par la matrice de projection</p>
<p><span class="math notranslate nohighlight">\(\mathbf P=\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\)</span></p>
<p>L‚Äôinertie du nuage projet√© sur <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span> vaut alors
<span class="math notranslate nohighlight">\(\begin{eqnarray*}
Tr({\bf VMP})&amp;=&amp;Tr\left ({\bf VM}\frac{\mathbf{a}\mathbf{a}^T{\bf M}}{\mathbf{a}^T{\bf M}\mathbf{a}}\right )\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr({\bf VM}\mathbf{a}\mathbf{a^T}{\bf M})\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}Tr(\mathbf{a^T}{\bf MVM}\mathbf{a})\quad \text{car } Tr(\mathbf{AB})=Tr(\mathbf{BA})\\
&amp;=&amp; \frac{1}{\mathbf{a^T}{\bf M}\mathbf{a}}\mathbf{a^T}{\bf MVM}\mathbf{a}\quad \text{car } \mathbf{a^T}{\bf MVM}\mathbf{a}\in\mathbb{R}
\end{eqnarray*}\)</span></p>
<p>La matrice <span class="math notranslate nohighlight">\({\bf MVM}\)</span> est la matrice d‚Äôinertie du nuage (√©gale √† la matrice de variance-covariance si <span class="math notranslate nohighlight">\({\bf M}=\mathbb I\)</span>).  Maximiser cette quantit√© revient √† annuler sa d√©riv√©e par rapport √† <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> d‚Äôo√π :</p>
<p><span class="math notranslate nohighlight">\(
\frac{d}{d\mathbf{a}}\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}}=\frac{(\mathbf{a^T}{\bf M}\mathbf{a})2{\bf MVM}\mathbf{a}-(\mathbf{a^T}{\bf MVM}\mathbf{a})2{\bf M}\mathbf{a}}{(\mathbf{a^T}{\bf M}\mathbf{a})^2}
\)</span>
et donc</p>
<p><span class="math notranslate nohighlight">\({\bf MVM}\mathbf{a}=\left (\frac{\mathbf{a^T}{\bf MVM}\mathbf{a}}{\mathbf{a^T}{\bf M}\mathbf{a}} \right ){\bf M}\mathbf{a}\)</span></p>
<p>soit <span class="math notranslate nohighlight">\({\bf VM}\mathbf{a}=\lambda \mathbf{a}\)</span> car <span class="math notranslate nohighlight">\({\bf M}\)</span> est de rang plein. Donc <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf VM}\)</span>, et <span class="math notranslate nohighlight">\(\lambda\)</span> est la plus grande des valeurs propres de <span class="math notranslate nohighlight">\({\bf VM}\)</span>. Or <span class="math notranslate nohighlight">\({\bf M}\)</span> est sym√©trique, elle est diagonalisable sur une base de vecteurs propres orthonorm√©s et on a le r√©sultat suivant :</p>
<div class="proof theorem admonition" id="theorem-1">
<p class="admonition-title"><span class="caption-number">Theorem 13 </span></p>
<div class="theorem-content section" id="proof-content">
<p>Le sous-espace <span class="math notranslate nohighlight">\(F_k\)</span> de dimension <span class="math notranslate nohighlight">\(k\)</span> portant l‚Äôinertie maximale est engendr√© par les <span class="math notranslate nohighlight">\(k\)</span> premiers vecteurs propres de <span class="math notranslate nohighlight">\({\bf VM}\)</span></p>
</div>
</div><p>Les droites port√©es par ces vecteurs propres sont les axes principaux. Dans la suite on supposera <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> <span class="math notranslate nohighlight">\(\mathbf M\)</span>-norm√©.</p>
</div>
<div class="section" id="facteurs-principaux">
<h4>Facteurs principaux<a class="headerlink" href="#facteurs-principaux" title="Permalink to this heading">#</a></h4>
<span class="target" id="index-4"></span><p id="index-5">On associe √†  <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span> la forme lin√©aire <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>, coordonn√©e orthogonale sur l‚Äôaxe <span class="math notranslate nohighlight">\(Lin(\mathbf{a})\)</span>. Le vecteur <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> d√©finit une combinaison lin√©aire des variables descriptives <span class="math notranslate nohighlight">\(x^1\cdots x^d\)</span>. A l‚Äôaxe principal <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est associ√© le facteur principal <span class="math notranslate nohighlight">\(\mathbf{u}=\mathbf{Ma}\)</span>. Puisque <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf VM}\)</span>, on peut alors √©crire</p>
<p><span class="math notranslate nohighlight">\({\bf MVM}\mathbf{a}=\lambda {\bf M}\mathbf{a}\)</span></p>
<p>et donc <span class="math notranslate nohighlight">\({\bf MV}\mathbf{u}=\lambda \mathbf{u}\)</span>.  Les facteurs principaux sont donc les vecteurs propres de <span class="math notranslate nohighlight">\({\bf MV}\)</span></p>
</div>
<div class="section" id="composantes-principales">
<h4>Composantes principales<a class="headerlink" href="#composantes-principales" title="Permalink to this heading">#</a></h4>
<p>Les composantes principales sont les √©l√©ments de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> d√©finis par <span class="math notranslate nohighlight">\(\mathbf{c_i}=\mathbf{Xu_i}\)</span>. Ce sont donc les vecteurs coordonn√©es des projections orthogonales des individus sur les axes propres <span class="math notranslate nohighlight">\(\mathbf{a_i}\)</span>.  Ce sont donc les combinaisons lin√©aires des <span class="math notranslate nohighlight">\(x^1\cdots x^p\)</span> de variance maximale sous la contrainte <span class="math notranslate nohighlight">\(\mathbf{u_i}^T{\bf M}\mathbf{u_i}=1\)</span>, et cette variance est √©gale √† la valeur propre <span class="math notranslate nohighlight">\(\lambda_i\)</span> associ√©e √† <span class="math notranslate nohighlight">\(\mathbf{a_i}\)</span>.</p>
<p>En pratique, l‚Äôanalyse en composantes principales consiste √† calculer les <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> par diagonalisation de <span class="math notranslate nohighlight">\({\bf MV}\)</span>, puis √† calculer les <span class="math notranslate nohighlight">\(\mathbf{c}=\mathbf{Xu}\)</span>. Le calcul explicite des vecteurs propres <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> n‚Äôa que peu d‚Äôint√©r√™t.</p>
</div>
<div class="section" id="reconstitution">
<h4>Reconstitution<a class="headerlink" href="#reconstitution" title="Permalink to this heading">#</a></h4>
<p>Il est possible de reconstituer le tableau <span class="math notranslate nohighlight">\({\bf X}\)</span> centr√© des donn√©es (ou une approximation par une matrice de rang <span class="math notranslate nohighlight">\(k\)</span>) en utilisant les composantes. En effet, puisque <span class="math notranslate nohighlight">\(\mathbf{Xu_j}=\mathbf{c_j}\)</span> on a</p>
<p><span class="math notranslate nohighlight">\({\bf X}\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}} = \displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)</span></p>
<p>Mais <span class="math notranslate nohighlight">\(\displaystyle\sum_j \mathbf{u_j}\mathbf{u_j}^T{\bf M^{-1}}=\mathbb{I}\)</span>  car les <span class="math notranslate nohighlight">\(\mathbf{u_j}\)</span> sont orthonorm√©s pour la m√©trique <span class="math notranslate nohighlight">\({\bf M^{-1}}\)</span> donc</p>
<p><span class="math notranslate nohighlight">\({\bf X}=\displaystyle\sum_j\mathbf{c_j}\mathbf{u_j}^T{\bf M^{-1}}\)</span>
et si l‚Äôon s‚Äôint√©resse √† l‚Äôapproximation de <span class="math notranslate nohighlight">\({\bf X}\)</span> on ne somme que les <span class="math notranslate nohighlight">\(k\)</span> premiers termes.</p>
<p>A noter que lorsque <span class="math notranslate nohighlight">\({\bf M}=\mathbb{I}, {\bf X}= \displaystyle\sum_j\sqrt{\lambda_j}\mathbf{z_j}\mathbf{v_j^T}\)</span> o√π les <span class="math notranslate nohighlight">\(\mathbf{z_j}\)</span> sont les vecteurs propres unitaires de <span class="math notranslate nohighlight">\({\bf XX^T}\)</span> et les <span class="math notranslate nohighlight">\(\mathbf{v_j}\)</span> les vecteurs propres unitaires de <span class="math notranslate nohighlight">\({\bf X^TX}\)</span> (d√©composition dite en valeurs singuli√®res).</p>
</div>
</div>
<div class="section" id="interpretation-des-resultats">
<h3>Interpr√©tation des r√©sultats<a class="headerlink" href="#interpretation-des-resultats" title="Permalink to this heading">#</a></h3>
</div>
<div class="section" id="quelle-dimension-pour-f-k">
<h3>Quelle dimension pour <span class="math notranslate nohighlight">\(F_k\)</span> ?<a class="headerlink" href="#quelle-dimension-pour-f-k" title="Permalink to this heading">#</a></h3>
<p>Le but premier de l‚ÄôACP est de r√©duire la dimension pour permettre une visualisation efficace des donn√©es, tout en pr√©servant l‚Äôinformation (ici repr√©sent√©e par la variance du nuage de points).  Il faut donc se doter d‚Äôoutils permettant de r√©pondre √† la question : quelle dimension pour <span class="math notranslate nohighlight">\(F_k\)</span> ? Il n‚Äôy a pas de r√©ponse th√©orique satisfaisante, l‚Äôessentiel √©tant d‚Äôavoir une repr√©sentation suffisamment expressive pour permettre une interpr√©tation correcte du nuage de points.
En pr√©ambule, il convient de remarquer que la r√©duction de dimension ne sera possible que si les variables <span class="math notranslate nohighlight">\(x^1\cdots x^d\)</span> ne sont pas ind√©pendantes.</p>
<div class="section" id="critere-theorique">
<h4>Crit√®re th√©orique<a class="headerlink" href="#critere-theorique" title="Permalink to this heading">#</a></h4>
<p>On d√©termine ici si les valeurs propres sont significativement diff√©rentes entre elles √† partir d‚Äôun certain rang : si la r√©ponse est n√©gative on conserve les
premi√®res valeurs propres.</p>
<p>On fait l‚Äôhypoth√®se que les <span class="math notranslate nohighlight">\(n\)</span> individus proviennent d‚Äôun tirage al√©atoire dans une population gaussienne  o√π <span class="math notranslate nohighlight">\(\lambda_{k+1}=\cdots =\lambda_{d}\)</span>. Si l‚Äôhypoth√®se est v√©rifi√©e, la moyenne arithm√©tique <span class="math notranslate nohighlight">\(\alpha\)</span> des <span class="math notranslate nohighlight">\(d-k\)</span> derni√®res valeurs propres et leur moyenne g√©om√©trique <span class="math notranslate nohighlight">\(\gamma\)</span> sont peu diff√©rentes. On admet que :</p>
<p><span class="math notranslate nohighlight">\(c=\left ( n-\frac{2p+11}{6}\right )(d-k) ln\frac{\alpha}{\gamma}\)</span>
suit une loi du <span class="math notranslate nohighlight">\(\chi^2\)</span> de degr√© de libert√© <span class="math notranslate nohighlight">\(\frac{(d-k+2)(d-k-1)}{2}\)</span> et on rejette l‚Äôhypoth√®se d‚Äô√©galit√© des <span class="math notranslate nohighlight">\(d-k\)</span> valeurs propres si <span class="math notranslate nohighlight">\(c\)</span> est trop grand.</p>
</div>
<div class="section" id="pourcentage-d-inertie">
<h4>Pourcentage d‚Äôinertie<a class="headerlink" href="#pourcentage-d-inertie" title="Permalink to this heading">#</a></h4>
<p>Le crit√®re couramment utilis√© est le pourcentage d‚Äôinertie totale expliqu√©e, qui s‚Äôexprime sur les <span class="math notranslate nohighlight">\(k\)</span> premiers axes par :</p>
<div class="math notranslate nohighlight">
\[\frac{\displaystyle\sum_{j=1}^k \lambda_j}{\displaystyle\sum_{j=1}^d \lambda_j}\]</div>
<p>Un seuil par exemple de 90% d‚Äôinertie totale expliqu√©e donne une valeur de <span class="math notranslate nohighlight">\(k\)</span> correspondante. Attention cependant, le pourcentage d‚Äôinertie doit faire intervenir le nombre de variables initiales.</p>
<p><img alt="" src="_images/scree.png" /></p>
</div>
<div class="section" id="mesures-locales">
<h4>Mesures locales<a class="headerlink" href="#mesures-locales" title="Permalink to this heading">#</a></h4>
<p>Le pourcentage d‚Äôinertie expliqu√©e est un crit√®re global qui doit √™tre compl√©t√© par d‚Äôautres consid√©rations. Supposons que le plan <span class="math notranslate nohighlight">\(F_2\)</span> explique une part importante d‚Äôinertie, et que, en projection sur ce plan, deux individus soient tr√®s proches. Cette proximit√© peut √™tre illusoire si les deux individus se trouvent √©loign√©s dans l‚Äôorthogonal de <span class="math notranslate nohighlight">\(F_2\)</span>. Pour prendre en compte ce ph√©nom√®ne, il faut envisager pour chaque individu <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> la qualit√© de sa repr√©sentation, souvent exprim√©e par le <strong>cosinus de l‚Äôangle entre le plan principal et le vecteur <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span></strong>. Si ce cosinus est grand, <span class="math notranslate nohighlight">\(\mathbf{e_i}\)</span> est voisin du plan, on peut  alors examiner la position de sa projection sur le plan par rapport √† d‚Äôautres points.</p>
<p>Dans la figure suivante, <span class="math notranslate nohighlight">\({\bf e_i} \)</span> et <span class="math notranslate nohighlight">\({\bf e_j}\)</span> se projettent sur <span class="math notranslate nohighlight">\(F_2\)</span> en <span class="math notranslate nohighlight">\({\bf p}\)</span> mais sont √©loign√©s dans <span class="math notranslate nohighlight">\(F_2^\perp\)</span>.</p>
<p><img alt="" src="_images/proj.png" /></p>
</div>
<div class="section" id="criteres-empiriques">
<h4>Crit√®res empiriques<a class="headerlink" href="#criteres-empiriques" title="Permalink to this heading">#</a></h4>
<p id="index-6">Lorsqu‚Äôon travaille sur donn√©es centr√©es r√©duites, on retient les composantes principales correspondant √† des valeurs propres sup√©rieures √† 1 (crit√®re de Kaiser) : en effet les composantes principales <span class="math notranslate nohighlight">\(c_j\)</span> √©tant des combinaisons lin√©aires des <span class="math notranslate nohighlight">\(z-j\)</span> de variance maximale <span class="math notranslate nohighlight">\(V(c_j)=\lambda\)</span>, seules les composantes de variance sup√©rieure √† celle des variables initiales pr√©sentent un int√©r√™t.</p>
</div>
</div>
<div class="section" id="interpretation-des-resultats-exemple">
<h3>Interpr√©tation des r√©sultats : exemple<a class="headerlink" href="#interpretation-des-resultats-exemple" title="Permalink to this heading">#</a></h3>
<p>Une analyse en composantes principales est r√©alis√©e sur un jeu de donn√©es compos√© de <span class="math notranslate nohighlight">\(d\)</span>=9 indicateurs de qualit√© pour <span class="math notranslate nohighlight">\(n\)</span>=329 villes am√©ricaines. Les paragraphes suivants sont illustr√©s par ces donn√©es.</p>
<div class="section" id="correlation-variables-facteurs">
<h4>Corr√©lation variables-facteurs<a class="headerlink" href="#correlation-variables-facteurs" title="Permalink to this heading">#</a></h4>
<p>Pour donner du sens aux composantes principales <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, il faut les relier aux variables initiales <span class="math notranslate nohighlight">\(x^j\)</span> en calculant les coefficients de corr√©lation lin√©aire  <span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)\)</span> et en seuillant ces coefficients en valeur absolue.</p>
<p>Lorsque l‚Äôon travaille sur des donn√©es centr√©es r√©duites (m√©trique <span class="math notranslate nohighlight">\(\mathbf D_{1/\sigma}\)</span>), le calcul de <span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)\)</span> se r√©duit √†</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\frac{\mathbf{c}^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}\)</span></p>
<p>Or <span class="math notranslate nohighlight">\(\mathbf{c}=Z\mathbf{u}\)</span> o√π <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>, facteur principal associ√© √† <span class="math notranslate nohighlight">\(\mathbf{c}\)</span>, est vecteur propre de la matrice de corr√©lation <span class="math notranslate nohighlight">\(\mathbf R\)</span> associ√© √† <span class="math notranslate nohighlight">\(\lambda\)</span>. Donc</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\frac{\mathbf{u}^T\mathbf Z^T\mathbf D\mathbf{z^j}}{\sqrt{\lambda}}=\frac{(\mathbf{z^j})^T\mathbf D\mathbf Z\mathbf{u}}{\sqrt{\lambda}}\)</span></p>
<p><span class="math notranslate nohighlight">\((\mathbf{z^j})^T\mathbf D\mathbf Z\)</span> est la <span class="math notranslate nohighlight">\(j^e\)</span> ligne de <span class="math notranslate nohighlight">\(\mathbf Z^T\mathbf D\mathbf Z=\mathbf R\)</span> donc <span class="math notranslate nohighlight">\((\mathbf{z^j})^T\mathbf D \mathbf Z \mathbf{u}\)</span> est la <span class="math notranslate nohighlight">\(j^e\)</span> composante de <span class="math notranslate nohighlight">\(\mathbf R\mathbf{u}=\lambda \mathbf{u}\)</span> d‚Äôo√π</p>
<p><span class="math notranslate nohighlight">\(r(\mathbf{c},x^j)=\sqrt{\lambda}u_j\)</span></p>
<p>Ces calculs s‚Äôeffectuent pour chaque composante principale. Pour un couple de composantes principales <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> et <span class="math notranslate nohighlight">\(\mathbf{c_2}\)</span> par exemple on repr√©sente fr√©quemment les corr√©lations sur une figure appel√©e ¬´ cercle des corr√©lations¬ª o√π chaque variable <span class="math notranslate nohighlight">\(x^j\)</span> est rep√©r√©e par un point d‚Äôabscisse <span class="math notranslate nohighlight">\(r(\mathbf{c_1},x^j)\)</span> et d‚Äôordonn√©e <span class="math notranslate nohighlight">\(r(\mathbf{c_2},x^j)\)</span>.</p>
<p><img alt="" src="_images/cercle.png" /></p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 14 </span></p>
<div class="remark-content section" id="proof-content">
<p>Attention de ne pas interpr√©ter des proximit√©s entre points variables, si ceux-ci ne sont pas proches de la circonf√©rence.</p>
</div>
</div><p>Notons que dans le cas de la m√©trique <span class="math notranslate nohighlight">\(D_{1/\sigma}\)</span>, le cercle des corr√©lations est la projection de l‚Äôensemble des variables centr√©es-r√©duites sur le sous-espace engendr√© par <span class="math notranslate nohighlight">\(\mathbf{c_1},\mathbf{c_2}\)</span>. En ce sens, le cercle de corr√©lation est le pendant, dans l‚Äôespace des variables, de la projection des individus sur le premier plan principal.</p>
</div>
</div>
<div class="section" id="positionnement-des-individus">
<h3>Positionnement des individus<a class="headerlink" href="#positionnement-des-individus" title="Permalink to this heading">#</a></h3>
<p>Dire que <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> est tr√®s corr√©l√©e √† <span class="math notranslate nohighlight">\(x^j\)</span> signifie que les individus ayant une forte coordonn√©e positive sur l‚Äôaxe 1 sont caract√©ris√©s par une valeur de <span class="math notranslate nohighlight">\(x^j\)</span> nettement sup√©rieure √† la moyenne.</p>
<p>Il est tr√®s utile aussi de calculer pour chaque axe la contribution apport√©e par les divers individus √† cet axe. Si <span class="math notranslate nohighlight">\(c_{ki}\)</span> est la valeur de la composante <span class="math notranslate nohighlight">\(k\)</span> pour le <span class="math notranslate nohighlight">\(i^e\)</span> individu, alors par construction</p>
<div class="math notranslate nohighlight">
\[\displaystyle\sum_{i=1}^np_ic_{ki}^2=\lambda_k\]</div>
<p>o√π <span class="math notranslate nohighlight">\(p_i\)</span> est le poids de l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span>. On appelle alors contribution de l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span> √† la composante <span class="math notranslate nohighlight">\(\mathbf{c_k}\)</span> la quantit√© <span class="math notranslate nohighlight">\(\frac{p_ic_{ki}^2}{\lambda_k}\)</span>. Dans le cas o√π le poids est diff√©rent de <span class="math notranslate nohighlight">\(1/n\)</span> (certains individus sont ‚Äúplus importants‚Äù que d‚Äôautres), la contribution est riche d‚Äôinterpr√©tation. Dans le cas contraire, elle n‚Äôapporte rien de plus que les coordonn√©es de l‚Äôindividu.</p>
<p>On peut alors positionner les individus sur les sous-espaces des premi√®res composantes principales (plans factoriels). La figure suivante pr√©sente le positionnement de 329 villes am√©ricaines, o√π les 9 variables de qualit√© de vie pr√©c√©dentes ont √©t√© mesur√©es. Par soucis de lisibilit√©, seul les villes qui contribuent le plus √† la cr√©ation de la premi√®re composante principale ont leurs noms inscrits.</p>
<p><img alt="" src="_images/individus.png" /></p>
<p>On peut √©galement superposer les deux informations pr√©c√©dentes pour corr√©ler le positionnement des villes selon les variables originales. La figure suivante pr√©sente les 329 villes pr√©c√©dentes, plong√©es dans <span class="math notranslate nohighlight">\(F_3\)</span>, les  anciennes variables √©tant mat√©rialis√©es par des vecteurs dont la direction et la norme indiquent √† quel point chaque variable contribue aux 3 premi√®res composantes principales.</p>
<p><img alt="" src="_images/biplot.png" /></p>
<p>Il n‚Äôest pas souhaitable, et ceci surtout pour les premi√®res composantes,  qu‚Äôun individu ait une contribution excessive car cela serait un facteur d‚Äôinstabilit√©, le fait de retirer cet individu modifiant profond√©ment le r√©sultat de l‚Äôanalyse. Si ce cas se produisait il y aurait int√©r√™t √† effectuer l‚Äôanalyse en √©liminant cet individu puis en le mettant en √©l√©ment suppl√©mentaire, s‚Äôil ne s‚Äôagit pas d‚Äôune donn√©e erron√©e qui a √©t√© ainsi mise en √©vidence.</p>
</div>
<div class="section" id="facteur-de-taille-facteur-de-forme">
<h3>Facteur de taille, facteur de forme<a class="headerlink" href="#facteur-de-taille-facteur-de-forme" title="Permalink to this heading">#</a></h3>
<p>Le th√©or√®me de Frobenius stipule qu‚Äôune matrice sym√©trique n‚Äôayant que des termes positifs admet un premier vecteur propre dont toutes les composantes sont de m√™me signe. Si ce signe est positif, la premi√®re composante est alors corr√©l√©e positivement avec toutes les variables et les individus sont rang√©s sur l‚Äôaxe 1 par valeurs croissantes de l‚Äôensemble des variables. Si de plus les corr√©lations entre variables sont du m√™me ordre de grandeur, la premi√®re composante principale est proportionnelle √† la moyenne des variables initiales. Cette premi√®re composante d√©finit alors un facteur de taille.</p>
<p>La deuxi√®me composante principale diff√©rencie alors des individus de ‚Äútaille‚Äù semblable, on l‚Äôappelle souvent facteur de forme.</p>
</div>
<div class="section" id="ajout-de-variable-et-ou-d-individu">
<h3>Ajout de variable et ou d‚Äôindividu<a class="headerlink" href="#ajout-de-variable-et-ou-d-individu" title="Permalink to this heading">#</a></h3>
<p>Toutes les interpr√©tations pr√©c√©dentes expliquent les r√©sultats √† l‚Äôaide des donn√©es initiales, qui ont permis de les calculer. On risque alors de prendre pour une propri√©t√© intrins√®que des donn√©es un simple artefact de la m√©thode (par exemple il existe de fortes corr√©lations entre la premi√®re composante principale et certaines variables, puisque <span class="math notranslate nohighlight">\(\mathbf{c_1}\)</span> maximise <span class="math notranslate nohighlight">\(\sum_j r^2(\mathbf{c},x^j)\)</span>).</p>
<p>En revanche une forte corr√©lation entre une composante principale et une variable qui n‚Äôa pas servi √† l‚Äôanalyse sera significative. D‚Äôo√π la pratique courante de partager en deux groupes l‚Äôensemble des variables : d‚Äôune part les variables actives qui servent √† d√©terminer les axes principaux, d‚Äôautre part les variables passives ou suppl√©mentaires que l‚Äôon relie a posteriori aux composantes principales. On distingue alors les variables suppl√©mentaires suivant leur type, num√©rique (√† placer dans les cercles de corr√©lation) ou qualitative (donn√©e d‚Äôune partition des <span class="math notranslate nohighlight">\(n\)</span> individus en <span class="math notranslate nohighlight">\(k\)</span> classes).</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this heading">#</a></h3>
<p>On √©tudie les consommations annuelles en 1972, exprim√©es en devises, de 8 denr√©es alimentaires (les variables), les individus √©tant 8 cat√©gories socio-professionnelles (CSP) . Les donn√©es sont des moyennes par CSP :</p>
<p><span class="math notranslate nohighlight">\(
\begin{array}{|c|cccccccc|}
\hline
  &amp;PAO  &amp;PAA  &amp;VIO&amp; VIA&amp;  POT&amp;  LEC &amp;RAI&amp; PLP\\
\hline
AGRI  &amp;167  &amp;1  &amp;163&amp; 23&amp; 41  &amp;8&amp; 6 &amp;6\\
SAAG  &amp;162&amp; 2 &amp;141&amp; 12  &amp;40 &amp;12&amp;  4&amp;  15\\
PRIN  &amp;119&amp; 6 &amp;69 &amp;56 &amp;39&amp;  5 &amp;13 &amp;41\\
CSUP  &amp;87 &amp;11 &amp;63 &amp;111&amp; 27&amp; 3 &amp;18 &amp;39\\
CMOY  &amp;103  &amp;5  &amp;68 &amp;77 &amp;32&amp;  4 &amp;11 &amp;30\\
EMPL  &amp;111  &amp;4  &amp;72 &amp;66 &amp;34&amp;  6 &amp;10 &amp;28\\
OUVR  &amp;130  &amp;3  &amp;76 &amp;52 &amp;43&amp;  7 &amp;7  &amp;16\\
INAC  &amp;138  &amp;7  &amp;117  &amp;74&amp;  53&amp; 8 &amp;12 &amp;20\\
\hline
\end{array}
\)</span></p>
<p>avec les notations suivantes :</p>
<p>AGRI = Exploitants agricoles, SAAG= Salari√©s agricoles,   PRIN = Professions ind√©pendantes, CSUP = Cadres sup√©rieurs, CMOY= Cadres moyens, EMPL= Employ√©s, OUVR = Ouvriers, INAC = Inactifs.</p>
<p>et</p>
<p>PAO = Pain ordinaire, PAA = Autre pain, VIO = Vin ordinaire, VIA=Autre vin, POT= Pommes de terre, LEC=L√©gumes secs, RAI=Raisin de table, PLP= Plats pr√©par√©s.</p>
<p>La matrice de corr√©lation des variables est alors</p>
<p><span class="math notranslate nohighlight">\(\begin{pmatrix}
   1.0000   &amp;  -.7737    &amp; 0.9262    &amp; -.9058    &amp; 0.6564  &amp;   0.8886   &amp;  -.8334  &amp;   -.8558\\
    -.7737    &amp; 1.0000    &amp; -.6040    &amp; 0.9044    &amp; -.3329    &amp; -.6734    &amp; 0.9588    &amp; 0.7712\\
   0.9262    &amp; -.6040    &amp; 1.0000    &amp; -.7502    &amp; 0.5171    &amp; 0.7917   &amp;  -.6690     &amp;-.8280\\
  -.9058    &amp; 0.9044    &amp; -.7502    &amp; 1.0000    &amp; -.4186    &amp; -.8386    &amp; 0.9239     &amp;0.7198\\
    0.6564   &amp;  -.3329    &amp; 0.5171    &amp; -.4186    &amp; 1.0000   &amp;  0.6029   &amp;  -.4099    &amp; -.5540\\
  0.8886   &amp;  -.6734    &amp; 0.7917   &amp;  -.8386    &amp; 0.6029   &amp;  1.0000   &amp;  -.8245    &amp; -.7509\\
  -.8334    &amp; 0.9588    &amp; -.6690    &amp; 0.9239    &amp; -.4099    &amp; -.8245    &amp; 1.0000     &amp;0.8344\\
   -.8558    &amp; 0.7712   &amp;  -.8280   &amp;  0.7198   &amp;  -.5540    &amp; -.7509  &amp;   0.8344    &amp; 1.0000\\
\end{pmatrix}\)</span></p>
<p>et son analyse spectrale donne</p>
<p><span class="math notranslate nohighlight">\(\begin{array}{|c||c|c|c|}
\hline
                      &amp;    \textrm{Valeur propre}  &amp;      \textrm{Variance expliqu√©e}  &amp;  \textrm{Variance cumulative expliqu√©e}\\
\hline
                     1  &amp;  6.20794684      &amp;      0.7760  &amp;      0.7760\\
                     2   &amp; 0.87968139      &amp;      0.1100    &amp;    0.8860\\
                     3    &amp;0.41596112    &amp;        0.0520      &amp;  0.9379\\
                     4    &amp;0.30645467    &amp;        0.0383      &amp;  0.9763\\
                     5    &amp;0.16844150    &amp;        0.0211      &amp;  0.9973\\
                     6    &amp;0.01806771    &amp;       0.0023       &amp; 0.9996\\
                     7    &amp;0.00344677    &amp;       0.0004       &amp; 1.0000\\
                     8    &amp;0.00000000        &amp;              0.0000      &amp;  1.0000\\
\hline
\end{array}\)</span></p>
<p>Le crit√®re de Kaiser  conduit √† s√©lectionner un seul axe, qui retient 77% de l‚Äôinertie totale. L‚Äôaxe 2 retenant 11% de l‚Äôinertie, il peut √™tre  int√©ressant de le rajouter √† l‚Äô√©tude pour expliquer pr√®s de 90% de la variance des donn√©es. Les suivantes repr√©sentent les variables et les individus dans le plan des deux premiers vecteurs propres.</p>
<p><img alt="" src="_images/ex1.png" /></p>
<p>L‚Äôinterpr√©tation de ce plan se fait s√©quentiellement, pour chaque axe et chaque nuage de points, en regardant les contributions √† la formation des axes:</p>
<ul class="simple">
<li><p>Axe 1 :</p></li>
</ul>
<p>1- <strong>Variables</strong> :  les variables contribuant le plus √† la formation de l‚Äôaxe 1 sont celles dont les coordonn√©es sur cet axe sont proches de 1 en valeur absolue.
PAA et VIO sont tr√®s proches de la contribution moyenne, on les int√®gre donc dans l‚Äôinterpr√©tation de l‚Äôaxe si elles vont dans le sens de l‚Äôinterpr√©tation que l‚Äôon peut en faire, sans elles. L‚Äôaxe 1 oppose les individus consommant du pain ordinaire, des l√©gumes secs (et √©ventuellement du vin ordinaire) √† ceux qui consomment du raisin, du vin (√©ventuellement du pain) plus sophistiqu√© et des plats pr√©par√©s. L‚Äôaxe 1, et donc la premi√®re composante principale, mesure la r√©partition entre aliments ordinaires bon march√© et aliments plus recherch√©s.</p>
<p>Toutes les variables sont bien repr√©sent√©es sur l‚Äôaxe (la qualit√© de repr√©sentation est √©gale √† la coordonn√©e au carr√©). D‚Äôun point de vue graphique, une variable bien repr√©sent√©e est proche du bord du cercle des corr√©lation et √† proximit√© de l‚Äôaxe. La premi√®re composante principale explique donc correctement tous les types de consommations alimentaires.</p>
<p>2- <strong>Individus</strong> : de m√™me, les individus contribuant le plus √† la formation de l‚Äôaxe 1 sont ceux dont les coordonn√©es sur cet axe sont les plus √©lev√©es en valeur absolue. Le premier axe met donc en opposition les agriculteurs et les cadres sup√©rieurs quant √† leurs habitudes alimentaires. Les autres cat√©gories socio-professionnelles, assez bien repr√©sent√©es sur l‚Äôaxe √† l‚Äôexception des inactifs (cf. contributions des individus sur l‚Äôaxe 1), s‚Äô√©chelonnent suivant la hi√©rarchie habituelle. Elles sont bien expliqu√©es par l‚Äôaxe.</p>
<ul class="simple">
<li><p>Axe 2 :</p></li>
</ul>
<p>1- <strong>Variables</strong> : L‚Äôaxe 2 est d√©fini par les variables POT et PAA. Compte tenu de la diff√©rence de contribution existant entre ces deux variables, de la contribution √©lev√©e de POT (55%), et de la qualit√© de repr√©sentation moyenne de PAA, la deuxi√®me composante principale peut √™tre consid√©r√©e comme essentiellement li√©e √† la consommation de pommes de terre. Les variables, √† l‚Äôexception de POT et de PAA (dans une moindre mesure) sont assez mal repr√©sent√©es sur l‚Äôaxe. La deuxi√®me composante principale n‚Äôexplique donc qu‚Äôun aspect tr√®s particulier de la consommation alimentaire.</p>
<p>2- <strong>Individus</strong> : Pour rep√©rer les individus ayant une contribution significative, on compare les coordonn√©es des individus sur l‚Äôaxe 2, √† la racine de la deuxi√®me valeur propre  =0,94, le signe donnant le sens de contribution.</p>
<p>L‚Äôaxe 1 refl√®te donc l‚Äôopposition qui existe entre les cat√©gories socio-professionnelles dans leur alimentation, opposant les CSP modestes qui consomment des produits basiques aux cat√©gories favoris√©es qui consomment des produits plus recherch√©s. L‚Äôaxe 2 refl√®te quant √† lui la particularit√© des inactifs quant √† leur alimentation, fortement compos√©e de pommes de terre (un retour aux donn√©es d‚Äôorigine vient confirmer cette conclusion).</p>
</div>
<div class="section" id="implementation">
<h3>Impl√©mentation<a class="headerlink" href="#implementation" title="Permalink to this heading">#</a></h3>
<p>De nombreuses librairies Python permettent d‚Äôutiliser facilement l‚ÄôACP, notamment <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> qui propose une m√©thode <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA">PCA</a>.</p>
<p>Nous proposons ici d‚Äôimpl√©menter enti√®rement l‚ÄôACP, pour bien comprendre les m√©canismes de cette approche.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 

<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">mode</span><span class="o">.</span><span class="n">chained_assignment</span> <span class="o">=</span> <span class="kc">None</span>

<span class="c1"># Donn√©es</span>
<span class="n">vins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./data/vins.csv&quot;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">cat_vins</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">vins</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">variables</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">% a</span><span class="s1">lcool&#39;</span><span class="p">,</span> <span class="s1">&#39;acide malique&#39;</span><span class="p">,</span> <span class="s1">&#39;cendres&#39;</span><span class="p">,</span> <span class="s1">&#39;alcalinit√©&#39;</span><span class="p">,</span> <span class="s1">&#39;magn√©sium&#39;</span><span class="p">,</span> <span class="s1">&#39;ph√©nols&#39;</span> <span class="p">,</span> 
                <span class="s1">&#39;flavono√Ødes&#39;</span><span class="p">,</span> <span class="s1">&#39;non flavano√Ødes&#39;</span><span class="p">,</span> <span class="s1">&#39;proanthocyanidines&#39;</span><span class="p">,</span> <span class="s1">&#39;couleur&#39;</span><span class="p">,</span> <span class="s1">&#39;teinte&#39;</span><span class="p">,</span> 
                <span class="s1">&#39;OD280/OD315&#39;</span><span class="p">,</span><span class="s1">&#39;proline&#39;</span><span class="p">]</span>

<span class="c1"># Affichage d&#39;un tableau</span>
<span class="k">def</span> <span class="nf">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">tab</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CP&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span>
    <span class="n">r</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; </span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;  </span><span class="si">%8.8s</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">ind</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">%.2f</span><span class="s2"> </span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">r</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="si">%.2f</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">r</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="preparation-des-donnees">
<h3>Pr√©paration des donn√©es<a class="headerlink" href="#preparation-des-donnees" title="Permalink to this heading">#</a></h3>
<div class="section" id="id1">
<h4>Donn√©es centr√©es<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(g=X^TD{\bf 1}\)</span> = Vecteur des moyennes arithm√©tiques de chaque variable</p>
<p><span class="math notranslate nohighlight">\(D=\frac{1}{n}I\)</span> = Matrice diagonale de poids, chaque <span class="math notranslate nohighlight">\(d_{ii}\)</span> donnant l‚Äôimportance de l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span> dans les donn√©es</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">un</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Xt</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">un</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;g = </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>g = 
 [[1.30006180e+01]
 [2.33634831e+00]
 [2.36651685e+00]
 [1.94949438e+01]
 [9.97415730e+01]
 [2.29511236e+00]
 [2.02926966e+00]
 [3.61853933e-01]
 [1.59089888e+00]
 [5.05808988e+00]
 [9.57449438e-01]
 [2.61168539e+00]
 [7.46893258e+02]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(Y = X - {\bf 1} g^T = (I - {\bf 11}^TD)X\)</span> = Tableau centr√© associ√© √† <span class="math notranslate nohighlight">\(X\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">un</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id2">
<h4>Donn√©es r√©duites<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(V=X^TDX-gg^T=Y^TDY\)</span> = Matrice de variance/covariance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Yt</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(Z = Y D_{1/\sigma}\)</span> = Matrice des donn√©es centr√©es r√©duites</p>
<p><span class="math notranslate nohighlight">\(R = D_{1/\sigma}VD_{1/\sigma} = Z^T  D  Z\)</span> = Matrice (sym√©trique) de variance/covariance des donn√©es centr√©es r√©duites.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sigma</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Xt</span><span class="p">]</span>
<span class="n">i_sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="o">/</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">]</span>
<span class="n">D_sigma</span> <span class="o">=</span> <span class="n">i_sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">D_sigma</span><span class="p">,</span> <span class="n">V</span><span class="p">),</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Matrice de corr√©lation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dadc8d6f0827a8670b00d3dd4b51016465ea4e90b225ace4cfd49e35483f74ba.png" src="_images/dadc8d6f0827a8670b00d3dd4b51016465ea4e90b225ace4cfd49e35483f74ba.png" />
</div>
</div>
</div>
<div class="section" id="inertie-du-nuage-de-points">
<h4>Inertie du nuage de points<a class="headerlink" href="#inertie-du-nuage-de-points" title="Permalink to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(M\)</span> est une matrice sym√©trique d√©finie positive correspondant √† la m√©trique</p>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(M=D^2_{1/\sigma}\)</span> on calcule <span class="math notranslate nohighlight">\( \frac{1}{n}\displaystyle\sum_{i=1}^n (e_i - g)^T M (e_i-g) = \frac{1}{n}\displaystyle\sum_{i=1}^n (y_i)^T M y_i = Tr(VM)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(M=I\)</span> on calcule <span class="math notranslate nohighlight">\( \frac{1}{n}\displaystyle\sum_{i=1}^n (z_i)^T M z_i = Tr(RM)\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calcul_inertie_somme</span> <span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">inertie</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">inertie</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> 
    <span class="k">return</span> <span class="n">inertie</span> <span class="o">/</span> <span class="n">n</span>
    
<span class="k">def</span> <span class="nf">calcul_inertie_trace</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

<span class="c1"># Si les donn√©es sont centr√©es mais pas encore r√©duites  on travaille avec Y et V</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">D_sigma</span><span class="p">,</span> <span class="n">D_sigma</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_somme</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_trace</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>

<span class="c1"># Si les donn√©es sont centr√©es r√©duites, on travaille avec Z et R</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_somme</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">calcul_inertie_trace</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12.999999999999995
13.000000000000004
12.999999999999998
13.000000000000004
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="analyse-spectrale">
<h3>Analyse spectrale<a class="headerlink" href="#analyse-spectrale" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigenvalues</span><span class="p">,</span><span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="p">[</span><span class="n">eigenvectors</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="calcul-des-composantes-principales">
<h4>Calcul des composantes principales<a class="headerlink" href="#calcul-des-composantes-principales" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span> <span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">u</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="pourcentage-d-inertie-expliquee-par-un-axe">
<h4>Pourcentage d‚Äôinertie expliqu√©e par un axe<a class="headerlink" href="#pourcentage-d-inertie-expliquee-par-un-axe" title="Permalink to this heading">#</a></h4>
<p>Pourcentage d‚Äôinertie cumul√©e expliqu√©e par les <span class="math notranslate nohighlight">\(k\)</span> premiers axes : <span class="math notranslate nohighlight">\(\frac{\displaystyle\sum_{j=1}^k\lambda_j}{\displaystyle\sum_{j=1}^d\lambda_j}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i_lambda</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">/</span><span class="n">d</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">eigenvalues</span><span class="p">]</span>
<span class="n">i_cum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="o">/</span><span class="n">d</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">i_lambda</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Valeurs propres&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">i_cum</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">% d</span><span class="s1">e variance expliqu√©e&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2dad4ba32c4808dbcf57b41305c7abe36a143e9f223385008bc71d67354fc559.png" src="_images/2dad4ba32c4808dbcf57b41305c7abe36a143e9f223385008bc71d67354fc559.png" />
</div>
</div>
</div>
<div class="section" id="critere-de-kaiser">
<h4>Crit√®re de Kaiser<a class="headerlink" href="#critere-de-kaiser" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On retient &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nb_l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; axes&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On retient 3 axes
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="analyse-des-resultats">
<h3>Analyse des r√©sultats<a class="headerlink" href="#analyse-des-resultats" title="Permalink to this heading">#</a></h3>
<div class="section" id="id3">
<h4>Corr√©lation variables/facteurs<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="cercle-des-correlations-pour-un-couple-de-composantes-principales">
<h4>Cercle des corr√©lations pour un couple de composantes principales<a class="headerlink" href="#cercle-des-correlations-pour-un-couple-de-composantes-principales" title="Permalink to this heading">#</a></h4>
<p>Pour <span class="math notranslate nohighlight">\(c_1\)</span> et <span class="math notranslate nohighlight">\(c_2\)</span>, chaque variable <span class="math notranslate nohighlight">\(x_j\)</span> est rep√©r√©e par un point d‚Äôabscisse <span class="math notranslate nohighlight">\(r(c_1,x^j)\)</span> et d‚Äôordonn√©e <span class="math notranslate nohighlight">\(r(c_2, x_j)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">i1</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i2</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i3</span> <span class="o">=</span> <span class="n">i_lambda</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">i12</span> <span class="o">=</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i2</span>
<span class="n">i13</span> <span class="o">=</span> <span class="n">i1</span> <span class="o">+</span> <span class="n">i3</span>
<span class="n">i23</span> <span class="o">=</span> <span class="n">i2</span> <span class="o">+</span> <span class="n">i3</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i12</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i13</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP2/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i23</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">radius</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/890895d5f844f44dec5929957163b9bab5ada18fafe8471de9dd2494e8c4585a.png" src="_images/890895d5f844f44dec5929957163b9bab5ada18fafe8471de9dd2494e8c4585a.png" />
</div>
</div>
</div>
<div class="section" id="contribution-des-variables">
<h4>Contribution des variables<a class="headerlink" href="#contribution-des-variables" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contributions_variables</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">u</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
    <span class="n">contributions_variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">variables</span><span class="p">,</span> <span class="n">contributions_variables</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
  % alcool	0.02 	0.23 	0.04 	0.00 	0.07 	0.05 	0.00 	0.00 	0.16 	0.07 	0.26 	0.05 	0.04
  acide ma	0.06 	0.05 	0.01 	0.29 	0.00 	0.29 	0.18 	0.00 	0.00 	0.01 	0.01 	0.01 	0.10
   cendres	0.00 	0.10 	0.39 	0.05 	0.02 	0.02 	0.02 	0.02 	0.03 	0.00 	0.09 	0.25 	0.00
  alcalini	0.06 	0.00 	0.37 	0.00 	0.00 	0.01 	0.08 	0.01 	0.18 	0.00 	0.04 	0.23 	0.00
  magn√©siu	0.02 	0.09 	0.02 	0.12 	0.53 	0.00 	0.10 	0.00 	0.02 	0.00 	0.07 	0.01 	0.00
   ph√©nols	0.16 	0.00 	0.02 	0.04 	0.02 	0.01 	0.00 	0.22 	0.16 	0.09 	0.08 	0.09 	0.10
  flavono√Ø	0.18 	0.00 	0.02 	0.02 	0.01 	0.00 	0.00 	0.69 	0.04 	0.00 	0.00 	0.00 	0.03
  non flav	0.09 	0.00 	0.03 	0.04 	0.25 	0.07 	0.35 	0.01 	0.05 	0.00 	0.04 	0.01 	0.05
  proantho	0.10 	0.00 	0.02 	0.16 	0.02 	0.28 	0.14 	0.01 	0.14 	0.01 	0.04 	0.06 	0.02
   couleur	0.01 	0.28 	0.02 	0.00 	0.01 	0.18 	0.05 	0.00 	0.00 	0.37 	0.00 	0.00 	0.08
    teinte	0.09 	0.08 	0.01 	0.18 	0.03 	0.01 	0.05 	0.01 	0.19 	0.07 	0.01 	0.00 	0.27
  OD280/OD	0.14 	0.03 	0.03 	0.03 	0.01 	0.07 	0.00 	0.02 	0.01 	0.36 	0.02 	0.00 	0.27
   proline	0.08 	0.13 	0.02 	0.05 	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.33 	0.29 	0.03
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="representation-des-individus">
<h4>Repr√©sentation des individus<a class="headerlink" href="#representation-des-individus" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i12</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 2 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP1/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i13</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i1</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CP2/CP3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i23</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Axe 1 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i2</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Axe 3 (</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i3</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">% d</span><span class="se">\&#39;</span><span class="s1">inertie)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ind</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9d89ee34ba73aa750b9b3fd9ca37bf3233ac94e9bf5e9a867c5763d97b7e1bcd.png" src="_images/9d89ee34ba73aa750b9b3fd9ca37bf3233ac94e9bf5e9a867c5763d97b7e1bcd.png" />
</div>
</div>
</div>
<div class="section" id="contribution-des-individus">
<h4>Contribution des individus<a class="headerlink" href="#contribution-des-individus" title="Permalink to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(\frac{p_ic_{ki}^2}{\lambda_k}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contributions_individus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">c</span><span class="p">)[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> 
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="n">contributions_individus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
    
<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">contributions_individus</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.06
         1	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
         2	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
         3	0.02 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
         4	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         5	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
         6	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.06
         7	0.01 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
         8	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
         9	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        10	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.01 	0.02 	0.00
        11	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        12	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        13	0.01 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.05 	0.00
        14	0.02 	0.01 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        15	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.03
        16	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00
        17	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01
        18	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.03
        19	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00
        20	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04 	0.01 	0.01
        21	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00
        22	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.04
        23	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01
        24	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        25	0.00 	0.00 	0.06 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.01
        27	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00
        28	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        29	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
        30	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.01
        31	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.04 	0.00
        32	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        33	0.00 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        34	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        35	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        36	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.04 	0.00
        37	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        38	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        39	0.01 	0.01 	0.00 	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.01 	0.05 	0.00
        40	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        41	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
        42	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.00
        43	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        44	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01
        45	0.00 	0.01 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        46	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        47	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        48	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        49	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
        50	0.01 	0.00 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.02
        51	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00
        52	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.01
        53	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
        54	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
        55	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        56	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        57	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        58	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        59	0.00 	0.02 	0.08 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.00 	0.00 	0.00 	0.05 	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
        61	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02 	0.00 	0.02
        62	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04 	0.00 	0.01
        63	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11
        64	0.00 	0.01 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.06
        65	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02 	0.03
        66	0.01 	0.01 	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        67	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
        68	0.00 	0.00 	0.00 	0.04 	0.00 	0.00 	0.00 	0.00 	0.03 	0.02 	0.02 	0.00 	0.00
        69	0.00 	0.00 	0.01 	0.01 	0.12 	0.00 	0.02 	0.00 	0.02 	0.01 	0.00 	0.00 	0.01
        70	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.00 	0.02
        71	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.05 	0.00 	0.03 	0.02 	0.03
        72	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.02 	0.00 	0.01
        73	0.01 	0.00 	0.04 	0.01 	0.03 	0.00 	0.00 	0.00 	0.02 	0.00 	0.03 	0.11 	0.00
        74	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.02 	0.01 	0.00 	0.01 	0.04 	0.00
        75	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.05
        76	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.01
        77	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        78	0.00 	0.00 	0.01 	0.01 	0.04 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02
        79	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.02
        80	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.02
        81	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
        82	0.00 	0.01 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
        83	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
        84	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01 	0.00 	0.04 	0.05 	0.04
        85	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        86	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
        87	0.00 	0.01 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
        88	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
        89	0.00 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        90	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
        91	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        92	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.03
        93	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01
        94	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00 	0.09
        95	0.01 	0.00 	0.00 	0.01 	0.09 	0.02 	0.07 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
        96	0.00 	0.00 	0.01 	0.01 	0.07 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.00
        97	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
        98	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.05
        99	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00 	0.01 	0.08
       100	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.02
       101	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.02
       102	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.02
       103	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00
       104	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.03
       105	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.11
       106	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       107	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.05
       108	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.04
       109	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.04 	0.01 	0.01
       110	0.00 	0.00 	0.00 	0.07 	0.03 	0.02 	0.03 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00
       112	0.00 	0.00 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01
       113	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01
       114	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01
       115	0.00 	0.03 	0.01 	0.00 	0.01 	0.01 	0.02 	0.00 	0.01 	0.01 	0.00 	0.00 	0.10
       116	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.04
       117	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01
       118	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       119	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       120	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01 	0.00 	0.00
       121	0.00 	0.00 	0.11 	0.00 	0.00 	0.01 	0.00 	0.04 	0.06 	0.02 	0.00 	0.00 	0.00
       122	0.00 	0.00 	0.04 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       123	0.00 	0.00 	0.00 	0.08 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       124	0.00 	0.00 	0.01 	0.09 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00
       125	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       126	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.01 	0.02 	0.01 	0.01 	0.00 	0.01
       127	0.00 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       128	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02
       129	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00
       130	0.00 	0.00 	0.01 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       131	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
       132	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.03 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00
       133	0.01 	0.00 	0.00 	0.00 	0.02 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.06
       134	0.01 	0.00 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
       135	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00
       136	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01 	0.04 	0.00 	0.01
       137	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       139	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00
       140	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03
       141	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.05
       142	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       143	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
       144	0.01 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.02 	0.02 	0.01 	0.00
       145	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.03
       146	0.02 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
       147	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       148	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02
       150	0.01 	0.01 	0.00 	0.00 	0.03 	0.00 	0.02 	0.01 	0.00 	0.00 	0.00 	0.01 	0.01
       151	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01
       152	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00
       153	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       154	0.01 	0.00 	0.01 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.04
       155	0.02 	0.01 	0.00 	0.01 	0.00 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       156	0.01 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       157	0.01 	0.00 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.06 	0.01
       158	0.00 	0.03 	0.01 	0.01 	0.01 	0.09 	0.00 	0.01 	0.01 	0.00 	0.02 	0.00 	0.01
       159	0.00 	0.01 	0.00 	0.00 	0.01 	0.07 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00
       160	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.02
       161	0.01 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       162	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       163	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       164	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       165	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00 	0.01 	0.00 	0.00
       166	0.01 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01 	0.00 	0.00 	0.04
       167	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.02
       168	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00
       169	0.01 	0.02 	0.01 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       171	0.02 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       172	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00
       173	0.01 	0.01 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       174	0.01 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00
       175	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00
       176	0.01 	0.01 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01
       177	0.01 	0.02 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="tableau-des-cosinus-carres">
<h4>Tableau des cosinus carr√©s<a class="headerlink" href="#tableau-des-cosinus-carres" title="Permalink to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(\frac{c_{ki}^2}{\displaystyle\sum_{j=1}^d c_{ji}^2}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cosinus_carres</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">line</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># on prend la repr√©sentation de l&#39;individu i sur chacune des composantes</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">x</span><span class="o">*</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">]])</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">line</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">c</span><span class="p">[:,</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="o">/</span><span class="n">tot</span><span class="p">)</span>
    <span class="n">cosinus_carres</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">print_tab</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">ind</span><span class="p">,</span> <span class="n">cosinus_carres</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>            	CP1	CP2	CP3	CP4	CP5	CP6	CP7	CP8	CP9	CP10	CP11	CP12	CP13 
         0	0.69 	0.13 	0.00 	0.00 	0.03 	0.00 	0.02 	0.00 	0.00 	0.02 	0.03 	0.01 	0.07
         1	0.43 	0.01 	0.36 	0.01 	0.01 	0.08 	0.00 	0.00 	0.09 	0.01 	0.01 	0.00 	0.00
         2	0.57 	0.10 	0.09 	0.05 	0.01 	0.03 	0.02 	0.00 	0.01 	0.00 	0.13 	0.01 	0.00
         3	0.60 	0.32 	0.00 	0.01 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.02 	0.00
         4	0.14 	0.11 	0.58 	0.02 	0.01 	0.02 	0.03 	0.00 	0.02 	0.01 	0.02 	0.04 	0.00
         5	0.60 	0.29 	0.03 	0.02 	0.03 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.00
         6	0.52 	0.12 	0.08 	0.00 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.02 	0.02 	0.09
         7	0.38 	0.23 	0.00 	0.13 	0.00 	0.18 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01 	0.05
         8	0.51 	0.07 	0.25 	0.00 	0.06 	0.00 	0.00 	0.00 	0.02 	0.03 	0.03 	0.02 	0.00
         9	0.71 	0.06 	0.09 	0.01 	0.02 	0.00 	0.07 	0.00 	0.00 	0.03 	0.00 	0.00 	0.00
        10	0.73 	0.10 	0.01 	0.00 	0.01 	0.00 	0.00 	0.00 	0.09 	0.00 	0.02 	0.03 	0.00
        11	0.39 	0.05 	0.18 	0.10 	0.07 	0.04 	0.02 	0.00 	0.12 	0.00 	0.03 	0.00 	0.01
        12	0.53 	0.05 	0.09 	0.02 	0.17 	0.01 	0.01 	0.00 	0.03 	0.00 	0.09 	0.00 	0.00
        13	0.50 	0.05 	0.06 	0.00 	0.17 	0.02 	0.09 	0.00 	0.01 	0.02 	0.00 	0.06 	0.00
        14	0.65 	0.15 	0.06 	0.00 	0.04 	0.02 	0.04 	0.00 	0.01 	0.00 	0.02 	0.00 	0.00
        15	0.45 	0.24 	0.00 	0.18 	0.02 	0.02 	0.00 	0.00 	0.00 	0.02 	0.01 	0.00 	0.05
        16	0.39 	0.44 	0.06 	0.07 	0.00 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00 	0.00 	0.00
        17	0.39 	0.29 	0.07 	0.13 	0.02 	0.01 	0.00 	0.03 	0.00 	0.01 	0.02 	0.01 	0.02
        18	0.54 	0.27 	0.01 	0.04 	0.06 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.04 	0.03
        19	0.45 	0.12 	0.00 	0.02 	0.08 	0.20 	0.00 	0.00 	0.03 	0.01 	0.00 	0.07 	0.00
        20	0.70 	0.04 	0.01 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00 	0.02 	0.13 	0.02 	0.01
        21	0.19 	0.01 	0.14 	0.17 	0.02 	0.24 	0.01 	0.02 	0.00 	0.08 	0.05 	0.07 	0.00
        22	0.74 	0.00 	0.01 	0.00 	0.02 	0.12 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00 	0.08
        23	0.50 	0.05 	0.00 	0.03 	0.03 	0.11 	0.08 	0.02 	0.01 	0.04 	0.09 	0.00 	0.04
        24	0.49 	0.02 	0.12 	0.00 	0.05 	0.13 	0.06 	0.01 	0.02 	0.02 	0.00 	0.02 	0.07
        25	0.05 	0.05 	0.77 	0.09 	0.00 	0.00 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.01
        26	0.44 	0.07 	0.00 	0.01 	0.18 	0.05 	0.03 	0.00 	0.12 	0.01 	0.08 	0.00 	0.03
        27	0.26 	0.00 	0.32 	0.04 	0.02 	0.06 	0.02 	0.00 	0.01 	0.02 	0.09 	0.13 	0.02
        28	0.51 	0.05 	0.21 	0.06 	0.07 	0.04 	0.00 	0.00 	0.00 	0.00 	0.02 	0.03 	0.00
        29	0.65 	0.00 	0.15 	0.01 	0.03 	0.01 	0.00 	0.03 	0.03 	0.00 	0.00 	0.00 	0.08
        30	0.49 	0.12 	0.15 	0.01 	0.03 	0.03 	0.01 	0.00 	0.09 	0.03 	0.02 	0.01 	0.01
        31	0.59 	0.18 	0.01 	0.01 	0.00 	0.00 	0.02 	0.01 	0.02 	0.00 	0.05 	0.10 	0.00
        32	0.52 	0.00 	0.01 	0.11 	0.08 	0.00 	0.19 	0.01 	0.05 	0.01 	0.00 	0.01 	0.01
        33	0.23 	0.17 	0.09 	0.35 	0.00 	0.01 	0.06 	0.00 	0.02 	0.00 	0.03 	0.03 	0.00
        34	0.42 	0.10 	0.05 	0.24 	0.00 	0.11 	0.01 	0.00 	0.01 	0.01 	0.03 	0.01 	0.01
        35	0.75 	0.01 	0.04 	0.02 	0.01 	0.01 	0.09 	0.00 	0.03 	0.01 	0.01 	0.01 	0.02
        36	0.28 	0.06 	0.03 	0.24 	0.01 	0.06 	0.00 	0.00 	0.13 	0.00 	0.01 	0.17 	0.01
        37	0.35 	0.00 	0.00 	0.25 	0.03 	0.05 	0.05 	0.00 	0.00 	0.04 	0.20 	0.00 	0.03
        38	0.37 	0.10 	0.34 	0.10 	0.00 	0.03 	0.00 	0.02 	0.00 	0.01 	0.01 	0.01 	0.01
        39	0.34 	0.17 	0.01 	0.08 	0.09 	0.13 	0.05 	0.00 	0.03 	0.00 	0.03 	0.08 	0.00
        40	0.67 	0.06 	0.00 	0.02 	0.02 	0.05 	0.03 	0.00 	0.05 	0.00 	0.07 	0.01 	0.01
        41	0.08 	0.01 	0.11 	0.30 	0.02 	0.30 	0.00 	0.03 	0.02 	0.00 	0.01 	0.12 	0.00
        42	0.67 	0.09 	0.01 	0.02 	0.01 	0.05 	0.07 	0.00 	0.06 	0.00 	0.00 	0.01 	0.00
        43	0.05 	0.03 	0.01 	0.47 	0.02 	0.24 	0.08 	0.01 	0.07 	0.00 	0.02 	0.00 	0.00
        44	0.66 	0.00 	0.06 	0.09 	0.03 	0.02 	0.00 	0.00 	0.06 	0.00 	0.01 	0.03 	0.03
        45	0.14 	0.34 	0.00 	0.05 	0.00 	0.39 	0.00 	0.00 	0.01 	0.00 	0.03 	0.04 	0.00
        46	0.56 	0.11 	0.02 	0.15 	0.02 	0.08 	0.04 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
        47	0.70 	0.04 	0.12 	0.09 	0.00 	0.01 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.01
        48	0.58 	0.22 	0.00 	0.01 	0.02 	0.04 	0.01 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        49	0.57 	0.24 	0.03 	0.00 	0.02 	0.04 	0.00 	0.02 	0.00 	0.04 	0.00 	0.02 	0.01
        50	0.53 	0.00 	0.20 	0.06 	0.00 	0.08 	0.00 	0.00 	0.01 	0.01 	0.08 	0.01 	0.02
        51	0.68 	0.05 	0.00 	0.00 	0.05 	0.00 	0.01 	0.00 	0.10 	0.01 	0.08 	0.03 	0.00
        52	0.68 	0.14 	0.02 	0.00 	0.02 	0.00 	0.00 	0.00 	0.11 	0.00 	0.01 	0.00 	0.01
        53	0.45 	0.32 	0.01 	0.13 	0.03 	0.01 	0.02 	0.00 	0.01 	0.00 	0.01 	0.01 	0.00
        54	0.56 	0.13 	0.11 	0.01 	0.09 	0.02 	0.02 	0.01 	0.01 	0.00 	0.01 	0.00 	0.03
        55	0.61 	0.18 	0.03 	0.01 	0.08 	0.03 	0.01 	0.01 	0.03 	0.00 	0.00 	0.01 	0.00
        56	0.66 	0.18 	0.03 	0.00 	0.02 	0.00 	0.00 	0.00 	0.01 	0.00 	0.08 	0.00 	0.01
        57	0.56 	0.17 	0.01 	0.03 	0.06 	0.01 	0.01 	0.00 	0.04 	0.00 	0.08 	0.00 	0.02
        58	0.66 	0.20 	0.01 	0.00 	0.00 	0.01 	0.05 	0.00 	0.03 	0.02 	0.01 	0.00 	0.00
        59	0.03 	0.28 	0.61 	0.03 	0.01 	0.00 	0.00 	0.01 	0.01 	0.01 	0.00 	0.00 	0.01
        60	0.14 	0.11 	0.04 	0.49 	0.06 	0.00 	0.07 	0.00 	0.07 	0.01 	0.00 	0.00 	0.02
        61	0.29 	0.06 	0.22 	0.18 	0.01 	0.04 	0.01 	0.01 	0.08 	0.00 	0.07 	0.00 	0.03
        62	0.00 	0.17 	0.35 	0.16 	0.02 	0.04 	0.04 	0.00 	0.04 	0.00 	0.16 	0.01 	0.01
        63	0.33 	0.29 	0.00 	0.04 	0.00 	0.05 	0.09 	0.01 	0.01 	0.00 	0.03 	0.00 	0.15
        64	0.03 	0.32 	0.04 	0.40 	0.00 	0.00 	0.02 	0.00 	0.00 	0.01 	0.01 	0.07 	0.10
        65	0.17 	0.12 	0.07 	0.10 	0.00 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.14 	0.10
        66	0.28 	0.20 	0.23 	0.11 	0.03 	0.09 	0.01 	0.00 	0.00 	0.00 	0.04 	0.00 	0.01
        67	0.00 	0.54 	0.11 	0.00 	0.03 	0.00 	0.17 	0.00 	0.00 	0.11 	0.01 	0.02 	0.00
        68	0.06 	0.00 	0.04 	0.53 	0.03 	0.01 	0.01 	0.01 	0.13 	0.09 	0.09 	0.00 	0.00
        69	0.13 	0.06 	0.05 	0.04 	0.57 	0.01 	0.07 	0.01 	0.04 	0.02 	0.00 	0.00 	0.01
        70	0.29 	0.09 	0.05 	0.16 	0.11 	0.01 	0.00 	0.02 	0.06 	0.02 	0.14 	0.01 	0.05
        71	0.18 	0.06 	0.25 	0.00 	0.04 	0.01 	0.14 	0.01 	0.15 	0.01 	0.09 	0.04 	0.03
        72	0.08 	0.17 	0.00 	0.00 	0.00 	0.04 	0.38 	0.01 	0.16 	0.00 	0.12 	0.00 	0.02
        73	0.23 	0.00 	0.39 	0.03 	0.14 	0.00 	0.01 	0.00 	0.04 	0.00 	0.04 	0.11 	0.00
        74	0.30 	0.15 	0.02 	0.00 	0.09 	0.00 	0.17 	0.09 	0.03 	0.02 	0.02 	0.10 	0.01
        75	0.06 	0.50 	0.20 	0.06 	0.04 	0.00 	0.01 	0.01 	0.00 	0.03 	0.00 	0.00 	0.07
        76	0.01 	0.35 	0.46 	0.00 	0.00 	0.02 	0.03 	0.00 	0.04 	0.00 	0.05 	0.01 	0.02
        77	0.26 	0.24 	0.01 	0.06 	0.14 	0.08 	0.11 	0.00 	0.09 	0.01 	0.00 	0.00 	0.00
        78	0.09 	0.03 	0.08 	0.05 	0.37 	0.14 	0.22 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
        79	0.02 	0.13 	0.38 	0.11 	0.00 	0.05 	0.17 	0.00 	0.01 	0.00 	0.08 	0.01 	0.04
        80	0.04 	0.83 	0.01 	0.01 	0.00 	0.00 	0.01 	0.00 	0.00 	0.02 	0.03 	0.00 	0.03
        81	0.25 	0.48 	0.03 	0.02 	0.02 	0.00 	0.05 	0.01 	0.08 	0.03 	0.02 	0.00 	0.00
        82	0.02 	0.49 	0.15 	0.09 	0.09 	0.00 	0.04 	0.01 	0.08 	0.00 	0.02 	0.00 	0.00
        83	0.61 	0.00 	0.02 	0.05 	0.12 	0.00 	0.13 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00
        84	0.07 	0.22 	0.04 	0.03 	0.08 	0.08 	0.07 	0.01 	0.03 	0.00 	0.15 	0.16 	0.07
        85	0.10 	0.64 	0.01 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.01 	0.04 	0.06 	0.00
        86	0.07 	0.56 	0.07 	0.12 	0.01 	0.01 	0.01 	0.00 	0.13 	0.00 	0.00 	0.00 	0.03
        87	0.02 	0.38 	0.36 	0.07 	0.02 	0.00 	0.02 	0.01 	0.05 	0.04 	0.01 	0.01 	0.00
        88	0.17 	0.45 	0.13 	0.02 	0.07 	0.00 	0.00 	0.00 	0.01 	0.00 	0.11 	0.01 	0.01
        89	0.03 	0.60 	0.06 	0.00 	0.15 	0.00 	0.05 	0.01 	0.02 	0.00 	0.01 	0.03 	0.04
        90	0.20 	0.49 	0.00 	0.02 	0.09 	0.03 	0.06 	0.00 	0.00 	0.00 	0.04 	0.05 	0.00
        91	0.28 	0.39 	0.07 	0.04 	0.03 	0.06 	0.00 	0.00 	0.02 	0.02 	0.02 	0.02 	0.04
        92	0.36 	0.23 	0.00 	0.03 	0.13 	0.08 	0.03 	0.02 	0.03 	0.02 	0.00 	0.02 	0.05
        93	0.06 	0.60 	0.00 	0.18 	0.01 	0.03 	0.00 	0.01 	0.00 	0.01 	0.00 	0.07 	0.02
        94	0.09 	0.47 	0.00 	0.01 	0.08 	0.01 	0.05 	0.04 	0.08 	0.00 	0.00 	0.00 	0.15
        95	0.20 	0.00 	0.02 	0.03 	0.44 	0.06 	0.22 	0.00 	0.01 	0.01 	0.00 	0.00 	0.01
        96	0.02 	0.01 	0.10 	0.09 	0.65 	0.03 	0.01 	0.01 	0.00 	0.00 	0.06 	0.03 	0.00
        97	0.11 	0.67 	0.12 	0.01 	0.00 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.02 	0.05
        98	0.42 	0.17 	0.00 	0.07 	0.00 	0.09 	0.06 	0.01 	0.05 	0.03 	0.00 	0.02 	0.07
        99	0.12 	0.29 	0.03 	0.09 	0.03 	0.02 	0.27 	0.00 	0.02 	0.00 	0.00 	0.02 	0.09
       100	0.06 	0.50 	0.22 	0.00 	0.03 	0.01 	0.01 	0.00 	0.01 	0.04 	0.00 	0.08 	0.03
       101	0.03 	0.56 	0.24 	0.01 	0.01 	0.00 	0.02 	0.00 	0.07 	0.00 	0.00 	0.00 	0.05
       102	0.01 	0.27 	0.20 	0.09 	0.02 	0.09 	0.05 	0.01 	0.18 	0.00 	0.02 	0.00 	0.07
       103	0.04 	0.74 	0.06 	0.02 	0.00 	0.02 	0.00 	0.01 	0.02 	0.03 	0.01 	0.05 	0.00
       104	0.02 	0.64 	0.03 	0.03 	0.01 	0.01 	0.03 	0.00 	0.02 	0.04 	0.00 	0.08 	0.08
       105	0.25 	0.23 	0.07 	0.01 	0.08 	0.00 	0.09 	0.02 	0.01 	0.02 	0.05 	0.00 	0.16
       106	0.02 	0.70 	0.03 	0.02 	0.03 	0.01 	0.01 	0.02 	0.01 	0.05 	0.02 	0.01 	0.06
       107	0.37 	0.26 	0.01 	0.00 	0.05 	0.05 	0.00 	0.06 	0.07 	0.01 	0.00 	0.01 	0.12
       108	0.00 	0.60 	0.02 	0.09 	0.01 	0.14 	0.01 	0.00 	0.01 	0.00 	0.04 	0.00 	0.07
       109	0.22 	0.19 	0.28 	0.03 	0.01 	0.06 	0.01 	0.00 	0.03 	0.00 	0.14 	0.03 	0.01
       110	0.07 	0.07 	0.00 	0.44 	0.18 	0.08 	0.10 	0.00 	0.00 	0.01 	0.01 	0.02 	0.00
       111	0.01 	0.55 	0.00 	0.13 	0.01 	0.07 	0.11 	0.00 	0.01 	0.05 	0.05 	0.00 	0.00
       112	0.12 	0.04 	0.29 	0.21 	0.04 	0.02 	0.09 	0.01 	0.05 	0.04 	0.04 	0.03 	0.02
       113	0.02 	0.53 	0.12 	0.07 	0.01 	0.10 	0.03 	0.00 	0.06 	0.01 	0.02 	0.00 	0.03
       114	0.03 	0.43 	0.20 	0.00 	0.08 	0.00 	0.11 	0.00 	0.11 	0.00 	0.02 	0.00 	0.02
       115	0.01 	0.61 	0.07 	0.03 	0.04 	0.04 	0.08 	0.00 	0.02 	0.02 	0.00 	0.00 	0.07
       116	0.01 	0.80 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00 	0.01 	0.00 	0.02 	0.07
       117	0.00 	0.57 	0.07 	0.01 	0.18 	0.00 	0.00 	0.01 	0.02 	0.00 	0.10 	0.00 	0.03
       118	0.44 	0.12 	0.27 	0.06 	0.02 	0.05 	0.00 	0.01 	0.03 	0.01 	0.00 	0.01 	0.00
       119	0.03 	0.54 	0.01 	0.20 	0.01 	0.04 	0.10 	0.00 	0.00 	0.00 	0.03 	0.01 	0.02
       120	0.07 	0.25 	0.16 	0.18 	0.02 	0.00 	0.01 	0.00 	0.24 	0.00 	0.05 	0.02 	0.01
       121	0.05 	0.00 	0.75 	0.00 	0.00 	0.02 	0.01 	0.06 	0.08 	0.02 	0.00 	0.00 	0.00
       122	0.10 	0.03 	0.63 	0.09 	0.01 	0.10 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00 	0.00
       123	0.01 	0.02 	0.01 	0.75 	0.00 	0.13 	0.03 	0.01 	0.00 	0.01 	0.01 	0.00 	0.00
       124	0.05 	0.10 	0.11 	0.69 	0.01 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00
       125	0.00 	0.60 	0.03 	0.15 	0.01 	0.00 	0.04 	0.01 	0.10 	0.00 	0.03 	0.02 	0.01
       126	0.00 	0.21 	0.06 	0.21 	0.02 	0.12 	0.09 	0.06 	0.13 	0.04 	0.04 	0.00 	0.03
       127	0.15 	0.09 	0.66 	0.00 	0.02 	0.04 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00
       128	0.03 	0.47 	0.21 	0.06 	0.00 	0.06 	0.02 	0.03 	0.01 	0.05 	0.02 	0.00 	0.05
       129	0.34 	0.14 	0.06 	0.23 	0.03 	0.10 	0.01 	0.00 	0.01 	0.01 	0.06 	0.02 	0.00
       130	0.14 	0.00 	0.11 	0.17 	0.41 	0.00 	0.05 	0.01 	0.01 	0.08 	0.00 	0.02 	0.00
       131	0.53 	0.01 	0.05 	0.01 	0.17 	0.05 	0.08 	0.02 	0.00 	0.01 	0.01 	0.03 	0.02
       132	0.63 	0.01 	0.00 	0.01 	0.09 	0.00 	0.19 	0.03 	0.02 	0.02 	0.01 	0.00 	0.00
       133	0.39 	0.01 	0.02 	0.00 	0.25 	0.09 	0.09 	0.00 	0.01 	0.04 	0.01 	0.00 	0.09
       134	0.44 	0.02 	0.10 	0.06 	0.09 	0.16 	0.01 	0.02 	0.06 	0.04 	0.01 	0.00 	0.01
       135	0.68 	0.01 	0.09 	0.04 	0.04 	0.05 	0.04 	0.00 	0.04 	0.00 	0.00 	0.01 	0.00
       136	0.78 	0.00 	0.00 	0.00 	0.01 	0.06 	0.04 	0.00 	0.00 	0.02 	0.07 	0.00 	0.01
       137	0.67 	0.02 	0.13 	0.01 	0.01 	0.03 	0.09 	0.00 	0.00 	0.00 	0.00 	0.01 	0.02
       138	0.74 	0.01 	0.08 	0.00 	0.09 	0.01 	0.05 	0.00 	0.00 	0.00 	0.02 	0.00 	0.00
       139	0.59 	0.01 	0.16 	0.06 	0.02 	0.02 	0.00 	0.06 	0.01 	0.01 	0.03 	0.03 	0.00
       140	0.71 	0.01 	0.03 	0.07 	0.03 	0.05 	0.00 	0.01 	0.02 	0.00 	0.00 	0.02 	0.05
       141	0.56 	0.01 	0.10 	0.01 	0.01 	0.09 	0.10 	0.01 	0.00 	0.01 	0.01 	0.00 	0.09
       142	0.65 	0.02 	0.07 	0.08 	0.02 	0.11 	0.00 	0.01 	0.01 	0.00 	0.02 	0.01 	0.01
       143	0.51 	0.02 	0.01 	0.04 	0.04 	0.22 	0.09 	0.01 	0.01 	0.01 	0.02 	0.00 	0.02
       144	0.37 	0.10 	0.12 	0.01 	0.22 	0.01 	0.00 	0.00 	0.01 	0.07 	0.08 	0.02 	0.00
       145	0.63 	0.03 	0.07 	0.01 	0.04 	0.02 	0.03 	0.00 	0.01 	0.07 	0.01 	0.04 	0.05
       146	0.70 	0.02 	0.08 	0.05 	0.00 	0.10 	0.00 	0.00 	0.02 	0.02 	0.00 	0.01 	0.00
       147	0.77 	0.10 	0.00 	0.07 	0.02 	0.02 	0.00 	0.00 	0.01 	0.00 	0.01 	0.00 	0.00
       148	0.68 	0.21 	0.02 	0.03 	0.01 	0.03 	0.02 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       149	0.51 	0.25 	0.02 	0.01 	0.13 	0.00 	0.01 	0.03 	0.00 	0.01 	0.00 	0.00 	0.02
       150	0.28 	0.29 	0.01 	0.00 	0.26 	0.00 	0.08 	0.03 	0.01 	0.00 	0.00 	0.02 	0.01
       151	0.34 	0.22 	0.01 	0.01 	0.18 	0.05 	0.14 	0.01 	0.00 	0.02 	0.00 	0.01 	0.01
       152	0.21 	0.15 	0.12 	0.00 	0.20 	0.06 	0.15 	0.00 	0.00 	0.07 	0.01 	0.01 	0.01
       153	0.41 	0.25 	0.05 	0.02 	0.01 	0.18 	0.06 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00
       154	0.53 	0.01 	0.10 	0.03 	0.01 	0.25 	0.00 	0.00 	0.01 	0.00 	0.00 	0.01 	0.05
       155	0.63 	0.16 	0.00 	0.08 	0.02 	0.00 	0.10 	0.00 	0.00 	0.00 	0.00 	0.02 	0.00
       156	0.53 	0.24 	0.04 	0.11 	0.02 	0.01 	0.00 	0.00 	0.00 	0.00 	0.01 	0.03 	0.00
       157	0.60 	0.09 	0.13 	0.01 	0.02 	0.03 	0.00 	0.00 	0.00 	0.00 	0.01 	0.09 	0.01
       158	0.04 	0.42 	0.05 	0.03 	0.03 	0.36 	0.01 	0.02 	0.01 	0.00 	0.03 	0.00 	0.00
       159	0.13 	0.29 	0.01 	0.03 	0.05 	0.42 	0.03 	0.03 	0.00 	0.00 	0.00 	0.01 	0.00
       160	0.72 	0.04 	0.00 	0.07 	0.01 	0.01 	0.00 	0.01 	0.09 	0.00 	0.00 	0.02 	0.03
       161	0.53 	0.15 	0.00 	0.14 	0.01 	0.07 	0.03 	0.02 	0.00 	0.00 	0.03 	0.01 	0.01
       162	0.73 	0.03 	0.06 	0.07 	0.01 	0.01 	0.07 	0.00 	0.01 	0.01 	0.01 	0.00 	0.01
       163	0.70 	0.05 	0.08 	0.01 	0.06 	0.05 	0.01 	0.00 	0.01 	0.00 	0.01 	0.00 	0.01
       164	0.60 	0.17 	0.07 	0.00 	0.01 	0.02 	0.07 	0.00 	0.04 	0.02 	0.01 	0.00 	0.00
       165	0.77 	0.05 	0.01 	0.02 	0.03 	0.01 	0.02 	0.00 	0.07 	0.00 	0.01 	0.00 	0.00
       166	0.38 	0.44 	0.01 	0.00 	0.01 	0.02 	0.00 	0.00 	0.03 	0.04 	0.00 	0.00 	0.05
       167	0.60 	0.11 	0.10 	0.01 	0.01 	0.01 	0.03 	0.00 	0.00 	0.08 	0.02 	0.00 	0.03
       168	0.40 	0.36 	0.05 	0.01 	0.01 	0.03 	0.05 	0.00 	0.07 	0.00 	0.00 	0.00 	0.00
       169	0.30 	0.36 	0.11 	0.02 	0.07 	0.05 	0.05 	0.01 	0.00 	0.00 	0.00 	0.00 	0.03
       170	0.82 	0.01 	0.06 	0.00 	0.03 	0.01 	0.01 	0.00 	0.03 	0.00 	0.03 	0.00 	0.00
       171	0.70 	0.04 	0.09 	0.00 	0.01 	0.04 	0.05 	0.00 	0.02 	0.03 	0.00 	0.00 	0.00
       172	0.43 	0.34 	0.06 	0.00 	0.03 	0.03 	0.05 	0.00 	0.01 	0.00 	0.01 	0.03 	0.01
       173	0.56 	0.24 	0.01 	0.06 	0.02 	0.06 	0.05 	0.00 	0.00 	0.00 	0.00 	0.00 	0.00
       174	0.64 	0.29 	0.00 	0.01 	0.01 	0.00 	0.00 	0.00 	0.03 	0.01 	0.00 	0.01 	0.00
       175	0.38 	0.41 	0.05 	0.01 	0.09 	0.00 	0.02 	0.00 	0.00 	0.03 	0.00 	0.01 	0.00
       176	0.38 	0.36 	0.02 	0.03 	0.04 	0.09 	0.03 	0.00 	0.01 	0.01 	0.00 	0.01 	0.02
       177	0.49 	0.36 	0.05 	0.02 	0.04 	0.00 	0.00 	0.00 	0.00 	0.00 	0.03 	0.00 	0.00
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-regression"></span><div class="tex2jax_ignore mathjax_ignore section" id="regression">
<h2>R√©gression<a class="headerlink" href="#regression" title="Permalink to this heading">#</a></h2>
<p id="index-0">On s‚Äôint√©resse ici √† l‚Äôexplication d‚Äôune variable (al√©atoire) <span class="math notranslate nohighlight">\(Y\)</span> (la variable expliqu√©e) par une (ou plusieurs) variable(s) al√©atoire(s) <span class="math notranslate nohighlight">\(X_j\)</span> (pr√©dicteurs, ou variables explicatives).</p>
<div class="section" id="regression-simple">
<h3>R√©gression simple<a class="headerlink" href="#regression-simple" title="Permalink to this heading">#</a></h3>
<p>On dispose de <span class="math notranslate nohighlight">\(n\)</span> couples de variables quantitatives <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span> constituant un √©chantillon d‚Äôobservations ind√©pendantes de <span class="math notranslate nohighlight">\((X,Y)\)</span> et on cherche une relation statistique pouvant exister entre <span class="math notranslate nohighlight">\(Y\)</span> et <span class="math notranslate nohighlight">\(X\)</span>.
On rappelle ici quelques r√©sultats √©l√©mentaires sur la r√©gression lin√©aire simple.</p>
<div class="section" id="modele-theorique">
<h4>Mod√®le th√©orique<a class="headerlink" href="#modele-theorique" title="Permalink to this heading">#</a></h4>
<p>Th√©oriquement, on cherche une fonction <span class="math notranslate nohighlight">\(f\)</span> telle que <span class="math notranslate nohighlight">\(f(X)\)</span> soit aussi proche que possible de <span class="math notranslate nohighlight">\(Y\)</span>. Par proximit√©, on entend ici au sens des moindres carr√©s, et donc on cherche <span class="math notranslate nohighlight">\(f\)</span> telle que <span class="math notranslate nohighlight">\(\mathbb{E}\left ( (Y-f(X))^2\right )\)</span> soit minimale. On sait alors que la fonction <span class="math notranslate nohighlight">\(f\)</span> qui satisfait cette propri√©t√© est :</p>
<p><span class="math notranslate nohighlight">\(f(X) = \mathbb{E}(Y\mid X)\)</span></p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 37 </span> (Fonction de r√©gression)</p>
<div class="definition-content section" id="proof-content">
<p>La fonction <span class="math notranslate nohighlight">\(x\mapsto \mathbb{E}(Y\mid X=x)\)</span> est la fonction de r√©gression de <span class="math notranslate nohighlight">\(Y\)</span> en <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div>
</div><p>La qualit√© de l‚Äôapproximation est mesur√©e par le rapport de corr√©lation.</p>
<div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 38 </span> (Rapport de corr√©lation)</p>
<div class="definition-content section" id="proof-content">
<p>Le rapport de corr√©lation entre deux variables al√©atoires <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> est d√©fini par le rapport entre la variation expliqu√©e et la variation totale :</p>
<p><span class="math notranslate nohighlight">\(\eta_{Y\mid X}^2 = \frac{\sigma_{\mathbb{E}(Y\mid X)}^2}{\sigma_Y^2}\)</span></p>
</div>
</div><p>En pratique, <span class="math notranslate nohighlight">\(Y\)</span> est approch√©e par <span class="math notranslate nohighlight">\(Y=\mathbb{E}(Y\mid X)+\varepsilon\)</span>, o√π <span class="math notranslate nohighlight">\(\varepsilon\)</span> est un r√©sidu al√©atoire de moyenne nulle, non corr√©l√© √† <span class="math notranslate nohighlight">\(X\)</span> et √† <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)\)</span> et tel que <span class="math notranslate nohighlight">\(\sigma_\varepsilon^2= (1-\eta_{Y\mid X}^2)\sigma_Y^2\)</span>.</p>
<p>Le cadre le plus utilis√© est celui de la r√©gression lin√©aire, c‚Äôest-√†-dire lorsque <span class="math notranslate nohighlight">\(Y=a+bX+\varepsilon\)</span> et donc <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)=a+bX\)</span>, ce qui est le cas lorsque <span class="math notranslate nohighlight">\((X,Y)\)</span> est un couple de variables al√©atoires gaussiennes.</p>
<p>Puisque <span class="math notranslate nohighlight">\(\mathbb{E}(\varepsilon)=0\)</span>, la droite de r√©gression passe par le point <span class="math notranslate nohighlight">\((\mathbb{E}(X),\mathbb{E}(Y))\)</span>. Ainsi</p>
<p><span class="math notranslate nohighlight">\(Y-\mathbb{E}(Y)=b(X-\mathbb{E}(X))+\varepsilon\)</span></p>
<p>En multipliant par <span class="math notranslate nohighlight">\(X-\mathbb{E}(X)\)</span> et en prenant l‚Äôesp√©rance, on trouve √† gauche la covariance de <span class="math notranslate nohighlight">\((X,Y)\)</span> et √† droite la variance de <span class="math notranslate nohighlight">\(X\)</span>, soit</p>
<p><span class="math notranslate nohighlight">\(\begin{array}{ccll}
\sigma_{XY}&amp;=&amp; b\sigma_X^2+\mathbb{E}(\varepsilon(X-\mathbb{E}(X)))&amp;\\
&amp;=&amp; b\sigma_X^2 + \sigma_{\varepsilon X}&amp;[\mathbb{E}(\varepsilon)=0]\\ 
&amp;=&amp; b\sigma_X^2 &amp;[X\text{ et } \varepsilon\text{ non corr√©l√©s}]\\ 
\end{array}
\)</span></p>
<p>d‚Äôo√π
<span class="math notranslate nohighlight">\(b = \frac{\sigma_{XY}}{\sigma_X^2} = r_{XY}\frac{\sigma_Y}{\sigma_X}\)</span></p>
<p>L‚Äô√©quation de la droite de r√©gression est donc finalement</p>
<p><span class="math notranslate nohighlight">\(Y-\mathbb{E}(Y)=r_{XY}\frac{\sigma_Y}{\sigma_X}(X-\mathbb{E}(X))+\varepsilon\)</span></p>
<p>En calculant la variance des deux termes, et puisque <span class="math notranslate nohighlight">\(\varepsilon\)</span> et <span class="math notranslate nohighlight">\(X\)</span> ne sont pas corr√©l√©s, on trouve</p>
<p><span class="math notranslate nohighlight">\(r_{XY}^2 = \eta_{Y\mid X}^2\)</span></p>
</div>
<div class="section" id="ajustement-aux-donnees">
<h4>Ajustement aux donn√©es<a class="headerlink" href="#ajustement-aux-donnees" title="Permalink to this heading">#</a></h4>
<p>On cherche ici √† ajuster le mod√®le lin√©aire th√©orique aux <span class="math notranslate nohighlight">\(n\)</span> couples d‚Äôobservations ind√©pendantes <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span>. Il s‚Äôagit donc de trouver <span class="math notranslate nohighlight">\(a,b\)</span> ainsi que la variance du r√©sidu <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p>
<p>La m√©thode la plus classique est la m√©thode des moindres carr√©s : on cherche √† ajuster au nuage de points  <span class="math notranslate nohighlight">\((\mathbf x_i,\mathbf y_i),i\in[\![1,n]\!]\)</span> une droite d‚Äô√©quation <span class="math notranslate nohighlight">\(y^*=\alpha +\beta x\)</span> de sorte √† minimiser</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n (y_i^*-y_i)^2 = \displaystyle\sum_{i=1}^n (\alpha + \beta x_i-y_i)^2\)</span></p>
<p>En annulant le gradient de cette fonction √† deux variables <span class="math notranslate nohighlight">\((\alpha,\beta)\)</span>, on trouve facilement</p>
<p><span class="math notranslate nohighlight">\(\beta = \frac{\sigma_{xy}}{\sigma_x^2} = r_{xy}\frac{\sigma_y}{\sigma_x}\)</span></p>
<p>de sorte que <span class="math notranslate nohighlight">\(y^* = \bar y + r_{xy}\frac{\sigma_y}{\sigma_x}(x-\bar x)\)</span>.</p>
<p>La droite de r√©gression lin√©aire passe donc par le centre de masse du nuage de points.</p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 15 </span></p>
<div class="remark-content section" id="proof-content">
<p>les <span class="math notranslate nohighlight">\(x_i\)</span> et <span class="math notranslate nohighlight">\(y_i\)</span> √©tant des r√©alisations de variables al√©atoires, tous les termes de l‚Äô√©quation de la droite de r√©gression lin√©aire le sont √©galement.</p>
</div>
</div><div class="proof remark dropdown admonition" id="remark-3">
<p class="admonition-title"><span class="caption-number">Remark 16 </span></p>
<div class="remark-content section" id="proof-content">
<p>On peut montrer que <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> et <span class="math notranslate nohighlight">\(y^*\)</span> sont des estimateurs sans biais de <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> et <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)\)</span>.</p>
</div>
</div><p>La figure suivante illustre la r√©gression lin√©aire d‚Äôun ensemble de points, d√©compos√© en un ensemble d‚Äôapprentissage (bleu) sur lequel la droite de r√©gression a √©t√© apprise et un ensemble de test (vert) sur lequel les valeurs ont √©t√© pr√©dites (magenta).</p>
<p><img alt="" src="_images/regressionlin.png" /></p>
</div>
</div>
<div class="section" id="regression-multiple">
<h3>R√©gression multiple<a class="headerlink" href="#regression-multiple" title="Permalink to this heading">#</a></h3>
<div class="section" id="ajustement-lineaire-d-un-ensemble-d-observations">
<span id="index-2"></span><h4>Ajustement lin√©aire d‚Äôun ensemble d‚Äôobservations<a class="headerlink" href="#ajustement-lineaire-d-un-ensemble-d-observations" title="Permalink to this heading">#</a></h4>
<p>La r√©gression multiple g√©n√©ralise la r√©gression simple au cas de <span class="math notranslate nohighlight">\(p\geq 2\)</span> pr√©dicteurs quantitatifs (ou variables explicatives). Ici on consid√®re un √©chantillon de <span class="math notranslate nohighlight">\(n\)</span> individus, sur lesquels <span class="math notranslate nohighlight">\(p+1\)</span> variables sont mesur√©es : une variable √† expliquer <span class="math notranslate nohighlight">\(\mathbf Y = (y_1\cdots y_n)^T\in\mathbb{R}^n\)</span> et <span class="math notranslate nohighlight">\(p\)</span> variables explicatives <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> lin√©airement ind√©pendantes, mais possiblement en relation.</p>
<p>On cherche</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\)</span></p>
<p>proche de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> au sens des moindres carr√©s. <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> est le vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> dont toutes les composantes valent 1.</p>
<p>En notant
<span class="math notranslate nohighlight">\(X = \begin{pmatrix}\mathbf{1} &amp; \mathbf{X_1}\cdots \mathbf{X_p}\end{pmatrix}\in\mathcal{M}_{n,p+1}(\mathbb{R})\quad\text{et}\quad \boldsymbol{\beta}=(\beta_0\cdots \beta_p)^T
\in\mathbb{R}^{p+1}\)</span></p>
<p>on a <span class="math notranslate nohighlight">\(\mathbf Y^*=\mathbf X\boldsymbol \beta\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^*\)</span> est par d√©finition des moindres carr√©s la projection de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sur <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, soit (Voir cours analyse num√©rique) :</p>
<p><span class="math notranslate nohighlight">\(\mathbf Y^* = \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)</span></p>
<p>et donc</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y\)</span></p>
<p>et on a donc les param√®tres de la r√©gression multiple.</p>
<div class="proof remark dropdown admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 17 </span></p>
<div class="remark-content section" id="proof-content">
<p>Dans le cas o√π la m√©trique utilis√©e est d√©finie par une matrice sym√©trique d√©finie positive <span class="math notranslate nohighlight">\(D\)</span> de taille <span class="math notranslate nohighlight">\(p\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf D \mathbf X)^{-1}\mathbf X^T \mathbf D \mathbf Y\)</span></p>
</div>
</div></div>
<div class="section" id="modele">
<h4>Mod√®le<a class="headerlink" href="#modele" title="Permalink to this heading">#</a></h4>
<p>On suppose que les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> et <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sont <span class="math notranslate nohighlight">\(n\)</span> r√©alisations ind√©pendantes de <span class="math notranslate nohighlight">\(p+1\)</span> variables al√©atoires <span class="math notranslate nohighlight">\(\chi_i\)</span> et <span class="math notranslate nohighlight">\(\omega\)</span>. De m√™me qu‚Äôen r√©gression simple, la recherche de la meilleure approximation de <span class="math notranslate nohighlight">\(\omega\)</span> par une fonction des <span class="math notranslate nohighlight">\(\chi_i\)</span> am√®ne √† <span class="math notranslate nohighlight">\(\mathbb{E}(\omega\mid \chi_1\cdots \chi_p)\)</span> et l‚Äôhypoth√®se de r√©gression multiple est</p>
<p><span class="math notranslate nohighlight">\(\mathbb{E}(\omega\mid \chi_0\cdots \chi_p) = b_0+\displaystyle\sum_{i=1}^p b_i\chi_i+\varepsilon\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(\mathbb{E}(\varepsilon)=0, \sigma_\varepsilon=\sigma^2\)</span> et <span class="math notranslate nohighlight">\(\varepsilon\)</span> non corr√©l√©e aux <span class="math notranslate nohighlight">\(\chi_i\)</span>.</p>
<p>On peut montrer que <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> est un estimateur sans biais du vecteur al√©atoire  <span class="math notranslate nohighlight">\((b_0\cdots b_p)\)</span>, et en est la meilleure approximation. De plus, la meilleure estimation sans biais de la variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> est</p>
<p><span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1}\)</span></p>
</div>
</div>
<div class="section" id="modele-lineaire-generalise">
<h3>Mod√®le lin√©aire g√©n√©ralis√©<a class="headerlink" href="#modele-lineaire-generalise" title="Permalink to this heading">#</a></h3>
<div class="section" id="position-du-probleme">
<h4>Position du probl√®me<a class="headerlink" href="#position-du-probleme" title="Permalink to this heading">#</a></h4>
<p>Dans le cas le plus g√©n√©ral, on ne cherche pas √† expliquer une seule variable mais <span class="math notranslate nohighlight">\(k\in\mathbb{N}\)</span>, obtenues par r√©p√©titions de l‚Äôexp√©rience, les <span class="math notranslate nohighlight">\(\mathbf X_j\)</span> restant identiques : pour <span class="math notranslate nohighlight">\(i\in[\![1,k]\!]\)</span> <span class="math notranslate nohighlight">\(\mathbf{Y}_i\in\mathbb{R}^n\)</span> est la <span class="math notranslate nohighlight">\(i^e\)</span> observation.</p>
</div>
<div class="section" id="solution-a-partir-des-donnees">
<h4>Solution √† partir des donn√©es<a class="headerlink" href="#solution-a-partir-des-donnees" title="Permalink to this heading">#</a></h4>
<p>Le mod√®le fait l‚Äôhypoth√®se que le centre de gravit√© <span class="math notranslate nohighlight">\(\mathbf g\)</span> des <span class="math notranslate nohighlight">\(k\)</span> observations se situe dans <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, soit <span class="math notranslate nohighlight">\(\mathbf g = \mathbf X \boldsymbol \beta\)</span>
La plupart du temps, on ne conna√Æt cependant qu‚Äôune seule des <span class="math notranslate nohighlight">\(k\)</span> observations <span class="math notranslate nohighlight">\(\mathbf Y\)</span>, et le probl√®me revient √† approximer le mieux possible <span class="math notranslate nohighlight">\(\mathbf g\)</span> en ne connaissant que <span class="math notranslate nohighlight">\(\mathbf Y\)</span>.</p>
<p>Cette approximation <span class="math notranslate nohighlight">\(\mathbf g^*\)</span> s‚Äôexprime comme la projection orthogonale de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> sur <span class="math notranslate nohighlight">\(Im(\mathbf X)\)</span>, selon une m√©trique <span class="math notranslate nohighlight">\(\mathbf M\)</span>, √† choisir de sorte que <span class="math notranslate nohighlight">\(\mathbf g^*\)</span> soit la plus proche possible de <span class="math notranslate nohighlight">\(\mathbf g\)</span>. Dit autrement, en r√©p√©tant la projection avec <span class="math notranslate nohighlight">\(\mathbf Y_1\cdots \mathbf Y_k\)</span>, les <span class="math notranslate nohighlight">\(k\)</span> approximations <span class="math notranslate nohighlight">\(g^*_i=\mathbf X (\mathbf X^T\mathbf M\mathbf X)^{-1} \mathbf X^T \mathbf M \mathbf Y_i, i\in[\![1,k]\!]\)</span> doivent √™tre le plus concentr√©es possible autour de <span class="math notranslate nohighlight">\(\mathbf g\)</span>.</p>
<p>Ceci revient donc √† trouver <span class="math notranslate nohighlight">\(\mathbf M\)</span> de sorte √† ce que l‚Äôinertie du nuage des <span class="math notranslate nohighlight">\(\mathbf g_i^*\)</span> soit minimale. On montre (th√©or√®me de Gauss-Markov g√©n√©ralis√©) que <span class="math notranslate nohighlight">\(\mathbf M=\mathbf V^{-1}\)</span>, o√π <span class="math notranslate nohighlight">\(\mathbf V\)</span> est la matrice de variance-covariance du nuage des <span class="math notranslate nohighlight">\(\mathbf Y_i\)</span>. Ainsi, pour une seule observation, on en d√©duit</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbf g^*&amp;=&amp;\mathbf X(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y\\
\boldsymbol \beta&amp;=&amp;(\mathbf X^T\mathbf V^{-1}\mathbf X)^{-1}\mathbf X^T\mathbf V^{-1}\mathbf Y
\end{eqnarray*}\)</span></p>
</div>
<div class="section" id="id1">
<h4>Mod√®le<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h4>
<p>En ayant une infinit√© d‚Äôobservations, on approche le mod√®le probabiliste. On suppose que <span class="math notranslate nohighlight">\(\mathbf Y\)</span> est une r√©alisation d‚Äôun vecteur al√©atoire d‚Äôesp√©rance <span class="math notranslate nohighlight">\(\mathbf X\mathbf b\)</span> et de matrice de variance-covariance <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>. Le mod√®le s‚Äô√©crit alors  <span class="math notranslate nohighlight">\(\mathbf Y=\mathbf X\mathbf b+\varepsilon\)</span>, avec <span class="math notranslate nohighlight">\(\varepsilon\)</span> centr√© de variance <span class="math notranslate nohighlight">\(\boldsymbol\Sigma\)</span>, et le probl√®me est donc d‚Äôestimer <span class="math notranslate nohighlight">\(\mathbf b\)</span>. On montre que  <span class="math notranslate nohighlight">\(\mathbf b = (\mathbf X^T \boldsymbol\Sigma^{-1}\mathbf X)^{-1}\mathbf X^T\boldsymbol\Sigma^{-1}\mathbf Y\)</span>, appel√© estimation des moindres carr√©s g√©n√©ralis√©s est, sous des hypoth√®ses larges, l‚Äôestimation de variance minimale de <span class="math notranslate nohighlight">\(\mathbf b\)</span>.</p>
</div>
</div>
<div class="section" id="modeles-regularises">
<h3>Mod√®les r√©gularis√©s<a class="headerlink" href="#modeles-regularises" title="Permalink to this heading">#</a></h3>
<p>On peut montrer que l‚Äôestimateur des moindres carr√©s est de variance minimale parmi les estimateurs lin√©aires sans biais. Cependant, la variance aboutit dans certains cas √† des erreurs de pr√©diction importantes. Dans ce cas, on cherche des estimateurs de variance plus petite quitte √† avoir un (l√©ger) biais. Pour ce faire, on peut supprimer l‚Äôeffet de certaines variables explicatives ce qui revient √† leur attribuer un poids nul.
Par ailleurs, dans le cas o√π <span class="math notranslate nohighlight">\(p\)</span> est grand, l‚Äôinterpr√©tation des r√©sultats obtenus est parfois complexe. Ainsi, on pourra pr√©f√©rer un mod√®le estim√© avec moins de variables explicatives afin de privil√©gier l‚Äôinterpr√©tation plut√¥t que la pr√©cision.</p>
<p>Dans cette section, on s‚Äôint√©resse √† des m√©thodes permettant de produire des estimateurs dont les valeurs sont d‚Äôamplitudes r√©duites. On parle de mod√®les parcimonieux lorsque des variables ont des coefficients nuls.</p>
<div class="section" id="regression-ridge">
<h4>R√©gression Ridge<a class="headerlink" href="#regression-ridge" title="Permalink to this heading">#</a></h4>
<span class="target" id="index-3"></span><p id="index-4">Dans l‚Äôapproche moindres carr√©s lin√©aires classique, on cherche <span class="math notranslate nohighlight">\(\mathbf Y^* = \beta_0 \mathbf{1} + \displaystyle\sum_{i=1}^p \beta_i\mathbf X_i\)</span> proche de <span class="math notranslate nohighlight">\(\mathbf Y\)</span> au sens de la minimisation de <span class="math notranslate nohighlight">\(\|\mathbf Y^*-\mathbf Y\|^2 \)</span>. On cherche donc <span class="math notranslate nohighlight">\(\boldsymbol\beta_{mc}\in\mathbb{R}^{p+1}\)</span> tel que :</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_{mc} = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2\right ]\)</span></p>
<p>Dans l‚Äôapproche Ridge regression (ou r√©gression de Tikhonov), on p√©nalise l‚Äôamplitude des coefficients <span class="math notranslate nohighlight">\(\beta_j\)</span>. Pour ce faire, on pose <span class="math notranslate nohighlight">\(\boldsymbol\beta_{\setminus 0}\)</span> le vecteur des <span class="math notranslate nohighlight">\(p\)</span> derni√®res composantes de <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> et on cherche le vecteur <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_r = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_2\right ]\)</span></p>
<p>Le r√©el positif <span class="math notranslate nohighlight">\(\lambda\)</span>, pond√©rant  <span class="math notranslate nohighlight">\(\| \boldsymbol\beta_{\setminus 0}\|^2_2\)</span> appel√©e fonction de p√©nalit√©, permet de r√©guler l‚Äôimportance du second terme sur la minimisation. Un <span class="math notranslate nohighlight">\(\lambda\)</span> grand impose √† la minimisation d‚Äôavoir une amplitude faible des coefficients <span class="math notranslate nohighlight">\(\beta_j,j\in[\![1,p]\!]\)</span>, et une variance faible de l‚Äôestimateur de <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span>.</p>
<p>Contrairement √† la r√©gression lin√©aire multiple classique o√π les variables ne sont pas n√©cessairement normalis√©es, ici il est n√©cessaire de r√©duire les variables explicatives. En pratique on les centre √©galement, et dans ce cas :</p>
<ol class="arabic simple">
<li><p>la premi√®re composante de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> est prise √©gale √† la moyenne empirique des <span class="math notranslate nohighlight">\(y_i\)</span> avant centrage</p></li>
<li><p>les <span class="math notranslate nohighlight">\(p\)</span> autres composantes de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span>  sont obtenues par minimisation :</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\hat{\boldsymbol \beta}_r = arg\displaystyle\min_{\mathbf v\in\mathbb{R}^{p}} \left ((\mathbf Y-\mathbf X\mathbf v)^T(\mathbf Y-\mathbf X\mathbf v) + \lambda \mathbf v^T\mathbf v\right )\)</span></p>
<p>dont la solution analytique est <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X + \lambda \mathbb{I})^{-1}\mathbf X^T\mathbf Y\)</span>.</p>
<p>Le choix de <span class="math notranslate nohighlight">\(\lambda\)</span> n‚Äôest pas √©vident. La solution la plus simple consiste √† prendre plusieurs valeurs, √† tester les solutions propos√©es par ces valeurs et √† retenir le <span class="math notranslate nohighlight">\(\lambda\)</span> ayant obtenu le meilleur score (par exemple la pr√©cision sur un ensemble de test). De mani√®re moins exp√©rimentale, il existe des algorithmes (bas√©s sur la d√©composition en valeurs singuli√®res) permettant de choisir une ‚Äò‚Äôbonne‚Äô‚Äô valeur de param√®tre.</p>
</div>
<div class="section" id="regression-lasso">
<h4>R√©gression Lasso<a class="headerlink" href="#regression-lasso" title="Permalink to this heading">#</a></h4>
<span class="target" id="index-5"></span><p id="index-6">La r√©gression Lasso (Least Absolute Shrinkage and Selection Operator) est, dans son principe, tr√®s proche de la r√©gression Ridge, la seule diff√©rence r√©sidant dans la norme utilis√©e dans la fonction de p√©nalit√© : on cherche <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> minimisant</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_l = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p+1}}\left [\displaystyle\sum_{i=1}^n \left (y_i-(\beta_0+\displaystyle\sum_{j=1}^p \beta_j x_{ij})\right )^2+\lambda \| \boldsymbol\beta_{\setminus 0}\|^2_1\right ]\)</span></p>
<p>Contrairement √† la r√©gression Ridge, il n‚Äôy a pas de solution analytique (la norme <span class="math notranslate nohighlight">\(\ell_1\)</span> rend la fonction non diff√©rentiable) et on doit donc recourir √† des m√©thodes de r√©solution num√©rique. Lorsque <span class="math notranslate nohighlight">\(\lambda\)</span> est grand, la minimisation force la fonction de p√©nalit√© √† √™tre petite : √©tant donn√© que cette derni√®re est une somme de valeurs absolues, la minimisation impose √† certains coefficients <span class="math notranslate nohighlight">\(\beta_j,j\in[\![1,p]\!]\)</span> d‚Äô√™tre nuls. On parle alors de r√©gression parcimonieuse (et la r√©gression peut donc √™tre vue comme une m√©thode de s√©lection de variables).</p>
<p>Quand <span class="math notranslate nohighlight">\(p&gt;n\)</span>, la m√©thode ne s√©lectionne que <span class="math notranslate nohighlight">\(n\)</span> variables. De plus, si plusieurs variables sont corr√©l√©es entre elles, Lasso ignore toutes sauf une. Et, pire, m√™me si <span class="math notranslate nohighlight">\(n&gt;p\)</span>, et s‚Äôil y a de fortes corr√©lations entre les variables explicatives, on trouve empiriquement que Ridge donne de meilleurs r√©sultats que Lasso.</p>
</div>
<div class="section" id="regression-elasticnet">
<h4>R√©gression Elasticnet<a class="headerlink" href="#regression-elasticnet" title="Permalink to this heading">#</a></h4>
<span class="target" id="index-7"></span><p id="index-8">On suppose ici que <span class="math notranslate nohighlight">\(\mathbf X\)</span> est centr√© r√©duit, et <span class="math notranslate nohighlight">\(\mathbf Y\)</span> est centr√© (donc <span class="math notranslate nohighlight">\(\beta_0=0\)</span>). La r√©gression Elasticnet est un m√©lange de Ridge et Lasso : on cherche <span class="math notranslate nohighlight">\(\boldsymbol\beta_e\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_e = arg\displaystyle\min_{\boldsymbol\beta\in\mathbb{R}^{p}}\left [\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda_1 \| \boldsymbol\beta\|^2_1 + \lambda_2 \| \boldsymbol\beta\|^2_2\right ]\)</span></p>
<p>En notant <span class="math notranslate nohighlight">\(\lambda =\lambda_1+\lambda_2\)</span> et <span class="math notranslate nohighlight">\( \alpha = \lambda_1/\lambda\)</span> on minimise alors</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n \left (y_i-\displaystyle\sum_{j=1}^p \beta_j x_{ij}\right )^2+\lambda(\alpha \| \boldsymbol\beta\|^2_1 + (1-\alpha) \| \boldsymbol\beta\|^2_2)\)</span></p>
<p>On montre alors que la solution de la r√©gression Elasticnet peut √™tre obtenue √† l‚Äôaide de la solution de la r√©gression Lasso.</p>
<div class="proof property admonition" id="property-5">
<p class="admonition-title"><span class="caption-number">Property 7 </span></p>
<div class="property-content section" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\(\mathbf X\in\mathcal{M}_{np}(\mathbb R)\)</span> la matrice des variables explicatives, et <span class="math notranslate nohighlight">\(\mathbf Y\in\mathbb{R}^n\)</span> le vecteur des valeurs de la variable expliqu√©e. Soient <span class="math notranslate nohighlight">\(\lambda_1,\lambda_2\in\mathbb{R}^+\)</span>. On pose</p>
<p><span class="math notranslate nohighlight">\(\mathbf X^*\in\mathcal{M}_{(n+p)p}(\mathbb R) = \frac{1}{\sqrt{1+\lambda_2}}\begin{pmatrix}\mathbf X\\\sqrt{\lambda_2 }\mathbb{I}\end{pmatrix}\quad\text{et}\quad \mathbf Y^*=\begin{pmatrix}\mathbf Y\\0\end{pmatrix}\)</span></p>
<p>et on note <span class="math notranslate nohighlight">\(\gamma=\lambda_1/(\lambda_1+\lambda_2)\)</span>.</p>
<p>Alors la fonction objectif de la r√©gression Elasticnet s‚Äô√©crit <span class="math notranslate nohighlight">\(\|\mathbf Y^*-\mathbf X^*\boldsymbol\beta^*\|_2^2+\gamma\|\boldsymbol\beta^*\|_1\)</span>.
Si <span class="math notranslate nohighlight">\(\hat{\boldsymbol\beta}\)</span> minimise cette fonction, alors l‚Äôestimateur na√Øf de la r√©gression Elasticnet est</p>
<p><span class="math notranslate nohighlight">\(\boldsymbol\beta_e = \frac{1}{\sqrt{1+\lambda_2}}\hat{\boldsymbol\beta}\)</span></p>
</div>
</div><p>Puisque <span class="math notranslate nohighlight">\(\mathbf X^*\)</span> est de rang <span class="math notranslate nohighlight">\(p\)</span>, la solution peut s√©lectionner <span class="math notranslate nohighlight">\(p\)</span> variables contrairement √† la r√©gression Lasso.</p>
<p>En pratique, cet estimateur na√Øf ne donne satisfaction que lorsqu‚Äôil est proche de <span class="math notranslate nohighlight">\(\boldsymbol\beta_r\)</span> ou de <span class="math notranslate nohighlight">\(\boldsymbol\beta_l\)</span>. On retient g√©n√©ralement l‚Äôestimateur r√©√©chelonn√© <span class="math notranslate nohighlight">\((1+\lambda_2)\boldsymbol\beta_e = \sqrt{1+\lambda_2}\hat{\boldsymbol\beta}\)</span> (Elasticnet peut √™tre vu comme un Lasso o√π la matrice de variance-covariance est proche de la matrice Identit√©, et on montre que le facteur <span class="math notranslate nohighlight">\(1+\lambda_2\)</span> intervient alors).</p>
<p>La figure suivante compare les diff√©rentes m√©thodes de r√©gression sur la fonction</p>
<p><span class="math notranslate nohighlight">\(f(x) = x-\frac35 x^2+\frac15x^3 + 18sin(x)\)</span></p>
<p>avec <span class="math notranslate nohighlight">\(p=8\)</span> et <span class="math notranslate nohighlight">\(n=20\)</span>. Les <span class="math notranslate nohighlight">\(n=20\)</span> points  √©chantillonn√©s sur la courbe <span class="math notranslate nohighlight">\(y=f(x)\)</span> sont utilis√©s pour faire la r√©gression sur l‚Äôintervalle [-10,10].</p>
<p><img alt="" src="_images/comparregression.png" /></p>
</div>
</div>
<div class="section" id="regression-logistique">
<h3>R√©gression logistique<a class="headerlink" href="#regression-logistique" title="Permalink to this heading">#</a></h3>
<p id="index-9">Dans les sections pr√©c√©dentes, nous n‚Äôavons pas abord√© les cas o√π les pr√©dicteurs exhibent des d√©pendances non lin√©aires ou lorsque la variable √† pr√©dire n‚Äôest pas quantitative.</p>
<p>La r√©gression logistique est un mod√®le lin√©aire g√©n√©ralis√© utilis√© pour pr√©dire une variable binaire, ou cat√©gorielle, √† partir de pr√©dicteurs quantitatifs ou cat√©goriels.</p>
<div class="section" id="regression-logistique-binaire">
<h4>R√©gression logistique binaire<a class="headerlink" href="#regression-logistique-binaire" title="Permalink to this heading">#</a></h4>
<p>Dans un premier temps, la variable √† pr√©dire est binaire : elle ne prend donc que deux valeurs 0/1 (ou -1/1). Dans le chapitre suivant, nous √©tudierons des algorithmes permettant d‚Äôaborder ce probl√®me sous un angle classification. Ici, nous nous int√©ressons √† une mod√©lisation probabiliste, permettant notamment de prendre en compte le bruit dans les donn√©es.</p>
<div class="section" id="id2">
<h5>Mod√®le<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h5>
<p>On recherche une distribution conditionnelle <span class="math notranslate nohighlight">\(P(Y|X)\)</span> de la variable √† pr√©dire sachant les pr√©dicteurs. Si le probl√®me est en 0/1, alors <span class="math notranslate nohighlight">\(Y\)</span> est une variable indicatrice et on a <span class="math notranslate nohighlight">\(P(Y=1)=\mathbb{E}(Y)\)</span> et <span class="math notranslate nohighlight">\(P(Y=1|X=x)=\mathbb{E}(Y|X=x)\)</span>. La probabilit√© conditionnelle est donc l‚Äôesp√©rance conditionnelle de l‚Äôindicatrice.</p>
<p>Supposons que <span class="math notranslate nohighlight">\(P(Y=1|X=x)=p(x,\boldsymbol\theta)\)</span> avec <span class="math notranslate nohighlight">\(p\)</span> fonction param√©tr√©e par <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span>. On suppose √©galement que les observations sont ind√©pendantes. La vraisemblance est alors donn√©e par</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n P(Y=y_i|X=x_i) = \displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)</span></p>
<div class="proof remark dropdown admonition" id="remark-6">
<p class="admonition-title"><span class="caption-number">Remark 18 </span></p>
<div class="remark-content section" id="proof-content">
<p>Pour <span class="math notranslate nohighlight">\(n\)</span> tirages d‚Äôune variable de Bernoulli dont la probabilit√© de succ√®s est constante et vaut <span class="math notranslate nohighlight">\(p\)</span>, la vraisemblance est <span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}\)</span>. Cette vraisemblance est maximis√©e lorsque
<span class="math notranslate nohighlight">\(p=n^{-1}\displaystyle\sum_{i=1}^n y_i\)</span>.</p>
</div>
</div><p>En notant <span class="math notranslate nohighlight">\(p_i=p(x_i,\boldsymbol\theta)\)</span>, maximiser la vraisemblance sans contrainte am√®ne √† la solution non informative <span class="math notranslate nohighlight">\(p_i=1\)</span> si <span class="math notranslate nohighlight">\(y_i=1\)</span> et 0 sinon. Si l‚Äôon essaye d‚Äôajouter des contraintes (relations entre les <span class="math notranslate nohighlight">\(p_i\)</span>), alors l‚Äôestimation du maximum de vraisemblance devient difficile.</p>
<p>Ici le mod√®le  <span class="math notranslate nohighlight">\(p_i=p(x_i,\boldsymbol\theta)\)</span> suppose que si <span class="math notranslate nohighlight">\(p\)</span> est continue, alors des valeurs proches de <span class="math notranslate nohighlight">\(x_i\)</span> am√®nent √† des valeurs proches de <span class="math notranslate nohighlight">\(p_i\)</span>. En supposant <span class="math notranslate nohighlight">\(p\)</span> connue comme fonction de <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span>, la vraisemblance est une fonction de <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> et on peut estimer ce param√®tre en maximisant la vraisemblance.</p>
</div>
<div class="section" id="id3">
<h5>R√©gression logistique<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h5>
<p>On recherche un ‚Äò‚Äôbon‚Äô‚Äô mod√®le pour <span class="math notranslate nohighlight">\(p\)</span> :</p>
<ol class="arabic simple">
<li><p>On peut dans un premier temps supposer que <span class="math notranslate nohighlight">\(p(\mathbf x)\)</span> est une fonction lin√©aire de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. Les fonctions lin√©aires √©tant non born√©es, elles ne peuvent mod√©liser des probabilit√©s.</p></li>
<li><p>On peut alors supposer que <span class="math notranslate nohighlight">\(log\ p(\mathbf x)\)</span> est une fonction lin√©aire de <span class="math notranslate nohighlight">\(\mathbf x\)</span>. L√† aussi, la fonction logarithme est non born√©e sup√©rieurement, et ne peut mod√©liser une probabilit√©.</p></li>
<li><p>Partant de cette id√©e, on borne le logarithme en utilisant la transformation logistique (ou logit) <span class="math notranslate nohighlight">\(log\frac{p(\mathbf x)}{1-p(\mathbf x)}\)</span>. Etant donn√© un √©v√©nement ayant une probabilit√© <span class="math notranslate nohighlight">\(p\)</span> de r√©ussir, le rapport <span class="math notranslate nohighlight">\(p/(1-p)\)</span> est appel√© la c√¥te de l‚Äô√©v√©nement (rapport de la probabilit√© qu‚Äôil se produise sur celle qu‚Äôil ne se produise pas. Si vous avez <span class="math notranslate nohighlight">\(p\)</span>=3/4 de chances de r√©ussir √† votre examen de permis, cotre c√¥te est <span class="math notranslate nohighlight">\(p/(1-p)=\frac{3/4}{1/4}\)</span>=3 contre un). On peut alors supposer que cette fonction de <span class="math notranslate nohighlight">\(p\)</span> est lin√©aire en <span class="math notranslate nohighlight">\(\mathbf x\)</span>.</p></li>
</ol>
<p>Le mod√®le de r√©gression logistique s‚Äô√©crit alors formellement</p>
<p><span class="math notranslate nohighlight">\(logit(p(\mathbf x)) = log \frac{p(\mathbf x)}{1-p(\mathbf x)} = \beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\)</span></p>
<p>En r√©solvant par rapport √† <span class="math notranslate nohighlight">\(p\)</span> on trouve alors</p>
<p><span class="math notranslate nohighlight">\(p(\mathbf x,\boldsymbol\theta) = \frac{e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}{1+e^{\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x}}=\frac{1}{1+e^{-(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x)}}\quad\text{avec }\boldsymbol\theta=(\beta_0,\boldsymbol\beta)^T\)</span></p>
<p>Pour minimiser les erreurs de pr√©diction, on doit pr√©dire <span class="math notranslate nohighlight">\(Y=1\)</span> si <span class="math notranslate nohighlight">\(p\geq 0.5\)</span>, soit <span class="math notranslate nohighlight">\(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x\geq 0\)</span> et <span class="math notranslate nohighlight">\(Y=0\)</span> sinon. La r√©gression logistique est donc un classifieur lin√©aire, dont la fronti√®re de d√©cision est justement l‚Äôhyperplan <span class="math notranslate nohighlight">\(\beta_0\mathbf 1 + \boldsymbol\beta\mathbf x= 0\)</span>. On peut montrer que la distance de <span class="math notranslate nohighlight">\(\mathbf x\)</span> √† cet hyperplan est <span class="math notranslate nohighlight">\(\beta_0/\|\boldsymbol\beta\| + \mathbf x^T\boldsymbol\beta/\|\boldsymbol\beta\|\)</span>. Les probabilit√©s d‚Äôappartenance de <span class="math notranslate nohighlight">\(\mathbf x\)</span> aux classes d√©croissent donc d‚Äôautant plus vite que <span class="math notranslate nohighlight">\(\|\boldsymbol\beta\|\)</span> est grand.</p>
<p>Dans la figure suivante, la probabilit√© d‚Äôappartenance √† la classe 1 (points rouges) est donn√©e en fausses couleurs.</p>
<p><img alt="" src="_images/regression.png" /></p>
</div>
</div>
<div class="section" id="regression-logistique-a-plusieurs-classes">
<h4>R√©gression logistique √† plusieurs classes<a class="headerlink" href="#regression-logistique-a-plusieurs-classes" title="Permalink to this heading">#</a></h4>
<p>Dans ce cas, <span class="math notranslate nohighlight">\(Y\)</span> peut prendre <span class="math notranslate nohighlight">\(k\)</span> valeurs. Le mod√®le reste le m√™me, chaque classe <span class="math notranslate nohighlight">\(c\in[\![0,k-1]\!]\)</span> ayant son jeu de param√®tres <span class="math notranslate nohighlight">\(\boldsymbol\theta_c=(\beta^c_0,\boldsymbol\beta^c)^T\)</span>. Les probabilit√©s conditionnelles pr√©dites sont alors</p>
<p><span class="math notranslate nohighlight">\((\forall c\in[\![0,k-1]\!])\;\;P(Y=c|X=\mathbf x) = \frac{e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}{1+e^{\beta^c_0\mathbf 1 + \boldsymbol\beta^c\mathbf x}}\)</span></p>
</div>
<div class="section" id="interpretation">
<h4>Interpr√©tation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h4>
<p>Si <span class="math notranslate nohighlight">\(\mathbf x=\mathbf 0\)</span>, alors <span class="math notranslate nohighlight">\(p(\mathbf x)=\frac{1}{1+e^{-\beta_0}}\)</span>. L‚Äôordonn√©e √† l‚Äôorigine fixe donc le taux d‚Äô√©v√©nements ‚Äúde base‚Äù.</p>
<p>Supposons <span class="math notranslate nohighlight">\(\boldsymbol\beta\in\mathbb{R}\)</span> (l‚Äôinterpr√©tation sera la m√™me dans le cas g√©n√©ral). Consid√©rons l‚Äôeffet sur la probabilit√© d‚Äôun √©v√®nement du changement de <span class="math notranslate nohighlight">\(x\in\mathbb{R}\)</span> d‚Äôune unit√©, passant de <span class="math notranslate nohighlight">\(x_0\)</span> √† <span class="math notranslate nohighlight">\(x_0+1\)</span>. Alors :</p>
<p><span class="math notranslate nohighlight">\(logit(p(x_0+1))-logit(p(x_0)) = \beta_0+\beta(x_0+1)-(\beta_0+\beta(x_0)) = \beta\)</span>
et en utilisant la d√©finition de la fonction logit :</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
log \frac{p( x_0+1)}{1-p(x_0+1)}-log \frac{p( x_0)}{1-p(x_0)} &amp;=&amp; \beta\\
log \left  [\frac{\frac{p( x_0+1)}{1-p(x_0+1)}}{\frac{p( x_0)}{1-p(x_0)}} \right ]&amp;=&amp; \beta\\
\end{eqnarray*}\)</span></p>
<p>En notant OR (Odds Ratio, ou rapport de c√¥te) le terme en argument du log, et en prenant l‚Äôexponentielle, on trouve <span class="math notranslate nohighlight">\(OR=e^\beta\)</span>. Le coefficient <span class="math notranslate nohighlight">\(\beta\)</span> est donc tel que <span class="math notranslate nohighlight">\(e^\beta\)</span> est le rapport de c√¥te pour un changement unitaire de l‚Äôentr√©e <span class="math notranslate nohighlight">\(x\)</span>. Si <span class="math notranslate nohighlight">\(x\)</span> est incr√©ment√© de deux unit√©s, alors le rapport de c√¥te est de <span class="math notranslate nohighlight">\(e^{2\beta}=(e^\beta)^2\)</span>, que l‚Äôon g√©n√©ralise facilement au cas d‚Äôun changement de <span class="math notranslate nohighlight">\(n\)</span> unit√©s √† OR=<span class="math notranslate nohighlight">\((e^\beta)^n\)</span>.</p>
<p>Dans le cas o√π <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> est un vecteur, sa i√®me composante est une estimation du changement de la probabilit√© d‚Äôun √©v√®nement correspondant √† une augmentation d‚Äôune unit√© de la i√®me composante de <span class="math notranslate nohighlight">\(\mathbf x\)</span>, les autres composantes √©tant constantes.</p>
</div>
<div class="section" id="estimation-des-coefficients-de-la-regression-logistique">
<h4>Estimation des coefficients de la r√©gression logistique<a class="headerlink" href="#estimation-des-coefficients-de-la-regression-logistique" title="Permalink to this heading">#</a></h4>
<p>D‚Äôapr√®s le mod√®le probabiliste, la distribution associ√©e √† la r√©gression logistique est la loi binomiale. Pour <span class="math notranslate nohighlight">\(n\)</span> √©chantillons <span class="math notranslate nohighlight">\((x_i,y_i),i\in[\![1,n]\!]\)</span>, la vraisemblance s‚Äô√©crit</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\prod_{i=1}^n p(x_i,\boldsymbol\theta)^{y_i}(1-p(x_i,\boldsymbol\theta))^{1-y_i}\)</span></p>
<p>Pour estimer les param√®tres <span class="math notranslate nohighlight">\(\beta_0\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol\beta\)</span> √† partir des donn√©es, on maximise cette vraisemblance. On prend son logarithme, on calcule son gradient et on en d√©duit un syst√®me d‚Äô√©quations √† r√©soudre. Cette approche am√®ne √† des calculs complexes, la formulation analytique n‚Äô√©tant pas simple, et une approximation num√©rique est en pratique mise en oeuvre pour trouver l‚Äôoptimal.</p>
</div>
</div>
<div class="section" id="analyse-des-resultats-d-une-regression">
<h3>Analyse des r√©sultats d‚Äôune r√©gression<a class="headerlink" href="#analyse-des-resultats-d-une-regression" title="Permalink to this heading">#</a></h3>
<div class="section" id="etude-des-residus">
<h4>Etude des r√©sidus<a class="headerlink" href="#etude-des-residus" title="Permalink to this heading">#</a></h4>
<p>L‚Äô√©tude des r√©sidus <span class="math notranslate nohighlight">\( y_i- y^*_i\)</span> permet de rep√©rer les observations aberrantes ou au contraire qui jouent un r√¥le fondamental dans la d√©termination de la r√©gression. Elle permet √©galement de v√©rifier que  le mod√®le lin√©aire est justifi√©.</p>
<p>Comme <span class="math notranslate nohighlight">\(\mathbf Y = \mathbf Y -\mathbf X\boldsymbol \beta +\mathbf X\boldsymbol \beta\)</span> , o√π <span class="math notranslate nohighlight">\(\mathbf Y-\mathbf X\boldsymbol \beta \)</span> est orthogonal √† <span class="math notranslate nohighlight">\(\mathbf X\boldsymbol \beta\)</span>, la matrice de variance des r√©sidus s‚Äô√©crit</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbb{V}(\mathbf Y) &amp;=&amp; \mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+\mathbb{V}(\mathbf X\boldsymbol \beta)\\
\sigma^2 \mathbf{I} &amp;=&amp;\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)+ \sigma^2 \mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\\ 
\text {soit }\mathbb{V}(\mathbf Y-\mathbf X\boldsymbol \beta)&amp;=&amp;\sigma^2(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T)
\end{eqnarray*}
\)</span></p>
<p>et les r√©sidus sont donc en g√©n√©ral corr√©l√©s entre eux.</p>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 19 </span></p>
<div class="remark-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\mathbf{I}-\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\)</span> est la projection orthogonale sur <span class="math notranslate nohighlight">\(Im(\mathbf X)^\perp\)</span></p>
</div>
</div><p>Si <span class="math notranslate nohighlight">\(p_i\)</span> est le <span class="math notranslate nohighlight">\(i^e\)</span> terme diagonal du projecteur <span class="math notranslate nohighlight">\(\mathbf X(\mathbf X^T\mathbf X)^{-1}\mathbf X^T\)</span>, alors</p>
<p><span class="math notranslate nohighlight">\(\mathbb{V}( y_i- y^*_i) = (1-p_i)\sigma^2\)</span></p>
<p>d‚Äôo√π l‚Äôestimation de la variance du r√©sidu <span class="math notranslate nohighlight">\(\hat{\mathbb{V}}(y_i-y^*_i) = (1-p_i)\hat{\sigma}^2\)</span>.</p>
<p>Si le mod√®le lin√©aire est justifi√©, alors la distribution des r√©sidus suit approximativement une loi normale. Un test statistique (par exemple le test de Jarque-Berra) viendra confirmer ou infirmer l‚Äôhypoth√®se selon laquelle la distribution peut √™tre consid√©r√©e comme telle.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/nuagelin.png" /></p></th>
<th class="head"><p><img alt="" src="_images/nuagepaslin.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="" src="_images/reslin.png" /></p></td>
<td><p><img alt="" src="_images/respaslin.png" /></p></td>
</tr>
</tbody>
</table>
<div class="proof definition admonition" id="definition-8">
<span id="index-10"></span><p class="admonition-title"><span class="caption-number">Definition 39 </span> (R√©sidu studentis√©)</p>
<div class="definition-content section" id="proof-content">
<p>On appelle r√©sidu studentis√© la quantit√© <span class="math notranslate nohighlight">\(\frac{y_i-y^*_i}{\hat{\sigma}\sqrt{1-hp}}\)</span></p>
</div>
</div><p>Lorsque <span class="math notranslate nohighlight">\(n\)</span> est grand, ces r√©sidus doivent √™tre compris dans l‚Äôintervalle [-2,2].</p>
<p>Un fort r√©sidu peut indiquer une valeur aberrante, mais la r√©ciproque n‚Äôest pas vraie. Il est donc n√©cessaire d‚Äô√©tudier l‚Äôinfluence de chaque observation sur les r√©sultats.</p>
</div>
<div class="section" id="influence-des-observations">
<h4>Influence des observations<a class="headerlink" href="#influence-des-observations" title="Permalink to this heading">#</a></h4>
<p>Pour √©tudier l‚Äôinfluence des observations sur la pr√©diction, deux approches sont possibles (et compl√©mentaires) :</p>
<ol class="arabic simple">
<li><p>√©tudier l‚Äôinfluence d‚Äôune observation sur sa propre pr√©diction. On calcule le r√©sidu pr√©dit <span class="math notranslate nohighlight">\(y_i-y_{\bar{i}}^*\)</span>, o√π <span class="math notranslate nohighlight">\(y_{\bar{i}}^*\)</span> est la pr√©vision obtenue avec les <span class="math notranslate nohighlight">\(n-1\)</span> autres observations que <span class="math notranslate nohighlight">\(y_i\)</span>. Il est facile de montrer que ce r√©sidu vaut <span class="math notranslate nohighlight">\(\frac{y_i-y_i^*}{1-p_i}\)</span></p></li>
</ol>
<div class="proof remark dropdown admonition" id="remark-9">
<p class="admonition-title"><span class="caption-number">Remark 20 </span></p>
<div class="remark-content section" id="proof-content">
<p>Il convient de rester prudent lorsque <span class="math notranslate nohighlight">\(p_i\)</span> est grand, et la quantit√©
<span class="math notranslate nohighlight">\(\displaystyle\sum_{i=1}^n  \frac{(y_i-y_i^*)^2}{(1-p_i)^2}\)</span>
est une mesure du pouvoir pr√©dictif du mod√®le.</p>
</div>
</div><ol class="arabic simple" start="2">
<li><p>√©tudier l‚Äôinfluence d‚Äôune observation sur les estimations des param√®tres de la r√©gression <span class="math notranslate nohighlight">\(\beta_i\)</span>. On peut par exemple calculer une distance, dite de Cook, entre <span class="math notranslate nohighlight">\(\boldsymbol \beta\)</span> et <span class="math notranslate nohighlight">\(\boldsymbol \beta_{\bar{i}}\)</span> :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}}) = \frac{(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})^T\mathbf X^T \mathbf X(\boldsymbol \beta-\boldsymbol\beta_{\bar{i}})}{\hat{\sigma}^2(p+1)}=\frac{\|\mathbf Y^*-\mathbf Y_{\bar{i}}^*\|^2}{\hat{\sigma}^2(p+1)}\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\mathbf Y_{\bar{i}}^*=\mathbf X\boldsymbol\beta_{\bar{i}}\)</span>. Si <span class="math notranslate nohighlight">\(d(\boldsymbol \beta,\boldsymbol\beta_{\bar{i}})&gt;1\)</span>, alors en g√©n√©ral l‚Äôobservation <span class="math notranslate nohighlight">\(i\)</span> a une influence anormale.</p>
</div>
<div class="section" id="stabilite-des-coefficients-de-regression">
<h4>Stabilit√© des coefficients de r√©gression<a class="headerlink" href="#stabilite-des-coefficients-de-regression" title="Permalink to this heading">#</a></h4>
<p>La source principale d‚Äôinstabilit√© dans l‚Äôestimation des param√®tres de r√©gression r√©side dans le fait que les variables explicatives sont tr√®s corr√©l√©es entre elles. Comme <span class="math notranslate nohighlight">\(\mathbb{V}(\boldsymbol \beta)=\sigma^2(\mathbf X^T\mathbf X)^{-1}\)</span> alors si les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> sont corr√©l√©s, la matrice <span class="math notranslate nohighlight">\(\mathbf X^T\mathbf X\)</span> est mal conditionn√©e. Dans ce cas, les param√®tres sont estim√©s avec impr√©cision et les pr√©dictions sont ent√¢ch√©es d‚Äôerreur. Il est donc essentiel de mesurer les colin√©arit√©s entre pr√©dicteurs. Par simplicit√© (sans que cela nuise √† la g√©n√©ralit√©), on suppose ici que les variables sont centr√©es et r√©duites : <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X)\)</span> est donc une matrice de taille <span class="math notranslate nohighlight">\(p\)</span> (le fait de centrer les donn√©es supprime la constante) et <span class="math notranslate nohighlight">\(\boldsymbol\beta\in\mathbb{R}^p\)</span>. Ainsi <span class="math notranslate nohighlight">\((\mathbf X^T\mathbf X)=n\mathbf R\)</span> o√π <span class="math notranslate nohighlight">\(\mathbf R\)</span> est la matrice de corr√©lation entre les pr√©dicteurs.</p>
<p>Deux strat√©gies sont classiquement propos√©es :</p>
<ol class="arabic simple">
<li><p>Facteur d‚Äôinflation de la variance : on a <span class="math notranslate nohighlight">\(\mathbb{V}(\boldsymbol \beta) = \sigma^2\frac{\mathbf{R}^{-1}}{n}\)</span> et <span class="math notranslate nohighlight">\(\sigma^2_{\beta_j} = \frac{\sigma^2}{n}(\mathbf{R}^{-1})_{jj}\)</span>. Or le <span class="math notranslate nohighlight">\(j^e\)</span> terme de <span class="math notranslate nohighlight">\(\mathbf{R}^{-1}\)</span> est <span class="math notranslate nohighlight">\(\frac{1}{1-R^2_j}\)</span> o√π <span class="math notranslate nohighlight">\(R^2_j\)</span> est le carr√© du coefficient de corr√©lation multiple de <span class="math notranslate nohighlight">\(\mathbf X_j\)</span> et des <span class="math notranslate nohighlight">\(p-1\)</span> autres variables explicatives. Ce terme est le facteur d‚Äôinflation de la variance. La moyenne de ces <span class="math notranslate nohighlight">\(p\)</span> termes est parfois utilis√©e comme indice global de colin√©arit√© multiple.</p></li>
</ol>
<div class="proof remark dropdown admonition" id="remark-10">
<p class="admonition-title"><span class="caption-number">Remark 21 </span></p>
<div class="remark-content section" id="proof-content">
<p>Si les variables explicatives sont orthogonales, la r√©gression multiple revient √† <span class="math notranslate nohighlight">\(p\)</span> r√©gressions simples.</p>
</div>
</div><ol class="arabic simple" start="2">
<li><p>La factorisation spectrale de <span class="math notranslate nohighlight">\(\mathbf R\)</span> s‚Äô√©crit <span class="math notranslate nohighlight">\(\mathbf R = \mathbf U\boldsymbol\Lambda \mathbf U^T\)</span>. Donc <span class="math notranslate nohighlight">\(\mathbf{R}^{-1}=\mathbf U\Lambda^{-1}\mathbf U^T\)</span> et la variance de <span class="math notranslate nohighlight">\(\beta_j\)</span> s‚Äô√©crit</p></li>
</ol>
<p><span class="math notranslate nohighlight">\(\mathbb{V}(\beta_j) = \frac{\sigma^2}{n}\displaystyle\sum_{i=1}^p \frac{u_{ji}^2}{\lambda_i}\)</span></p>
<p>et d√©pend donc des inverses des valeurs propres de <span class="math notranslate nohighlight">\(\mathbf R\)</span>. Dans le cas o√π les pr√©dicteurs sont fortement corr√©l√©s, les derni√®res valeurs propres sont proches de 0 ce qui entra√Æne l‚Äôinstabilit√© des param√®tres de r√©gression.</p>
<p>Pour am√©liorer la stabilit√© des param√®tres de r√©gression, on peut alors :</p>
<ul class="simple">
<li><p>rejeter certains termes de la somme pr√©c√©dente, par exemple en rempla√ßant les <span class="math notranslate nohighlight">\(p\)</span> pr√©dicteurs par leurs <span class="math notranslate nohighlight">\(p\)</span> composantes principales (Ceci revient √† effectuer <span class="math notranslate nohighlight">\(p\)</span> r√©gressions simples).</p></li>
<li><p>r√©gulariser la r√©gression en utilisant des approche de type Ridge regression.</p></li>
</ul>
</div>
</div>
<div class="section" id="selection-des-variables">
<h3>S√©lection des variables<a class="headerlink" href="#selection-des-variables" title="Permalink to this heading">#</a></h3>
<p>Plut√¥t que d‚Äôexpliquer <span class="math notranslate nohighlight">\(\mathbf Y\)</span> par l‚Äôensemble des pr√©dicteurs, on peut chercher un sous-ensemble de ces <span class="math notranslate nohighlight">\(p\)</span> variables permettant d‚Äôobtenir quasiment le m√™me r√©sultat (r√©gression). Nous avons d√©j√† abord√© la s√©lection de variables dans un chapitre pr√©c√©dent.</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this heading">#</a></h3>
<div class="section" id="donnees">
<h4>Donn√©es<a class="headerlink" href="#donnees" title="Permalink to this heading">#</a></h4>
<p>On s‚Äôint√©resse aux donn√©es suivantes et on cherche s‚Äôil existe une relation entre la production <span class="math notranslate nohighlight">\(Y\)</span> et les deux variables pr√©dictives <span class="math notranslate nohighlight">\(X_1\)</span> et <span class="math notranslate nohighlight">\(X_2\)</span>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Usine</p></th>
<th class="head"><p>Travail (h) <span class="math notranslate nohighlight">\(X_1\)</span></p></th>
<th class="head"><p>Capital (machines/h) <span class="math notranslate nohighlight">\(X_2\)</span></p></th>
<th class="head"><p>Production (<span class="math notranslate nohighlight">\(10^2\)</span> T)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1100</p></td>
<td><p>300</p></td>
<td><p>60</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1200</p></td>
<td><p>400</p></td>
<td><p>120</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1430</p></td>
<td><p>420</p></td>
<td><p>190</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>1500</p></td>
<td><p>400</p></td>
<td><p>250</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>1520</p></td>
<td><p>510</p></td>
<td><p>300</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>1620</p></td>
<td><p>590</p></td>
<td><p>360</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>1800</p></td>
<td><p>600</p></td>
<td><p>380</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>1820</p></td>
<td><p>630</p></td>
<td><p>430</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>1800</p></td>
<td><p>610</p></td>
<td><p>440</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id4">
<h4>Mod√®le<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<p>On fait l‚Äôhypoth√®se d‚Äôun mod√®le lin√©aire</p>
<p><span class="math notranslate nohighlight">\(y = \beta_0+\beta_1 X_1 + \beta_2 X_2+\varepsilon = \mathbf X \boldsymbol\beta+\boldsymbol\varepsilon\)</span></p>
<p>On a alors <span class="math notranslate nohighlight">\(\boldsymbol\beta = (\mathbf X^T\mathbf X)^{-1}\mathbf X^T \mathbf Y = \begin{pmatrix} -437.714\\0.336\\0.410\end{pmatrix}\)</span> et l‚Äô√©quation du mod√®le lin√©aire (hyperplan) aux moindres carr√©s est</p>
<p><span class="math notranslate nohighlight">\(y = -437.714+0.336 X_1+0.41X_2\)</span></p>
<p>De plus
<span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{\|\mathbf Y -\mathbf Y^*\|^2}{n-p-1} = \frac{3194}{6} = 639\)</span></p>
<p>de sorte que la covariance des param√®tres de r√©gression vaut</p>
<p><span class="math notranslate nohighlight">\(\hat{\sigma}^2 (\mathbf X^T\mathbf X)^{-1} = \begin{pmatrix} 3355.56 &amp; -4.152 &amp; 6.184\\-4.152 &amp; 0.008 &amp; -0.016 \\ 6.184 &amp; -0.016 &amp; 0.038\end{pmatrix}\)</span></p>
<p>Dans la figure suivante, les points au-dessus du plan regresseur sont en bleu, les autres en vert.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/plan.png" /></p></th>
<th class="head"><p><img alt="" src="_images/plan2.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Un point de vue‚Ä¶</p></td>
<td><p>Un autre</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-clustering"></span><div class="tex2jax_ignore mathjax_ignore section" id="quelques-methodes-de-classification">
<h2>Quelques m√©thodes de classification<a class="headerlink" href="#quelques-methodes-de-classification" title="Permalink to this heading">#</a></h2>
<div class="section" id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h3>
<p>La classification automatique a pour but d‚Äôobtenir une repr√©sentation simplifi√©e des donn√©es initiales. Elle consiste √† organiser un ensemble de donn√©es en classes homog√®nes ou classes naturelles.</p>
<p>Une d√©finition formelle de la classification, qui puisse servir de base √† un processus automatis√©, am√®ne √† se poser les questions suivantes :</p>
<ul class="simple">
<li><p>Comment les objets √† classer sont-ils d√©finis ?</p></li>
<li><p>Comment d√©finir la notion de ressemblance entre objets ?</p></li>
<li><p>Qu‚Äôest-ce qu‚Äôune classe ?</p></li>
<li><p>Comment sont structur√©es les classes ?</p></li>
<li><p>Comment juger une classification par rapport √† une autre ?</p></li>
</ul>
<p>Pour effectuer cette classification, deux d√©marches sont g√©n√©ralement utilis√©es :</p>
<ul class="simple">
<li><p>on regroupe en classes les objets qui partagent certaines caract√©ristiques.</p></li>
<li><p>on regroupe en classes les objets qui poss√®dent des caract√©ristiques proches. C‚Äôest cette approche qui est √©tudi√©e ici</p></li>
</ul>
</div>
<div class="section" id="structures-de-classification">
<h3>Structures de classification<a class="headerlink" href="#structures-de-classification" title="Permalink to this heading">#</a></h3>
<div class="section" id="partition">
<h4>Partition<a class="headerlink" href="#partition" title="Permalink to this heading">#</a></h4>
<div class="proof definition admonition" id="definition-0">
<span id="index-0"></span><p class="admonition-title"><span class="caption-number">Definition 40 </span> (Partition)</p>
<div class="definition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\Omega\)</span> √©tant un ensemble fini, un ensemble <span class="math notranslate nohighlight">\(P =(P_1 ,P_2 ,\cdots  P_g )\)</span> de parties non vides de   <span class="math notranslate nohighlight">\(\Omega\)</span> est une partition si :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall k\neq l) P_k \cap P_l=\emptyset\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\displaystyle\cup_{i=1}^gP_i=\Omega\)</span></p></li>
</ul>
</div>
</div><p>Dans un ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> partitionn√© en <span class="math notranslate nohighlight">\(g\)</span> classes, chaque √©l√©ment de l‚Äôensemble appartient √† une classe et une seule. Une mani√®re pratique de d√©crire cette partition <span class="math notranslate nohighlight">\(P\)</span> consiste √† lui associer la matrice de classification <span class="math notranslate nohighlight">\({\bf C}=(c_{ij}), i\in [\![1,n]\!], j\in [\![1,g]\!]\)</span>, avec <span class="math notranslate nohighlight">\(c_{ij}=1\)</span> si l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span> appartient √† <span class="math notranslate nohighlight">\(P_j\)</span>, et <span class="math notranslate nohighlight">\(c_{ij}=0\)</span> sinon. Dans le cas o√π l‚Äôon accepte qu‚Äôun individu appartienne √† plusieurs classes (avec des degr√©s d‚Äôappartenance), on autorise <span class="math notranslate nohighlight">\(c_{ij}\)</span> √† couvrir l‚Äôintervalle [0,1] et on parle alors de classification floue.</p>
</div>
<div class="section" id="hierarchie-indicee">
<h4>Hi√©rarchie indic√©e<a class="headerlink" href="#hierarchie-indicee" title="Permalink to this heading">#</a></h4>
<div class="proof definition admonition" id="definition-1">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 41 </span> (Hi√©rarchie)</p>
<div class="definition-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(\Omega\)</span> √©tant un ensemble fini, un ensemble <span class="math notranslate nohighlight">\(H\)</span> de parties non vides de <span class="math notranslate nohighlight">\(\Omega\)</span> est une hi√©rarchie sur <span class="math notranslate nohighlight">\(\Omega\)</span> si :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega \in H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall x\in \Omega) \{x\}\in H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall h,h'\in H) h\cap h'=\emptyset\)</span> ou <span class="math notranslate nohighlight">\(h\subset h'\)</span> ou <span class="math notranslate nohighlight">\(h'\subset h\)</span></p></li>
</ul>
</div>
</div><p>Une hi√©rarchie est souvent repr√©sent√©e par l‚Äôinterm√©diaire d‚Äôun indice, fonction <span class="math notranslate nohighlight">\(i\)</span> de <span class="math notranslate nohighlight">\(H\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}^+\)</span>, strictement croissante vis √† vis de l‚Äôinclusion et de noyau l‚Äôensemble des singletons de <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
</div>
<div class="section" id="partition-et-hierarchie">
<h4>Partition et hi√©rarchie<a class="headerlink" href="#partition-et-hierarchie" title="Permalink to this heading">#</a></h4>
<p>Si <span class="math notranslate nohighlight">\(P =(P_1 \cdots,P_g)\)</span> est une partition de <span class="math notranslate nohighlight">\(\Omega\)</span>, l‚Äôensemble <span class="math notranslate nohighlight">\(H\)</span> form√© des classes <span class="math notranslate nohighlight">\(P_k\)</span> de <span class="math notranslate nohighlight">\(P\)</span>, des singletons de   <span class="math notranslate nohighlight">\(\Omega\)</span> et de l‚Äôensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> lui-m√™me forme une hi√©rarchie. Remarquons qu‚Äôinversement, il est possible d‚Äôassocier √† chaque niveau d‚Äôune hi√©rarchie indic√©e une partition. Une hi√©rarchie indic√©e correspond donc √† un ensemble de partitions embo√Æt√©es.</p>
</div>
</div>
<div class="section" id="objectifs-de-la-classification">
<h3>Objectifs de la classification<a class="headerlink" href="#objectifs-de-la-classification" title="Permalink to this heading">#</a></h3>
<div class="section" id="difficultes-de-caracteriser-les-objectifs">
<h4>Difficult√©s de caract√©riser les objectifs<a class="headerlink" href="#difficultes-de-caracteriser-les-objectifs" title="Permalink to this heading">#</a></h4>
<p>L‚Äôobjectif de la classification automatique est l‚Äôorganisation en classes homog√®nes des √©l√©ments d‚Äôun ensemble  <span class="math notranslate nohighlight">\(\Omega\)</span>. Pour d√©finir cette notion de classes homog√®nes, on utilise le plus souvent une mesure de similarit√© (ou de dissimilarit√©) sur  <span class="math notranslate nohighlight">\(\Omega\)</span>. Par exemple, on peut imposer √† un couple quelconque d‚Äôindividus d‚Äôune m√™me classe d‚Äô√™tre plus ‚Äúproches‚Äù que n‚Äôimporte quel couple form√© par un individu de la classe et un individu d‚Äôune autre classe. En pratique, cet objectif est inutilisable, et plusieurs d√©marches sont alors utilis√©es pour remplacer cet objectif trop difficile √† atteindre.</p>
</div>
<div class="section" id="demarche-numerique">
<h4>D√©marche num√©rique<a class="headerlink" href="#demarche-numerique" title="Permalink to this heading">#</a></h4>
<div class="section" id="id1">
<h5>Partition<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h5>
<p>On remplace cette condition trop exigeante par une fonction num√©rique (crit√®re) qui mesure la qualit√© d‚Äôhomog√©n√©it√© d‚Äôune partition. Le probl√®me peut para√Ætre alors tr√®s simple. En effet, par exemple, dans le cas de la recherche d‚Äôune partition, il suffit de chercher parmi l‚Äôensemble fini de toutes les partitions celle qui optimise le crit√®re num√©rique. Malheureusement, le nombre de ces partitions √©tant tr√®s grand, leur √©num√©ration est impossible dans un temps raisonnable.
Le nombre de partitions en <span class="math notranslate nohighlight">\(g\)</span> classes d‚Äôun ensemble √† <span class="math notranslate nohighlight">\(n\)</span> √©l√©ments, que l‚Äôon note <span class="math notranslate nohighlight">\(S_n^g\)</span> est le nombre de Stirling de deuxi√®me esp√®ce. En posant <span class="math notranslate nohighlight">\(S_0^0=1\)</span> et pour tout <span class="math notranslate nohighlight">\(n&gt;0\)</span>, <span class="math notranslate nohighlight">\(S_n^0=S_0^n=0\)</span>, il peut √™tre calcul√© par r√©currence gr√¢ce √† la relation <span class="math notranslate nohighlight">\(S_n^g=S_{n-1}^{g-1}+gS_{n-1}^g\)</span>. On peut montrer que</p>
<div class="math notranslate nohighlight">
\[\begin{split}S_n^g = \frac{1}{g!}\displaystyle\sum_{i=1}^g (-1)^{g-i}\begin{pmatrix}g\\ i \end{pmatrix}i^n\end{split}\]</div>
<p>et donc <span class="math notranslate nohighlight">\(S_n^g\sim \frac{g^n}{g!}\)</span> lorsque <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>. En pratique, sur un ordinateur calculant <span class="math notranslate nohighlight">\(10^6\)</span> partitions par seconde, il faut 126 000 ans pour calculer l‚Äôensemble des partitions d‚Äôun ensemble √† <span class="math notranslate nohighlight">\(n=25\)</span> √©l√©ments.</p>
<p>On utilise alors des heuristiques qui donnent, non pas la meilleure solution, mais une ‚Äúbonne solution‚Äù, proche de la solution optimale. On parle alors d‚Äôoptimisation locale. Lorsqu‚Äôil existe une structure d‚Äôordre sur l‚Äôensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> et que celle-ci doit √™tre respect√©e par la partition, il existe un algorithme de programmation dynamique (algorithme de Fisher), qui fournit la solution optimale.</p>
</div>
<div class="section" id="hierarchie">
<h5>Hi√©rarchie<a class="headerlink" href="#hierarchie" title="Permalink to this heading">#</a></h5>
<p>Dans le cas d‚Äôune hi√©rarchie, on cherche √† obtenir des classes d‚Äôautant plus homog√®nes qu‚Äôelles sont situ√©es dans le bas de la hi√©rarchie. La d√©finition d‚Äôun crit√®re est moins facile. Nous verrons qu‚Äôil est possible de le faire en utilisant la
notion d‚Äôultram√©trique (ultram√©trique optimale).</p>
</div>
</div>
<div class="section" id="demarche-algorithmique">
<h4>D√©marche algorithmique<a class="headerlink" href="#demarche-algorithmique" title="Permalink to this heading">#</a></h4>
<p>Il s‚Äôagit cette fois de d√©finir directement un algorithme qui construit des classes homog√®nes en tenant compte de la mesure de similarit√©. Il est relativement facile de proposer de tels algorithmes, le probl√®me est de pouvoir v√©rifier que les r√©sultats fournis sont int√©ressants et r√©pondent au probl√®me pos√©. En r√©alit√©, cette d√©marche rejoint assez souvent la pr√©c√©dente.</p>
</div>
<div class="section" id="mesure-de-dissimilarite-et-distance">
<h4>Mesure de dissimilarit√© et distance<a class="headerlink" href="#mesure-de-dissimilarite-et-distance" title="Permalink to this heading">#</a></h4>
<p>Les algorithmes de classification d√©pendent d‚Äôune m√©trique qui d√©finit implicitement la forme des classes qui seront calcul√©es. Si la distance euclidienne suppose une isotropie dans les axes (et donc une repr√©sentation sph√©rique des classes), d‚Äôautres distances ou indices de dissimilarit√© peuvent √™tre utilis√©s.</p>
<div class="section" id="indice-de-dissimilarite">
<h5>Indice de dissimilarit√©<a class="headerlink" href="#indice-de-dissimilarite" title="Permalink to this heading">#</a></h5>
<p>On se place dans <span class="math notranslate nohighlight">\(\mathbb R^d\)</span>, et on consid√®re <span class="math notranslate nohighlight">\(n\)</span> individus √† classer <span class="math notranslate nohighlight">\({\bf x_1}\ldots {\bf x_n}\)</span>.</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 42 </span> (Dissimilarit√© - ultram√©trique)</p>
<div class="definition-content section" id="proof-content">
<p>Une mesure de dissimilarit√© <span class="math notranslate nohighlight">\(\delta\)</span> est une fonction de</p>
<p><span class="math notranslate nohighlight">\(
 \delta: \begin{array}{ccc}
\mathbb{R}^d\times\mathbb{R}^d &amp;\rightarrow &amp;\mathbb{R}^+\\
(\mathbf x_i,\mathbf x_j)&amp;\mapsto &amp; \delta_{ij} = \delta(\mathbf x_i,\mathbf x_j)
\end{array}
\)</span></p>
<p>v√©rifiant :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\forall i,j\in[\![1, n]\!])\ \delta_{ij}=\delta_{ji}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\forall i\in[\![1, n]\!])\ \delta_{ii}= 0\)</span></p></li>
</ul>
<p>Si l‚Äôin√©galit√© triangulaire <span class="math notranslate nohighlight">\(\delta_{ij}\leq \delta_{ik}+\delta_{kj}\)</span> est de plus v√©rifi√©e pour tout <span class="math notranslate nohighlight">\(i,j,k\)</span>, alors <span class="math notranslate nohighlight">\(\delta\)</span> est une distance.</p>
<p>Si enfin l‚Äôin√©galit√© ultram√©trique  <span class="math notranslate nohighlight">\(\delta_{ij}\leq max(\delta_{ik}+\delta_{jk})\)</span> est  v√©rifi√©e pour tout <span class="math notranslate nohighlight">\(i,j,k\)</span>, <span class="math notranslate nohighlight">\(\delta\)</span> est une ultram√©trique.</p>
</div>
</div><p>A partir des mesures de dissimilarit√©, on d√©duit des mesures de similarit√© <span class="math notranslate nohighlight">\(s_{ij}\)</span> le passage de l‚Äôune √† l‚Äôautre se faisant par exemple par <span class="math notranslate nohighlight">\(\delta_{ij} = s_{max}-s_{ij}\)</span>.</p>
</div>
<div class="section" id="cas-de-variables-qualitatives">
<h5>Cas de variables qualitatives<a class="headerlink" href="#cas-de-variables-qualitatives" title="Permalink to this heading">#</a></h5>
<p>On suppose que les <span class="math notranslate nohighlight">\(d\)</span> composantes des <span class="math notranslate nohighlight">\({\bf x_i}\)</span> sont qualitatives, et on se limite ici au cas de variables bimodales.
√âtant donn√©s <span class="math notranslate nohighlight">\({\bf x_i}=\begin{pmatrix} x_i^1\ldots x_i^d\end{pmatrix}\)</span> et <span class="math notranslate nohighlight">\({\bf x_j}\)</span>, on note :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a_{ij}\)</span> le nombre de co-occurences entre les individus <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_{ij}\)</span> le nombre de co-absences entre les individus <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c_{ij}\)</span> le nombre d‚Äôattributs pr√©sents chez <span class="math notranslate nohighlight">\(i\)</span> et absents chez <span class="math notranslate nohighlight">\(j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d_{ij}\)</span> le nombre d‚Äôattributs absents chez <span class="math notranslate nohighlight">\(i\)</span> et pr√©sents chez <span class="math notranslate nohighlight">\(j\)</span></p></li>
</ul>
<p>les mesures suivantes sont des exemples de dissimilarit√© :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \sqrt{b_{ij}+c_{ij}}\)</span> [distance ‚Äúeuclidienne‚Äù binaire]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [diff√©rence binaire de taille]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}c_{ij})}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [diff√©rence binaire de motif]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(a_{ij}+b_{ij}+c_{ij}+d_{ij})(b_{ij}+c_{ij})-(b_{ij}-c_{ij})^2}{(a_{ij}+b_{ij}+c_{ij}+d_{ij})^2}\)</span> [diff√©rence binaire de forme]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{4(a_{ij}+b_{ij}+c_{ij}+d_{ij})}\)</span> [dissimilarit√© binaire de variance]</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta_{ij} = \frac{(b_{ij}+c_{ij})}{2a_{ij}+b_{ij}+c_{ij}}\)</span> [dissimilarit√© binaire de Lance et Williams]</p></li>
</ul>
</div>
<div class="section" id="cas-de-variables-quantitatives">
<h5>Cas de variables quantitatives<a class="headerlink" href="#cas-de-variables-quantitatives" title="Permalink to this heading">#</a></h5>
<p>Dans le cas de variables quantitatives, les normes  <span class="math notranslate nohighlight">\(L_p\)</span> :</p>
<p><span class="math notranslate nohighlight">\(\|{\bf x_i}\|_p=\left (\displaystyle\sum_{j=1}^d|x_i^j|^p\right ) ^\frac{1}{p}\)</span></p>
<p>sont classiquement utilis√©es, et par exemple</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p=1\)</span> : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_1=\displaystyle\sum_{k=1}^d|x_i^k-x_j^k|\)</span> est la norme <span class="math notranslate nohighlight">\(L_1\)</span> (ou city block).</p></li>
<li><p><span class="math notranslate nohighlight">\(p=2\)</span> : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_2=\sqrt{\displaystyle\sum_{k=1}^d(x_i^k-x_j^k)^2}\)</span> est la norme <span class="math notranslate nohighlight">\(L_2\)</span> (ou norme euclidienne).</p></li>
<li><p>‚Äú<span class="math notranslate nohighlight">\(p=\infty\)</span>‚Äù : <span class="math notranslate nohighlight">\(\|{\bf x_i}-{\bf x_j}\|_\infty = \displaystyle\max_{1\leq k\leq d}\{|x_i^k-x_j^k|\}\)</span> est la norme du max (ou norme de Tchebychev)</p></li>
</ul>
<p>Si les variables ne sont pas normalis√©es, on peut utiliser la distance de Mahalanobis</p>
<p><span class="math notranslate nohighlight">\(\delta_{ij} = \displaystyle\sum_{k=1}^d\displaystyle\sum_{l=1}^dw_{kl}(x_i^k-x_j^k)(x_i^l-x_j^l)\)</span></p>
<p>o√π la matrice des <span class="math notranslate nohighlight">\(w_{kl}\)</span> est l‚Äôinverse de la matrice de covariance empirique. Cette distance √©limine √©galement les corr√©lations entre variables.</p>
<p>Enfin, on peut utiliser une m√©trique issue du coefficient de corr√©lation, dite distance de Pearson : <span class="math notranslate nohighlight">\(\delta_{ij} =\sqrt{1-r^2_{ij}}\)</span>, avec</p>
<p><span class="math notranslate nohighlight">\(r^2_{ij} = \frac{\left (\displaystyle\sum_{k=1}^d (x_i^k-\bar{x_i})(x_j^k-\bar{x_j})\right )^2}{\displaystyle\sum_{k=1}^d(x_i^k-\bar{x_i})^2\displaystyle\sum_{k=1}^d(x_j^k-\bar{x_j})^2}\)</span></p>
</div>
<div class="section" id="variables-de-comptage">
<h5>Variables de comptage<a class="headerlink" href="#variables-de-comptage" title="Permalink to this heading">#</a></h5>
<p>Dans le cas particulier de variables de comptage (<span class="math notranslate nohighlight">\(x_i^k\)</span> effectif de la classe <span class="math notranslate nohighlight">\(k\)</span> pour l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span>), une mesure naturelle de dissimilarit√© entre <span class="math notranslate nohighlight">\({\bf x_i}\)</span> et <span class="math notranslate nohighlight">\({\bf x_j}\)</span> est le <span class="math notranslate nohighlight">\(\chi^2\)</span> du tableau de contingence 2<span class="math notranslate nohighlight">\(\times d\)</span> associ√©.</p>
</div>
<div class="section" id="quelle-mesure-choisir">
<h5>Quelle mesure choisir ?<a class="headerlink" href="#quelle-mesure-choisir" title="Permalink to this heading">#</a></h5>
<p>Une r√©flexion  sur le type de dissimilarit√© √† choisir est n√©cessaire. Il est en particulier int√©ressant de r√©pondre aux questions suivantes:</p>
<ul class="simple">
<li><p>de quelles variables initiales (qualitatives et/ou quantitatives) doit d√©pendre la dissimilarit√©?</p></li>
<li><p>est-il souhaitable (et possible) d‚Äôobtenir des variables pertinentes suppl√©mentaires? Si oui par mesure ? par analyse lin√©aire (ACP,‚Ä¶) ou non lin√©aire (manifold learning) ?</p></li>
<li><p>quelles doivent √™tre les importances relatives des diverses variables retenues dans la constitution de la dissimilarit√© ?</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="classification-ascendante-hierarchique">
<h3>Classification ascendante hi√©rarchique<a class="headerlink" href="#classification-ascendante-hierarchique" title="Permalink to this heading">#</a></h3>
<p>L‚Äôobjectif est de construire une hi√©rarchie indic√©e d‚Äôun ensemble <span class="math notranslate nohighlight">\(\Omega\)</span> sur lequel on conna√Æt une mesure de dissimilarit√© <span class="math notranslate nohighlight">\(\delta\)</span> telle que les points les plus proches soient regroup√©s dans les classes de plus petit indice. La hi√©rarchie est alors construite en appliquant it√©rativement ce principe, et l‚Äôarbre obtenu sur l‚Äôensemble des it√©rations est appel√© un dendrogramme.</p>
<p>Il existe essentiellement
deux approches :</p>
<ul class="simple">
<li><p>la classification descendante : on divise <span class="math notranslate nohighlight">\(\Omega\)</span> en classes, puis on recommence sur chacune de ces classes it√©rativement jusqu‚Äô√† ce que les classes soient r√©duites √† des singletons.</p></li>
<li><p>la classification ascendante : cette fois on part de la partition de <span class="math notranslate nohighlight">\(\Omega\)</span>  o√π chaque classe est un singleton. On proc√®de alors par fusions successives des classes jusqu‚Äô√† obtenir une seule classe, c‚Äôest-√† -dire l‚Äôensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> lui-m√™me. Nous insistons sur ce type de classification dans la suite.</p></li>
</ul>
<div class="section" id="algorithme">
<h4>Algorithme<a class="headerlink" href="#algorithme" title="Permalink to this heading">#</a></h4>
<div class="section" id="construction-de-la-hierarchie">
<span id="index-2"></span><h5>Construction de la hi√©rarchie<a class="headerlink" href="#construction-de-la-hierarchie" title="Permalink to this heading">#</a></h5>
<p><span class="math notranslate nohighlight">\(\Omega\)</span>  √©tant l‚Äôensemble √† classifier et <span class="math notranslate nohighlight">\(\delta\)</span> une mesure de dissimilarit√© sur cet ensemble, on d√©finit √† partir de <span class="math notranslate nohighlight">\(\delta\)</span> une  distance <span class="math notranslate nohighlight">\(D\)</span> entre les parties de  <span class="math notranslate nohighlight">\(\Omega\)</span>. Cette distance est en r√©alit√© une mesure de dissimilarit√© qui ne v√©rifie pas n√©cessairement toutes les propri√©t√©s d‚Äôune distance sur l‚Äôensemble des parties de <span class="math notranslate nohighlight">\(\Omega\)</span>. En g√©n√©ral, <span class="math notranslate nohighlight">\(D\)</span> est appel√© crit√®re d‚Äôagr√©gation.
L‚Äôalgorithme est alors le suivant :</p>
<div class="proof algorithm admonition" id="algorithm-3">
<p class="admonition-title"><span class="caption-number">Algorithm 4 </span> (Algorithme de clustering hi√©rarchique ascendant)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> Les √©l√©ments de <span class="math notranslate nohighlight">\(\Omega\)</span></p>
<p><strong>Sortie :</strong> Une hi√©rarchie</p>
<ol class="arabic simple">
<li><p>Initialisation : partition des singletons</p></li>
<li><p>Calcul des distances entre classes.</p></li>
<li><p>Tant que le nombre de classes est <span class="math notranslate nohighlight">\(&gt;\)</span>1</p>
<ol class="arabic simple">
<li><p>Regroupement des 2 classes les plus proches au sens de <span class="math notranslate nohighlight">\(D\)</span></p></li>
<li><p>Calcul des distances entre la nouvelle classe et les anciennes classes non regroup√©es.</p></li>
</ol>
</li>
</ol>
</div>
</div><p>Il est facile de montrer que l‚Äôensemble des classes d√©finies au cours de cet algorithme forme une hi√©rarchie.</p>
</div>
<div class="section" id="construction-de-l-indice">
<h5>Construction de l‚Äôindice<a class="headerlink" href="#construction-de-l-indice" title="Permalink to this heading">#</a></h5>
<p>Apr√®s avoir d√©fini une hi√©rarchie, il est n√©cessaire de lui associer un indice. Pour les classes du bas de la hi√©rarchie, c‚Äôest-√†-dire les singletons, cet indice est n√©cessairement la valeur 0. Pour les autres classes, cet indice est g√©n√©ralement
d√©fini en associant √† chacune des classes construites au cours de l‚Äôalgorithme la distance <span class="math notranslate nohighlight">\(D\)</span> qui s√©parait les deux classes fusionn√©es pour former cette nouvelle classe. Pour que cette d√©finition conduise bien √† un indice, il est n√©cessaire que
les indices obtenus soient strictement croissants avec le niveau de la hi√©rarchie. Plusieurs difficult√©s peuvent alors appara√Ætre :</p>
<ul class="simple">
<li><p>pour certains crit√®res d‚Äôagr√©gation, l‚Äôindice ainsi d√©fini n‚Äôest pas n√©cessairement croissant. On parle alors d‚Äôinversion. Par exemple, si les donn√©es sont form√©es par trois points du plan situ√©s au sommet d‚Äôun triangle √©quilat√©ral de c√¥t√© 1 et si on prend comme distance <span class="math notranslate nohighlight">\(D\)</span> entre classes la distance entre les centres de gravit√©, on obtient une inversion.</p></li>
<li><p>lorsqu‚Äôil y a √©galit√© de l‚Äôindice pour plusieurs niveaux embo√Æt√©s, il suffit de filtrer la hi√©rarchie, c‚Äôest-√†-dire conserver une seule classe qui regroupe toutes les classes embo√Æt√©es ayant le m√™me indice.</p></li>
</ul>
</div>
</div>
<div class="section" id="criteres-d-agregation">
<h4>Crit√®res d‚Äôagr√©gation<a class="headerlink" href="#criteres-d-agregation" title="Permalink to this heading">#</a></h4>
<p>Il existe de nombreux crit√®res d‚Äôagr√©gation, mais les plus utilis√©s sont les suivants :</p>
<ul class="simple">
<li><p>crit√®re du lien commun : <span class="math notranslate nohighlight">\(D_{min}(A,B)=\displaystyle\min_{i\in A,j\in B}\delta_{ij}\)</span></p></li>
<li><p>crit√®re du lien maximum: <span class="math notranslate nohighlight">\(D_{max}(A,B)=\displaystyle\max_{i\in A,j\in B}\delta_{ij}\)</span></p></li>
<li><p>crit√®re du lien moyen : <span class="math notranslate nohighlight">\(D_{moy}(A,B)=\frac{\displaystyle\sum_{i\in A}\displaystyle\sum_{j\in B}\delta_{ij}}{|A||B|}\)</span></p></li>
</ul>
<p><img alt="" src="_images/agreg.png" /></p>
</div>
<div class="section" id="formule-de-recurrence-de-lance-et-williams">
<h4>Formule de r√©currence de Lance et Williams<a class="headerlink" href="#formule-de-recurrence-de-lance-et-williams" title="Permalink to this heading">#</a></h4>
<p>Pour les trois crit√®res d‚Äôagr√©gation pr√©c√©dents, il existe des relations de simplification du calcul des distances entre classes essentielles pour la mise en place pratique de l‚Äôalgorithme de classification ascendante :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D_{min}(A,B\cup C)=min(D_{min}(A,B),D_{min}(A,C))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{max}(A,B\cup C)=max(D_{max}(A,B),D_{min}(A,C))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(D_{moy}(A,B\cup C)=\frac{|B|D_{moy}(A,B)+|C|D_{moy}(A,C)}{|B|+|C|}\)</span></p></li>
</ul>
</div>
<div class="section" id="critere-de-ward">
<h4>Crit√®re de Ward<a class="headerlink" href="#critere-de-ward" title="Permalink to this heading">#</a></h4>
<p id="index-3">Lorsque l‚Äôensemble  <span class="math notranslate nohighlight">\(\Omega\)</span> √† classifier est mesur√© par <span class="math notranslate nohighlight">\(d\)</span> variables quantitatives, il est possible de lui associer un nuage de points pond√©r√©s dans <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> muni de la distance euclidienne. G√©n√©ralement, les pond√©rations seront toutes √©gales √† 1. Le crit√®re d‚Äôagr√©gation le plus utilis√© dans cette situation est alors le crit√®re d‚Äôinertie de Ward :</p>
<p><span class="math notranslate nohighlight">\(D(A,B)=\frac{p_Ap_B}{p_A+p_B}\|({\bf g}(A),{\bf g}(B))\|_2^2\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\(p_E\)</span> repr√©sente la somme des pond√©rations des √©l√©ments d‚Äôune classe <span class="math notranslate nohighlight">\(E\)</span> et <span class="math notranslate nohighlight">\({\bf g}(E)\)</span> est le centre de gravit√© d‚Äôune classe <span class="math notranslate nohighlight">\(E\)</span>.</p>
</div>
<div class="section" id="proprietes-d-optimalite">
<h4>Propri√©t√©s d‚Äôoptimalit√©<a class="headerlink" href="#proprietes-d-optimalite" title="Permalink to this heading">#</a></h4>
<p>La notion de hi√©rarchie indic√©e est √©quivalente √† la notion d‚Äôultram√©trique. La classification hi√©rarchique ascendante transforme donc la mesure de dissimilarit√© <span class="math notranslate nohighlight">\(d\)</span> initiale en une mesure de dissimilarit√© <span class="math notranslate nohighlight">\(\delta\)</span> qui poss√®de la propri√©t√© d‚Äô√™tre une ultram√©trique.</p>
<p>Le probl√®me de la classification hi√©rarchique peut donc √©galement se poser en ces termes : trouver l‚Äôultram√©trique <span class="math notranslate nohighlight">\(\delta^*\)</span> la plus proche de <span class="math notranslate nohighlight">\(\delta\)</span>. Il reste √† munir l‚Äôespace des mesures de dissimilarit√© sur  <span class="math notranslate nohighlight">\(\Omega\)</span> d‚Äôune distance. On pourra utiliser, par exemple :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}(\delta_{ij}-\delta^*_{ij})^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\Delta(\delta,\delta^**)=\displaystyle\sum_{i,j\in \Omega}|\delta_{ij}-\delta^*_{ij}|\)</span></p></li>
</ul>
</div>
<div class="section" id="critere-d-arret-et-partition">
<h4>Crit√®re d‚Äôarr√™t et partition<a class="headerlink" href="#critere-d-arret-et-partition" title="Permalink to this heading">#</a></h4>
<p id="index-4">L‚Äôensemble des it√©rations peut √™tre visualis√© sous la forme d‚Äôun arbre, appel√© dendrogramme. La figure suivante pr√©sente un exemple de dendrogramme en clustering hi√©rarchique descendant sur <span class="math notranslate nohighlight">\(X = \{a, b, c, d, e\}\)</span>. La distance <span class="math notranslate nohighlight">\(D\)</span> n‚Äôest pas report√©e</p>
<p><img alt="" src="_images/dendro1.png" /></p>
<p>Le crit√®re d‚Äôarr√™t permet de d√©terminer la partition  de <span class="math notranslate nohighlight">\(X\)</span> la plus appropri√©e. Ici encore, plusieurs choix sont possibles :</p>
<ul class="simple">
<li><p>en fixant a priori un nombre de classes</p></li>
<li><p>en fixant une borne sup√©rieure <span class="math notranslate nohighlight">\(r\)</span> pour <span class="math notranslate nohighlight">\(D\)</span>, et en stoppant les it√©rations d√®s que les distances calcul√©es par les liens d√©passent <span class="math notranslate nohighlight">\(r\)</span>. A noter que <span class="math notranslate nohighlight">\(r\)</span> peut √™tre √©galement calcul√© par <span class="math notranslate nohighlight">\(r=\alpha max\{\delta(x,y),x,y\in X\}\)</span> (crit√®re dit ‚Äúscale distance upper bound‚Äù).</p></li>
<li><p>en coupant le dendrogramme au saut de distance <span class="math notranslate nohighlight">\(D\)</span> maximal.</p></li>
</ul>
<p><img alt="" src="_images/dendro2.png" /></p>
</div>
<div class="section" id="utilisation-des-methodes">
<h4>Utilisation des m√©thodes<a class="headerlink" href="#utilisation-des-methodes" title="Permalink to this heading">#</a></h4>
<p>La premi√®re difficult√© est le choix de la mesure de dissimilarit√© sur  <span class="math notranslate nohighlight">\(\Omega\)</span> et du crit√®re d‚Äôagr√©gation. G√©n√©ralement, lorsque l‚Äôon dispose de variables quantitatives, le crit√®re conseill√© est le crit√®re d‚Äôinertie. Ensuite, il est souvent n√©cessaire de disposer d‚Äôoutils d‚Äôaide √† l‚Äôinterpr√©tation et d‚Äôoutils permettant de diminuer le nombre de niveaux de hi√©rarchie. Il est d‚Äôautre part conseill√© d‚Äôutiliser conjointement d‚Äôautres m√©thodes d‚Äôanalyse des donn√©es comme l‚ÄôAnalyse en Composantes Principales.</p>
</div>
<div class="section" id="exemple">
<h4>Exemple<a class="headerlink" href="#exemple" title="Permalink to this heading">#</a></h4>
<p>On √©tudie ici un jeu de donn√©es correspondant aux achats dans un supermarch√©. On cherche √† caract√©riser les comportements des acheteurs en fonction de leurs revenus</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./data/Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>Genre</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>On affiche les donn√©es</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Score/Revenu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;Revenu annuel (k$)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution des √¢ges et des scores d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score d&#39;achat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/40af125764252cecf7df0609a0d09aea389d627bca442ec7639bad8eb392434d.png" src="_images/40af125764252cecf7df0609a0d09aea389d627bca442ec7639bad8eb392434d.png" />
</div>
</div>
<p>L‚Äôobjectif est de trouver des cat√©gories de population ayant les m√™mes comportements d‚Äôachat. Le nombre de classes √©tant inconnu, la classification h√©ararchique va permettre de donner des indications sur le nombre de groupes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="k">as</span> <span class="nn">sch</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Dendrogramme&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Clients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Indice&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">190</span><span class="p">,</span><span class="n">xmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">xmax</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">linestyles</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">220</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="s1">&#39;Cut&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">dendrogram</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">dendrogram</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">linkage</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;ward&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/78d4a9a45386b64f29780bb351aae8389a5aa0d02e0f3b4e0007c6466783e532.png" src="_images/78d4a9a45386b64f29780bb351aae8389a5aa0d02e0f3b4e0007c6466783e532.png" />
</div>
</div>
<p>On projette ensuite le r√©sultat de la classification</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">AgglomerativeClustering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">linkage</span> <span class="o">=</span> <span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="n">y_model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Radins&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Prudents&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Riches&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;D√©pensiers modestes&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_model</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;magenta&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Conscients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Classification&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="p">(</span><span class="s2">&quot;revenu annuel (k$)&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span> <span class="p">(</span><span class="s2">&quot;Score (1-100)&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/85f8f2ed254f16dc77c66f71965aac224a56a48f2667c2a76e212b3e1629d6ea.png" src="_images/85f8f2ed254f16dc77c66f71965aac224a56a48f2667c2a76e212b3e1629d6ea.png" />
</div>
</div>
</div>
</div>
<div class="section" id="recherche-de-partitions">
<h3>Recherche de partitions<a class="headerlink" href="#recherche-de-partitions" title="Permalink to this heading">#</a></h3>
<div class="section" id="methode-des-centres-mobiles">
<h4>M√©thode des centres mobiles<a class="headerlink" href="#methode-des-centres-mobiles" title="Permalink to this heading">#</a></h4>
<span class="target" id="index-5"></span><p id="index-6">La m√©thode des centres mobiles est encore connue sous le nom de m√©thode de r√©allocation-centrage ou des k-means lorsque l‚Äôensemble √† classifier est mesur√© par <span class="math notranslate nohighlight">\(d\)</span> variables. Ici, <span class="math notranslate nohighlight">\(\Omega \in \mathbb{R}^d\)</span> est muni de sa distance euclidienne <span class="math notranslate nohighlight">\(\delta\)</span>. Pour simplifier la pr√©sentation, les pond√©rations des individus seront toutes √©gales √† 1, mais la g√©n√©ralisation √† des pond√©rations quelconques ne pose aucun probl√®me.</p>
<div class="section" id="id2">
<h5>Algorithme<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h5>
<p>L‚Äôalgorithme des centres-mobiles peut se d√©finir ainsi :</p>
<div class="proof algorithm admonition" id="algorithm-4">
<p class="admonition-title"><span class="caption-number">Algorithm 5 </span> (Algorithme des centres mobiles)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(\Omega\)</span>,<span class="math notranslate nohighlight">\(g\)</span>, m√©trique</p>
<p><strong>Sortie :</strong> Une partition de <span class="math notranslate nohighlight">\(\Omega\)</span></p>
<ol class="arabic simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points de  <span class="math notranslate nohighlight">\(\Omega\)</span> (centres initiaux des <span class="math notranslate nohighlight">\(g\)</span> classes)</p></li>
<li><p>Tant que (non convergence)</p>
<ol class="arabic simple">
<li><p>√âtape E : Construction de la partition en affectant chaque point de <span class="math notranslate nohighlight">\(\Omega\)</span> √† la classe dont il est le plus pr√®s du centre (en cas d‚Äô√©galit√©, l‚Äôaffectation se fait √† la classe de plus petit indice).</p></li>
<li><p>√âtape M : Les centres de gravit√© de la partition qui vient d‚Äô√™tre calcul√©e deviennent les nouveaux centres</p></li>
</ol>
</li>
</ol>
</div>
</div><p><img alt="" src="_images/kmeans1.png" /></p>
<p>L‚Äôinitialisation des centres de classe √©tant al√©atoire, il convient de r√©pliquer l‚Äôalgorithme plusieurs fois et de, par exemple, retenir la partition majoritaire. La figure suivante pr√©sente deux r√©sultats des k-means, sur un m√™me jeu de donn√©es (5 classes, 50 points par classes), avec une initialisation al√©atoire diff√©rente.</p>
<p><img alt="" src="_images/kmeans2.png" /></p>
</div>
<div class="section" id="critere-et-convergence">
<h5>Crit√®re et convergence<a class="headerlink" href="#critere-et-convergence" title="Permalink to this heading">#</a></h5>
<p>La qualit√© d‚Äôun couple partition-centres est mesur√©e par la somme des inerties des classes par rapport √† leur centre. On peut montrer qu‚Äô√† chacune des deux √©tapes de l‚Äôalgorithme, on am√©liore ce crit√®re.</p>
</div>
<div class="section" id="lien-avec-la-methode-de-ward">
<h5>Lien avec la m√©thode de Ward<a class="headerlink" href="#lien-avec-la-methode-de-ward" title="Permalink to this heading">#</a></h5>
<p>La m√©thode des centres mobiles et la m√©thode de Ward optimisent toutes deux, √† leur fa√ßon, le crit√®re d‚Äôinertie intra-classe. Cette situation conduit √† proposer des strat√©gies utilisant les deux approches comme, par exemple :</p>
<ul class="simple">
<li><p>appliquer les centres-mobiles pour regrouper l‚Äôensemble initial en un nombre ‚Äúimportant‚Äù de classes</p></li>
<li><p>appliquer la m√©thode de Ward en partant de ces classes</p></li>
<li><p>rechercher quelques ‚Äúbons‚Äù niveaux de la hi√©rarchie</p></li>
<li><p>√©ventuellement, appliquer de nouveau la m√©thode des centres-mobiles sur les partitions obtenues pour am√©liorer encore leur crit√®re.</p></li>
</ul>
</div>
</div>
<div class="section" id="generalisation-les-nuees-dynamiques">
<h4>G√©n√©ralisation : les nu√©es dynamiques<a class="headerlink" href="#generalisation-les-nuees-dynamiques" title="Permalink to this heading">#</a></h4>
<p id="index-7">L‚Äôid√©e de base consiste √† remplacer les centres   qui √©taient des √©l√©ments de <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> jouant le r√¥le de repr√©sentant ou encore de noyau de la classe par des √©l√©ments de nature tr√®s diverse adapt√©s au probl√®me que l‚Äôon cherche √† r√©soudre.</p>
<div class="section" id="formalisation">
<h5>Formalisation<a class="headerlink" href="#formalisation" title="Permalink to this heading">#</a></h5>
<p>On note <span class="math notranslate nohighlight">\(L=\{\lambda_i\}\)</span> l‚Äôensemble des noyaux, <span class="math notranslate nohighlight">\(D:\Omega\times L\rightarrow \mathbb{R}^+\)</span> une mesure de ressemblance entre √©l√©ments de <span class="math notranslate nohighlight">\(\Omega\)</span> et de <span class="math notranslate nohighlight">\(L\)</span>. L‚Äôobjectif est alors de trouver la partition en <span class="math notranslate nohighlight">\(g\)</span> classes (<span class="math notranslate nohighlight">\(g\)</span> fix√© a priori) de <span class="math notranslate nohighlight">\(\Omega\)</span> minimisant le crit√®re <span class="math notranslate nohighlight">\(\displaystyle\sum_{k}\displaystyle\sum_{x\in P_k}D(x,\lambda_k)\)</span></p>
<p>Cette minimisation est r√©alis√©e de fa√ßon altern√©e, comme pour les centres mobiles.</p>
</div>
<div class="section" id="choix-du-nombre-de-classes">
<h5>Choix du nombre de classes<a class="headerlink" href="#choix-du-nombre-de-classes" title="Permalink to this heading">#</a></h5>
<p>En g√©n√©ral, le crit√®re n‚Äôest pas ind√©pendant du nombre de classes. Par exemple, le crit√®re de l‚Äôinertie s‚Äôannule pour la partition triviale pour laquelle chaque point forme une classe. Il s‚Äôagit donc de la meilleure partition. Il est donc
n√©cessaire de fixer a priori le nombre de classes. Pour r√©soudre ce probl√®me tr√®s difficile, plusieurs solutions sont utilis√©es :</p>
<ul class="simple">
<li><p>on a une id√©e du nombre de classes d√©sir√©es</p></li>
<li><p>on recherche la meilleure partition pour plusieurs nombres de classes et on √©tudie la d√©croissance du crit√®re en fonction du nombre de classes (m√©thode du coude)</p></li>
<li><p>on d√©finit une fonction <span class="math notranslate nohighlight">\(f(\Omega)\)</span> qui rend le crit√®re ind√©pendant du nombre de classes</p></li>
<li><p>on ajoute des contraintes suppl√©mentaires (nombre d‚Äôindividus par classe, volume d‚Äôune classe‚Ä¶). C‚Äôest l‚Äôoption retenue par la m√©thode Isodata</p></li>
<li><p>on effectue des tests statistiques sur les classes</p></li>
</ul>
</div>
</div>
<div class="section" id="quelques-variantes">
<h4>Quelques variantes<a class="headerlink" href="#quelques-variantes" title="Permalink to this heading">#</a></h4>
<div class="section" id="k-means">
<h5>K-means++<a class="headerlink" href="#k-means" title="Permalink to this heading">#</a></h5>
<p>Plut√¥t que d‚Äôinitialiser les centres de mani√®re al√©atoire, l‚Äôalgorithme K-means++ propose de partitionner <span class="math notranslate nohighlight">\(\Omega=\{\mathbf x_1\cdots \mathbf x_n\}\)</span> selon l‚Äôalgorithme suivant :</p>
<ol class="arabic simple">
<li><p>Tirer uniform√©ment le premier centre de classe <span class="math notranslate nohighlight">\(c_1\)</span> dans <span class="math notranslate nohighlight">\(\Omega\)</span>\</p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(i\in[\![2,g]\!]\)</span>, choisir <span class="math notranslate nohighlight">\(\mathbf{c_i}\)</span> √† partir de <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> selon la probabilit√© <span class="math notranslate nohighlight">\(D(\mathbf{x}_i)^2\)</span> / <span class="math notranslate nohighlight">\(\displaystyle\sum\limits_{j=1}^{m}{D(\mathbf{x}_j)}^2\)</span> o√π  <span class="math notranslate nohighlight">\(D(\mathbf{x}_i)\)</span> est la distance entre <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> et le centre de classe le plus proche d√©j√† choisi. Ceci assure de tirer des centres de classe √©loign√©s avec forte probabilit√©.</p></li>
</ol>
</div>
<div class="section" id="acceleration-des-k-means">
<h5>Acc√©l√©ration des k-means<a class="headerlink" href="#acceleration-des-k-means" title="Permalink to this heading">#</a></h5>
<p>L‚Äôalgorithme original peut √™tre am√©lior√© de mani√®re significative en √©vitant les calculs de distances non n√©cessaires. En exploitant l‚Äôin√©galit√© triangulaire, et en conservant les bornes inf√©rieures et sup√©rieures des distances entre les points et les centres de classe, l‚Äôalgorithme correspondant est performant, y compris pour de grandes valeurs de <span class="math notranslate nohighlight">\(k\)</span> (<a class="reference internal" href="#km">Algorithm 6</a>)</p>
<div class="proof algorithm admonition" id="km">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span> (Acc√©l√©ration des k-means)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(\Omega, g\)</span></p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(P\)</span> une partition de <span class="math notranslate nohighlight">\(X\)</span> en <span class="math notranslate nohighlight">\(g\)</span> classes</p>
<ol class="arabic simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points <span class="math notranslate nohighlight">\(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)</span></p></li>
<li><p>Pour <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(l(\mathbf x,\mathbf c)=0\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega\)</span></p>
<ol class="arabic simple">
<li><p>Affecter <span class="math notranslate nohighlight">\(\mathbf x\)</span> √† la classe du centre le plus proche : <span class="math notranslate nohighlight">\(\mathbf c(x) = Arg \displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p>A chaque calcul de <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)\)</span>,<span class="math notranslate nohighlight">\( l(\mathbf x,\mathbf c)=\delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(u(\mathbf x,\mathbf c)=\displaystyle\min_{\mathbf c\in C} \delta(\mathbf x,\mathbf c)\)</span></p></li>
</ol>
</li>
<li><p>Tant que (non convergence)</p>
<ol class="arabic simple">
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c,\mathbf {c'}\in C\)</span> calculer <span class="math notranslate nohighlight">\(\delta (\mathbf c,\mathbf {c'})\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\)</span> <span class="math notranslate nohighlight">\(s(c)= \frac{1}{2}\displaystyle\min_{\mathbf {c'}\neq \mathbf c} \delta(\mathbf c,\mathbf {c'})\)</span></p></li>
<li><p>Identifier les <span class="math notranslate nohighlight">\(\mathbf x\)</span> tels que <span class="math notranslate nohighlight">\(u(\mathbf x)\leq s(\mathbf c(\mathbf x))\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span> tels que <span class="math notranslate nohighlight">\(\mathbf c\neq \mathbf c(\mathbf x)\)</span> et <span class="math notranslate nohighlight">\(u(\mathbf x)&gt;l(\mathbf x,\mathbf c)\)</span> et <span class="math notranslate nohighlight">\(u(\mathbf x)&gt;\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)</span></p>
<ol class="arabic simple">
<li><p>Si <span class="math notranslate nohighlight">\(r(\mathbf x)\)</span></p>
<ol class="arabic simple">
<li><p>Calculer <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r(\mathbf x)=Faux\)</span></p></li>
</ol>
</li>
<li><p>Sinon</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)=u(\mathbf x)\)</span></p></li>
</ol>
</li>
<li><p>Si <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)&gt;l(\mathbf x,\mathbf c)\)</span>  ou <span class="math notranslate nohighlight">\(\delta(\mathbf c(\mathbf x),\mathbf x)&gt;\frac{1}{2}\delta(\mathbf c(\mathbf x),\mathbf c)\)</span></p>
<ol class="arabic simple">
<li><p>Calculer <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)\)</span></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(\delta(\mathbf x,\mathbf c)&lt;\delta(\mathbf c(\mathbf x),\mathbf x)\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c(\mathbf x)= \mathbf c\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\in C\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf m(\mathbf c)\)</span> : centre de masse des points de <span class="math notranslate nohighlight">\(\Omega\)</span> plus proches de <span class="math notranslate nohighlight">\(\mathbf c\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega,\mathbf c\in C\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(l(\mathbf x,\mathbf c)=max\left (l(\mathbf x,\mathbf c)-\delta(\mathbf m(\mathbf c),\mathbf c),0 \right )\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \Omega\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(u(\mathbf x)=u(\mathbf x)+\delta(\mathbf m(\mathbf c(\mathbf x)),\mathbf c(\mathbf x))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r(\mathbf x)=Vrai\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf c\in C\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c = \mathbf m(\mathbf c)\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
<div class="section" id="k-means-a-mini-batchs">
<h5>k-means √† mini batchs<a class="headerlink" href="#k-means-a-mini-batchs" title="Permalink to this heading">#</a></h5>
<p>Il est √©galement possible d‚Äôappliquer une optimisation par mini-batchs dans l‚Äôalgorithme des k-means (<a class="reference internal" href="#kmbatch">Algorithm 7</a>).</p>
<div class="proof algorithm admonition" id="kmbatch">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (Acc√©l√©ration des k-means)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(\Omega, g\)</span>, <span class="math notranslate nohighlight">\(b\)</span> taille des batchs</p>
<p><strong>Sortie :</strong> <span class="math notranslate nohighlight">\(P\)</span> une partition de <span class="math notranslate nohighlight">\(X\)</span> en <span class="math notranslate nohighlight">\(g\)</span> classes</p>
<ol class="arabic simple">
<li><p>Initialisation : tirage au hasard de <span class="math notranslate nohighlight">\(g\)</span> points <span class="math notranslate nohighlight">\(C =\{\mathbf {c_1}\cdots \mathbf {c_g\}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf v=0\in\mathbb{R}^g\)</span></p></li>
<li><p>Tant que non convergence</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\leftarrow\)</span> batch de <span class="math notranslate nohighlight">\(b\)</span> exemples tir√©s de <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \mathcal{B}\)</span></p>
<ol class="arabic simple">
<li><p>Affecter <span class="math notranslate nohighlight">\(\mathbf x\)</span> √† la classe du centre le plus proche <span class="math notranslate nohighlight">\(\mathbf T(\mathbf x)\)</span></p></li>
</ol>
</li>
<li><p>Pour tout <span class="math notranslate nohighlight">\(\mathbf x\in \mathcal{B}\)</span></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf c = \mathbf T(\mathbf x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_c = v_c + 1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\eta = \frac{1}{v_c}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf c = (1-\eta)\mathbf c + \eta \mathbf x\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
</div>
<div class="section" id="id3">
<h4>Exemple<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<p>On g√©n√®re des donn√©es</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">nb_classes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
        <span class="p">[[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],[</span><span class="mi">1</span> <span class="p">,</span>  <span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">]])</span>
<span class="n">cluster_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>    

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">centers</span><span class="o">=</span><span class="n">center</span><span class="p">,</span><span class="n">cluster_std</span> <span class="o">=</span> <span class="n">cluster_std</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d2f13817276b12842d3ef7f2a6c79d8ba48f2b22ac3edf3ae55bb2e97113f22f.png" src="_images/d2f13817276b12842d3ef7f2a6c79d8ba48f2b22ac3edf3ae55bb2e97113f22f.png" />
</div>
</div>
<p>Puis on applique l‚Äôalgorithme des <span class="math notranslate nohighlight">\(k\)</span>-means.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">nb_classes</span><span class="p">,</span><span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Vraies classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;K means √† </span><span class="si">{0:d}</span><span class="s2"> classes&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb_classes</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/282a71541470bf599f673324fd8333f803863bae5ab287f44588409753ee6b0a.png" src="_images/282a71541470bf599f673324fd8333f803863bae5ab287f44588409753ee6b0a.png" />
</div>
</div>
</div>
</div>
<div class="section" id="modeles-de-melange">
<h3>Mod√®les de m√©lange<a class="headerlink" href="#modeles-de-melange" title="Permalink to this heading">#</a></h3>
<p>Les mod√®les de m√©lange supposent que les donn√©es proviennent d‚Äôun m√©lange de distributions (g√©n√©ralement gaussiennes), et l‚Äôobjectif est alors d‚Äôestimer les param√®tres du mod√®le de m√©lange en maximisant la fonction de vraisemblance pour les donn√©es.
L‚Äôoptimisation directe de la fonction de vraisemblance dans ce cas n‚Äôest pas une t√¢che simple, en raison des contraintes n√©cessaires sur les param√®tres et de la nature complexe de la fonction de vraisemblance, qui pr√©sente g√©n√©ralement un grand nombre de maxima locaux et de points de selle. Une m√©thode courante pour estimer les param√®tres du mod√®le de m√©lange est l‚Äôalgorithme EM.</p>
<div class="section" id="definition">
<h4>D√©finition<a class="headerlink" href="#definition" title="Permalink to this heading">#</a></h4>
<p>Soient <span class="math notranslate nohighlight">\(\mathcal S = \{\mathbf X_1\cdots X_n\}\)</span> <span class="math notranslate nohighlight">\(n\)</span> vecteurs al√©atoires i.i.d. √† valeur dans <span class="math notranslate nohighlight">\(\mathcal X\subset \mathbb{R}^d\)</span> , chaque <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> √©tant distribu√© selon</p>
<div class="math notranslate nohighlight">
\[g(\mathbf x|\boldsymbol \theta) = \displaystyle\sum_{i=1}^K w_i\Phi_i(\mathbf x)\]</div>
<p>o√π <span class="math notranslate nohighlight">\(\Phi_i,i\in[\![1,K]\!]\)</span> sont des densit√©s de probabilit√© sur <span class="math notranslate nohighlight">\(\mathcal X\)</span> et les <span class="math notranslate nohighlight">\(w_i\)</span> sont des poids positifs, sommant √† 1. <span class="math notranslate nohighlight">\(g\)</span> peut √™tre interpr√©t√©e comme suit : soit <span class="math notranslate nohighlight">\(Z\)</span> une variable al√©atoire discr√®te prenant les valeurs <span class="math notranslate nohighlight">\(i\in[\![1,K]\!]\)</span> avec probabilit√© <span class="math notranslate nohighlight">\(w_i\)</span>, et soit <span class="math notranslate nohighlight">\(\mathbf X\)</span> un vecteur al√©atoire dont la distribution conditionnelle, √©tant donn√©e <span class="math notranslate nohighlight">\(Z=z\)</span> est <span class="math notranslate nohighlight">\(\Phi_z\)</span>. Alors</p>
<div class="math notranslate nohighlight">
\[\Phi_{Z,\mathbf X}(z,\mathbf x) = \Phi_Z(z)\Phi_{\mathbf X|Z}(\mathbf x,z) = w_z(\mathbf x)\]</div>
<p>et la distribution marginale de <span class="math notranslate nohighlight">\(\mathbf X\)</span> est calcul√©e en sommant sur <span class="math notranslate nohighlight">\(z\)</span> les probabilit√©s jointes.</p>
<p>Un vecteur al√©atoire <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(g\)</span> peut donc √™tre simul√© d‚Äôabord en tirant <span class="math notranslate nohighlight">\(Z\)</span> suivant <span class="math notranslate nohighlight">\(P(Z=z)=w_z,z\in[\![1,K]\!]\)</span>, puis en tirant <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(\Phi_Z\)</span>. La famille <span class="math notranslate nohighlight">\(\mathcal S\)</span> ne contenant que les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span>, les <span class="math notranslate nohighlight">\(Z_i\)</span> sont des variables latentes, interpr√©t√©es comme les √©tiquettes cach√©es des classes auxquelles les <span class="math notranslate nohighlight">\(\mathbf X_i\)</span> appartiennent.</p>
<p>Typiquement, les <span class="math notranslate nohighlight">\(\Phi_k\)</span> sont des lois param√©triques. Classiquement ce sont des lois gaussiennes <span class="math notranslate nohighlight">\(\mathcal N(\boldsymbol \mu_k,\boldsymbol \Sigma_k)\)</span> et donc en rassemblant tous les param√®tres des lois, incluant les <span class="math notranslate nohighlight">\(w_k\)</span>, dans un vecteur de param√®tre <span class="math notranslate nohighlight">\(\boldsymbol \theta = (\mu_k,\boldsymbol \Sigma_k,w_k,k\in[\!1,K]\!])\)</span>, on peut √©crire</p>
<div class="math notranslate nohighlight">
\[g(s|\boldsymbol \theta) = \prod_{i=1}^n g(\mathbf x_i|\boldsymbol \theta) = \prod_{i=1}^n \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k)\]</div>
<p>o√π <span class="math notranslate nohighlight">\(s=(\mathbf x_1\cdots \mathbf x_n)\)</span> d√©note une r√©alisation de <span class="math notranslate nohighlight">\(\mathcal S\)</span>.</p>
<p>On estime alors <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> en maximisant la log vraisemblance</p>
<div class="math notranslate nohighlight">
\[\ell(\boldsymbol\theta|s) = \displaystyle\sum_{i=1}^n ln(g(\mathbf x_i|\boldsymbol \theta)) = \displaystyle\sum_{i=1}^n ln \left ( \displaystyle\sum_{k=1}^K w_k \Phi_k(\mathbf x_i|\boldsymbol\mu_k \boldsymbol\Sigma_k) \right )\]</div>
<p>ce qui est en g√©n√©ral complexe, la fonction <span class="math notranslate nohighlight">\(\ell\)</span> admettant de nombreux extrema locaux.</p>
</div>
<div class="section" id="algorithme-em">
<h4>Algorithme EM<a class="headerlink" href="#algorithme-em" title="Permalink to this heading">#</a></h4>
<p>Plut√¥t que d‚Äôoptimiser <span class="math notranslate nohighlight">\(\ell\)</span> directement depuis les donn√©es <span class="math notranslate nohighlight">\(s\)</span>, l‚Äôalgorithme EM (<a class="reference internal" href="#EM">Algorithm 8</a>) augmente d‚Äôabord les donn√©es des variables latentes (les √©tiquettes <span class="math notranslate nohighlight">\(\mathbf z=(z_1\cdots z_n)\)</span> des classes). L‚Äôid√©e est que <span class="math notranslate nohighlight">\(s\)</span> est uniquement la partie observ√©e des donn√©es al√©atoires <span class="math notranslate nohighlight">\((\mathcal S,\mathbf Z)\)</span> g√©n√©r√©es d‚Äôabord en tirant <span class="math notranslate nohighlight">\(Z\)</span> suivant <span class="math notranslate nohighlight">\(P(Z=z)\)</span>, puis en tirant <span class="math notranslate nohighlight">\(\mathbf X\)</span> suivant <span class="math notranslate nohighlight">\(\Phi_z\)</span>, de sorte √† avoir</p>
<div class="math notranslate nohighlight">
\[g(s,z|\boldsymbol \theta) = \displaystyle\prod_{i=1}^n w_{z_i} \Phi_{z_i}(\mathbf x_i)\]</div>
<p>Ainsi, la log vraisemblance des donn√©es compl√®tes, en g√©n√©ral plus facile √† optimiser, est</p>
<div class="math notranslate nohighlight">
\[\bar\ell(\boldsymbol\theta|s,z) =\displaystyle\sum{i=1}^n ln(w_{z_i} \Phi_{z_i}(\mathbf x_i))\]</div>
<p>Cependant, les <span class="math notranslate nohighlight">\(z\)</span> ne sont pas observ√©es et <span class="math notranslate nohighlight">\(\bar\ell\)</span> ne peut √™tre √©valu√©e. Dans l‚Äô√©tape E de l‚Äôalgorithme EM, <span class="math notranslate nohighlight">\(\bar\ell\)</span> est remplac√©e par <span class="math notranslate nohighlight">\(\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\)</span>, o√π l‚Äôindice <span class="math notranslate nohighlight">\(p\)</span> indique que <span class="math notranslate nohighlight">\(\mathbf Z\)</span> est distribu√©e selon la distribution conditionnelle de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> √©tant donn√©e <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, soit</p>
<div class="math notranslate nohighlight">
\[p(z)=g(z|s,\boldsymbol \theta) \propto g(s,z|\boldsymbol \theta)\]</div>
<div class="proof remark dropdown admonition" id="remark-7">
<p class="admonition-title"><span class="caption-number">Remark 22 </span></p>
<div class="remark-content section" id="proof-content">
<p><span class="math notranslate nohighlight">\(p(z)\)</span> est de la forme <span class="math notranslate nohighlight">\(p_1(z_1)\cdots p_n(z_n)\)</span> de telle sorte que, √©tant donn√© <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, les composantes de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> sont deux √† deux ind√©pendantes.</p>
</div>
</div><div class="proof algorithm admonition" id="EM">
<p class="admonition-title"><span class="caption-number">Algorithm 8 </span> (Algorithmes EM)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Entr√©e :</strong> <span class="math notranslate nohighlight">\(s,\boldsymbol\theta^{(0)}\)</span></p>
<p><strong>Sortie :</strong> Approximation de la log vraisemblance maximale</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(i=1\)</span></p></li>
<li><p>Tant que (not stop)</p>
<ol class="arabic simple">
<li><p>Etape E : Trouver <span class="math notranslate nohighlight">\(p^{(i)}(z) = g(s|s,\boldsymbol\theta^{(i-1)})\)</span> et <span class="math notranslate nohighlight">\(Q^{(i)}(\boldsymbol\theta)=\mathbb{E}_p \bar\ell(\boldsymbol \theta |s,\mathbf Z)\)</span></p></li>
<li><p>Etape M : <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i)} = Arg \displaystyle\max_{\boldsymbol\theta} Q^{(i)}(\boldsymbol\theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i = i+1\)</span></p></li>
</ol>
</li>
<li><p>Retourner <span class="math notranslate nohighlight">\(\boldsymbol\theta{(i)}\)</span></p></li>
</ol>
</div>
</div><p>Dans l‚Äô<a class="reference internal" href="#EM">Algorithm 8</a>, un crit√®re d‚Äôarr√™t est par exemple</p>
<div class="math notranslate nohighlight">
\[\frac{\ell(\boldsymbol\theta{(i)}|s)-\ell(\boldsymbol\theta^{(i-1)}|s)}{\ell(\boldsymbol\theta{(i)}|s)}&lt;\epsilon\]</div>
<p>Sous certaines conditions, la suite des <span class="math notranslate nohighlight">\(\ell(\boldsymbol\theta{(i)}|s)\)</span> converge vers un maximum local de la log vraisemblance <span class="math notranslate nohighlight">\(\ell\)</span>. La convergence vers le maximum global d√©pend bien s√ªr du choix de <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(0)}\)</span>, de sorte qu‚Äôune strat√©gie possible est d‚Äôex√©cuter plusieurs fois l‚Äôalgorithme avec des initialisations diff√©rentes.</p>
<p>Dans le cas d‚Äôun m√©lange gaussien, <span class="math notranslate nohighlight">\(\Phi_k=\mathcal N(\boldsymbol\mu_k,\boldsymbol\Sigma_k),k\in[\![1,K]\!]\)</span>. Si <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i-1)}\)</span> est le vecteur optimal √† l‚Äôit√©ration courante, constitu√© des poids <span class="math notranslate nohighlight">\(w_k^{(i-1)}\)</span>, des vecteurs moyenne <span class="math notranslate nohighlight">\((\boldsymbol\mu_k)^{(i-1)}\)</span> et des matrices de covariances <span class="math notranslate nohighlight">\((\boldsymbol\Sigma_k)^{(i-1)}\)</span>, alors on d√©termine <span class="math notranslate nohighlight">\(p^{(i)}\)</span>, la distribution de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> conditionnelement √† <span class="math notranslate nohighlight">\(\mathcal S=s\)</span>, pour le param√®tre <span class="math notranslate nohighlight">\(\boldsymbol\theta^{(i-1)}\)</span>. Puisque les composantes de <span class="math notranslate nohighlight">\(\mathbf Z\)</span> √©tant donn√© <span class="math notranslate nohighlight">\(\mathcal S=s\)</span> sont ind√©pendantes, il suffit de sp√©cifier la distribution discr√®te <span class="math notranslate nohighlight">\(p_j^{(i)}\)</span> de chaque <span class="math notranslate nohighlight">\(Z_j\)</span>, √©tant donn√©es l‚Äôobservation <span class="math notranslate nohighlight">\(\mathbf X_j=\mathbf x_j\)</span>, calcul√©e √† l‚Äôaide de la forule de Bayes</p>
<div class="math notranslate nohighlight">
\[p_j^{(i)}(k)\propto w_k^{(i-1)}\Phi_k(\mathbf x_j|\boldsymbol\mu_k^{(i-1)},\boldsymbol\Sigma_k^{(i-1)}),k\in[\![1,K]\!]\]</div>
<p>Alors</p>
<ol class="arabic simple">
<li><p>Pour l‚Äô√©tape E</p></li>
</ol>
<div class="math notranslate nohighlight">
\[Q^{(i)}(\boldsymbol\theta) = \mathbb{E}_p \displaystyle\sum_{j=1}^n \left (ln w_{z_j} + ln \Phi_{z_j}(\mathbf x_j|\boldsymbol\mu_{Z_j},\boldsymbol\Sigma_{Z_j}) \right )\]</div>
<p>o√π les <span class="math notranslate nohighlight">\(Z_j\)</span> sont ind√©pendants et distribu√©s selon <span class="math notranslate nohighlight">\(p_j^{(i)}\)</span>.</p>
<ol class="arabic simple" start="2">
<li><p>Pour l‚Äô√©tape M, on maximise</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ \displaystyle\sum_{j=1}^n\displaystyle\sum_{k=1}^K p_j^{(i)}(k)\left (ln w_k + ln \Phi_k(\mathbf x_j|\boldsymbol\mu_{k},\boldsymbol\Sigma_{k})\right )\]</div>
<p>sous la contrainte <span class="math notranslate nohighlight">\(\displaystyle\sum_{k=1}^K w_k=1\)</span>. En utilisant une relxation lagrangienne, et le fait que <span class="math notranslate nohighlight">\(\displaystyle\sum_{k=1}^K p_j^{(i)}(k)=1\)</span> on trouve pour tout <span class="math notranslate nohighlight">\(k\in[\![1,K]\!]\)</span></p>
<div class="math notranslate nohighlight">
\[w_k = \frac1n\displaystyle\sum_{j=1}^n p_j^{(i)}(k)\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol\mu_k = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) \mathbf x_j}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol\Sigma_{k} = \frac{\displaystyle\sum_{j=1}^n p_j^{(i)}(k) (\mathbf x_j-\boldsymbol\mu_k)(\mathbf x_j-\boldsymbol\mu_k)^T}{\displaystyle\sum_{j=1}^n p_j^{(i)}(k)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s1">&#39;./data/mixture.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

<span class="c1"># Param√®tres initiaux</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">]])</span> <span class="c1"># poids</span>
<span class="n">M</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span> <span class="c1"># Moyennes</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># Co</span>

<span class="n">C</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">C</span><span class="p">[:,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">300</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">):</span> 

    <span class="c1"># Etape E</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>  
        <span class="n">mvn</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span> <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:]</span> <span class="p">)</span>
        <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">mvn</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Etape M</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>   
    <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">K</span><span class="p">):</span>
        <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span>
        <span class="n">xm</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">M</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">xm</span> <span class="o">@</span> <span class="p">(</span><span class="n">xm</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="n">v</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">angle</span> <span class="o">=</span> <span class="mf">180.0</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  
    <span class="n">ell</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">M</span><span class="p">[:,</span><span class="n">i</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="mf">180.0</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
    <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e82489a2b59d3c7873ab58b0cc2164a3c6ec3d3afb14eb796120c6bc39ad98d5.png" src="_images/e82489a2b59d3c7873ab58b0cc2164a3c6ec3d3afb14eb796120c6bc39ad98d5.png" />
</div>
</div>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper compound">
<span id="document-afc"></span><div class="tex2jax_ignore mathjax_ignore section" id="analyse-factorielle-des-correspondances">
<h2>Analyse Factorielle des correspondances<a class="headerlink" href="#analyse-factorielle-des-correspondances" title="Permalink to this heading">#</a></h2>
<p id="index-0">On cherche √† expliquer la liaison entre deux variables qualitatives <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, caract√©ris√©es par un ensemble de couples de modalit√©s <span class="math notranslate nohighlight">\((x_i,y_i)\)</span>. On note <span class="math notranslate nohighlight">\(x_1\cdots x_J\)</span> et <span class="math notranslate nohighlight">\(y_1\cdots y_K\)</span> les modalit√©s distinctes de <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> respectivement.</p>
<p>Plus pr√©cis√©ment, l‚Äôanalyse factorielle des correspondances (AFC) vise √† d√©finir un mod√®le statistique permettant de fournir des param√®tres dont la repr√©sentation graphique illustre les correspondances entre les modalit√©s de ces variables. Dans sa version ‚Äúanalyse de donn√©es‚Äù, l‚ÄôAFC cherche √† r√©duire la dimension des donn√©es en effectuant la d√©composition factorielle des nuages de points associ√©s aux profils lignes et aux profils colonnes du tableau de contingence croisant les modalit√©s des deux variables (L‚ÄôAFC est une double ACP sur les deux tableaux de profils). On aborde √† la fin du chapitre une mod√©lisation statistique de l‚ÄôAFC, en supposant que les fr√©quences d‚Äôobservation correspondent √† l‚Äôobservation d‚Äôune probabilit√© th√©orique, dont la distribution mod√©lise le tableau de contingence des deux variables.</p>
<div class="section" id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this heading">#</a></h3>
<p>Le tableau de contingence <span class="math notranslate nohighlight">\({\bf T}\)</span> entre les <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>, vu comme une matrice,  est d√©fini par</p>
<span class="target" id="index-1"></span><table class="colwidths-auto table" id="index-2">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_k\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_K\)</span></p></th>
<th class="head"><p>total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{11}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1K}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{1.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_j\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{j1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{jk}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{jK}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{j.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x_J\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{J1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{Jk}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{JK}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{J.}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>total</p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.1}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.k}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\cdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{.K}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n\)</span></p></td>
</tr>
</tbody>
</table>
<p>o√π <span class="math notranslate nohighlight">\(n_{j.}\)</span> (resp <span class="math notranslate nohighlight">\(n_{.k}\)</span> )sont les effectifs marginaux repr√©sentant le nombre de fois o√π <span class="math notranslate nohighlight">\(x_j\)</span> (resp. <span class="math notranslate nohighlight">\(y_k\)</span>) appara√Æt, et <span class="math notranslate nohighlight">\(n_{jk}\)</span> le nombre d‚Äôapparitions du couple <span class="math notranslate nohighlight">\((x_j,y_k)\)</span>.</p>
<p>Les fr√©quences conjointes <span class="math notranslate nohighlight">\(f_{jk}=\frac{n_{jk}}{n}\)</span> et les fr√©quences marginales sont stock√©es dans des vecteurs <span class="math notranslate nohighlight">\({\bf g_J}=\begin{pmatrix}f_{1.}\ldots f_{J.} \end{pmatrix}^T\)</span> et <span class="math notranslate nohighlight">\({\bf g_K}=\begin{pmatrix}f_{.1}\ldots f_{.K} \end{pmatrix}^T\)</span>.</p>
<p>On note aussi <span class="math notranslate nohighlight">\({\bf D_J}=diag\left (f_{1.}\ldots f_{J.}\right )\)</span> et <span class="math notranslate nohighlight">\({\bf D_K}=diag\left (f_{.1}\ldots f_{.K} \right )\)</span>.</p>
<p>Dans le tableau de contingence <span class="math notranslate nohighlight">\(\mathbf T\)</span>, on lit le <span class="math notranslate nohighlight">\(j^e\)</span> profil ligne <span class="math notranslate nohighlight">\([\frac{n_{j1}}{n_{j.}}\ldots \frac{n_{jK}}{n_{j.}}]\)</span>, consid√©r√© comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span> et le <span class="math notranslate nohighlight">\(k^e\)</span> profil colonne <span class="math notranslate nohighlight">\([\frac{n_{1k}}{n_{.k}}\ldots \frac{n_{Jk}}{n_{.k}}]\)</span> consid√©r√© comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span>. Ces profils sont rang√©s dans des matrices de profils lignes <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R})\)</span> et de colonnes <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\)</span> d√©finies par <span class="math notranslate nohighlight">\({\bf A}=\frac{1}{n}{\bf T^TD_J^{-1}}\textrm{  et  } {\bf B}=\frac{1}{n}{\bf T D_K^{-1}}\)</span></p>
</div>
<div class="section" id="double-acp">
<h3>Double ACP<a class="headerlink" href="#double-acp" title="Permalink to this heading">#</a></h3>
<p>L‚Äôanalyse factorielle des correspondances peut √™tre consid√©r√©e comme le r√©sultat d‚Äôune double ACP :</p>
<ul class="simple">
<li><p>une effectu√©e sur les profils colonnes dans <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span></p></li>
<li><p>une effectu√©e sur les profils lignes dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span></p></li>
</ul>
<p>relativement √† la m√©trique du <span class="math notranslate nohighlight">\(\chi^2\)</span> de matrice <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> pour l‚Äôanalyse en lignes et <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span> pour l‚Äôanalyse en colonnes.</p>
<p>Ainsi, par exemple, la distance entre deux modalit√©s <span class="math notranslate nohighlight">\(x_l\)</span> et <span class="math notranslate nohighlight">\(x_p\)</span> de <span class="math notranslate nohighlight">\(X\)</span> est donn√©e par :</p>
<p><span class="math notranslate nohighlight">\(\Vert {\bf A_{.l}}-{\bf A_{.p}}\Vert^2_{{\bf D_K^{-1}}} = \displaystyle\sum_{i=1}^K\frac{1}{f_{.i}}\left (A_{i,l}-A_{i,p} \right )^2\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\({\bf A_{.l}}\)</span> est la <span class="math notranslate nohighlight">\(l^e\)</span> colonne de <span class="math notranslate nohighlight">\({\bf A}\)</span>. La m√©trique du <span class="math notranslate nohighlight">\(\chi^2\)</span> introduit les inverses
des fr√©quences marginales des modalit√©s de <span class="math notranslate nohighlight">\(Y\)</span> comme pond√©rations des √©carts
entre √©l√©ments de deux profils relatifs √† <span class="math notranslate nohighlight">\(X\)</span> (et r√©ciproquement). Elle attribue
donc plus de poids aux √©carts correspondants √† des modalit√©s de faible effectif
(rares) pour <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</div>
<div class="section" id="acp-dans-mathbb-r-j">
<h3>ACP dans <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span><a class="headerlink" href="#acp-dans-mathbb-r-j" title="Permalink to this heading">#</a></h3>
<p>L‚ÄôACP sur les profils colonnes est r√©alis√©e en recherchant les √©l√©ments propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span>, sym√©trique par rapport √† la m√©trique <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span> et semi d√©finie positive. On note <span class="math notranslate nohighlight">\(\bf U\)</span> la matrice des vecteurs propres. Cette ACP fournit une repr√©sentation des modalit√©s de <span class="math notranslate nohighlight">\(Y\)</span>, r√©alis√©e au moyen des lignes de la matrice des composantes principales <span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TD_J^{-1}U}\)</span>.</p>
</div>
<div class="section" id="acp-dans-mathbb-r-k">
<h3>ACP dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span><a class="headerlink" href="#acp-dans-mathbb-r-k" title="Permalink to this heading">#</a></h3>
<p>L‚ÄôACP sur les profils lignes est r√©alis√©e en recherchant les √©l√©ments propres de <span class="math notranslate nohighlight">\({\bf AB}\)</span>, sym√©trique par rapport √† la m√©trique <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> et semi d√©finie positive. On note <span class="math notranslate nohighlight">\(\bf V\)</span> la matrice des vecteurs propres. Cette ACP fournit une repr√©sentation des modalit√©s de <span class="math notranslate nohighlight">\(X\)</span>, r√©alis√©e au moyen des lignes de la matrice des composantes principales <span class="math notranslate nohighlight">\({\bf C_J}={\bf A^TD_K^{-1}V}\)</span>.</p>
<p>Puisque <span class="math notranslate nohighlight">\(\bf U\)</span> contient les vecteurs propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span> et <span class="math notranslate nohighlight">\(\bf V\)</span> ceux de <span class="math notranslate nohighlight">\({\bf AB}\)</span>, il suffit de r√©aliser en fait une seule ACP, les r√©sultats de l‚Äôautre s‚Äôen d√©duisant simplement : si <span class="math notranslate nohighlight">\(\bf \Lambda\)</span> est la matrice des valeurs propres (hors <span class="math notranslate nohighlight">\(\lambda_0=0\)</span>) communes aux deux ACP :</p>
<p><span class="math notranslate nohighlight">\({\bf V} ={\bf AU\Lambda^{-\frac{1}{2}}}\textrm {  et  } {\bf U} ={\bf BV\Lambda^{-\frac{1}{2}}}\)</span>
Alors</p>
<p><span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TD_J^{-1}U }= {\bf B^TD_J^{-1}BV\Lambda^{-\frac{1}{2}}} = {\bf D_K^{-1}ABV \Lambda^{-\frac{1}{2}}} =  {\bf D_K^{-1}V \Lambda^{\frac{1}{2}}}\)</span>
et</p>
<p><span class="math notranslate nohighlight">\({\bf C_J}={\bf A^TD_K^{-1}V}= {\bf D_J^{-1}U \Lambda^{\frac{1}{2}}}\)</span>
d‚Äôo√π</p>
<p><span class="math notranslate nohighlight">\({\bf C_K}={\bf B^TC_J\Lambda^{-\frac{1}{2}}} \textrm{    et    } {\bf C_J}={\bf A^TC_K\Lambda^{-\frac{1}{2}}}\)</span></p>
<div class="proof remark dropdown admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 23 </span></p>
<div class="remark-content section" id="proof-content">
<p>Soit deux matrices <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{KJ}(\mathbb{R)}\)</span>  et <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{JK}(\mathbb{R})\)</span>. Les valeurs propres non nulles de <span class="math notranslate nohighlight">\({\bf AB}\)</span> et <span class="math notranslate nohighlight">\({\bf BA}\)</span> sont identiques avec le m√™me degr√© de multiplicit√©. De plus, si <span class="math notranslate nohighlight">\(\bf u\)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf BA}\)</span> associ√© √† la valeur propre <span class="math notranslate nohighlight">\(\lambda\neq 0\)</span>, alors <span class="math notranslate nohighlight">\({\bf v} = {\bf Au} \)</span> est vecteur propre de <span class="math notranslate nohighlight">\({\bf AB}\)</span> associ√© √† la m√™me valeur
propre.</p>
</div>
</div></div>
<div class="section" id="representation-graphique">
<h3>Repr√©sentation graphique<a class="headerlink" href="#representation-graphique" title="Permalink to this heading">#</a></h3>
<p>La d√©composition de <span class="math notranslate nohighlight">\(\mathbf T/n\)</span> donne :</p>
<p><span class="math notranslate nohighlight">\(\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}} = \displaystyle\sum_{i=0}^{min(J-1,K-1)}\sqrt{\lambda_i}\frac{u^i_jv^i_k}{f_{j.}f_{.k}}\)</span>
Cette quantit√© est appel√©e taux de liaison entre les modalit√©s <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span>. En se limitant au rang <span class="math notranslate nohighlight">\(q\)</span> on obtient pour chaque couple de modalit√© <span class="math notranslate nohighlight">\((j,k)\)</span> de <span class="math notranslate nohighlight">\(\mathbf T\)</span> une approximation de son √©cart relatif √† l‚Äôind√©pendance, comme produit scalaire des deux vecteurs <span class="math notranslate nohighlight">\(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{j.}}u_j\)</span> et <span class="math notranslate nohighlight">\(\frac{(\lambda_i)^{\frac{1}{4}}}{f_{.k}}v_k\)</span>, termes g√©n√©riques des matrices <span class="math notranslate nohighlight">\({\bf D_J^{-1}U\Lambda^{\frac{1}{4}}}\)</span> et <span class="math notranslate nohighlight">\({\bf D_K^{-1}V\Lambda^{\frac{1}{4}}}\)</span></p>
<p>La repr√©sentation graphique de ces vecteurs (par exemple avec <span class="math notranslate nohighlight">\(q=2\)</span>), appel√©e biplot, donne la correspondance entre les deux modalit√©s <span class="math notranslate nohighlight">\(x_j\)</span> et <span class="math notranslate nohighlight">\(y_k\)</span>. Lorsque ces deux modalit√©s, √©loign√©es de l‚Äôorigine, sont
voisines (resp. oppos√©es), leur produit scalaire est de valeur absolue importante ; leur cellule conjointe contribue alors fortement et de mani√®re positive
(resp. n√©gative) √† la d√©pendance entre les deux variables. L‚Äôanalyse factorielle des correspondances appara√Æt ainsi comme la meilleure reconstitution des fr√©quences <span class="math notranslate nohighlight">\(f_{jk}\)</span>, ou encore la meilleure repr√©sentation des √©carts relatifs √† l‚Äôind√©pendance.</p>
</div>
<div class="section" id="interpretation">
<h3>Interpr√©tation<a class="headerlink" href="#interpretation" title="Permalink to this heading">#</a></h3>
<p>Les qualit√©s de repr√©sentation dans la dimension choisie et les contributions
des modalit√©s de <span class="math notranslate nohighlight">\(X\)</span> ou de <span class="math notranslate nohighlight">\(Y\)</span> se d√©duisent facilement de celles de l‚ÄôACP. Ces
quantit√©s sont utilis√©es √† la fois pour choisir la dimension de l‚Äôanalyse factorielle des correspondances  et pour interpr√©ter ses r√©sultats dans la dimension choisie.</p>
</div>
<div class="section" id="inertie-et-test-d-independance">
<h3>Inertie et test d‚Äôind√©pendance<a class="headerlink" href="#inertie-et-test-d-independance" title="Permalink to this heading">#</a></h3>
<p>En analyse en composantes principales centr√©e r√©duite, l‚Äôinertie totale du nuage de points est  √©gale au nombre de variables. En AFC,  l‚Äôinertie totale du nuage des profils lignes est  √©gale √† l‚Äôinertie totale du nuage des profils colonnes, √©gale au <span class="math notranslate nohighlight">\(\chi^2\)</span> d‚Äôind√©pendance entre les deux variables qualitatives.<br />
La valeur de l‚Äôinertie est donc un indicateur de la dispersion des nuages de points et une mesure de liaison entre les deux variables qualitatives,  appel√©e mesure d‚Äô√©cart √† l‚Äôind√©pendance.</p>
</div>
<div class="section" id="interpretation-des-valeurs-propres">
<h3>Interpr√©tation des valeurs propres<a class="headerlink" href="#interpretation-des-valeurs-propres" title="Permalink to this heading">#</a></h3>
<p>Les valeurs propres des ACP renseignent sur la dispersion des nuages de profils lignes et colonnes :</p>
<ul class="simple">
<li><p>Une valeur propre proche de 1 indique une dichotomie parfaite du tableau <span class="math notranslate nohighlight">\(\mathbf T\)</span>, qui peut √™tre d√©compos√© apr√®s reclassement des modalit√©s en deux blocs distincts</p></li>
<li><p>Plus g√©n√©ralement <span class="math notranslate nohighlight">\(p\)</span> valeurs propres proches am√®nent √† <span class="math notranslate nohighlight">\(k+1\)</span> blocs distincts</p></li>
<li><p>Si toutes les valeurs propres sont proches de 1, on aboutit √† l‚Äôeffet Guttman : il existe une correspondance entre chaque modalit√© ligne et une modalit√© colonne ‚Äúassoci√©e‚Äù. Avec une r√©organisation des modalit√©s, les effectifs importants se trouvent alors le long de la diagonale.</p></li>
</ul>
</div>
<div class="section" id="qualite-globale">
<h3>Qualit√© globale<a class="headerlink" href="#qualite-globale" title="Permalink to this heading">#</a></h3>
<p>A <span class="math notranslate nohighlight">\(q\)</span> fix√©, la qualit√© globale de la repr√©sentation se mesure comme dans le cadre de l‚ÄôACP, comme le rapport entre les <span class="math notranslate nohighlight">\(q\)</span> premi√®res valeurs propres <span class="math notranslate nohighlight">\(\lambda_i\)</span> et la somme sur tout le spectre.</p>
<p>On montre que la qualit√© de la repr√©sentation dans la <span class="math notranslate nohighlight">\(i^e\)</span> dimension s‚Äô√©crit <span class="math notranslate nohighlight">\(\frac{n\lambda_i}{\chi^2}\)</span></p>
</div>
<div class="section" id="qualite-de-chaque-modalite">
<h3>Qualit√© de chaque modalit√©<a class="headerlink" href="#qualite-de-chaque-modalite" title="Permalink to this heading">#</a></h3>
<p>Comme dans l‚ÄôACP √©galement, la qualit√© d‚Äôune modalit√© de <span class="math notranslate nohighlight">\(X\)</span> (resp. <span class="math notranslate nohighlight">\(Y\)</span>) se quantifie par le carr√© du cosinus de l‚Äôangle entre le vecteur repr√©sentant cette modalit√© dans <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span> (resp. <span class="math notranslate nohighlight">\(\mathbb{R}^J\)</span>)  et sa projection orthogonale au sens de <span class="math notranslate nohighlight">\({\bf D_K^{-1}}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf D_J^{-1}}\)</span>) dans le sous-espace principal de dimension <span class="math notranslate nohighlight">\(q\)</span>. Ces cosinus se calculent tr√®s simplement en faisant le rapport des sommes appropri√©es des carr√©s des coordonn√©es extraites des lignes de <span class="math notranslate nohighlight">\({\bf C_J}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf C_K}\)</span>).</p>
</div>
<div class="section" id="inertie-expliquee">
<h3>Inertie expliqu√©e<a class="headerlink" href="#inertie-expliquee" title="Permalink to this heading">#</a></h3>
<p>L‚Äôinertie totale du nuage des profils lignes (resp. colonnes) est √©gale √† la somme de toutes les valeurs propres <span class="math notranslate nohighlight">\(\lambda_i\)</span>. La part due au <span class="math notranslate nohighlight">\(j^e\)</span> profil ligne (resp. <span class="math notranslate nohighlight">\(k^e\)</span> profil colonne) est <span class="math notranslate nohighlight">\(f_{j.}\displaystyle\sum_i \left (\mathbf{C_J}(ji) \right )^2\)</span> (resp. <span class="math notranslate nohighlight">\(f_{.k}\displaystyle\sum_i \left (\mathbf{C_K}(ik) \right )^2\)</span>).</p>
<p>Les contributions √† l‚Äôinertie selon chaque axe se calculent de la m√™me mani√®re, sans sommation sur <span class="math notranslate nohighlight">\(i\)</span>. Elles sont utilis√©es pour s√©lectionner les modalit√©s les plus importantes (i.e. celles qui importent le plus dans la d√©finition de la liaison entre <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span>).</p>
</div>
<div class="section" id="choix-de-q">
<h3>Choix de q<a class="headerlink" href="#choix-de-q" title="Permalink to this heading">#</a></h3>
<p>Comme dans le cas de l‚ÄôACP, le choix de l‚Äôespace de repr√©sentation est important. On peut estimer <span class="math notranslate nohighlight">\(q\)</span> comme en ACP (pourcentage de l‚Äôinertie expliqu√©e, d√©croissance des valeurs propres), ou utiliser une approche probabiliste : si</p>
<p><span class="math notranslate nohighlight">\(\nu_{jk}^q = n f_{j.}f_{.k} + n\displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\)</span>
est l‚Äôestimation d‚Äôordre <span class="math notranslate nohighlight">\(q\)</span> de <span class="math notranslate nohighlight">\(n_{jk}\)</span> alors sous certaines conditions (<span class="math notranslate nohighlight">\(n\)</span> grand, mod√®le multinomial‚Ä¶), on montre que</p>
<p><span class="math notranslate nohighlight">\(\displaystyle\sum_{j=1}^J\displaystyle\sum_{k=1}^K\frac{\left (n_{jk}-\nu_{jk}^q \right )^2}{\nu_{jk}^q}\approx \displaystyle\sum_{i\geq q+1} \lambda_i\)</span></p>
<p>suit approximativement une loi <span class="math notranslate nohighlight">\(\chi^2\)</span> √† <span class="math notranslate nohighlight">\((J-q-1)(K-q-1)\)</span> degr√©s de libert√©. On peut donc retenir <span class="math notranslate nohighlight">\(q\)</span> comme √©tant la plus petite dimension telle que cette quantit√© est inf√©rieure √† la valeur limite de cette loi.</p>
</div>
<div class="section" id="modele-statistique">
<h3>Mod√®le statistique<a class="headerlink" href="#modele-statistique" title="Permalink to this heading">#</a></h3>
<p>On suppose que chaque fr√©quence <span class="math notranslate nohighlight">\(f_{jk}\)</span> correspond √† l‚Äôobservation d‚Äôune probabilit√©   th√©orique <span class="math notranslate nohighlight">\(\pi_{jk}\)</span> et on mod√©lise donc <span class="math notranslate nohighlight">\(\bf T\)</span> par la distribution correspondante. Le mod√®le d√©crivant cette distribution permet d‚Äôexpliciter la probabilit√©.</p>
</div>
<div class="section" id="modele-log-lineaire">
<h3>Mod√®le log lin√©aire<a class="headerlink" href="#modele-log-lineaire" title="Permalink to this heading">#</a></h3>
<p>Souvent, le nombre <span class="math notranslate nohighlight">\(n\)</span> est fix√© a priori. La distribution conjointe des effectifs <span class="math notranslate nohighlight">\(n_{jk}\)</span> est alors conditionn√©e par <span class="math notranslate nohighlight">\(n\)</span> et est une loi multinomiale de param√®tre <span class="math notranslate nohighlight">\(\pi_{jk}\)</span> et d‚Äôesp√©rance <span class="math notranslate nohighlight">\(n\pi_{jk}\)</span>.</p>
<p>Par d√©finition, les variables <span class="math notranslate nohighlight">\(X\)</span> et <span class="math notranslate nohighlight">\(Y\)</span> sont ind√©pendantes si <span class="math notranslate nohighlight">\(\pi_{jk}=\pi_{j.}\pi_{.k}\)</span>. Dans le cas contraire, on peut √©crire</p>
<p><span class="math notranslate nohighlight">\(\pi_{jk} = \pi_{j.}\pi_{.k}\frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\)</span></p>
<p>En passant au log, on lin√©arise en</p>
<p><span class="math notranslate nohighlight">\(ln\left (\pi_{jk}\right ) =  ln (\pi_{j.}) + ln (\pi_{.k})  + ln \left( \frac{\pi_{jk}}{\pi_{j.}\pi_{.k}}\right )\)</span></p>
<p>Ce mod√®le est satur√© car il comporte autant de param√®tres que de donn√©es.  L‚Äôind√©pendance est v√©rifi√©e si le dernier terme de couplage est nul pour tout <span class="math notranslate nohighlight">\((j,k)\)</span>. Les param√®tres du mod√®le sont estim√©s en maximisant la log vraisemblance.</p>
</div>
<div class="section" id="modele-de-correlation">
<h3>Mod√®le de corr√©lation<a class="headerlink" href="#modele-de-correlation" title="Permalink to this heading">#</a></h3>
<p>Dans ce mod√®le, on √©crit</p>
<p><span class="math notranslate nohighlight">\(\pi_{jk} = \pi_{j.}\pi_{.k} + \displaystyle\sum_{i=1}^q \sqrt{\lambda_i} u^i_jv^i_k\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\({\bf u^i}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf v^i}\)</span>) sont les vecteurs propres de <span class="math notranslate nohighlight">\({\bf BA}\)</span> (resp. <span class="math notranslate nohighlight">\({\bf AB}\)</span>),  <span class="math notranslate nohighlight">\(\lambda_i\)</span> les valeurs propres associ√©es (qui sont identiques pour les deux matrices), et <span class="math notranslate nohighlight">\(q\leq min(J-1,K-1)\)</span>.</p>
<p>Les contraintes <span class="math notranslate nohighlight">\(\displaystyle\sum_{j=1}^J u^i_j =  \displaystyle\sum_{k=1}^K v^i_k = 0\)</span>  et <span class="math notranslate nohighlight">\({\bf (u^i)^TD_J^{-1} u^l }= {\bf (v^i)^TD_K^{-1} v^l} = \delta_{il}\)</span> (vecteurs propres orthonorm√©s) permettent d‚Äôidentifier les param√®tres du mod√®le. \
Les estimations des param√®tres <span class="math notranslate nohighlight">\(\pi_{j.}\pi_{.k} ,\lambda_i,u^i,v^i\)</span> peuvent √™tre r√©alis√©es par maximum de vraisemblance ou par moindres carr√©s.</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this heading">#</a></h3>
<p>On utilise ici des donn√©es open source du <a class="reference external" href="https://www.data.gouv.fr/fr/posts/les-donnees-des-elections/">gouvernement</a>, pr√©sentant le r√©sultat du premier tour des √©lections pr√©sidentielles de 2017.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>HAMON</p></th>
<th class="head"><p>MACRON</p></th>
<th class="head"><p>ASSELINEAU</p></th>
<th class="head"><p>FILLON</p></th>
<th class="head"><p>CHEMINADE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>151296</p></td>
<td><p>615775</p></td>
<td><p>30223</p></td>
<td><p>586390</p></td>
<td><p>6078</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>240175</p></td>
<td><p>851372</p></td>
<td><p>26667</p></td>
<td><p>602884</p></td>
<td><p>6264</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>256620</p></td>
<td><p>1026255</p></td>
<td><p>41352</p></td>
<td><p>846252</p></td>
<td><p>7602</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>87382</p></td>
<td><p>338187</p></td>
<td><p>14330</p></td>
<td><p>304387</p></td>
<td><p>2842</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>180827</p></td>
<td><p>581076</p></td>
<td><p>13419</p></td>
<td><p>380815</p></td>
<td><p>3400</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>83552</p></td>
<td><p>323724</p></td>
<td><p>12075</p></td>
<td><p>300324</p></td>
<td><p>2882</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>5780</p></td>
<td><p>28528</p></td>
<td><p>965</p></td>
<td><p>39453</p></td>
<td><p>253</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>430404</p></td>
<td><p>1612816</p></td>
<td><p>64406</p></td>
<td><p>1249770</p></td>
<td><p>9796</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>216362</p></td>
<td><p>740037</p></td>
<td><p>28603</p></td>
<td><p>566045</p></td>
<td><p>5524</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>166640</p></td>
<td><p>630300</p></td>
<td><p>26043</p></td>
<td><p>521389</p></td>
<td><p>5688</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>113744</p></td>
<td><p>423075</p></td>
<td><p>14303</p></td>
<td><p>370188</p></td>
<td><p>3544</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>143491</p></td>
<td><p>575832</p></td>
<td><p>15529</p></td>
<td><p>516428</p></td>
<td><p>3731</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>113344</p></td>
<td><p>520909</p></td>
<td><p>25948</p></td>
<td><p>615455</p></td>
<td><p>4569</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>101948</p></td>
<td><p>389440</p></td>
<td><p>18725</p></td>
<td><p>314017</p></td>
<td><p>3425</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>MELENCHON</p></th>
<th class="head"><p>LASSALLE</p></th>
<th class="head"><p>FILLON</p></th>
<th class="head"><p>DUPONT-AIGNAN</p></th>
<th class="head"><p>POUTOU</p></th>
<th class="head"><p>LEPEN</p></th>
<th class="head"><p>ARTHAUD</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>484810</p></td>
<td><p>30508</p></td>
<td><p>586390</p></td>
<td><p>182200</p></td>
<td><p>34468</p></td>
<td><p>825600</p></td>
<td><p>24272</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>703505</p></td>
<td><p>91915</p></td>
<td><p>602884</p></td>
<td><p>155600</p></td>
<td><p>49649</p></td>
<td><p>640228</p></td>
<td><p>21442</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>805846</p></td>
<td><p>53282</p></td>
<td><p>846252</p></td>
<td><p>215951</p></td>
<td><p>43530</p></td>
<td><p>867874</p></td>
<td><p>24670</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>276954</p></td>
<td><p>15843</p></td>
<td><p>304387</p></td>
<td><p>87263</p></td>
<td><p>18529</p></td>
<td><p>387658</p></td>
<td><p>11492</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>385736</p></td>
<td><p>19097</p></td>
<td><p>380815</p></td>
<td><p>87928</p></td>
<td><p>27092</p></td>
<td><p>306644</p></td>
<td><p>14296</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>252307</p></td>
<td><p>13570</p></td>
<td><p>300324</p></td>
<td><p>82060</p></td>
<td><p>16282</p></td>
<td><p>329470</p></td>
<td><p>11365</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>21314</p></td>
<td><p>8711</p></td>
<td><p>39453</p></td>
<td><p>4462</p></td>
<td><p>1374</p></td>
<td><p>43041</p></td>
<td><p>495</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>1225311</p></td>
<td><p>36358</p></td>
<td><p>1249770</p></td>
<td><p>226266</p></td>
<td><p>45715</p></td>
<td><p>708340</p></td>
<td><p>23592</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>734223</p></td>
<td><p>75483</p></td>
<td><p>566045</p></td>
<td><p>135405</p></td>
<td><p>35219</p></td>
<td><p>762104</p></td>
<td><p>16777</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>633322</p></td>
<td><p>22411</p></td>
<td><p>521389</p></td>
<td><p>160722</p></td>
<td><p>33653</p></td>
<td><p>1003221</p></td>
<td><p>29194</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>362535</p></td>
<td><p>13900</p></td>
<td><p>370188</p></td>
<td><p>98957</p></td>
<td><p>23816</p></td>
<td><p>452702</p></td>
<td><p>15196</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>403454</p></td>
<td><p>16988</p></td>
<td><p>516428</p></td>
<td><p>109842</p></td>
<td><p>26340</p></td>
<td><p>364267</p></td>
<td><p>16018</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>515419</p></td>
<td><p>29551</p></td>
<td><p>615455</p></td>
<td><p>119025</p></td>
<td><p>21316</p></td>
<td><p>774791</p></td>
<td><p>10439</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>256149</p></td>
<td><p>7748</p></td>
<td><p>314017</p></td>
<td><p>29505</p></td>
<td><p>17599</p></td>
<td><p>213553</p></td>
<td><p>13180</p></td>
</tr>
</tbody>
</table>
<p>On d√©cide dans l‚Äôanalyse d‚Äôenlever le candidat LASSALLE, dont les votes sont concentr√©s dans les Pyren√©es et en Corse (et qui introduit un biais dans l‚Äô√©tude).</p>
<p>A partir de ce tableau de donn√©es <span class="math notranslate nohighlight">\(\bf T\)</span>, on calcule les tableaux de fr√©quences en lignes et en colonnes, ainsi que les profils ligne et colonne moyens.
Comme en ACP, on s‚Äôint√©resse √† l‚Äôinertie du nuage de points, mais pour ce faire on utilise la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span>. Avec cette m√©trique, la distance entre deux lignes (ou deux colonnes) ne d√©pend pas des poids respectifs des colonnes (ou lignes). Par exemple, les diff√©rents candidats obtiennent des scores tr√®s diff√©rents et l‚Äôusage de la m√©trique euclidienne aurait donn√© trop de poids aux candidats qui ont obtenu des scores √©lev√©s. De plus, la m√©trique du <span class="math notranslate nohighlight">\(\chi^2\)</span> poss√®de la propri√©t√© d‚Äô√©quivalence distributionnelle : si on regroupe deux modalit√©s lignes (colonnes), les distances entre les profils-colonne (lignes), ou entre les autres profils-lignes (colonnes) restent inchang√©es.</p>
<p><img alt="" src="_images/presdist.png" /></p>
<p>On peut √©galement calculer les taux de liaisons, d√©finis pour deux individus <span class="math notranslate nohighlight">\(j\)</span> et <span class="math notranslate nohighlight">\(k\)</span> par <span class="math notranslate nohighlight">\(\frac{f_{jk}-f_{j.}f_{.k}}{f_{j.}f_{.k}}\)</span>. Par exemple, le taux de liaison entre HAMON et Grand-Est est √©gal √†  -0.2003, tandis que le taux de liaison entre CHEMINADE et Nouvelle-Aquitaine est √©gal √† 0.2068. Le taux de liaison s‚Äôinterpr√©te comme suit : le score du candidat dans la r√©gion est 20% moins √©lev√© (ou 20.6% moins √©lev√©) que le score th√©orique que l‚Äôon observerait si les votes √©taient ind√©pendants des r√©gions.</p>
<p>Notons que <span class="math notranslate nohighlight">\(f_{j.}f_{.k}\)</span> repr√©sente le poids th√©orique de chaque case du tableau des fr√©quences. La somme de ces coefficients vaut 1. La moyenne de la s√©rie des taux de liaisons pond√©r√©e par les <span class="math notranslate nohighlight">\(f_{j.}f_{.k}\)</span> est nulle. De m√™me, la variance de cette s√©rie avec la m√™me pond√©ration vaut <span class="math notranslate nohighlight">\(\chi^2\)</span>, et ici est √©gale √† 0.0301.</p>
<p>On r√©alise ensuite une AFC, par analyse spectrale des matrices <span class="math notranslate nohighlight">\(\bf X^T\bf X\)</span> et <span class="math notranslate nohighlight">\(\bf X\bf X^T\)</span>, o√π <span class="math notranslate nohighlight">\(\bf X\)</span> est la matrice de terme g√©n√©ral <span class="math notranslate nohighlight">\(f_{jk}/\sqrt{f_{j.}f_{.k}}\)</span>.</p>
<p>Le nombre de valeurs propres produites par la recherche des facteurs principaux est √©gal au minimum du nombre de lignes et du nombre de colonnes du tableau de contingence. La premi√®re valeur propre est syst√©matiquement √©gale √† 1, et n‚Äôest pas utilis√©e dans les r√©sultats. Les autres valeurs propres sont des nombres positifs inf√©rieurs √† 1 et leur somme est √©gale √† <span class="math notranslate nohighlight">\(\chi^2\)</span>.</p>
<p><img alt="" src="_images/spectralelig.png" /> <img alt="" src="_images/spectralecol.png" /></p>
<p>On utilise alors les vecteurs propres (axes factoriels) pour analyser les donn√©es lignes et colonnes. Pour chaque analyse, on reporte (illustr√© ici sur l‚Äôanalyse en lignes) :</p>
<ul class="simple">
<li><p>La masse, qui rappelle les fr√©quences marginales des lignes c‚Äôest-√†-dire le profil colonne moyen. Contrairement √† l‚ÄôACP norm√©e, dans laquelle chaque individu √©tait affect√© du m√™me poids, les r√©gions ont ici un poids d√©pendant de l‚Äôeffectif total d‚Äô√©lecteurs inscrits dans la r√©gion.</p></li>
<li><p>La qualit√© qui indique les qualit√©s de repr√©sentation des individus ligne sur les deux premiers axes factoriels : c‚Äôest la somme des carr√©s des composantes de chaque individu sur les 2 axes, normalis√©e par la somme des carr√©s des composantes sur tous les axes.</p></li>
<li><p>La contribution de chaque individu √† la formation de chaque axe factoriel</p></li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Region</p></th>
<th class="head"><p>Masse</p></th>
<th class="head"><p>Coord1</p></th>
<th class="head"><p>Coord2</p></th>
<th class="head"><p>Qualit√©</p></th>
<th class="head"><p>contrib1</p></th>
<th class="head"><p>contrib2</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Grand-Est</p></td>
<td><p>0.082561</p></td>
<td><p>-0.081478</p></td>
<td><p>0.030469</p></td>
<td><p>0.033810</p></td>
<td><p>0.1031</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Nelle-Aquitaine</p></td>
<td><p>0.092573</p></td>
<td><p>0.022322</p></td>
<td><p>-0.009848</p></td>
<td><p>0.002607</p></td>
<td><p>0.0129</p></td>
<td><p>0.1404</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>AURA</p></td>
<td><p>0.116102</p></td>
<td><p>0.002463</p></td>
<td><p>0.020096</p></td>
<td><p>0.000032</p></td>
<td><p>0.0005</p></td>
<td><p>0.0074</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Bourgogne-FC</p></td>
<td><p>0.042922</p></td>
<td><p>-0.049796</p></td>
<td><p>0.021913</p></td>
<td><p>0.012884</p></td>
<td><p>0.0203</p></td>
<td><p>0.0048</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Bretagne</p></td>
<td><p>0.055616</p></td>
<td><p>0.074398</p></td>
<td><p>-0.000143</p></td>
<td><p>0.028325</p></td>
<td><p>0.0727</p></td>
<td><p>0.00599</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>Centre-Val-de-Loire</p></td>
<td><p>0.039694</p></td>
<td><p>-0.027564</p></td>
<td><p>0.034140</p></td>
<td><p>0.003982</p></td>
<td><p>0.0057</p></td>
<td><p>0.026</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>Corse</p></td>
<td><p>0.004089</p></td>
<td><p>-0.089963</p></td>
<td><p>0.097463</p></td>
<td><p>0.036278</p></td>
<td><p>0.0061</p></td>
<td><p>0.0528</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>Ile-de-France</p></td>
<td><p>0.157099</p></td>
<td><p>0.099907</p></td>
<td><p>0.022705</p></td>
<td><p>0.048773</p></td>
<td><p>0.3461</p></td>
<td><p>0.0099</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>Occitanie</p></td>
<td><p>0.090960</p></td>
<td><p>-0.024738</p></td>
<td><p>-0.020736</p></td>
<td><p>0.003126</p></td>
<td><p>0.0066</p></td>
<td><p>0.2115</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>Hauts-de-France</p></td>
<td><p>0.090114</p></td>
<td><p>-0.113101</p></td>
<td><p>-0.015944</p></td>
<td><p>0.061802</p></td>
<td><p>0.2215</p></td>
<td><p>0.1268</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>Normandie</p></td>
<td><p>0.052720</p></td>
<td><p>-0.035129</p></td>
<td><p>0.014011</p></td>
<td><p>0.006437</p></td>
<td><p>0.0116</p></td>
<td><p>0.0004</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>Pays-de-la-Loire</p></td>
<td><p>0.061053</p></td>
<td><p>0.049348</p></td>
<td><p>0.050417</p></td>
<td><p>0.012418</p></td>
<td><p>0.0311</p></td>
<td><p>0.1115</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>PACA</p></td>
<td><p>0.076388</p></td>
<td><p>-0.085523</p></td>
<td><p>0.044826</p></td>
<td><p>0.035070</p></td>
<td><p>0.1061</p></td>
<td><p>0.1766</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>Outremer</p></td>
<td><p>0.038108</p></td>
<td><p>0.073346</p></td>
<td><p>0.038647</p></td>
<td><p>0.026537</p></td>
<td><p>0.051</p></td>
<td><p>0.0222</p></td>
</tr>
</tbody>
</table>
<p>On repr√©sente alors graphiquement les individus ligne et colonne sur le premier plan factoriel.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><img alt="" src="_images/planlig.png" /></p></th>
<th class="head"><p><img alt="" src="_images/plancol.png" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Individus ligne</p></td>
<td><p>Individus colonne</p></td>
</tr>
</tbody>
</table>
<p>On en d√©duit alors l‚Äôanalyse suivante (ici propos√©e sur les candidats) :</p>
<ul class="simple">
<li><p>Premier axe : Le Pen repr√©sente 67% de l‚Äôinertie de cet axe. Macron, √† l‚Äôoppos√© en repr√©sente 19%. Clairement, cet axe oppose Le Pen √† Macron, mais les autres candidats ‚Äúles plus importants‚Äù ont un score du m√™me signe que celui de Macron.</p></li>
<li><p>Fillon repr√©sente 63% de son inertie et s‚Äôoppose √† Hamon, Poutou et M√©lenchon. Macron et Le Pen sont insignifiants sur cet axe. Cet axe repr√©sente l‚Äôopposition classique droite / gauche.</p></li>
<li><p>Le Pen est plac√©e assez loin de l‚Äôorigine sur la repr√©sentation graphique:  l‚Äôinertie de la modalit√© Le Pen (54%) est bien plus importante que sa masse (21%). Dit autrement, les scores de Le Pen pr√©sentent une grande variabilit√© selon les r√©gions, plus √©lev√©e que celle des scores de Macron (inertie 16% pour une masse de 24,3%) ou encore M√©lenchon (inertie 5%, inertie 19,8%). Les √©lecteurs de Le Pen, m√™me s‚Äôils sont plus nombreux que lors des scrutins pr√©c√©dents, restent in√©galement r√©partis sur le territoire.</p></li>
<li><p>Enfin, la premi√®re source de variation dans les votes est une opposition Le Pen / Macron, ind√©pendante des oppositions  droite / gauche traditionnels.</p></li>
</ul>
</div>
</div>
<span id="document-acm"></span><div class="tex2jax_ignore mathjax_ignore section" id="analyse-des-correspondances-multiples">
<h2>Analyse des correspondances multiples<a class="headerlink" href="#analyse-des-correspondances-multiples" title="Permalink to this heading">#</a></h2>
<p id="index-0">Tandis que l‚Äôanalyse factorielle des correspondances permet d‚Äôexpliquer la liaison entre deux variables qualitatives, l‚Äôanalyse des correspondances multiples (ACM) s‚Äôint√©resse au cas o√π l‚Äôon dispose de <span class="math notranslate nohighlight">\(p\geq 2\)</span> variables. C‚Äôest l‚Äô√©quivalent de l‚ÄôACP pour les variables qualitatives.</p>
<div class="section" id="notations">
<h3>Notations<a class="headerlink" href="#notations" title="Permalink to this heading">#</a></h3>
<p>On dispose d‚Äôun tableau de donn√©es <span class="math notranslate nohighlight">\(\mathbf{H}=(h_{i,j})\)</span> √† <span class="math notranslate nohighlight">\(n\)</span> lignes et <span class="math notranslate nohighlight">\(p\)</span> colonnes, o√π <span class="math notranslate nohighlight">\(n\)</span> est le nombre d‚Äôindividus, <span class="math notranslate nohighlight">\(p\)</span> le nombre de variables qualitatives mesur√©es et pour <span class="math notranslate nohighlight">\(i\in[\![1,n]\!],j\in[\![1,p]\!],h_{ij}\in\mathcal{M}_j\)</span>, <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span> √©tant l‚Äôensemble des modalit√©s de la j<span class="math notranslate nohighlight">\(^e\)</span> variable. Si <span class="math notranslate nohighlight">\(m_j\)</span> est le cardinal de <span class="math notranslate nohighlight">\(\mathcal{M}_j\)</span>, alors <span class="math notranslate nohighlight">\(m=\sum_{k=1}^p m_k\)</span> est le nombre total de modalit√©s.</p>
<div class="proof definition admonition" id="definition-0">
<span id="index-1"></span><p class="admonition-title"><span class="caption-number">Definition 43 </span> (Tableau disjonctif complet)</p>
<div class="definition-content section" id="proof-content">
<p>Le tableau disjonctif complet <span class="math notranslate nohighlight">\(\mathbf T\)</span> des donn√©es est un tableau <span class="math notranslate nohighlight">\(n\times m\)</span> tel que</p>
<p><span class="math notranslate nohighlight">\((\forall i\in[\![1,n]\!],j\in[\![1, m]\!])\; \mathbf T_{ij} = \left \{ \begin{array}{cl} 1&amp;\textrm{si l'individu i poss√®de la modalit√© j}\\0 &amp; \textrm{sinon}\end{array}\right .\)</span></p>
</div>
</div><span class="target" id="index-2"></span><p id="index-3">On d√©duit de ce tableau disjonctif le tableau de Burt correspondant, <span class="math notranslate nohighlight">\(\mathbf B=\mathbf T^T \mathbf T\)</span>, qui rassemble les croisements deux √†  deux de toutes les variables, i.e tous les tableaux de contingence des variables deux √† deux. Sur la diagonale de <span class="math notranslate nohighlight">\(\mathbf B\)</span> se trouvent les coefficients <span class="math notranslate nohighlight">\(B_{ii}=n_i\)</span>, donnant le nombre d‚Äôindividus poss√©dant la modalit√© <span class="math notranslate nohighlight">\(i\)</span>.  Les autres coefficients <span class="math notranslate nohighlight">\(B_{ij} = \mathbf{T_{\bullet i}}^T \mathbf {T_{\bullet j}}\)</span> quantifient le nombre d‚Äôindividus ayant les modalit√©s <span class="math notranslate nohighlight">\(i\)</span> et <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="section" id="analyse">
<h3>Analyse<a class="headerlink" href="#analyse" title="Permalink to this heading">#</a></h3>
</div>
<div class="section" id="tableau-de-contingence-de-l-acm">
<h3>Tableau de contingence de l‚ÄôACM<a class="headerlink" href="#tableau-de-contingence-de-l-acm" title="Permalink to this heading">#</a></h3>
<p>En analyse des correspondances multiples, on traite <span class="math notranslate nohighlight">\(\mathbf T\)</span> comme un tableau de contingence. Les totaux en ligne sont alors √©gaux au nombre de variables <span class="math notranslate nohighlight">\(p\)</span>, les totaux en colonne correspondent au nombre d‚Äôindividus ayant la modalit√© correspondant √† la colonne trait√©e. Pour une colonne <span class="math notranslate nohighlight">\(j\)</span>, on note ce total <span class="math notranslate nohighlight">\(n_j\)</span> Le total de tous les coefficients de <span class="math notranslate nohighlight">\(\mathbf T\)</span> vaut donc <span class="math notranslate nohighlight">\(np\)</span>.</p>
<p>Comme dans le cas de l‚ÄôAFC, l‚ÄôACM consid√®re les fr√©quences, les profils ligne et les profils colonne.</p>
<p>Pour les fr√©quences :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f_{ij}=T_{ij}/np\)</span> est la fr√©quence conjointe et vaut donc <span class="math notranslate nohighlight">\(1/np\)</span> si l‚Äôindividu <span class="math notranslate nohighlight">\(i\)</span> poss√®de la modalit√© <span class="math notranslate nohighlight">\(j\)</span> et 0 sinon. On range ces coefficients dans une matrice <span class="math notranslate nohighlight">\(\mathbf{F}\in\mathcal{M}_{nm}(\mathbb{R})\)</span></p></li>
<li><p>le poids des lignes est constant et vaut <span class="math notranslate nohighlight">\(1/n\)</span>. On note alors <span class="math notranslate nohighlight">\(\mathbf{a} = (\frac{1}{n}\cdots \frac{1}{n})^T\in\mathbb{R}^n\)</span> le vecteur des poids des individus.</p></li>
<li><p>le poids des colonnes vaut <span class="math notranslate nohighlight">\(n_j/np\)</span>, et est d‚Äôautant plus fort que la modalit√© <span class="math notranslate nohighlight">\(j\)</span> est fr√©quente. On note alors <span class="math notranslate nohighlight">\(\mathbf{b} = (\frac{n_1}{np}\cdots \frac{n_m}{np})^T\in\mathbb{R}^m\)</span> le vecteur des poids des modalit√©s.</p></li>
</ul>
<p>Comme en analyse factorielle des correspondances, on note <span class="math notranslate nohighlight">\({\bf D_n}=diag\left ({\bf a}\right )\)</span> et <span class="math notranslate nohighlight">\({\bf D_m}=diag\left ({\bf b} \right )\)</span>.</p>
<p>Pour les profils ligne et colonne :</p>
<ul class="simple">
<li><p>on lit dans <span class="math notranslate nohighlight">\(\mathbf T\)</span> le i<span class="math notranslate nohighlight">\(^e\)</span> profil ligne, consid√©r√© comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, de composantes <span class="math notranslate nohighlight">\(T_{ij}/p,j\in[\![1,m]\!]\)</span>. Ces profils sont rang√©s dans une matrice <span class="math notranslate nohighlight">\({\bf A}\in\mathcal{M}_{nm}(\mathbb{R})\)</span> et on a <span class="math notranslate nohighlight">\({\bf A}={\bf D_n^{-1}F}\)</span>.</p></li>
<li><p>on lit dans <span class="math notranslate nohighlight">\(\mathbf T\)</span> le j<span class="math notranslate nohighlight">\(^e\)</span> profil colonne, consid√©r√© comme un vecteur de <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, de composantes <span class="math notranslate nohighlight">\(T_{ij}/n_j,i\in[\![1,n]\!]\)</span>. Ces profils sont rang√©s dans <span class="math notranslate nohighlight">\({\bf B}\in\mathcal{M}_{nm}(\mathbb{R})\)</span> et on a <span class="math notranslate nohighlight">\({\bf B}={\bf FD_m^{-1}}\)</span>.</p></li>
</ul>
<p>L‚ÄôACM consid√®re, comme l‚ÄôAFC, deux nuages de points centr√©s :</p>
<ol class="arabic simple">
<li><p>le nuage des <span class="math notranslate nohighlight">\(n\)</span> individus dans <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, i.e. les <span class="math notranslate nohighlight">\(n\)</span> lignes de la matrice <span class="math notranslate nohighlight">\({\bf D_n^{-1}(F-ab^T})\)</span>. Chaque individu est pond√©r√© par <span class="math notranslate nohighlight">\(1/n\)</span></p></li>
<li><p>le nuage des <span class="math notranslate nohighlight">\(m\)</span> modalit√©s dans <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>, i.e. les <span class="math notranslate nohighlight">\(n\)</span> lignes de la matrice <span class="math notranslate nohighlight">\({\bf (F-ab^T)D_m^{-1}}\)</span>. Chaque modalit√© <span class="math notranslate nohighlight">\(j\)</span> est pond√©r√©e par <span class="math notranslate nohighlight">\(n_j/np\)</span>.</p></li>
</ol>
</div>
<div class="section" id="distances-entre-individus-et-entre-modalites">
<h3>Distances entre individus et entre modalit√©s<a class="headerlink" href="#distances-entre-individus-et-entre-modalites" title="Permalink to this heading">#</a></h3>
<p>En analyse des correspondances multiples, on utilise la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span> dans <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> et <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> :</p>
<ul class="simple">
<li><p>dans l‚Äôespace des individus, la m√©trique est <span class="math notranslate nohighlight">\(\mathbf {D_m}^{-1}\)</span> :</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\chi^2(i,i') = (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet})^T\mathbf {D_m}^{-1} (\mathbf{A}_{i\bullet}-\mathbf{A}_{i'\bullet}) = \displaystyle\sum_{j=1}^m\frac{1}{f_{\bullet j}}\left (\frac{T_{ij}-T_{i'j}}{p} \right )^2 = \frac {n}{p}\displaystyle\sum_{j=1}^m\frac{1}{{n_j}}\left (T_{ij}-T_{i'j}\right )^2\)</span>
Deux individus sont proches s‚Äôils poss√®dent les m√™mes modalit√©s, sachant que l‚Äôon donne plus de poids au fait que ces deux individus ont en commun une modalit√© rare (<span class="math notranslate nohighlight">\(n_s\)</span> petit).</p>
<ul class="simple">
<li><p>dans l‚Äôespace des modalit√©s, la m√©trique est <span class="math notranslate nohighlight">\(\mathbf {D_n}^{-1}\)</span> :</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\chi^2(j,j') = (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'})^T\mathbf {D_n}^{-1} (\mathbf{B}_{\bullet j}-\mathbf{B}_{\bullet j'}) = n\displaystyle\sum_{i=1}^n\left (\frac{T_{ij}}{n_j} -\frac{T_{ij'}}{n_{j'}}\right )^2\)</span>
et deux modalit√©s sont proches si elles sont poss√©d√©es par les m√™mes individus.</p>
<div class="proof remark dropdown admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 24 </span></p>
<div class="remark-content section" id="proof-content">
<p>On a de plus  <span class="math notranslate nohighlight">\(\chi^2(j,\mathbf{a}) = \frac{n}{n_j}-1\)</span> et  <span class="math notranslate nohighlight">\(f_{\bullet j}\chi^2(j,\mathbf{a}) = \frac{1}{p}\left( 1-\frac{n_j}{n}\right )\)</span>. Donc la distance d‚Äôune modalit√© au centre du nuage est d‚Äôautant plus grande que la modalit√© est rare et la part de l‚Äôinertie totale, due √† une modalit√© est d‚Äôautant plus grande que la modalit√© est rare.  On  √©vite donc en pratique de conserver dans l‚Äôanalyse les modalit√©s trop rares.</p>
<p>De m√™me, puisque <span class="math notranslate nohighlight">\(\displaystyle\sum_{k\in \mathcal{M}_j}f_{\bullet k}\chi^2(k,\mathbf{a}) = \frac{1}{p}\left( m_j-1\right )\)</span>, la part de l‚Äôinertie totale, due √† une variable  <span class="math notranslate nohighlight">\(j\)</span> est d‚Äôautant plus grande que le nombre de modalit√©s  de cette variable est grand. L√† aussi, on  √©vite en pratique de conserver dans l‚Äôanalyse des variables ayant des nombres de modalit√©s trop diff√©rents.</p>
</div>
</div></div>
<div class="section" id="principe-de-l-acm">
<h3>Principe de l‚ÄôACM<a class="headerlink" href="#principe-de-l-acm" title="Permalink to this heading">#</a></h3>
<p>L‚Äôanalyse en composantes multiples consiste alors √† appliquer l‚Äôanalyse factorielle des correspondances  du tableau des  contingences <span class="math notranslate nohighlight">\(\mathbf T\)</span>, c‚Äôest-√†-dire effectuer une ACP pond√©r√©e des nuages des point-individus et des point-modalit√©s .</p>
<p>Une diff√©rence notable vient cependant de l‚Äôinterpr√©tation de l‚Äôinertie de ces nuages de points  individus (<span class="math notranslate nohighlight">\(I(\mathbf{A})\)</span>) et modalit√©s (<span class="math notranslate nohighlight">\(I(\mathbf{B})\)</span>). En AFC, on pouvait interpr√©ter statistiquement cette inertie en terme de <span class="math notranslate nohighlight">\(\chi^2/n\)</span> mesurant l‚Äôind√©pendance entre les deux variables qualitatives. Ici, ce n‚Äôest plus le cas puisque l‚Äôon peut montrer que <span class="math notranslate nohighlight">\(I(\mathbf{A}) = I(\mathbf{B})= m/p-1\)</span>. L‚Äôinertie d√©pend donc du nombre moyen <span class="math notranslate nohighlight">\(m/p\)</span> de cat√©gories par variable.</p>
<div class="proof remark dropdown admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 25 </span></p>
<div class="remark-content section" id="proof-content">
<p>Les anglo-saxons consid√®re que l‚ÄôACM consiste √† effectuer l‚Äôanalyse factorielle des correspondances  du tableau de Burt <span class="math notranslate nohighlight">\(\mathbf T^T \mathbf T\)</span>, matrice sym√©trique de taille <span class="math notranslate nohighlight">\(m\)</span>. Les profils ligne et colonne sont alors identiques et correspondent aux modalit√©s que l‚Äôon veut analyser. On ne peut donc pas effectuer d‚Äôanalyse des individus.</p>
</div>
</div></div>
<div class="section" id="interpretation-des-resultats">
<h3>Interpr√©tation des r√©sultats<a class="headerlink" href="#interpretation-des-resultats" title="Permalink to this heading">#</a></h3>
</div>
<div class="section" id="inertie-expliquee">
<h3>Inertie expliqu√©e<a class="headerlink" href="#inertie-expliquee" title="Permalink to this heading">#</a></h3>
<p>L‚Äôinertie totale, √©gale comme nous l‚Äôavons vu √† <span class="math notranslate nohighlight">\(m/p-1\)</span> se calcule  √©galement comme la somme des valeurs propres <span class="math notranslate nohighlight">\(\lambda_1+\cdots +\lambda_r\)</span>, o√π <span class="math notranslate nohighlight">\(r=min(n-1,m-p)\)</span> est le nombre de valeurs propres non nulles issues de l‚ÄôACP. La part d‚Äôinertie expliqu√©e par l‚Äôaxe <span class="math notranslate nohighlight">\(z\)</span> est alors <span class="math notranslate nohighlight">\(\lambda_z/(\lambda_1+\cdots +\lambda_r)\)</span>. En revanche, point important, le nombre d‚Äôaxes retenus pour l‚Äôinterpr√©tation ou le recodage ne peut pas √™tre choisi √† partir de ces pourcentages d‚Äôinertie expliqu√©es, contrairement √† l‚ÄôACP.</p>
</div>
<div class="section" id="contributions-et-representation">
<h3>Contributions et repr√©sentation<a class="headerlink" href="#contributions-et-representation" title="Permalink to this heading">#</a></h3>
<p>En reprenant les r√©sultats de l‚ÄôAFC, on montre que :</p>
<ul class="simple">
<li><p>les individus les plus excentr√©s sur les plans factoriels sont ceux qui contribuent le plus</p></li>
<li><p>les modalit√©s les plus excentr√©es ne sont pas n√©cessairement celles qui
contribuent le plus. En effet, leur contribution d√©pend de leur fr√©quence.</p></li>
<li><p>la contribution d‚Äôune variable qualitative <span class="math notranslate nohighlight">\(j\)</span> √† un axe <span class="math notranslate nohighlight">\(z\)</span> donne une id√©e de la liaison entre cette variable et la composante principale correspondant √† <span class="math notranslate nohighlight">\(z\)</span></p></li>
<li><p>une repr√©sentation graphique consiste alors √† repr√©senter les variables qualitatives sur un plan factoriel <span class="math notranslate nohighlight">\((z,z')\)</span> : on propose en abscisses (respectivement ordonn√©es) les contributions des variables √† l‚Äôaxe <span class="math notranslate nohighlight">\(z\)</span> (resp. <span class="math notranslate nohighlight">\(z'\)</span>)</p></li>
<li><p>on √©value la qualit√© de la repr√©sentation de la m√™me mani√®re qu‚Äôen ACP, √† l‚Äôaide des cosinus carr√©s. Si deux individus sont bien projet√©s alors s‚Äôils sont proches en projections, ils sont effectivement proches dans leur espace d‚Äôorigine et on peut alors interpr√©ter leur proximit√© :
deux individus se ressemblent (au sens de la distance du <span class="math notranslate nohighlight">\(\chi^2\)</span>) s‚Äôils ont choisis les m√™mes modalit√©s et ; deux modalit√©s se ressemblent (en terme de <span class="math notranslate nohighlight">\(\chi^2\)</span>) si elles sont poss√©d√©es par les m√™mes individus.</p></li>
</ul>
</div>
<div class="section" id="cas-particulier-p-2">
<h3>Cas particulier <span class="math notranslate nohighlight">\(p\)</span>=2<a class="headerlink" href="#cas-particulier-p-2" title="Permalink to this heading">#</a></h3>
<p>Dans le cas <span class="math notranslate nohighlight">\(p=2\)</span>, on observe <span class="math notranslate nohighlight">\(2\)</span> variables ayant respectivement <span class="math notranslate nohighlight">\(m_1\)</span> et <span class="math notranslate nohighlight">\(m_2\)</span> modalit√©s. On se retrouve donc dans le cas o√π l‚ÄôAFC s‚Äôapplique et on peut :</p>
<ul class="simple">
<li><p>soit analyser le tableau <span class="math notranslate nohighlight">\(\mathbf{T}\in\mathcal{M}_{n,m_1+m_2}(\mathbb{R})\)</span> par analyse en composantes multiples,</p></li>
<li><p>soit analyser le tableau de contingence <span class="math notranslate nohighlight">\(\mathbf{K}\in\mathcal{M}_{m_1,m_2}(\mathbb{R})\)</span> par AFC.</p></li>
</ul>
<p>Si on note <span class="math notranslate nohighlight">\(Sp(\mathbf{K}) = (\mu_i)\)</span> et <span class="math notranslate nohighlight">\(Sp(\mathbf{T}) = (\lambda_i)\)</span> alors on montre  que  <span class="math notranslate nohighlight">\(\mu_r = (2\lambda_r - 1)^2\)</span>
On en d√©duit qu‚Äô√† chaque valeur  propre de l‚ÄôAFC correspondent deux valeurs propres de l‚ÄôACM <span class="math notranslate nohighlight">\(\lambda_r = \frac{1\pm \sqrt{\mu_r}}{2}\)</span>, et une relation entre les coordonn√©es factorielles des deux analyses</p>
<p><span class="math notranslate nohighlight">\(\begin{eqnarray*}
\mathbf{c}_1 = \begin{pmatrix} \mathbf{x_K}\\\mathbf{y_K}\end{pmatrix}\ &amp;\textrm{pour}&amp; \lambda_r = \frac{1+ \sqrt{\mu_r}}{2}\\
\mathbf{c}_2 = \begin{pmatrix} \mathbf{x_K}\\-\mathbf{y_K}\end{pmatrix}\ &amp;\textrm{pour}&amp; \lambda_r = \frac{1- \sqrt{\mu_r}}{2}
\end{eqnarray*}\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\(\mathbf{x_K},\mathbf{y_K}\)</span> sont les composantes principales des profils ligne et colonne de <span class="math notranslate nohighlight">\(K\)</span>.
De l√† viennent deux constats :</p>
<ul class="simple">
<li><p>dans l‚Äôanalyse en composantes multiples de 2 variables, on ne retient que les valeurs propres sup√©rieures strictement √† 1/2, les composantes correspondant  aux valeurs propres inf√©rieures se d√©duisant facilement</p></li>
<li><p>Les pourcentages d‚Äôinertie expliqu√©s par les axes en ACM sont souvent tr√®s faibles et
ne peuvent donc pas √™tre interpr√©t√©s comme en AFC et en ACP.</p></li>
</ul>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Permalink to this heading">#</a></h3>
<p>On souhaite √©valuer l‚Äôeffet de l‚Äôesp√®ce de ch√™ne sur des vins rouges vieillis en barrique. Un m√™me vin a √©t√© vieilli dans six barriques diff√©rentes fabriqu√©es avec deux types de ch√™ne : les vins <span class="math notranslate nohighlight">\(V_1\)</span>, <span class="math notranslate nohighlight">\(V_5\)</span> et <span class="math notranslate nohighlight">\(V_6\)</span> ont √©t√© √©lev√©s avec le premier type de ch√™ne, tandis que les vins <span class="math notranslate nohighlight">\(V_2\)</span>, <span class="math notranslate nohighlight">\(V_3\)</span> et <span class="math notranslate nohighlight">\(V_4\)</span> ont √©t√© √©lev√©s avec le second. Trois experts <span class="math notranslate nohighlight">\(E_1,E_2,E_3\)</span> ont ensuite choisi entre deux et cinq variables pour d√©crire les vins. Pour chaque vin et pour chaque variable, l‚Äôexpert √©value l‚Äôintensit√©, cod√©e soit comme une r√©ponse binaire (i.e. fruit√© vs. non fruit√©), soit comme une r√©ponse ternaire (i.e. pas fruit√©, un peu fruit√©, tr√®s fruit√©). On code le tout par un tableau disjonctif complet :</p>
<p><img alt="" src="_images/tab0.png" /></p>
<p>L‚Äôobjectif de l‚Äô√©tude est d‚Äôune part de proposer une typologie des vins et d‚Äôautre part de savoir s‚Äôil y a un accord entre les √©chelles utilis√©es par les experts.</p>
<p>La figure suivante pr√©sente le r√©sultat de l‚Äôanalyse spectrale en lignes. Les tableaux qui suivent donnent les coordonn√©es des individus (<span class="math notranslate nohighlight">\(S\)</span>), la qualit√© de leur repr√©sentation et leur contribution  (C<span class="math notranslate nohighlight">\(\times 10^3\)</span>) sur les trois premiers axes factoriels.</p>
<p><img alt="" src="_images/spectralACM.png" /></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Vin</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_4\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_5\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(V_6\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(S\)</span></p></td>
<td><p>1</p></td>
<td><p>-0.951</p></td>
<td><p>0.787</p></td>
<td><p>1.018</p></td>
<td><p>0.951</p></td>
<td><p>-1.018</p></td>
<td><p>-0.787</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>-0.316</p></td>
<td><p>0.632</p></td>
<td><p>-0.316</p></td>
<td><p>-0.316</p></td>
<td><p>-0.316</p></td>
<td><p>0.632</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>3</p></td>
<td><p>0.43</p></td>
<td><p>0.387</p></td>
<td><p>0.103</p></td>
<td><p>-0.43</p></td>
<td><p>-0.103</p></td>
<td><p>-0.387</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(cos^2\)</span></p></td>
<td><p>1</p></td>
<td><p>0.754</p></td>
<td><p>0.516</p></td>
<td><p>0.863</p></td>
<td><p>0.754</p></td>
<td><p>0.863</p></td>
<td><p>0.516</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>2</p></td>
<td><p>0.083</p></td>
<td><p>0.333</p></td>
<td><p>0.083</p></td>
<td><p>0.083</p></td>
<td><p>0.083</p></td>
<td><p>0.333</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>3</p></td>
<td><p>0.154</p></td>
<td><p>0.125</p></td>
<td><p>0.009</p></td>
<td><p>0.154</p></td>
<td><p>0.009</p></td>
<td><p>0.125</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(C\)</span></p></td>
<td><p>1</p></td>
<td><p>176.68</p></td>
<td><p>120.986</p></td>
<td><p>202.334</p></td>
<td><p>176.68</p></td>
<td><p>202.334</p></td>
<td><p>120.986</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>2</p></td>
<td><p>83.333</p></td>
<td><p>333.333</p></td>
<td><p>83.333</p></td>
<td><p>83.333</p></td>
<td><p>83.333</p></td>
<td><p>333.333</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>3</p></td>
<td><p>267.821</p></td>
<td><p>216.945</p></td>
<td><p>15.234</p></td>
<td><p>267.821</p></td>
<td><p>15.234</p></td>
<td><p>216.945</p></td>
</tr>
</tbody>
</table>
<p><img alt="" src="_images/tab.png" /></p>
<p>On peut alors projeter les individus lignes ou colonnes sur le premier plan factoriel.</p>
<p><img alt="" src="_images/plan12lig.png" /><img alt="" src="_images/plan12col.png" /></p>
<p>On peut alors par exemple utiliser le type de ch√™ne (MONCHENE) comme une variable suppl√©mentaire (ou illustrative) √† projeter sur l‚Äôanalyse apr√®s coup. On peut √©galement projeter apr√®s analyse un nouveau vin (MONVIN, donc une observation suppl√©mentaire), test√© par les experts. Lorsque ces derniers n‚Äô√©taient pas s√ªrs de la fa√ßon d‚Äôutiliser un descripteur, un mod√®le de r√©ponse (1/2, 1/2) est utilis√© pour repr√©senter la r√©ponse.</p>
<p><img alt="" src="_images/plan12lig2.png" /><img alt="" src="_images/plan12col2.png" /></p>
</div>
</div>
<span id="document-genindex"></span><div class="tex2jax_ignore mathjax_ignore section" id="index">
<h2>Index<a class="headerlink" href="#index" title="Permalink to this heading">#</a></h2>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <p aria-level="2" class="caption" role="heading"><span class="caption-text">Cours</span></p>
<ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-Rappels">Rappels de probabilit√©</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-elemstats">Elements de statistiques</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-statsdescriptives">Statistique descriptive</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-selection">S√©lection de variables</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-acp">Analyse en composantes principales</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-regression">R√©gression</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-clustering">Quelques m√©thodes de classification</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Annexes</span></p>
<ul class="nav section-nav flex-column">
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-afc">Analyse Factorielle des correspondances</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-acm">Analyse des correspondances multiples</a></li>
<li class="toctree-l1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-genindex">Index</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Vincent BARRA
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      ¬© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>